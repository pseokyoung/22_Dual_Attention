{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"..\") \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.utility import loadfile, savefile, exists\n",
    "from src.dataprocessing import *\n",
    "from src import rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "input_var   = [\"FT-3061-2\", \"FT-3061-3\", \"FT-3061-4\", \"FT-3062-1\"]\n",
    "output_var  = [\"TT-3061-3\", \"TT-3061-5\", \"LT-3061-2\"]\n",
    "process_var = input_var + output_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_1.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_2.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_3.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_4.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_5.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_6.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_7.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_8.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_9.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_10.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_11.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_12.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_13.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_14.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_15.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_16.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_17.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_18.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_19.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_20.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_21.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_22.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_23.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_24.csv\n"
     ]
    }
   ],
   "source": [
    "min_len = 100\n",
    "continuous_path = './data/3_continuous'\n",
    "\n",
    "cts_list = []\n",
    "i = 1\n",
    "while exists(continuous_path, f\"cts_{min_len}/dataset {min_len}_{i}\", 'csv'):\n",
    "    cts_df = loadfile(continuous_path, f\"cts_{min_len}/dataset {min_len}_{i}\", 'csv')\n",
    "    cts_list.append(cts_df)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3th iteration\n",
      "history size: 10\n",
      "future size: 10\n",
      "Epoch 1/10000\n",
      "581/581 - 9s - loss: 0.2352 - val_loss: 0.1614 - 9s/epoch - 16ms/step\n",
      "Epoch 2/10000\n",
      "581/581 - 5s - loss: 0.1545 - val_loss: 0.1459 - 5s/epoch - 9ms/step\n",
      "Epoch 3/10000\n",
      "581/581 - 5s - loss: 0.1438 - val_loss: 0.1322 - 5s/epoch - 9ms/step\n",
      "Epoch 4/10000\n",
      "581/581 - 5s - loss: 0.1350 - val_loss: 0.1297 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "581/581 - 5s - loss: 0.1276 - val_loss: 0.1116 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "581/581 - 5s - loss: 0.1212 - val_loss: 0.1122 - 5s/epoch - 9ms/step\n",
      "Epoch 7/10000\n",
      "581/581 - 5s - loss: 0.1144 - val_loss: 0.1046 - 5s/epoch - 9ms/step\n",
      "Epoch 8/10000\n",
      "581/581 - 5s - loss: 0.1090 - val_loss: 0.1010 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "581/581 - 5s - loss: 0.1050 - val_loss: 0.0975 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "581/581 - 5s - loss: 0.1015 - val_loss: 0.0962 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "581/581 - 5s - loss: 0.0979 - val_loss: 0.0925 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "581/581 - 5s - loss: 0.0939 - val_loss: 0.0915 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "581/581 - 5s - loss: 0.0919 - val_loss: 0.0923 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "581/581 - 5s - loss: 0.0901 - val_loss: 0.0888 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "581/581 - 5s - loss: 0.0869 - val_loss: 0.0875 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "581/581 - 5s - loss: 0.0876 - val_loss: 0.0863 - 5s/epoch - 9ms/step\n",
      "Epoch 17/10000\n",
      "581/581 - 5s - loss: 0.0846 - val_loss: 0.0840 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "581/581 - 5s - loss: 0.0826 - val_loss: 0.0815 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "581/581 - 5s - loss: 0.0818 - val_loss: 0.0815 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "581/581 - 5s - loss: 0.0781 - val_loss: 0.0787 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "581/581 - 5s - loss: 0.0765 - val_loss: 0.0771 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "581/581 - 5s - loss: 0.0760 - val_loss: 0.0788 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "581/581 - 5s - loss: 0.0745 - val_loss: 0.0780 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "581/581 - 5s - loss: 0.0745 - val_loss: 0.0766 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "581/581 - 5s - loss: 0.0730 - val_loss: 0.0824 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "581/581 - 5s - loss: 0.0724 - val_loss: 0.0771 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "581/581 - 5s - loss: 0.0709 - val_loss: 0.0737 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "581/581 - 5s - loss: 0.0696 - val_loss: 0.0702 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "581/581 - 5s - loss: 0.0677 - val_loss: 0.0731 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "581/581 - 5s - loss: 0.0674 - val_loss: 0.0726 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "581/581 - 5s - loss: 0.0668 - val_loss: 0.0726 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "581/581 - 5s - loss: 0.0676 - val_loss: 0.0702 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "581/581 - 5s - loss: 0.0653 - val_loss: 0.0714 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "581/581 - 5s - loss: 0.0646 - val_loss: 0.0721 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "581/581 - 5s - loss: 0.0667 - val_loss: 0.0695 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "581/581 - 5s - loss: 0.0638 - val_loss: 0.0728 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "581/581 - 5s - loss: 0.0631 - val_loss: 0.0669 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "581/581 - 5s - loss: 0.0623 - val_loss: 0.0681 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "581/581 - 5s - loss: 0.0611 - val_loss: 0.0688 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "581/581 - 5s - loss: 0.0592 - val_loss: 0.0658 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "581/581 - 5s - loss: 0.0600 - val_loss: 0.0716 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "581/581 - 5s - loss: 0.0609 - val_loss: 0.0655 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "581/581 - 5s - loss: 0.0586 - val_loss: 0.0721 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "581/581 - 5s - loss: 0.0589 - val_loss: 0.0645 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "581/581 - 5s - loss: 0.0577 - val_loss: 0.0653 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "581/581 - 5s - loss: 0.0611 - val_loss: 0.0638 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "581/581 - 5s - loss: 0.0552 - val_loss: 0.0661 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "581/581 - 5s - loss: 0.0548 - val_loss: 0.0629 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "581/581 - 5s - loss: 0.0568 - val_loss: 0.0663 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "581/581 - 5s - loss: 0.0539 - val_loss: 0.0635 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "581/581 - 5s - loss: 0.0543 - val_loss: 0.0657 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "581/581 - 5s - loss: 0.0566 - val_loss: 0.0626 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "581/581 - 5s - loss: 0.0534 - val_loss: 0.0713 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "581/581 - 5s - loss: 0.0553 - val_loss: 0.0647 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "581/581 - 5s - loss: 0.0516 - val_loss: 0.0622 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "581/581 - 5s - loss: 0.0513 - val_loss: 0.0611 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "581/581 - 5s - loss: 0.0547 - val_loss: 0.0613 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "581/581 - 4s - loss: 0.0530 - val_loss: 0.0627 - 4s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "581/581 - 5s - loss: 0.0494 - val_loss: 0.0606 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "581/581 - 5s - loss: 0.0491 - val_loss: 0.0635 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "581/581 - 5s - loss: 0.0503 - val_loss: 0.0609 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "581/581 - 5s - loss: 0.0512 - val_loss: 0.0601 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "581/581 - 5s - loss: 0.0497 - val_loss: 0.0613 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "581/581 - 5s - loss: 0.0490 - val_loss: 0.0626 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "581/581 - 5s - loss: 0.0487 - val_loss: 0.0598 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "581/581 - 5s - loss: 0.0478 - val_loss: 0.0617 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "581/581 - 5s - loss: 0.0483 - val_loss: 0.0587 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "581/581 - 5s - loss: 0.0477 - val_loss: 0.0604 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "581/581 - 5s - loss: 0.0498 - val_loss: 0.0611 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "581/581 - 5s - loss: 0.0483 - val_loss: 0.0595 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "581/581 - 5s - loss: 0.0460 - val_loss: 0.0594 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "581/581 - 5s - loss: 0.0463 - val_loss: 0.0610 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "581/581 - 5s - loss: 0.0462 - val_loss: 0.0620 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "581/581 - 5s - loss: 0.0503 - val_loss: 0.0631 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "581/581 - 5s - loss: 0.0517 - val_loss: 0.0613 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "581/581 - 5s - loss: 0.0453 - val_loss: 0.0568 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "581/581 - 5s - loss: 0.0437 - val_loss: 0.0576 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "581/581 - 5s - loss: 0.0439 - val_loss: 0.0577 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "581/581 - 5s - loss: 0.0443 - val_loss: 0.0586 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "581/581 - 5s - loss: 0.0468 - val_loss: 0.0590 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "581/581 - 5s - loss: 0.0442 - val_loss: 0.0610 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "581/581 - 5s - loss: 0.0435 - val_loss: 0.0577 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "581/581 - 5s - loss: 0.0434 - val_loss: 0.0567 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "581/581 - 5s - loss: 0.0452 - val_loss: 0.0577 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "581/581 - 5s - loss: 0.0439 - val_loss: 0.0589 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "581/581 - 5s - loss: 0.0432 - val_loss: 0.0581 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "581/581 - 5s - loss: 0.0436 - val_loss: 0.0581 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "581/581 - 5s - loss: 0.0433 - val_loss: 0.0586 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "581/581 - 5s - loss: 0.0481 - val_loss: 0.0574 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "581/581 - 5s - loss: 0.0452 - val_loss: 0.0579 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "581/581 - 5s - loss: 0.0415 - val_loss: 0.0564 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "581/581 - 5s - loss: 0.0432 - val_loss: 0.0615 - 5s/epoch - 9ms/step\n",
      "Epoch 93/10000\n",
      "581/581 - 5s - loss: 0.0430 - val_loss: 0.0576 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "581/581 - 5s - loss: 0.0429 - val_loss: 0.0573 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "581/581 - 5s - loss: 0.0420 - val_loss: 0.0570 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "581/581 - 5s - loss: 0.0414 - val_loss: 0.0606 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "581/581 - 5s - loss: 0.0413 - val_loss: 0.0570 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "581/581 - 5s - loss: 0.0420 - val_loss: 0.0576 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "581/581 - 5s - loss: 0.0411 - val_loss: 0.0583 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "581/581 - 5s - loss: 0.0426 - val_loss: 0.0548 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "581/581 - 5s - loss: 0.0404 - val_loss: 0.0553 - 5s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "581/581 - 5s - loss: 0.0406 - val_loss: 0.0560 - 5s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "581/581 - 5s - loss: 0.0409 - val_loss: 0.0556 - 5s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "581/581 - 5s - loss: 0.0404 - val_loss: 0.0546 - 5s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "581/581 - 5s - loss: 0.0397 - val_loss: 0.0579 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "581/581 - 5s - loss: 0.0434 - val_loss: 0.0576 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "581/581 - 4s - loss: 0.0410 - val_loss: 0.0551 - 4s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "581/581 - 5s - loss: 0.0392 - val_loss: 0.0548 - 5s/epoch - 8ms/step\n",
      "Epoch 109/10000\n",
      "581/581 - 5s - loss: 0.0395 - val_loss: 0.0562 - 5s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "581/581 - 5s - loss: 0.0391 - val_loss: 0.0539 - 5s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "581/581 - 5s - loss: 0.0385 - val_loss: 0.0540 - 5s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "581/581 - 5s - loss: 0.0405 - val_loss: 0.0576 - 5s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "581/581 - 5s - loss: 0.0390 - val_loss: 0.0562 - 5s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "581/581 - 5s - loss: 0.0390 - val_loss: 0.0542 - 5s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "581/581 - 5s - loss: 0.0448 - val_loss: 0.0543 - 5s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "581/581 - 5s - loss: 0.0378 - val_loss: 0.0524 - 5s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "581/581 - 5s - loss: 0.0371 - val_loss: 0.0534 - 5s/epoch - 8ms/step\n",
      "Epoch 118/10000\n",
      "581/581 - 5s - loss: 0.0378 - val_loss: 0.0524 - 5s/epoch - 8ms/step\n",
      "Epoch 119/10000\n",
      "581/581 - 5s - loss: 0.0389 - val_loss: 0.0573 - 5s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "581/581 - 5s - loss: 0.0443 - val_loss: 0.0586 - 5s/epoch - 8ms/step\n",
      "Epoch 121/10000\n",
      "581/581 - 5s - loss: 0.0380 - val_loss: 0.0523 - 5s/epoch - 8ms/step\n",
      "Epoch 122/10000\n",
      "581/581 - 5s - loss: 0.0368 - val_loss: 0.0549 - 5s/epoch - 8ms/step\n",
      "Epoch 123/10000\n",
      "581/581 - 5s - loss: 0.0373 - val_loss: 0.0543 - 5s/epoch - 8ms/step\n",
      "Epoch 124/10000\n",
      "581/581 - 5s - loss: 0.0375 - val_loss: 0.0544 - 5s/epoch - 8ms/step\n",
      "Epoch 125/10000\n",
      "581/581 - 5s - loss: 0.0379 - val_loss: 0.0536 - 5s/epoch - 8ms/step\n",
      "Epoch 126/10000\n",
      "581/581 - 5s - loss: 0.0379 - val_loss: 0.0522 - 5s/epoch - 8ms/step\n",
      "Epoch 127/10000\n",
      "581/581 - 5s - loss: 0.0375 - val_loss: 0.0615 - 5s/epoch - 8ms/step\n",
      "Epoch 128/10000\n",
      "581/581 - 5s - loss: 0.0422 - val_loss: 0.0543 - 5s/epoch - 8ms/step\n",
      "Epoch 129/10000\n",
      "581/581 - 5s - loss: 0.0365 - val_loss: 0.0536 - 5s/epoch - 8ms/step\n",
      "Epoch 130/10000\n",
      "581/581 - 5s - loss: 0.0365 - val_loss: 0.0539 - 5s/epoch - 8ms/step\n",
      "Epoch 131/10000\n",
      "581/581 - 5s - loss: 0.0377 - val_loss: 0.0513 - 5s/epoch - 8ms/step\n",
      "Epoch 132/10000\n",
      "581/581 - 5s - loss: 0.0365 - val_loss: 0.0514 - 5s/epoch - 8ms/step\n",
      "Epoch 133/10000\n",
      "581/581 - 5s - loss: 0.0367 - val_loss: 0.0529 - 5s/epoch - 8ms/step\n",
      "Epoch 134/10000\n",
      "581/581 - 5s - loss: 0.0371 - val_loss: 0.0519 - 5s/epoch - 8ms/step\n",
      "Epoch 135/10000\n",
      "581/581 - 5s - loss: 0.0367 - val_loss: 0.0509 - 5s/epoch - 8ms/step\n",
      "Epoch 136/10000\n",
      "581/581 - 5s - loss: 0.0361 - val_loss: 0.0535 - 5s/epoch - 8ms/step\n",
      "Epoch 137/10000\n",
      "581/581 - 5s - loss: 0.0360 - val_loss: 0.0528 - 5s/epoch - 8ms/step\n",
      "Epoch 138/10000\n",
      "581/581 - 6s - loss: 0.0376 - val_loss: 0.0559 - 6s/epoch - 10ms/step\n",
      "Epoch 139/10000\n",
      "581/581 - 5s - loss: 0.0373 - val_loss: 0.0524 - 5s/epoch - 8ms/step\n",
      "Epoch 140/10000\n",
      "581/581 - 4s - loss: 0.0356 - val_loss: 0.0509 - 4s/epoch - 8ms/step\n",
      "Epoch 141/10000\n",
      "581/581 - 5s - loss: 0.0371 - val_loss: 0.0535 - 5s/epoch - 8ms/step\n",
      "Epoch 142/10000\n",
      "581/581 - 5s - loss: 0.0365 - val_loss: 0.0511 - 5s/epoch - 8ms/step\n",
      "Epoch 143/10000\n",
      "581/581 - 5s - loss: 0.0378 - val_loss: 0.0517 - 5s/epoch - 8ms/step\n",
      "Epoch 144/10000\n",
      "581/581 - 5s - loss: 0.0358 - val_loss: 0.0516 - 5s/epoch - 8ms/step\n",
      "Epoch 145/10000\n",
      "581/581 - 5s - loss: 0.0360 - val_loss: 0.0542 - 5s/epoch - 8ms/step\n",
      "Epoch 146/10000\n",
      "581/581 - 5s - loss: 0.0351 - val_loss: 0.0517 - 5s/epoch - 8ms/step\n",
      "Epoch 147/10000\n",
      "581/581 - 5s - loss: 0.0358 - val_loss: 0.0545 - 5s/epoch - 8ms/step\n",
      "Epoch 148/10000\n",
      "581/581 - 5s - loss: 0.0362 - val_loss: 0.0520 - 5s/epoch - 8ms/step\n",
      "Epoch 149/10000\n",
      "581/581 - 5s - loss: 0.0351 - val_loss: 0.0498 - 5s/epoch - 8ms/step\n",
      "Epoch 150/10000\n",
      "581/581 - 5s - loss: 0.0348 - val_loss: 0.0531 - 5s/epoch - 8ms/step\n",
      "Epoch 151/10000\n",
      "581/581 - 5s - loss: 0.0431 - val_loss: 0.0519 - 5s/epoch - 8ms/step\n",
      "Epoch 152/10000\n",
      "581/581 - 5s - loss: 0.0347 - val_loss: 0.0509 - 5s/epoch - 8ms/step\n",
      "Epoch 153/10000\n",
      "581/581 - 5s - loss: 0.0347 - val_loss: 0.0514 - 5s/epoch - 8ms/step\n",
      "Epoch 154/10000\n",
      "581/581 - 5s - loss: 0.0352 - val_loss: 0.0518 - 5s/epoch - 8ms/step\n",
      "Epoch 155/10000\n",
      "581/581 - 5s - loss: 0.0355 - val_loss: 0.0524 - 5s/epoch - 8ms/step\n",
      "Epoch 156/10000\n",
      "581/581 - 5s - loss: 0.0410 - val_loss: 0.0516 - 5s/epoch - 8ms/step\n",
      "Epoch 157/10000\n",
      "581/581 - 5s - loss: 0.0351 - val_loss: 0.0506 - 5s/epoch - 8ms/step\n",
      "Epoch 158/10000\n",
      "581/581 - 5s - loss: 0.0335 - val_loss: 0.0493 - 5s/epoch - 8ms/step\n",
      "Epoch 159/10000\n",
      "581/581 - 5s - loss: 0.0335 - val_loss: 0.0507 - 5s/epoch - 8ms/step\n",
      "Epoch 160/10000\n",
      "581/581 - 5s - loss: 0.0342 - val_loss: 0.0504 - 5s/epoch - 8ms/step\n",
      "Epoch 161/10000\n",
      "581/581 - 5s - loss: 0.0339 - val_loss: 0.0519 - 5s/epoch - 8ms/step\n",
      "Epoch 162/10000\n",
      "581/581 - 5s - loss: 0.0354 - val_loss: 0.0533 - 5s/epoch - 8ms/step\n",
      "Epoch 163/10000\n",
      "581/581 - 5s - loss: 0.0353 - val_loss: 0.0518 - 5s/epoch - 8ms/step\n",
      "Epoch 164/10000\n",
      "581/581 - 5s - loss: 0.0340 - val_loss: 0.0519 - 5s/epoch - 8ms/step\n",
      "Epoch 165/10000\n",
      "581/581 - 5s - loss: 0.0332 - val_loss: 0.0511 - 5s/epoch - 8ms/step\n",
      "Epoch 166/10000\n",
      "581/581 - 5s - loss: 0.0338 - val_loss: 0.0499 - 5s/epoch - 8ms/step\n",
      "Epoch 167/10000\n",
      "581/581 - 5s - loss: 0.0353 - val_loss: 0.0510 - 5s/epoch - 8ms/step\n",
      "Epoch 168/10000\n",
      "581/581 - 5s - loss: 0.0394 - val_loss: 0.0504 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_10_1_50_1_50_datt_seq2seq_gru_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_10_1_50_1_50_datt_seq2seq_gru_3\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002646DDD9940> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002640A536FA0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.932547</td>\n",
       "      <td>0.929228</td>\n",
       "      <td>0.962515</td>\n",
       "      <td>0.94143</td>\n",
       "      <td>4.216931</td>\n",
       "      <td>4.652152</td>\n",
       "      <td>3.018687</td>\n",
       "      <td>3.96259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.854473</td>\n",
       "      <td>0.877838</td>\n",
       "      <td>0.903776</td>\n",
       "      <td>0.878696</td>\n",
       "      <td>6.154257</td>\n",
       "      <td>6.112773</td>\n",
       "      <td>4.835263</td>\n",
       "      <td>5.700764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.752587</td>\n",
       "      <td>0.832637</td>\n",
       "      <td>0.825899</td>\n",
       "      <td>0.803708</td>\n",
       "      <td>7.915257</td>\n",
       "      <td>7.155482</td>\n",
       "      <td>6.50239</td>\n",
       "      <td>7.191043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.648692</td>\n",
       "      <td>0.805187</td>\n",
       "      <td>0.767005</td>\n",
       "      <td>0.740295</td>\n",
       "      <td>9.306822</td>\n",
       "      <td>7.721072</td>\n",
       "      <td>7.520281</td>\n",
       "      <td>8.182725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.554263</td>\n",
       "      <td>0.785425</td>\n",
       "      <td>0.739352</td>\n",
       "      <td>0.693013</td>\n",
       "      <td>10.380806</td>\n",
       "      <td>8.104377</td>\n",
       "      <td>7.951497</td>\n",
       "      <td>8.812227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.471005</td>\n",
       "      <td>0.766227</td>\n",
       "      <td>0.736972</td>\n",
       "      <td>0.658068</td>\n",
       "      <td>11.199981</td>\n",
       "      <td>8.46057</td>\n",
       "      <td>7.985714</td>\n",
       "      <td>9.215422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.401735</td>\n",
       "      <td>0.745662</td>\n",
       "      <td>0.743771</td>\n",
       "      <td>0.63039</td>\n",
       "      <td>11.797866</td>\n",
       "      <td>8.825626</td>\n",
       "      <td>7.880397</td>\n",
       "      <td>9.501296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.342002</td>\n",
       "      <td>0.725256</td>\n",
       "      <td>0.746316</td>\n",
       "      <td>0.604525</td>\n",
       "      <td>12.343449</td>\n",
       "      <td>9.173175</td>\n",
       "      <td>7.840391</td>\n",
       "      <td>9.785672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.292902</td>\n",
       "      <td>0.706322</td>\n",
       "      <td>0.739063</td>\n",
       "      <td>0.579429</td>\n",
       "      <td>12.810507</td>\n",
       "      <td>9.484457</td>\n",
       "      <td>7.951082</td>\n",
       "      <td>10.082015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.251999</td>\n",
       "      <td>0.687274</td>\n",
       "      <td>0.717859</td>\n",
       "      <td>0.552377</td>\n",
       "      <td>13.191294</td>\n",
       "      <td>9.787511</td>\n",
       "      <td>8.26724</td>\n",
       "      <td>10.415348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.550221</td>\n",
       "      <td>0.786106</td>\n",
       "      <td>0.788253</td>\n",
       "      <td>0.708193</td>\n",
       "      <td>9.931717</td>\n",
       "      <td>7.94772</td>\n",
       "      <td>6.975294</td>\n",
       "      <td>8.28491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3 TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE     nRMSE     nRMSE   \n",
       "0      0.932547  0.929228  0.962515   0.94143   4.216931  4.652152  3.018687   \n",
       "1      0.854473  0.877838  0.903776  0.878696   6.154257  6.112773  4.835263   \n",
       "2      0.752587  0.832637  0.825899  0.803708   7.915257  7.155482   6.50239   \n",
       "3      0.648692  0.805187  0.767005  0.740295   9.306822  7.721072  7.520281   \n",
       "4      0.554263  0.785425  0.739352  0.693013  10.380806  8.104377  7.951497   \n",
       "5      0.471005  0.766227  0.736972  0.658068  11.199981   8.46057  7.985714   \n",
       "6      0.401735  0.745662  0.743771   0.63039  11.797866  8.825626  7.880397   \n",
       "7      0.342002  0.725256  0.746316  0.604525  12.343449  9.173175  7.840391   \n",
       "8      0.292902  0.706322  0.739063  0.579429  12.810507  9.484457  7.951082   \n",
       "9      0.251999  0.687274  0.717859  0.552377  13.191294  9.787511   8.26724   \n",
       "mean   0.550221  0.786106  0.788253  0.708193   9.931717   7.94772  6.975294   \n",
       "\n",
       "            mean  \n",
       "index      nRMSE  \n",
       "0        3.96259  \n",
       "1       5.700764  \n",
       "2       7.191043  \n",
       "3       8.182725  \n",
       "4       8.812227  \n",
       "5       9.215422  \n",
       "6       9.501296  \n",
       "7       9.785672  \n",
       "8      10.082015  \n",
       "9      10.415348  \n",
       "mean     8.28491  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/10_10_1_50_1_50_datt_seq2seq_gru_3.csv\n",
      "\n",
      "\n",
      "3th iteration\n",
      "history size: 10\n",
      "future size: 20\n",
      "Epoch 1/10000\n",
      "577/577 - 8s - loss: 0.3059 - val_loss: 0.2578 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "577/577 - 4s - loss: 0.2273 - val_loss: 0.2382 - 4s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "577/577 - 4s - loss: 0.2132 - val_loss: 0.2307 - 4s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "577/577 - 4s - loss: 0.2043 - val_loss: 0.2214 - 4s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "577/577 - 4s - loss: 0.1978 - val_loss: 0.2127 - 4s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "577/577 - 5s - loss: 0.1892 - val_loss: 0.2054 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "577/577 - 4s - loss: 0.1826 - val_loss: 0.1980 - 4s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "577/577 - 4s - loss: 0.1762 - val_loss: 0.1919 - 4s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "577/577 - 5s - loss: 0.1711 - val_loss: 0.1819 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "577/577 - 4s - loss: 0.1628 - val_loss: 0.1870 - 4s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "577/577 - 4s - loss: 0.1602 - val_loss: 0.1756 - 4s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "577/577 - 5s - loss: 0.1522 - val_loss: 0.1687 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "577/577 - 4s - loss: 0.1480 - val_loss: 0.1649 - 4s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "577/577 - 5s - loss: 0.1453 - val_loss: 0.1615 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "577/577 - 4s - loss: 0.1385 - val_loss: 0.1660 - 4s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "577/577 - 4s - loss: 0.1357 - val_loss: 0.1631 - 4s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "577/577 - 4s - loss: 0.1308 - val_loss: 0.1538 - 4s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "577/577 - 4s - loss: 0.1279 - val_loss: 0.1444 - 4s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "577/577 - 5s - loss: 0.1244 - val_loss: 0.1438 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "577/577 - 4s - loss: 0.1235 - val_loss: 0.1439 - 4s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "577/577 - 4s - loss: 0.1205 - val_loss: 0.1617 - 4s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "577/577 - 4s - loss: 0.1217 - val_loss: 0.1354 - 4s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "577/577 - 4s - loss: 0.1151 - val_loss: 0.1346 - 4s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "577/577 - 5s - loss: 0.1119 - val_loss: 0.1258 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "577/577 - 4s - loss: 0.1090 - val_loss: 0.1295 - 4s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "577/577 - 4s - loss: 0.1118 - val_loss: 0.1312 - 4s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "577/577 - 4s - loss: 0.1065 - val_loss: 0.1266 - 4s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "577/577 - 4s - loss: 0.1055 - val_loss: 0.1319 - 4s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "577/577 - 4s - loss: 0.1054 - val_loss: 0.1194 - 4s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "577/577 - 5s - loss: 0.1018 - val_loss: 0.1542 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "577/577 - 4s - loss: 0.1050 - val_loss: 0.1240 - 4s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "577/577 - 4s - loss: 0.1010 - val_loss: 0.1153 - 4s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "577/577 - 4s - loss: 0.0982 - val_loss: 0.1255 - 4s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "577/577 - 4s - loss: 0.0976 - val_loss: 0.1197 - 4s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "577/577 - 4s - loss: 0.0962 - val_loss: 0.1225 - 4s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "577/577 - 4s - loss: 0.0954 - val_loss: 0.1153 - 4s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "577/577 - 4s - loss: 0.0948 - val_loss: 0.1165 - 4s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "577/577 - 4s - loss: 0.0933 - val_loss: 0.1334 - 4s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "577/577 - 4s - loss: 0.0926 - val_loss: 0.1119 - 4s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "577/577 - 4s - loss: 0.0909 - val_loss: 0.1208 - 4s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "577/577 - 4s - loss: 0.0914 - val_loss: 0.1207 - 4s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "577/577 - 4s - loss: 0.0896 - val_loss: 0.1103 - 4s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "577/577 - 4s - loss: 0.0896 - val_loss: 0.1094 - 4s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "577/577 - 4s - loss: 0.0868 - val_loss: 0.1195 - 4s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "577/577 - 5s - loss: 0.0886 - val_loss: 0.1184 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "577/577 - 5s - loss: 0.0855 - val_loss: 0.1062 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "577/577 - 4s - loss: 0.0858 - val_loss: 0.1080 - 4s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "577/577 - 4s - loss: 0.0841 - val_loss: 0.1096 - 4s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "577/577 - 4s - loss: 0.0851 - val_loss: 0.1143 - 4s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "577/577 - 4s - loss: 0.0886 - val_loss: 0.1229 - 4s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "577/577 - 4s - loss: 0.0879 - val_loss: 0.1087 - 4s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "577/577 - 4s - loss: 0.0813 - val_loss: 0.1025 - 4s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "577/577 - 4s - loss: 0.0831 - val_loss: 0.1028 - 4s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "577/577 - 4s - loss: 0.0818 - val_loss: 0.1091 - 4s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "577/577 - 4s - loss: 0.0799 - val_loss: 0.1010 - 4s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "577/577 - 4s - loss: 0.0830 - val_loss: 0.1067 - 4s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "577/577 - 4s - loss: 0.0805 - val_loss: 0.1010 - 4s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "577/577 - 4s - loss: 0.0802 - val_loss: 0.1069 - 4s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "577/577 - 4s - loss: 0.0787 - val_loss: 0.0991 - 4s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "577/577 - 4s - loss: 0.0759 - val_loss: 0.1021 - 4s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "577/577 - 4s - loss: 0.0778 - val_loss: 0.1068 - 4s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "577/577 - 4s - loss: 0.0782 - val_loss: 0.1005 - 4s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "577/577 - 5s - loss: 0.0763 - val_loss: 0.1041 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "577/577 - 5s - loss: 0.0778 - val_loss: 0.0983 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "577/577 - 5s - loss: 0.0755 - val_loss: 0.1057 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "577/577 - 4s - loss: 0.0788 - val_loss: 0.1004 - 4s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "577/577 - 4s - loss: 0.0743 - val_loss: 0.0930 - 4s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "577/577 - 4s - loss: 0.0737 - val_loss: 0.0976 - 4s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "577/577 - 4s - loss: 0.0737 - val_loss: 0.1007 - 4s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "577/577 - 4s - loss: 0.0727 - val_loss: 0.0950 - 4s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "577/577 - 4s - loss: 0.0761 - val_loss: 0.0966 - 4s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "577/577 - 4s - loss: 0.0720 - val_loss: 0.0982 - 4s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "577/577 - 4s - loss: 0.0722 - val_loss: 0.0936 - 4s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "577/577 - 4s - loss: 0.0731 - val_loss: 0.0923 - 4s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "577/577 - 4s - loss: 0.0753 - val_loss: 0.1065 - 4s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "577/577 - 5s - loss: 0.0733 - val_loss: 0.0971 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "577/577 - 4s - loss: 0.0705 - val_loss: 0.0949 - 4s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "577/577 - 4s - loss: 0.0720 - val_loss: 0.0906 - 4s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "577/577 - 4s - loss: 0.0684 - val_loss: 0.0931 - 4s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "577/577 - 4s - loss: 0.0704 - val_loss: 0.0976 - 4s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "577/577 - 4s - loss: 0.0700 - val_loss: 0.0909 - 4s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "577/577 - 4s - loss: 0.0708 - val_loss: 0.0951 - 4s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "577/577 - 4s - loss: 0.0693 - val_loss: 0.0923 - 4s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "577/577 - 5s - loss: 0.0667 - val_loss: 0.0920 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "577/577 - 4s - loss: 0.0700 - val_loss: 0.0918 - 4s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "577/577 - 4s - loss: 0.0677 - val_loss: 0.0911 - 4s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "577/577 - 4s - loss: 0.0665 - val_loss: 0.0897 - 4s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "577/577 - 4s - loss: 0.0669 - val_loss: 0.0904 - 4s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "577/577 - 5s - loss: 0.0658 - val_loss: 0.0898 - 5s/epoch - 9ms/step\n",
      "Epoch 90/10000\n",
      "577/577 - 5s - loss: 0.0679 - val_loss: 0.0992 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "577/577 - 5s - loss: 0.0669 - val_loss: 0.0909 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "577/577 - 5s - loss: 0.0641 - val_loss: 0.0919 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "577/577 - 5s - loss: 0.0681 - val_loss: 0.0899 - 5s/epoch - 9ms/step\n",
      "Epoch 94/10000\n",
      "577/577 - 5s - loss: 0.0644 - val_loss: 0.0917 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "577/577 - 5s - loss: 0.0644 - val_loss: 0.0913 - 5s/epoch - 9ms/step\n",
      "Epoch 96/10000\n",
      "577/577 - 5s - loss: 0.0641 - val_loss: 0.0957 - 5s/epoch - 9ms/step\n",
      "Epoch 97/10000\n",
      "577/577 - 5s - loss: 0.0677 - val_loss: 0.0912 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_20_1_50_1_50_datt_seq2seq_gru_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_20_1_50_1_50_datt_seq2seq_gru_3\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002643D7421F0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002643D709EB0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.905768</td>\n",
       "      <td>0.915556</td>\n",
       "      <td>0.961293</td>\n",
       "      <td>0.927539</td>\n",
       "      <td>6.063228</td>\n",
       "      <td>5.076893</td>\n",
       "      <td>3.061092</td>\n",
       "      <td>4.733738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.842657</td>\n",
       "      <td>0.869505</td>\n",
       "      <td>0.928861</td>\n",
       "      <td>0.880341</td>\n",
       "      <td>7.692173</td>\n",
       "      <td>6.313111</td>\n",
       "      <td>4.149431</td>\n",
       "      <td>6.051571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.743919</td>\n",
       "      <td>0.830255</td>\n",
       "      <td>0.882684</td>\n",
       "      <td>0.818953</td>\n",
       "      <td>9.384822</td>\n",
       "      <td>7.202883</td>\n",
       "      <td>5.32785</td>\n",
       "      <td>7.305185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.628859</td>\n",
       "      <td>0.799336</td>\n",
       "      <td>0.836876</td>\n",
       "      <td>0.755024</td>\n",
       "      <td>10.920878</td>\n",
       "      <td>7.834166</td>\n",
       "      <td>6.281019</td>\n",
       "      <td>8.345354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.518161</td>\n",
       "      <td>0.772994</td>\n",
       "      <td>0.793308</td>\n",
       "      <td>0.694821</td>\n",
       "      <td>12.245098</td>\n",
       "      <td>8.336259</td>\n",
       "      <td>7.068064</td>\n",
       "      <td>9.216474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.415911</td>\n",
       "      <td>0.750272</td>\n",
       "      <td>0.756149</td>\n",
       "      <td>0.640778</td>\n",
       "      <td>13.437841</td>\n",
       "      <td>8.747793</td>\n",
       "      <td>7.674383</td>\n",
       "      <td>9.953339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.32694</td>\n",
       "      <td>0.728431</td>\n",
       "      <td>0.726097</td>\n",
       "      <td>0.593823</td>\n",
       "      <td>14.143597</td>\n",
       "      <td>9.125094</td>\n",
       "      <td>8.13131</td>\n",
       "      <td>10.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.248568</td>\n",
       "      <td>0.706778</td>\n",
       "      <td>0.702529</td>\n",
       "      <td>0.552625</td>\n",
       "      <td>14.658534</td>\n",
       "      <td>9.484934</td>\n",
       "      <td>8.471997</td>\n",
       "      <td>10.871822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.183799</td>\n",
       "      <td>0.684613</td>\n",
       "      <td>0.681565</td>\n",
       "      <td>0.516659</td>\n",
       "      <td>15.050574</td>\n",
       "      <td>9.84013</td>\n",
       "      <td>8.763557</td>\n",
       "      <td>11.218087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.131823</td>\n",
       "      <td>0.662616</td>\n",
       "      <td>0.658055</td>\n",
       "      <td>0.484165</td>\n",
       "      <td>15.296725</td>\n",
       "      <td>10.179464</td>\n",
       "      <td>9.07876</td>\n",
       "      <td>11.518316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.089508</td>\n",
       "      <td>0.639587</td>\n",
       "      <td>0.627455</td>\n",
       "      <td>0.452183</td>\n",
       "      <td>15.502273</td>\n",
       "      <td>10.52288</td>\n",
       "      <td>9.474145</td>\n",
       "      <td>11.833099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.051682</td>\n",
       "      <td>0.615838</td>\n",
       "      <td>0.588249</td>\n",
       "      <td>0.418589</td>\n",
       "      <td>15.720478</td>\n",
       "      <td>10.865403</td>\n",
       "      <td>9.958382</td>\n",
       "      <td>12.181421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.01436</td>\n",
       "      <td>0.590053</td>\n",
       "      <td>0.544989</td>\n",
       "      <td>0.383134</td>\n",
       "      <td>15.808928</td>\n",
       "      <td>11.225274</td>\n",
       "      <td>10.466284</td>\n",
       "      <td>12.500162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.020406</td>\n",
       "      <td>0.563739</td>\n",
       "      <td>0.506129</td>\n",
       "      <td>0.349821</td>\n",
       "      <td>15.871424</td>\n",
       "      <td>11.581719</td>\n",
       "      <td>10.901698</td>\n",
       "      <td>12.784947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.05308</td>\n",
       "      <td>0.538921</td>\n",
       "      <td>0.477239</td>\n",
       "      <td>0.321026</td>\n",
       "      <td>15.963403</td>\n",
       "      <td>11.908252</td>\n",
       "      <td>11.213334</td>\n",
       "      <td>13.02833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.08185</td>\n",
       "      <td>0.514185</td>\n",
       "      <td>0.461116</td>\n",
       "      <td>0.297817</td>\n",
       "      <td>16.020455</td>\n",
       "      <td>12.22533</td>\n",
       "      <td>11.382822</td>\n",
       "      <td>13.209536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.104625</td>\n",
       "      <td>0.491319</td>\n",
       "      <td>0.453281</td>\n",
       "      <td>0.279991</td>\n",
       "      <td>16.031329</td>\n",
       "      <td>12.510369</td>\n",
       "      <td>11.463983</td>\n",
       "      <td>13.335227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.120996</td>\n",
       "      <td>0.468331</td>\n",
       "      <td>0.449146</td>\n",
       "      <td>0.265494</td>\n",
       "      <td>16.108953</td>\n",
       "      <td>12.790185</td>\n",
       "      <td>11.506728</td>\n",
       "      <td>13.468622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.132947</td>\n",
       "      <td>0.444157</td>\n",
       "      <td>0.4425</td>\n",
       "      <td>0.251237</td>\n",
       "      <td>16.21064</td>\n",
       "      <td>13.078217</td>\n",
       "      <td>11.575481</td>\n",
       "      <td>13.621446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.143906</td>\n",
       "      <td>0.419786</td>\n",
       "      <td>0.42982</td>\n",
       "      <td>0.235233</td>\n",
       "      <td>16.304766</td>\n",
       "      <td>13.362074</td>\n",
       "      <td>11.706085</td>\n",
       "      <td>13.790975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.222207</td>\n",
       "      <td>0.650314</td>\n",
       "      <td>0.645367</td>\n",
       "      <td>0.505963</td>\n",
       "      <td>13.921806</td>\n",
       "      <td>10.110521</td>\n",
       "      <td>8.88282</td>\n",
       "      <td>10.971716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.905768  0.915556  0.961293  0.927539   6.063228   5.076893   \n",
       "1      0.842657  0.869505  0.928861  0.880341   7.692173   6.313111   \n",
       "2      0.743919  0.830255  0.882684  0.818953   9.384822   7.202883   \n",
       "3      0.628859  0.799336  0.836876  0.755024  10.920878   7.834166   \n",
       "4      0.518161  0.772994  0.793308  0.694821  12.245098   8.336259   \n",
       "5      0.415911  0.750272  0.756149  0.640778  13.437841   8.747793   \n",
       "6       0.32694  0.728431  0.726097  0.593823  14.143597   9.125094   \n",
       "7      0.248568  0.706778  0.702529  0.552625  14.658534   9.484934   \n",
       "8      0.183799  0.684613  0.681565  0.516659  15.050574    9.84013   \n",
       "9      0.131823  0.662616  0.658055  0.484165  15.296725  10.179464   \n",
       "10     0.089508  0.639587  0.627455  0.452183  15.502273   10.52288   \n",
       "11     0.051682  0.615838  0.588249  0.418589  15.720478  10.865403   \n",
       "12      0.01436  0.590053  0.544989  0.383134  15.808928  11.225274   \n",
       "13    -0.020406  0.563739  0.506129  0.349821  15.871424  11.581719   \n",
       "14     -0.05308  0.538921  0.477239  0.321026  15.963403  11.908252   \n",
       "15     -0.08185  0.514185  0.461116  0.297817  16.020455   12.22533   \n",
       "16    -0.104625  0.491319  0.453281  0.279991  16.031329  12.510369   \n",
       "17    -0.120996  0.468331  0.449146  0.265494  16.108953  12.790185   \n",
       "18    -0.132947  0.444157    0.4425  0.251237   16.21064  13.078217   \n",
       "19    -0.143906  0.419786   0.42982  0.235233  16.304766  13.362074   \n",
       "mean   0.222207  0.650314  0.645367  0.505963  13.921806  10.110521   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       3.061092   4.733738  \n",
       "1       4.149431   6.051571  \n",
       "2        5.32785   7.305185  \n",
       "3       6.281019   8.345354  \n",
       "4       7.068064   9.216474  \n",
       "5       7.674383   9.953339  \n",
       "6        8.13131  10.466667  \n",
       "7       8.471997  10.871822  \n",
       "8       8.763557  11.218087  \n",
       "9        9.07876  11.518316  \n",
       "10      9.474145  11.833099  \n",
       "11      9.958382  12.181421  \n",
       "12     10.466284  12.500162  \n",
       "13     10.901698  12.784947  \n",
       "14     11.213334   13.02833  \n",
       "15     11.382822  13.209536  \n",
       "16     11.463983  13.335227  \n",
       "17     11.506728  13.468622  \n",
       "18     11.575481  13.621446  \n",
       "19     11.706085  13.790975  \n",
       "mean     8.88282  10.971716  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/10_20_1_50_1_50_datt_seq2seq_gru_3.csv\n",
      "\n",
      "\n",
      "3th iteration\n",
      "history size: 10\n",
      "future size: 30\n",
      "Epoch 1/10000\n",
      "572/572 - 8s - loss: 0.3673 - val_loss: 0.3000 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "572/572 - 5s - loss: 0.2986 - val_loss: 0.2917 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "572/572 - 5s - loss: 0.2793 - val_loss: 0.2905 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "572/572 - 5s - loss: 0.2668 - val_loss: 0.2611 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "572/572 - 5s - loss: 0.2530 - val_loss: 0.2423 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "572/572 - 5s - loss: 0.2411 - val_loss: 0.2393 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "572/572 - 5s - loss: 0.2310 - val_loss: 0.2355 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "572/572 - 5s - loss: 0.2195 - val_loss: 0.2205 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "572/572 - 5s - loss: 0.2104 - val_loss: 0.2082 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "572/572 - 5s - loss: 0.2034 - val_loss: 0.2081 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "572/572 - 5s - loss: 0.1956 - val_loss: 0.1987 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "572/572 - 5s - loss: 0.1881 - val_loss: 0.1887 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "572/572 - 5s - loss: 0.1809 - val_loss: 0.1876 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "572/572 - 5s - loss: 0.1783 - val_loss: 0.1805 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "572/572 - 5s - loss: 0.1724 - val_loss: 0.1762 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "572/572 - 5s - loss: 0.1694 - val_loss: 0.1781 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "572/572 - 5s - loss: 0.1657 - val_loss: 0.1745 - 5s/epoch - 9ms/step\n",
      "Epoch 18/10000\n",
      "572/572 - 5s - loss: 0.1608 - val_loss: 0.1697 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "572/572 - 5s - loss: 0.1595 - val_loss: 0.1627 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "572/572 - 5s - loss: 0.1591 - val_loss: 0.1642 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "572/572 - 5s - loss: 0.1514 - val_loss: 0.1570 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "572/572 - 5s - loss: 0.1540 - val_loss: 0.1562 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "572/572 - 5s - loss: 0.1469 - val_loss: 0.1635 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "572/572 - 5s - loss: 0.1484 - val_loss: 0.1590 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "572/572 - 5s - loss: 0.1445 - val_loss: 0.1531 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "572/572 - 5s - loss: 0.1448 - val_loss: 0.1565 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "572/572 - 5s - loss: 0.1406 - val_loss: 0.1498 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "572/572 - 5s - loss: 0.1371 - val_loss: 0.1541 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "572/572 - 5s - loss: 0.1352 - val_loss: 0.1473 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "572/572 - 5s - loss: 0.1392 - val_loss: 0.1455 - 5s/epoch - 9ms/step\n",
      "Epoch 31/10000\n",
      "572/572 - 5s - loss: 0.1298 - val_loss: 0.1417 - 5s/epoch - 9ms/step\n",
      "Epoch 32/10000\n",
      "572/572 - 5s - loss: 0.1324 - val_loss: 0.1426 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "572/572 - 5s - loss: 0.1299 - val_loss: 0.1462 - 5s/epoch - 9ms/step\n",
      "Epoch 34/10000\n",
      "572/572 - 5s - loss: 0.1287 - val_loss: 0.1482 - 5s/epoch - 9ms/step\n",
      "Epoch 35/10000\n",
      "572/572 - 5s - loss: 0.1259 - val_loss: 0.1345 - 5s/epoch - 9ms/step\n",
      "Epoch 36/10000\n",
      "572/572 - 5s - loss: 0.1225 - val_loss: 0.1451 - 5s/epoch - 9ms/step\n",
      "Epoch 37/10000\n",
      "572/572 - 5s - loss: 0.1231 - val_loss: 0.1356 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "572/572 - 5s - loss: 0.1214 - val_loss: 0.1456 - 5s/epoch - 9ms/step\n",
      "Epoch 39/10000\n",
      "572/572 - 5s - loss: 0.1268 - val_loss: 0.1403 - 5s/epoch - 9ms/step\n",
      "Epoch 40/10000\n",
      "572/572 - 5s - loss: 0.1191 - val_loss: 0.1388 - 5s/epoch - 9ms/step\n",
      "Epoch 41/10000\n",
      "572/572 - 5s - loss: 0.1219 - val_loss: 0.1400 - 5s/epoch - 9ms/step\n",
      "Epoch 42/10000\n",
      "572/572 - 5s - loss: 0.1180 - val_loss: 0.1283 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "572/572 - 5s - loss: 0.1143 - val_loss: 0.1280 - 5s/epoch - 9ms/step\n",
      "Epoch 44/10000\n",
      "572/572 - 5s - loss: 0.1150 - val_loss: 0.1349 - 5s/epoch - 9ms/step\n",
      "Epoch 45/10000\n",
      "572/572 - 5s - loss: 0.1167 - val_loss: 0.1270 - 5s/epoch - 9ms/step\n",
      "Epoch 46/10000\n",
      "572/572 - 5s - loss: 0.1144 - val_loss: 0.1295 - 5s/epoch - 9ms/step\n",
      "Epoch 47/10000\n",
      "572/572 - 5s - loss: 0.1114 - val_loss: 0.1244 - 5s/epoch - 9ms/step\n",
      "Epoch 48/10000\n",
      "572/572 - 5s - loss: 0.1113 - val_loss: 0.1328 - 5s/epoch - 9ms/step\n",
      "Epoch 49/10000\n",
      "572/572 - 5s - loss: 0.1164 - val_loss: 0.1304 - 5s/epoch - 9ms/step\n",
      "Epoch 50/10000\n",
      "572/572 - 5s - loss: 0.1119 - val_loss: 0.1254 - 5s/epoch - 9ms/step\n",
      "Epoch 51/10000\n",
      "572/572 - 5s - loss: 0.1083 - val_loss: 0.1281 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "572/572 - 5s - loss: 0.1156 - val_loss: 0.1389 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "572/572 - 5s - loss: 0.1098 - val_loss: 0.1260 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "572/572 - 5s - loss: 0.1072 - val_loss: 0.1217 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "572/572 - 5s - loss: 0.1050 - val_loss: 0.1221 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "572/572 - 5s - loss: 0.1044 - val_loss: 0.1228 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "572/572 - 5s - loss: 0.1058 - val_loss: 0.1240 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "572/572 - 5s - loss: 0.1002 - val_loss: 0.1136 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "572/572 - 5s - loss: 0.1020 - val_loss: 0.1205 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "572/572 - 5s - loss: 0.0999 - val_loss: 0.1158 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "572/572 - 5s - loss: 0.0991 - val_loss: 0.1171 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "572/572 - 5s - loss: 0.1050 - val_loss: 0.1190 - 5s/epoch - 9ms/step\n",
      "Epoch 63/10000\n",
      "572/572 - 5s - loss: 0.0983 - val_loss: 0.1114 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "572/572 - 5s - loss: 0.0977 - val_loss: 0.1118 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "572/572 - 5s - loss: 0.0987 - val_loss: 0.1145 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "572/572 - 5s - loss: 0.1005 - val_loss: 0.1098 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "572/572 - 5s - loss: 0.0953 - val_loss: 0.1109 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "572/572 - 5s - loss: 0.0946 - val_loss: 0.1190 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "572/572 - 5s - loss: 0.0939 - val_loss: 0.1165 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "572/572 - 5s - loss: 0.0933 - val_loss: 0.1092 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "572/572 - 5s - loss: 0.0950 - val_loss: 0.1075 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "572/572 - 5s - loss: 0.0932 - val_loss: 0.1166 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "572/572 - 5s - loss: 0.0992 - val_loss: 0.1153 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "572/572 - 5s - loss: 0.0918 - val_loss: 0.1077 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "572/572 - 5s - loss: 0.0906 - val_loss: 0.1102 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "572/572 - 5s - loss: 0.0929 - val_loss: 0.1068 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "572/572 - 5s - loss: 0.0895 - val_loss: 0.1039 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "572/572 - 5s - loss: 0.0925 - val_loss: 0.1081 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "572/572 - 5s - loss: 0.0958 - val_loss: 0.1193 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "572/572 - 5s - loss: 0.0883 - val_loss: 0.0993 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "572/572 - 5s - loss: 0.0905 - val_loss: 0.1099 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "572/572 - 5s - loss: 0.0867 - val_loss: 0.0994 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "572/572 - 5s - loss: 0.0894 - val_loss: 0.1032 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "572/572 - 5s - loss: 0.0887 - val_loss: 0.1019 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "572/572 - 5s - loss: 0.0950 - val_loss: 0.1193 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "572/572 - 5s - loss: 0.0987 - val_loss: 0.1032 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "572/572 - 5s - loss: 0.0922 - val_loss: 0.1054 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "572/572 - 5s - loss: 0.0841 - val_loss: 0.0979 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "572/572 - 5s - loss: 0.0850 - val_loss: 0.1007 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "572/572 - 5s - loss: 0.0841 - val_loss: 0.0992 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "572/572 - 5s - loss: 0.0850 - val_loss: 0.1031 - 5s/epoch - 9ms/step\n",
      "Epoch 92/10000\n",
      "572/572 - 5s - loss: 0.0882 - val_loss: 0.1190 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "572/572 - 5s - loss: 0.0914 - val_loss: 0.0975 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "572/572 - 5s - loss: 0.0905 - val_loss: 0.1171 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "572/572 - 5s - loss: 0.0881 - val_loss: 0.1003 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "572/572 - 5s - loss: 0.0831 - val_loss: 0.1061 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "572/572 - 5s - loss: 0.0835 - val_loss: 0.1081 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "572/572 - 5s - loss: 0.0948 - val_loss: 0.1311 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "572/572 - 5s - loss: 0.0871 - val_loss: 0.0960 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "572/572 - 5s - loss: 0.0805 - val_loss: 0.0994 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "572/572 - 5s - loss: 0.0828 - val_loss: 0.0972 - 5s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "572/572 - 5s - loss: 0.0834 - val_loss: 0.0999 - 5s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "572/572 - 5s - loss: 0.0812 - val_loss: 0.0997 - 5s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "572/572 - 5s - loss: 0.0840 - val_loss: 0.1002 - 5s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "572/572 - 5s - loss: 0.0797 - val_loss: 0.0968 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "572/572 - 5s - loss: 0.0820 - val_loss: 0.1023 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "572/572 - 5s - loss: 0.0806 - val_loss: 0.0978 - 5s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "572/572 - 5s - loss: 0.0793 - val_loss: 0.0925 - 5s/epoch - 8ms/step\n",
      "Epoch 109/10000\n",
      "572/572 - 5s - loss: 0.0775 - val_loss: 0.0949 - 5s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "572/572 - 5s - loss: 0.0815 - val_loss: 0.1103 - 5s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "572/572 - 5s - loss: 0.0862 - val_loss: 0.0914 - 5s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "572/572 - 5s - loss: 0.0770 - val_loss: 0.0962 - 5s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "572/572 - 5s - loss: 0.0836 - val_loss: 0.0983 - 5s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "572/572 - 5s - loss: 0.0837 - val_loss: 0.0945 - 5s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "572/572 - 5s - loss: 0.0774 - val_loss: 0.1014 - 5s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "572/572 - 5s - loss: 0.0775 - val_loss: 0.0908 - 5s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "572/572 - 5s - loss: 0.0808 - val_loss: 0.1070 - 5s/epoch - 8ms/step\n",
      "Epoch 118/10000\n",
      "572/572 - 5s - loss: 0.0801 - val_loss: 0.0900 - 5s/epoch - 8ms/step\n",
      "Epoch 119/10000\n",
      "572/572 - 5s - loss: 0.0768 - val_loss: 0.0910 - 5s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "572/572 - 5s - loss: 0.0748 - val_loss: 0.0967 - 5s/epoch - 8ms/step\n",
      "Epoch 121/10000\n",
      "572/572 - 5s - loss: 0.0767 - val_loss: 0.0933 - 5s/epoch - 9ms/step\n",
      "Epoch 122/10000\n",
      "572/572 - 5s - loss: 0.0801 - val_loss: 0.1180 - 5s/epoch - 8ms/step\n",
      "Epoch 123/10000\n",
      "572/572 - 5s - loss: 0.0784 - val_loss: 0.0973 - 5s/epoch - 9ms/step\n",
      "Epoch 124/10000\n",
      "572/572 - 5s - loss: 0.0759 - val_loss: 0.0935 - 5s/epoch - 8ms/step\n",
      "Epoch 125/10000\n",
      "572/572 - 5s - loss: 0.0741 - val_loss: 0.0903 - 5s/epoch - 8ms/step\n",
      "Epoch 126/10000\n",
      "572/572 - 5s - loss: 0.0777 - val_loss: 0.0964 - 5s/epoch - 8ms/step\n",
      "Epoch 127/10000\n",
      "572/572 - 5s - loss: 0.0749 - val_loss: 0.0904 - 5s/epoch - 8ms/step\n",
      "Epoch 128/10000\n",
      "572/572 - 5s - loss: 0.0807 - val_loss: 0.0999 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_4_layer_call_fn, gru_cell_4_layer_call_and_return_conditional_losses, gru_cell_5_layer_call_fn, gru_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_30_1_50_1_50_datt_seq2seq_gru_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_30_1_50_1_50_datt_seq2seq_gru_3\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002643D0F0400> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002640AA539A0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.907579</td>\n",
       "      <td>0.925709</td>\n",
       "      <td>0.929711</td>\n",
       "      <td>0.921</td>\n",
       "      <td>6.001595</td>\n",
       "      <td>4.760987</td>\n",
       "      <td>4.109403</td>\n",
       "      <td>4.957328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.831134</td>\n",
       "      <td>0.884269</td>\n",
       "      <td>0.906305</td>\n",
       "      <td>0.873903</td>\n",
       "      <td>8.112566</td>\n",
       "      <td>5.942564</td>\n",
       "      <td>4.74462</td>\n",
       "      <td>6.266583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.727294</td>\n",
       "      <td>0.849452</td>\n",
       "      <td>0.883431</td>\n",
       "      <td>0.820059</td>\n",
       "      <td>10.31099</td>\n",
       "      <td>6.778276</td>\n",
       "      <td>5.291667</td>\n",
       "      <td>7.460311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.613956</td>\n",
       "      <td>0.821255</td>\n",
       "      <td>0.857087</td>\n",
       "      <td>0.764099</td>\n",
       "      <td>12.270263</td>\n",
       "      <td>7.386754</td>\n",
       "      <td>5.857746</td>\n",
       "      <td>8.504921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.503776</td>\n",
       "      <td>0.796285</td>\n",
       "      <td>0.834814</td>\n",
       "      <td>0.711625</td>\n",
       "      <td>13.913301</td>\n",
       "      <td>7.887279</td>\n",
       "      <td>6.295738</td>\n",
       "      <td>9.365439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.404591</td>\n",
       "      <td>0.773216</td>\n",
       "      <td>0.817861</td>\n",
       "      <td>0.665223</td>\n",
       "      <td>15.24182</td>\n",
       "      <td>8.324227</td>\n",
       "      <td>6.608869</td>\n",
       "      <td>10.058305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.316169</td>\n",
       "      <td>0.748439</td>\n",
       "      <td>0.801725</td>\n",
       "      <td>0.622111</td>\n",
       "      <td>16.335762</td>\n",
       "      <td>8.769764</td>\n",
       "      <td>6.893813</td>\n",
       "      <td>10.666446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.238776</td>\n",
       "      <td>0.723675</td>\n",
       "      <td>0.784244</td>\n",
       "      <td>0.582231</td>\n",
       "      <td>17.235237</td>\n",
       "      <td>9.194069</td>\n",
       "      <td>7.190106</td>\n",
       "      <td>11.206471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.170688</td>\n",
       "      <td>0.699245</td>\n",
       "      <td>0.765339</td>\n",
       "      <td>0.545091</td>\n",
       "      <td>17.988622</td>\n",
       "      <td>9.595739</td>\n",
       "      <td>7.497088</td>\n",
       "      <td>11.693816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.11063</td>\n",
       "      <td>0.674715</td>\n",
       "      <td>0.743519</td>\n",
       "      <td>0.509622</td>\n",
       "      <td>18.629592</td>\n",
       "      <td>9.982796</td>\n",
       "      <td>7.83636</td>\n",
       "      <td>12.149583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.053522</td>\n",
       "      <td>0.649242</td>\n",
       "      <td>0.718543</td>\n",
       "      <td>0.473769</td>\n",
       "      <td>19.22265</td>\n",
       "      <td>10.369088</td>\n",
       "      <td>8.207772</td>\n",
       "      <td>12.599837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.002486</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>0.6916</td>\n",
       "      <td>0.437671</td>\n",
       "      <td>19.424348</td>\n",
       "      <td>10.740584</td>\n",
       "      <td>8.590477</td>\n",
       "      <td>12.91847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.055245</td>\n",
       "      <td>0.598646</td>\n",
       "      <td>0.663492</td>\n",
       "      <td>0.402297</td>\n",
       "      <td>19.059369</td>\n",
       "      <td>11.099471</td>\n",
       "      <td>8.972167</td>\n",
       "      <td>13.043669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.106987</td>\n",
       "      <td>0.574979</td>\n",
       "      <td>0.634567</td>\n",
       "      <td>0.36752</td>\n",
       "      <td>18.868915</td>\n",
       "      <td>11.426059</td>\n",
       "      <td>9.348553</td>\n",
       "      <td>13.214509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.158943</td>\n",
       "      <td>0.553232</td>\n",
       "      <td>0.606586</td>\n",
       "      <td>0.333625</td>\n",
       "      <td>18.996773</td>\n",
       "      <td>11.719987</td>\n",
       "      <td>9.698914</td>\n",
       "      <td>13.471891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.207257</td>\n",
       "      <td>0.532437</td>\n",
       "      <td>0.579431</td>\n",
       "      <td>0.301537</td>\n",
       "      <td>19.321247</td>\n",
       "      <td>11.99562</td>\n",
       "      <td>10.026518</td>\n",
       "      <td>13.781128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.250719</td>\n",
       "      <td>0.512301</td>\n",
       "      <td>0.552233</td>\n",
       "      <td>0.271272</td>\n",
       "      <td>19.278263</td>\n",
       "      <td>12.254706</td>\n",
       "      <td>10.344273</td>\n",
       "      <td>13.959081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.291878</td>\n",
       "      <td>0.492733</td>\n",
       "      <td>0.524669</td>\n",
       "      <td>0.241842</td>\n",
       "      <td>19.214959</td>\n",
       "      <td>12.502106</td>\n",
       "      <td>10.655736</td>\n",
       "      <td>14.124267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.330397</td>\n",
       "      <td>0.473655</td>\n",
       "      <td>0.496945</td>\n",
       "      <td>0.213401</td>\n",
       "      <td>19.206347</td>\n",
       "      <td>12.739264</td>\n",
       "      <td>10.959332</td>\n",
       "      <td>14.301648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.368069</td>\n",
       "      <td>0.454477</td>\n",
       "      <td>0.467898</td>\n",
       "      <td>0.184768</td>\n",
       "      <td>19.18886</td>\n",
       "      <td>12.971762</td>\n",
       "      <td>11.267751</td>\n",
       "      <td>14.476124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.406719</td>\n",
       "      <td>0.434192</td>\n",
       "      <td>0.43766</td>\n",
       "      <td>0.155044</td>\n",
       "      <td>19.249821</td>\n",
       "      <td>13.212964</td>\n",
       "      <td>11.580342</td>\n",
       "      <td>14.681042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.445195</td>\n",
       "      <td>0.413022</td>\n",
       "      <td>0.40686</td>\n",
       "      <td>0.124896</td>\n",
       "      <td>19.381776</td>\n",
       "      <td>13.459512</td>\n",
       "      <td>11.891207</td>\n",
       "      <td>14.910831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.481174</td>\n",
       "      <td>0.392634</td>\n",
       "      <td>0.376928</td>\n",
       "      <td>0.096129</td>\n",
       "      <td>19.349444</td>\n",
       "      <td>13.692553</td>\n",
       "      <td>12.186611</td>\n",
       "      <td>15.076203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.515335</td>\n",
       "      <td>0.371522</td>\n",
       "      <td>0.348442</td>\n",
       "      <td>0.06821</td>\n",
       "      <td>19.307613</td>\n",
       "      <td>13.93031</td>\n",
       "      <td>12.462193</td>\n",
       "      <td>15.233372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.549049</td>\n",
       "      <td>0.350204</td>\n",
       "      <td>0.322398</td>\n",
       "      <td>0.041185</td>\n",
       "      <td>19.32745</td>\n",
       "      <td>14.166467</td>\n",
       "      <td>12.70846</td>\n",
       "      <td>15.400793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.582486</td>\n",
       "      <td>0.33152</td>\n",
       "      <td>0.299319</td>\n",
       "      <td>0.016118</td>\n",
       "      <td>19.345908</td>\n",
       "      <td>14.370878</td>\n",
       "      <td>12.92318</td>\n",
       "      <td>15.546655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.615</td>\n",
       "      <td>0.313812</td>\n",
       "      <td>0.279858</td>\n",
       "      <td>-0.00711</td>\n",
       "      <td>19.358887</td>\n",
       "      <td>14.561016</td>\n",
       "      <td>13.102035</td>\n",
       "      <td>15.673979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.647062</td>\n",
       "      <td>0.295936</td>\n",
       "      <td>0.26228</td>\n",
       "      <td>-0.029615</td>\n",
       "      <td>19.507115</td>\n",
       "      <td>14.749846</td>\n",
       "      <td>13.262153</td>\n",
       "      <td>15.839705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.678363</td>\n",
       "      <td>0.278805</td>\n",
       "      <td>0.24529</td>\n",
       "      <td>-0.051422</td>\n",
       "      <td>19.719251</td>\n",
       "      <td>14.928627</td>\n",
       "      <td>13.414899</td>\n",
       "      <td>16.020926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.705738</td>\n",
       "      <td>0.261829</td>\n",
       "      <td>0.226883</td>\n",
       "      <td>-0.072342</td>\n",
       "      <td>19.907997</td>\n",
       "      <td>15.103404</td>\n",
       "      <td>13.579049</td>\n",
       "      <td>16.196817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.560178</td>\n",
       "      <td>0.582197</td>\n",
       "      <td>0.352792</td>\n",
       "      <td>17.409225</td>\n",
       "      <td>11.287223</td>\n",
       "      <td>9.583568</td>\n",
       "      <td>12.760005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.907579  0.925709  0.929711     0.921   6.001595   4.760987   \n",
       "1      0.831134  0.884269  0.906305  0.873903   8.112566   5.942564   \n",
       "2      0.727294  0.849452  0.883431  0.820059   10.31099   6.778276   \n",
       "3      0.613956  0.821255  0.857087  0.764099  12.270263   7.386754   \n",
       "4      0.503776  0.796285  0.834814  0.711625  13.913301   7.887279   \n",
       "5      0.404591  0.773216  0.817861  0.665223   15.24182   8.324227   \n",
       "6      0.316169  0.748439  0.801725  0.622111  16.335762   8.769764   \n",
       "7      0.238776  0.723675  0.784244  0.582231  17.235237   9.194069   \n",
       "8      0.170688  0.699245  0.765339  0.545091  17.988622   9.595739   \n",
       "9       0.11063  0.674715  0.743519  0.509622  18.629592   9.982796   \n",
       "10     0.053522  0.649242  0.718543  0.473769   19.22265  10.369088   \n",
       "11    -0.002486    0.6239    0.6916  0.437671  19.424348  10.740584   \n",
       "12    -0.055245  0.598646  0.663492  0.402297  19.059369  11.099471   \n",
       "13    -0.106987  0.574979  0.634567   0.36752  18.868915  11.426059   \n",
       "14    -0.158943  0.553232  0.606586  0.333625  18.996773  11.719987   \n",
       "15    -0.207257  0.532437  0.579431  0.301537  19.321247   11.99562   \n",
       "16    -0.250719  0.512301  0.552233  0.271272  19.278263  12.254706   \n",
       "17    -0.291878  0.492733  0.524669  0.241842  19.214959  12.502106   \n",
       "18    -0.330397  0.473655  0.496945  0.213401  19.206347  12.739264   \n",
       "19    -0.368069  0.454477  0.467898  0.184768   19.18886  12.971762   \n",
       "20    -0.406719  0.434192   0.43766  0.155044  19.249821  13.212964   \n",
       "21    -0.445195  0.413022   0.40686  0.124896  19.381776  13.459512   \n",
       "22    -0.481174  0.392634  0.376928  0.096129  19.349444  13.692553   \n",
       "23    -0.515335  0.371522  0.348442   0.06821  19.307613   13.93031   \n",
       "24    -0.549049  0.350204  0.322398  0.041185   19.32745  14.166467   \n",
       "25    -0.582486   0.33152  0.299319  0.016118  19.345908  14.370878   \n",
       "26       -0.615  0.313812  0.279858  -0.00711  19.358887  14.561016   \n",
       "27    -0.647062  0.295936   0.26228 -0.029615  19.507115  14.749846   \n",
       "28    -0.678363  0.278805   0.24529 -0.051422  19.719251  14.928627   \n",
       "29    -0.705738  0.261829  0.226883 -0.072342  19.907997  15.103404   \n",
       "mean     -0.084  0.560178  0.582197  0.352792  17.409225  11.287223   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       4.109403   4.957328  \n",
       "1        4.74462   6.266583  \n",
       "2       5.291667   7.460311  \n",
       "3       5.857746   8.504921  \n",
       "4       6.295738   9.365439  \n",
       "5       6.608869  10.058305  \n",
       "6       6.893813  10.666446  \n",
       "7       7.190106  11.206471  \n",
       "8       7.497088  11.693816  \n",
       "9        7.83636  12.149583  \n",
       "10      8.207772  12.599837  \n",
       "11      8.590477   12.91847  \n",
       "12      8.972167  13.043669  \n",
       "13      9.348553  13.214509  \n",
       "14      9.698914  13.471891  \n",
       "15     10.026518  13.781128  \n",
       "16     10.344273  13.959081  \n",
       "17     10.655736  14.124267  \n",
       "18     10.959332  14.301648  \n",
       "19     11.267751  14.476124  \n",
       "20     11.580342  14.681042  \n",
       "21     11.891207  14.910831  \n",
       "22     12.186611  15.076203  \n",
       "23     12.462193  15.233372  \n",
       "24      12.70846  15.400793  \n",
       "25      12.92318  15.546655  \n",
       "26     13.102035  15.673979  \n",
       "27     13.262153  15.839705  \n",
       "28     13.414899  16.020926  \n",
       "29     13.579049  16.196817  \n",
       "mean    9.583568  12.760005  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/10_30_1_50_1_50_datt_seq2seq_gru_3.csv\n",
      "\n",
      "\n",
      "3th iteration\n",
      "history size: 20\n",
      "future size: 10\n",
      "Epoch 1/10000\n",
      "577/577 - 8s - loss: 0.2407 - val_loss: 0.1673 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "577/577 - 4s - loss: 0.1518 - val_loss: 0.1456 - 4s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "577/577 - 4s - loss: 0.1391 - val_loss: 0.1435 - 4s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "577/577 - 4s - loss: 0.1297 - val_loss: 0.1342 - 4s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "577/577 - 4s - loss: 0.1210 - val_loss: 0.1219 - 4s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "577/577 - 4s - loss: 0.1113 - val_loss: 0.1119 - 4s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "577/577 - 4s - loss: 0.1060 - val_loss: 0.1055 - 4s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "577/577 - 4s - loss: 0.1012 - val_loss: 0.1064 - 4s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "577/577 - 4s - loss: 0.0971 - val_loss: 0.1028 - 4s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "577/577 - 4s - loss: 0.0935 - val_loss: 0.1022 - 4s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "577/577 - 4s - loss: 0.0906 - val_loss: 0.1027 - 4s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "577/577 - 4s - loss: 0.0882 - val_loss: 0.0996 - 4s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "577/577 - 4s - loss: 0.0863 - val_loss: 0.0912 - 4s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "577/577 - 5s - loss: 0.0835 - val_loss: 0.0900 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "577/577 - 4s - loss: 0.0813 - val_loss: 0.0909 - 4s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "577/577 - 4s - loss: 0.0814 - val_loss: 0.0957 - 4s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "577/577 - 4s - loss: 0.0777 - val_loss: 0.0885 - 4s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "577/577 - 4s - loss: 0.0752 - val_loss: 0.0885 - 4s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "577/577 - 4s - loss: 0.0724 - val_loss: 0.0881 - 4s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "577/577 - 4s - loss: 0.0726 - val_loss: 0.0909 - 4s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "577/577 - 4s - loss: 0.0741 - val_loss: 0.0879 - 4s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "577/577 - 4s - loss: 0.0689 - val_loss: 0.0785 - 4s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "577/577 - 4s - loss: 0.0664 - val_loss: 0.0766 - 4s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "577/577 - 4s - loss: 0.0696 - val_loss: 0.0769 - 4s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "577/577 - 4s - loss: 0.0652 - val_loss: 0.0770 - 4s/epoch - 7ms/step\n",
      "Epoch 26/10000\n",
      "577/577 - 4s - loss: 0.0630 - val_loss: 0.0765 - 4s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "577/577 - 4s - loss: 0.0631 - val_loss: 0.0721 - 4s/epoch - 7ms/step\n",
      "Epoch 28/10000\n",
      "577/577 - 4s - loss: 0.0633 - val_loss: 0.0758 - 4s/epoch - 7ms/step\n",
      "Epoch 29/10000\n",
      "577/577 - 4s - loss: 0.0599 - val_loss: 0.0727 - 4s/epoch - 7ms/step\n",
      "Epoch 30/10000\n",
      "577/577 - 4s - loss: 0.0589 - val_loss: 0.0716 - 4s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "577/577 - 4s - loss: 0.0589 - val_loss: 0.0766 - 4s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "577/577 - 4s - loss: 0.0609 - val_loss: 0.0759 - 4s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "577/577 - 4s - loss: 0.0574 - val_loss: 0.0682 - 4s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "577/577 - 4s - loss: 0.0560 - val_loss: 0.0667 - 4s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "577/577 - 4s - loss: 0.0556 - val_loss: 0.0722 - 4s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "577/577 - 4s - loss: 0.0548 - val_loss: 0.0685 - 4s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "577/577 - 4s - loss: 0.0546 - val_loss: 0.0692 - 4s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "577/577 - 4s - loss: 0.0538 - val_loss: 0.0657 - 4s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "577/577 - 4s - loss: 0.0534 - val_loss: 0.0653 - 4s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "577/577 - 4s - loss: 0.0533 - val_loss: 0.0643 - 4s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "577/577 - 4s - loss: 0.0519 - val_loss: 0.0650 - 4s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "577/577 - 4s - loss: 0.0506 - val_loss: 0.0631 - 4s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "577/577 - 4s - loss: 0.0508 - val_loss: 0.0624 - 4s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "577/577 - 4s - loss: 0.0506 - val_loss: 0.0670 - 4s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "577/577 - 4s - loss: 0.0623 - val_loss: 0.0686 - 4s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "577/577 - 4s - loss: 0.0523 - val_loss: 0.0627 - 4s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "577/577 - 4s - loss: 0.0487 - val_loss: 0.0614 - 4s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "577/577 - 4s - loss: 0.0512 - val_loss: 0.0612 - 4s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "577/577 - 4s - loss: 0.0479 - val_loss: 0.0617 - 4s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "577/577 - 4s - loss: 0.0484 - val_loss: 0.0630 - 4s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "577/577 - 4s - loss: 0.0474 - val_loss: 0.0584 - 4s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "577/577 - 4s - loss: 0.0467 - val_loss: 0.0585 - 4s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "577/577 - 4s - loss: 0.0458 - val_loss: 0.0574 - 4s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "577/577 - 4s - loss: 0.0467 - val_loss: 0.0595 - 4s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "577/577 - 4s - loss: 0.0464 - val_loss: 0.0756 - 4s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "577/577 - 4s - loss: 0.0494 - val_loss: 0.0594 - 4s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "577/577 - 4s - loss: 0.0446 - val_loss: 0.0563 - 4s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "577/577 - 4s - loss: 0.0445 - val_loss: 0.0585 - 4s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "577/577 - 4s - loss: 0.0450 - val_loss: 0.0556 - 4s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "577/577 - 4s - loss: 0.0435 - val_loss: 0.0549 - 4s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "577/577 - 4s - loss: 0.0431 - val_loss: 0.0597 - 4s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "577/577 - 5s - loss: 0.0435 - val_loss: 0.0604 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "577/577 - 4s - loss: 0.0435 - val_loss: 0.0562 - 4s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "577/577 - 4s - loss: 0.0432 - val_loss: 0.0557 - 4s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "577/577 - 4s - loss: 0.0414 - val_loss: 0.0555 - 4s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "577/577 - 4s - loss: 0.0413 - val_loss: 0.0537 - 4s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "577/577 - 4s - loss: 0.0416 - val_loss: 0.0594 - 4s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "577/577 - 4s - loss: 0.0420 - val_loss: 0.0537 - 4s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "577/577 - 4s - loss: 0.0417 - val_loss: 0.0582 - 4s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "577/577 - 4s - loss: 0.0411 - val_loss: 0.0537 - 4s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "577/577 - 4s - loss: 0.0395 - val_loss: 0.0554 - 4s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "577/577 - 4s - loss: 0.0407 - val_loss: 0.0536 - 4s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "577/577 - 4s - loss: 0.0413 - val_loss: 0.0533 - 4s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "577/577 - 4s - loss: 0.0400 - val_loss: 0.0546 - 4s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "577/577 - 4s - loss: 0.0405 - val_loss: 0.0624 - 4s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "577/577 - 4s - loss: 0.0411 - val_loss: 0.0534 - 4s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "577/577 - 4s - loss: 0.0386 - val_loss: 0.0543 - 4s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "577/577 - 4s - loss: 0.0395 - val_loss: 0.0521 - 4s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "577/577 - 5s - loss: 0.0407 - val_loss: 0.0543 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "577/577 - 5s - loss: 0.0391 - val_loss: 0.0510 - 5s/epoch - 9ms/step\n",
      "Epoch 81/10000\n",
      "577/577 - 5s - loss: 0.0380 - val_loss: 0.0511 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "577/577 - 5s - loss: 0.0381 - val_loss: 0.0517 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "577/577 - 4s - loss: 0.0374 - val_loss: 0.0527 - 4s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "577/577 - 4s - loss: 0.0382 - val_loss: 0.0527 - 4s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "577/577 - 4s - loss: 0.0375 - val_loss: 0.0552 - 4s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "577/577 - 4s - loss: 0.0381 - val_loss: 0.0497 - 4s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "577/577 - 4s - loss: 0.0366 - val_loss: 0.0513 - 4s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "577/577 - 4s - loss: 0.0373 - val_loss: 0.0565 - 4s/epoch - 7ms/step\n",
      "Epoch 89/10000\n",
      "577/577 - 4s - loss: 0.0370 - val_loss: 0.0505 - 4s/epoch - 7ms/step\n",
      "Epoch 90/10000\n",
      "577/577 - 4s - loss: 0.0368 - val_loss: 0.0492 - 4s/epoch - 7ms/step\n",
      "Epoch 91/10000\n",
      "577/577 - 4s - loss: 0.0365 - val_loss: 0.0528 - 4s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "577/577 - 4s - loss: 0.0371 - val_loss: 0.0486 - 4s/epoch - 7ms/step\n",
      "Epoch 93/10000\n",
      "577/577 - 4s - loss: 0.0361 - val_loss: 0.0490 - 4s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "577/577 - 4s - loss: 0.0377 - val_loss: 0.0521 - 4s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "577/577 - 4s - loss: 0.0359 - val_loss: 0.0510 - 4s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "577/577 - 4s - loss: 0.0353 - val_loss: 0.0484 - 4s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "577/577 - 4s - loss: 0.0347 - val_loss: 0.0490 - 4s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "577/577 - 4s - loss: 0.0349 - val_loss: 0.0512 - 4s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "577/577 - 4s - loss: 0.0360 - val_loss: 0.0478 - 4s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "577/577 - 4s - loss: 0.0354 - val_loss: 0.0512 - 4s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "577/577 - 4s - loss: 0.0354 - val_loss: 0.0501 - 4s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "577/577 - 4s - loss: 0.0339 - val_loss: 0.0471 - 4s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "577/577 - 4s - loss: 0.0356 - val_loss: 0.0478 - 4s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "577/577 - 4s - loss: 0.0345 - val_loss: 0.0473 - 4s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "577/577 - 4s - loss: 0.0353 - val_loss: 0.0481 - 4s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "577/577 - 4s - loss: 0.0336 - val_loss: 0.0485 - 4s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "577/577 - 4s - loss: 0.0341 - val_loss: 0.0547 - 4s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "577/577 - 4s - loss: 0.0339 - val_loss: 0.0472 - 4s/epoch - 8ms/step\n",
      "Epoch 109/10000\n",
      "577/577 - 4s - loss: 0.0343 - val_loss: 0.0494 - 4s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "577/577 - 4s - loss: 0.0340 - val_loss: 0.0493 - 4s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "577/577 - 4s - loss: 0.0333 - val_loss: 0.0498 - 4s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "577/577 - 4s - loss: 0.0341 - val_loss: 0.0488 - 4s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_6_layer_call_fn, gru_cell_6_layer_call_and_return_conditional_losses, gru_cell_7_layer_call_fn, gru_cell_7_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_10_1_50_1_50_datt_seq2seq_gru_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_10_1_50_1_50_datt_seq2seq_gru_3\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002660083C0A0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002640A5192E0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.930984</td>\n",
       "      <td>0.941173</td>\n",
       "      <td>0.947488</td>\n",
       "      <td>0.939882</td>\n",
       "      <td>4.268084</td>\n",
       "      <td>4.251298</td>\n",
       "      <td>3.556967</td>\n",
       "      <td>4.02545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.834771</td>\n",
       "      <td>0.881906</td>\n",
       "      <td>0.908402</td>\n",
       "      <td>0.875026</td>\n",
       "      <td>6.561926</td>\n",
       "      <td>6.024249</td>\n",
       "      <td>4.696932</td>\n",
       "      <td>5.761036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.728427</td>\n",
       "      <td>0.831094</td>\n",
       "      <td>0.859861</td>\n",
       "      <td>0.806461</td>\n",
       "      <td>8.298252</td>\n",
       "      <td>7.205366</td>\n",
       "      <td>5.808462</td>\n",
       "      <td>7.104027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.624992</td>\n",
       "      <td>0.796328</td>\n",
       "      <td>0.815704</td>\n",
       "      <td>0.745675</td>\n",
       "      <td>9.621645</td>\n",
       "      <td>7.913443</td>\n",
       "      <td>6.659558</td>\n",
       "      <td>8.064882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.532201</td>\n",
       "      <td>0.772043</td>\n",
       "      <td>0.780421</td>\n",
       "      <td>0.694888</td>\n",
       "      <td>10.639582</td>\n",
       "      <td>8.37311</td>\n",
       "      <td>7.267389</td>\n",
       "      <td>8.760027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.448696</td>\n",
       "      <td>0.754378</td>\n",
       "      <td>0.755309</td>\n",
       "      <td>0.652794</td>\n",
       "      <td>11.43634</td>\n",
       "      <td>8.692776</td>\n",
       "      <td>7.67028</td>\n",
       "      <td>9.266466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.372847</td>\n",
       "      <td>0.739508</td>\n",
       "      <td>0.740934</td>\n",
       "      <td>0.617763</td>\n",
       "      <td>12.079495</td>\n",
       "      <td>8.952504</td>\n",
       "      <td>7.891479</td>\n",
       "      <td>9.641159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.302896</td>\n",
       "      <td>0.723221</td>\n",
       "      <td>0.735962</td>\n",
       "      <td>0.58736</td>\n",
       "      <td>12.703223</td>\n",
       "      <td>9.228322</td>\n",
       "      <td>7.966487</td>\n",
       "      <td>9.96601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.243477</td>\n",
       "      <td>0.705049</td>\n",
       "      <td>0.736574</td>\n",
       "      <td>0.5617</td>\n",
       "      <td>13.246662</td>\n",
       "      <td>9.526816</td>\n",
       "      <td>7.956932</td>\n",
       "      <td>10.24347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.193626</td>\n",
       "      <td>0.685987</td>\n",
       "      <td>0.731654</td>\n",
       "      <td>0.537089</td>\n",
       "      <td>13.689505</td>\n",
       "      <td>9.830003</td>\n",
       "      <td>8.030697</td>\n",
       "      <td>10.516735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.521292</td>\n",
       "      <td>0.783069</td>\n",
       "      <td>0.801231</td>\n",
       "      <td>0.701864</td>\n",
       "      <td>10.254471</td>\n",
       "      <td>7.999789</td>\n",
       "      <td>6.750518</td>\n",
       "      <td>8.334926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3 TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE     nRMSE     nRMSE   \n",
       "0      0.930984  0.941173  0.947488  0.939882   4.268084  4.251298  3.556967   \n",
       "1      0.834771  0.881906  0.908402  0.875026   6.561926  6.024249  4.696932   \n",
       "2      0.728427  0.831094  0.859861  0.806461   8.298252  7.205366  5.808462   \n",
       "3      0.624992  0.796328  0.815704  0.745675   9.621645  7.913443  6.659558   \n",
       "4      0.532201  0.772043  0.780421  0.694888  10.639582   8.37311  7.267389   \n",
       "5      0.448696  0.754378  0.755309  0.652794   11.43634  8.692776   7.67028   \n",
       "6      0.372847  0.739508  0.740934  0.617763  12.079495  8.952504  7.891479   \n",
       "7      0.302896  0.723221  0.735962   0.58736  12.703223  9.228322  7.966487   \n",
       "8      0.243477  0.705049  0.736574    0.5617  13.246662  9.526816  7.956932   \n",
       "9      0.193626  0.685987  0.731654  0.537089  13.689505  9.830003  8.030697   \n",
       "mean   0.521292  0.783069  0.801231  0.701864  10.254471  7.999789  6.750518   \n",
       "\n",
       "            mean  \n",
       "index      nRMSE  \n",
       "0        4.02545  \n",
       "1       5.761036  \n",
       "2       7.104027  \n",
       "3       8.064882  \n",
       "4       8.760027  \n",
       "5       9.266466  \n",
       "6       9.641159  \n",
       "7        9.96601  \n",
       "8       10.24347  \n",
       "9      10.516735  \n",
       "mean    8.334926  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/20_10_1_50_1_50_datt_seq2seq_gru_3.csv\n",
      "\n",
      "\n",
      "3th iteration\n",
      "history size: 20\n",
      "future size: 20\n",
      "Epoch 1/10000\n",
      "572/572 - 8s - loss: 0.3340 - val_loss: 0.2429 - 8s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "572/572 - 5s - loss: 0.2301 - val_loss: 0.2294 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "572/572 - 5s - loss: 0.2138 - val_loss: 0.2126 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "572/572 - 5s - loss: 0.2034 - val_loss: 0.2002 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "572/572 - 5s - loss: 0.1923 - val_loss: 0.1958 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "572/572 - 5s - loss: 0.1827 - val_loss: 0.1929 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "572/572 - 5s - loss: 0.1733 - val_loss: 0.1787 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "572/572 - 5s - loss: 0.1670 - val_loss: 0.1718 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "572/572 - 5s - loss: 0.1586 - val_loss: 0.1658 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "572/572 - 5s - loss: 0.1501 - val_loss: 0.1703 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "572/572 - 5s - loss: 0.1462 - val_loss: 0.1508 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "572/572 - 5s - loss: 0.1392 - val_loss: 0.1443 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "572/572 - 5s - loss: 0.1339 - val_loss: 0.1414 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "572/572 - 5s - loss: 0.1285 - val_loss: 0.1356 - 5s/epoch - 9ms/step\n",
      "Epoch 15/10000\n",
      "572/572 - 5s - loss: 0.1241 - val_loss: 0.1370 - 5s/epoch - 9ms/step\n",
      "Epoch 16/10000\n",
      "572/572 - 5s - loss: 0.1190 - val_loss: 0.1361 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "572/572 - 5s - loss: 0.1213 - val_loss: 0.1276 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "572/572 - 5s - loss: 0.1115 - val_loss: 0.1246 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "572/572 - 5s - loss: 0.1090 - val_loss: 0.1266 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "572/572 - 5s - loss: 0.1066 - val_loss: 0.1235 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "572/572 - 5s - loss: 0.1054 - val_loss: 0.1250 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "572/572 - 5s - loss: 0.1080 - val_loss: 0.1172 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "572/572 - 5s - loss: 0.1016 - val_loss: 0.1173 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "572/572 - 5s - loss: 0.1011 - val_loss: 0.1126 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "572/572 - 5s - loss: 0.0971 - val_loss: 0.1147 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "572/572 - 5s - loss: 0.0975 - val_loss: 0.1080 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "572/572 - 5s - loss: 0.0965 - val_loss: 0.1131 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "572/572 - 5s - loss: 0.0945 - val_loss: 0.1091 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "572/572 - 5s - loss: 0.0917 - val_loss: 0.1111 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "572/572 - 5s - loss: 0.0912 - val_loss: 0.1056 - 5s/epoch - 9ms/step\n",
      "Epoch 31/10000\n",
      "572/572 - 5s - loss: 0.0908 - val_loss: 0.1101 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "572/572 - 5s - loss: 0.0908 - val_loss: 0.1047 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "572/572 - 5s - loss: 0.0879 - val_loss: 0.1013 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "572/572 - 5s - loss: 0.0860 - val_loss: 0.1035 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "572/572 - 5s - loss: 0.0860 - val_loss: 0.1023 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "572/572 - 5s - loss: 0.0850 - val_loss: 0.1105 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "572/572 - 5s - loss: 0.0853 - val_loss: 0.1041 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "572/572 - 5s - loss: 0.0825 - val_loss: 0.0981 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "572/572 - 5s - loss: 0.0819 - val_loss: 0.1036 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "572/572 - 5s - loss: 0.0817 - val_loss: 0.0994 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "572/572 - 5s - loss: 0.0811 - val_loss: 0.1011 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "572/572 - 5s - loss: 0.0807 - val_loss: 0.1013 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "572/572 - 5s - loss: 0.0785 - val_loss: 0.0996 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "572/572 - 5s - loss: 0.0782 - val_loss: 0.0968 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "572/572 - 5s - loss: 0.0776 - val_loss: 0.0940 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "572/572 - 5s - loss: 0.0771 - val_loss: 0.0921 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "572/572 - 5s - loss: 0.0759 - val_loss: 0.0933 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "572/572 - 5s - loss: 0.0778 - val_loss: 0.0948 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "572/572 - 5s - loss: 0.0741 - val_loss: 0.1047 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "572/572 - 5s - loss: 0.0744 - val_loss: 0.0875 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "572/572 - 5s - loss: 0.0710 - val_loss: 0.0949 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "572/572 - 5s - loss: 0.0737 - val_loss: 0.0917 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "572/572 - 5s - loss: 0.0719 - val_loss: 0.1207 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "572/572 - 5s - loss: 0.0725 - val_loss: 0.0838 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "572/572 - 5s - loss: 0.0703 - val_loss: 0.0856 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "572/572 - 5s - loss: 0.0681 - val_loss: 0.1030 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "572/572 - 5s - loss: 0.0718 - val_loss: 0.0919 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "572/572 - 5s - loss: 0.0686 - val_loss: 0.0832 - 5s/epoch - 9ms/step\n",
      "Epoch 59/10000\n",
      "572/572 - 5s - loss: 0.0707 - val_loss: 0.0890 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "572/572 - 5s - loss: 0.0664 - val_loss: 0.0816 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "572/572 - 5s - loss: 0.0656 - val_loss: 0.0856 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "572/572 - 5s - loss: 0.0679 - val_loss: 0.0884 - 5s/epoch - 9ms/step\n",
      "Epoch 63/10000\n",
      "572/572 - 5s - loss: 0.0653 - val_loss: 0.0814 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "572/572 - 5s - loss: 0.0671 - val_loss: 0.0879 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "572/572 - 5s - loss: 0.0657 - val_loss: 0.0878 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "572/572 - 5s - loss: 0.0671 - val_loss: 0.0861 - 5s/epoch - 9ms/step\n",
      "Epoch 67/10000\n",
      "572/572 - 5s - loss: 0.0642 - val_loss: 0.0805 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "572/572 - 5s - loss: 0.0649 - val_loss: 0.0899 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "572/572 - 5s - loss: 0.0623 - val_loss: 0.0830 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "572/572 - 5s - loss: 0.0622 - val_loss: 0.0770 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "572/572 - 5s - loss: 0.0604 - val_loss: 0.0806 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "572/572 - 5s - loss: 0.0596 - val_loss: 0.0890 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "572/572 - 5s - loss: 0.0612 - val_loss: 0.0772 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "572/572 - 5s - loss: 0.0582 - val_loss: 0.0820 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "572/572 - 5s - loss: 0.0614 - val_loss: 0.0829 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "572/572 - 5s - loss: 0.0647 - val_loss: 0.0892 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "572/572 - 5s - loss: 0.0590 - val_loss: 0.0757 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "572/572 - 5s - loss: 0.0560 - val_loss: 0.0720 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "572/572 - 5s - loss: 0.0565 - val_loss: 0.0741 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "572/572 - 5s - loss: 0.0569 - val_loss: 0.0769 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "572/572 - 5s - loss: 0.0558 - val_loss: 0.0728 - 5s/epoch - 9ms/step\n",
      "Epoch 82/10000\n",
      "572/572 - 5s - loss: 0.0562 - val_loss: 0.0729 - 5s/epoch - 9ms/step\n",
      "Epoch 83/10000\n",
      "572/572 - 5s - loss: 0.0592 - val_loss: 0.0787 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "572/572 - 5s - loss: 0.0558 - val_loss: 0.0759 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "572/572 - 5s - loss: 0.0601 - val_loss: 0.0704 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "572/572 - 5s - loss: 0.0561 - val_loss: 0.0717 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "572/572 - 5s - loss: 0.0550 - val_loss: 0.0709 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "572/572 - 5s - loss: 0.0526 - val_loss: 0.0684 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "572/572 - 5s - loss: 0.0571 - val_loss: 0.0678 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "572/572 - 5s - loss: 0.0525 - val_loss: 0.0673 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "572/572 - 5s - loss: 0.0548 - val_loss: 0.0718 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "572/572 - 5s - loss: 0.0536 - val_loss: 0.0732 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "572/572 - 5s - loss: 0.0519 - val_loss: 0.0696 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "572/572 - 5s - loss: 0.0530 - val_loss: 0.0718 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "572/572 - 5s - loss: 0.0524 - val_loss: 0.0742 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "572/572 - 5s - loss: 0.0581 - val_loss: 0.0849 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "572/572 - 5s - loss: 0.0575 - val_loss: 0.0727 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "572/572 - 5s - loss: 0.0542 - val_loss: 0.0698 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "572/572 - 5s - loss: 0.0520 - val_loss: 0.0695 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "572/572 - 5s - loss: 0.0508 - val_loss: 0.0727 - 5s/epoch - 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_8_layer_call_fn, gru_cell_8_layer_call_and_return_conditional_losses, gru_cell_9_layer_call_fn, gru_cell_9_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_20_1_50_1_50_datt_seq2seq_gru_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_20_1_50_1_50_datt_seq2seq_gru_3\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002641506E040> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002647920AC40> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.907186</td>\n",
       "      <td>0.912263</td>\n",
       "      <td>0.940481</td>\n",
       "      <td>0.919976</td>\n",
       "      <td>6.019569</td>\n",
       "      <td>5.185964</td>\n",
       "      <td>3.77439</td>\n",
       "      <td>4.993308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.795416</td>\n",
       "      <td>0.861263</td>\n",
       "      <td>0.914488</td>\n",
       "      <td>0.857056</td>\n",
       "      <td>8.774927</td>\n",
       "      <td>6.523361</td>\n",
       "      <td>4.523474</td>\n",
       "      <td>6.607254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.660939</td>\n",
       "      <td>0.816515</td>\n",
       "      <td>0.87818</td>\n",
       "      <td>0.785211</td>\n",
       "      <td>10.803657</td>\n",
       "      <td>7.504799</td>\n",
       "      <td>5.398327</td>\n",
       "      <td>7.902261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.539735</td>\n",
       "      <td>0.777792</td>\n",
       "      <td>0.84776</td>\n",
       "      <td>0.721762</td>\n",
       "      <td>12.166894</td>\n",
       "      <td>8.261735</td>\n",
       "      <td>6.03399</td>\n",
       "      <td>8.820873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.433706</td>\n",
       "      <td>0.743747</td>\n",
       "      <td>0.828189</td>\n",
       "      <td>0.668547</td>\n",
       "      <td>13.279132</td>\n",
       "      <td>8.876065</td>\n",
       "      <td>6.409488</td>\n",
       "      <td>9.521562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.343075</td>\n",
       "      <td>0.71813</td>\n",
       "      <td>0.811074</td>\n",
       "      <td>0.624093</td>\n",
       "      <td>14.252567</td>\n",
       "      <td>9.313794</td>\n",
       "      <td>6.720129</td>\n",
       "      <td>10.095497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.26586</td>\n",
       "      <td>0.697744</td>\n",
       "      <td>0.789662</td>\n",
       "      <td>0.584422</td>\n",
       "      <td>14.769896</td>\n",
       "      <td>9.647496</td>\n",
       "      <td>7.089782</td>\n",
       "      <td>10.502391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.198129</td>\n",
       "      <td>0.678131</td>\n",
       "      <td>0.763503</td>\n",
       "      <td>0.546587</td>\n",
       "      <td>15.138438</td>\n",
       "      <td>9.958749</td>\n",
       "      <td>7.5162</td>\n",
       "      <td>10.871129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.140178</td>\n",
       "      <td>0.658287</td>\n",
       "      <td>0.733118</td>\n",
       "      <td>0.510528</td>\n",
       "      <td>15.440397</td>\n",
       "      <td>10.264547</td>\n",
       "      <td>7.982441</td>\n",
       "      <td>11.229128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.091769</td>\n",
       "      <td>0.639675</td>\n",
       "      <td>0.697832</td>\n",
       "      <td>0.476425</td>\n",
       "      <td>15.634833</td>\n",
       "      <td>10.542406</td>\n",
       "      <td>8.491106</td>\n",
       "      <td>11.556115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.05006</td>\n",
       "      <td>0.622366</td>\n",
       "      <td>0.66139</td>\n",
       "      <td>0.444605</td>\n",
       "      <td>15.818711</td>\n",
       "      <td>10.794472</td>\n",
       "      <td>8.986113</td>\n",
       "      <td>11.866432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.012484</td>\n",
       "      <td>0.606918</td>\n",
       "      <td>0.629714</td>\n",
       "      <td>0.416372</td>\n",
       "      <td>16.021475</td>\n",
       "      <td>11.014376</td>\n",
       "      <td>9.39541</td>\n",
       "      <td>12.143754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.021852</td>\n",
       "      <td>0.59203</td>\n",
       "      <td>0.607687</td>\n",
       "      <td>0.392622</td>\n",
       "      <td>16.071613</td>\n",
       "      <td>11.222071</td>\n",
       "      <td>9.670083</td>\n",
       "      <td>12.321256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.051738</td>\n",
       "      <td>0.57755</td>\n",
       "      <td>0.59229</td>\n",
       "      <td>0.372701</td>\n",
       "      <td>16.085256</td>\n",
       "      <td>11.420983</td>\n",
       "      <td>9.858108</td>\n",
       "      <td>12.454782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.079863</td>\n",
       "      <td>0.562843</td>\n",
       "      <td>0.579868</td>\n",
       "      <td>0.354283</td>\n",
       "      <td>16.137131</td>\n",
       "      <td>11.619612</td>\n",
       "      <td>10.006882</td>\n",
       "      <td>12.587875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.10769</td>\n",
       "      <td>0.548332</td>\n",
       "      <td>0.5695</td>\n",
       "      <td>0.336714</td>\n",
       "      <td>16.185581</td>\n",
       "      <td>11.812691</td>\n",
       "      <td>10.129678</td>\n",
       "      <td>12.709317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.13183</td>\n",
       "      <td>0.531433</td>\n",
       "      <td>0.560042</td>\n",
       "      <td>0.319882</td>\n",
       "      <td>16.206331</td>\n",
       "      <td>12.032496</td>\n",
       "      <td>10.240834</td>\n",
       "      <td>12.826554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.151813</td>\n",
       "      <td>0.512532</td>\n",
       "      <td>0.548806</td>\n",
       "      <td>0.303175</td>\n",
       "      <td>16.31282</td>\n",
       "      <td>12.273102</td>\n",
       "      <td>10.371698</td>\n",
       "      <td>12.985873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.16969</td>\n",
       "      <td>0.492335</td>\n",
       "      <td>0.536832</td>\n",
       "      <td>0.286492</td>\n",
       "      <td>16.462008</td>\n",
       "      <td>12.525137</td>\n",
       "      <td>10.509131</td>\n",
       "      <td>13.165425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.188176</td>\n",
       "      <td>0.472644</td>\n",
       "      <td>0.526335</td>\n",
       "      <td>0.270268</td>\n",
       "      <td>16.615436</td>\n",
       "      <td>12.76581</td>\n",
       "      <td>10.628755</td>\n",
       "      <td>13.336667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.176794</td>\n",
       "      <td>0.651126</td>\n",
       "      <td>0.700838</td>\n",
       "      <td>0.509586</td>\n",
       "      <td>14.409834</td>\n",
       "      <td>10.177983</td>\n",
       "      <td>8.186801</td>\n",
       "      <td>10.924873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.907186  0.912263  0.940481  0.919976   6.019569   5.185964   \n",
       "1      0.795416  0.861263  0.914488  0.857056   8.774927   6.523361   \n",
       "2      0.660939  0.816515   0.87818  0.785211  10.803657   7.504799   \n",
       "3      0.539735  0.777792   0.84776  0.721762  12.166894   8.261735   \n",
       "4      0.433706  0.743747  0.828189  0.668547  13.279132   8.876065   \n",
       "5      0.343075   0.71813  0.811074  0.624093  14.252567   9.313794   \n",
       "6       0.26586  0.697744  0.789662  0.584422  14.769896   9.647496   \n",
       "7      0.198129  0.678131  0.763503  0.546587  15.138438   9.958749   \n",
       "8      0.140178  0.658287  0.733118  0.510528  15.440397  10.264547   \n",
       "9      0.091769  0.639675  0.697832  0.476425  15.634833  10.542406   \n",
       "10      0.05006  0.622366   0.66139  0.444605  15.818711  10.794472   \n",
       "11     0.012484  0.606918  0.629714  0.416372  16.021475  11.014376   \n",
       "12    -0.021852   0.59203  0.607687  0.392622  16.071613  11.222071   \n",
       "13    -0.051738   0.57755   0.59229  0.372701  16.085256  11.420983   \n",
       "14    -0.079863  0.562843  0.579868  0.354283  16.137131  11.619612   \n",
       "15     -0.10769  0.548332    0.5695  0.336714  16.185581  11.812691   \n",
       "16     -0.13183  0.531433  0.560042  0.319882  16.206331  12.032496   \n",
       "17    -0.151813  0.512532  0.548806  0.303175   16.31282  12.273102   \n",
       "18     -0.16969  0.492335  0.536832  0.286492  16.462008  12.525137   \n",
       "19    -0.188176  0.472644  0.526335  0.270268  16.615436   12.76581   \n",
       "mean   0.176794  0.651126  0.700838  0.509586  14.409834  10.177983   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0        3.77439   4.993308  \n",
       "1       4.523474   6.607254  \n",
       "2       5.398327   7.902261  \n",
       "3        6.03399   8.820873  \n",
       "4       6.409488   9.521562  \n",
       "5       6.720129  10.095497  \n",
       "6       7.089782  10.502391  \n",
       "7         7.5162  10.871129  \n",
       "8       7.982441  11.229128  \n",
       "9       8.491106  11.556115  \n",
       "10      8.986113  11.866432  \n",
       "11       9.39541  12.143754  \n",
       "12      9.670083  12.321256  \n",
       "13      9.858108  12.454782  \n",
       "14     10.006882  12.587875  \n",
       "15     10.129678  12.709317  \n",
       "16     10.240834  12.826554  \n",
       "17     10.371698  12.985873  \n",
       "18     10.509131  13.165425  \n",
       "19     10.628755  13.336667  \n",
       "mean    8.186801  10.924873  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/20_20_1_50_1_50_datt_seq2seq_gru_3.csv\n",
      "\n",
      "\n",
      "3th iteration\n",
      "history size: 20\n",
      "future size: 30\n",
      "Epoch 1/10000\n",
      "568/568 - 8s - loss: 0.3881 - val_loss: 0.3229 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "568/568 - 4s - loss: 0.2888 - val_loss: 0.2982 - 4s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "568/568 - 5s - loss: 0.2673 - val_loss: 0.2790 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "568/568 - 5s - loss: 0.2473 - val_loss: 0.2577 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "568/568 - 5s - loss: 0.2310 - val_loss: 0.2479 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "568/568 - 5s - loss: 0.2183 - val_loss: 0.2347 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "568/568 - 5s - loss: 0.2061 - val_loss: 0.2322 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "568/568 - 5s - loss: 0.1983 - val_loss: 0.2206 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "568/568 - 5s - loss: 0.1882 - val_loss: 0.2049 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "568/568 - 5s - loss: 0.1801 - val_loss: 0.2296 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "568/568 - 4s - loss: 0.1816 - val_loss: 0.1952 - 4s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "568/568 - 5s - loss: 0.1673 - val_loss: 0.1862 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "568/568 - 4s - loss: 0.1613 - val_loss: 0.1817 - 4s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "568/568 - 5s - loss: 0.1578 - val_loss: 0.1848 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "568/568 - 5s - loss: 0.1539 - val_loss: 0.1752 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "568/568 - 4s - loss: 0.1526 - val_loss: 0.1853 - 4s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "568/568 - 4s - loss: 0.1493 - val_loss: 0.1711 - 4s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "568/568 - 4s - loss: 0.1438 - val_loss: 0.1716 - 4s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "568/568 - 5s - loss: 0.1409 - val_loss: 0.1692 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "568/568 - 5s - loss: 0.1405 - val_loss: 0.1615 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "568/568 - 5s - loss: 0.1365 - val_loss: 0.1566 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "568/568 - 4s - loss: 0.1336 - val_loss: 0.1519 - 4s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "568/568 - 4s - loss: 0.1307 - val_loss: 0.1558 - 4s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "568/568 - 4s - loss: 0.1299 - val_loss: 0.1500 - 4s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "568/568 - 4s - loss: 0.1264 - val_loss: 0.1601 - 4s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "568/568 - 4s - loss: 0.1258 - val_loss: 0.1444 - 4s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "568/568 - 5s - loss: 0.1227 - val_loss: 0.1492 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "568/568 - 4s - loss: 0.1224 - val_loss: 0.1439 - 4s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "568/568 - 5s - loss: 0.1207 - val_loss: 0.1402 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "568/568 - 4s - loss: 0.1193 - val_loss: 0.1392 - 4s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "568/568 - 5s - loss: 0.1168 - val_loss: 0.1393 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "568/568 - 4s - loss: 0.1143 - val_loss: 0.1362 - 4s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "568/568 - 5s - loss: 0.1140 - val_loss: 0.1424 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "568/568 - 5s - loss: 0.1119 - val_loss: 0.1404 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "568/568 - 4s - loss: 0.1101 - val_loss: 0.1326 - 4s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "568/568 - 4s - loss: 0.1103 - val_loss: 0.1278 - 4s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "568/568 - 5s - loss: 0.1084 - val_loss: 0.1257 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "568/568 - 4s - loss: 0.1060 - val_loss: 0.1304 - 4s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "568/568 - 5s - loss: 0.1049 - val_loss: 0.1212 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "568/568 - 5s - loss: 0.1031 - val_loss: 0.1290 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "568/568 - 4s - loss: 0.1022 - val_loss: 0.1202 - 4s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "568/568 - 4s - loss: 0.1013 - val_loss: 0.1218 - 4s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "568/568 - 5s - loss: 0.1016 - val_loss: 0.1244 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "568/568 - 4s - loss: 0.1015 - val_loss: 0.1165 - 4s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "568/568 - 5s - loss: 0.0966 - val_loss: 0.1143 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "568/568 - 5s - loss: 0.0967 - val_loss: 0.1127 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "568/568 - 4s - loss: 0.0968 - val_loss: 0.1104 - 4s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "568/568 - 5s - loss: 0.0957 - val_loss: 0.1128 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "568/568 - 4s - loss: 0.0931 - val_loss: 0.1178 - 4s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "568/568 - 4s - loss: 0.0947 - val_loss: 0.1183 - 4s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "568/568 - 4s - loss: 0.0926 - val_loss: 0.1111 - 4s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "568/568 - 4s - loss: 0.0919 - val_loss: 0.1179 - 4s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "568/568 - 4s - loss: 0.0924 - val_loss: 0.1090 - 4s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "568/568 - 5s - loss: 0.0896 - val_loss: 0.1055 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "568/568 - 5s - loss: 0.0933 - val_loss: 0.1046 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "568/568 - 5s - loss: 0.0885 - val_loss: 0.1156 - 5s/epoch - 9ms/step\n",
      "Epoch 57/10000\n",
      "568/568 - 5s - loss: 0.0921 - val_loss: 0.1051 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "568/568 - 4s - loss: 0.0877 - val_loss: 0.1103 - 4s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "568/568 - 5s - loss: 0.0895 - val_loss: 0.1065 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "568/568 - 5s - loss: 0.0914 - val_loss: 0.1031 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "568/568 - 5s - loss: 0.0843 - val_loss: 0.0985 - 5s/epoch - 9ms/step\n",
      "Epoch 62/10000\n",
      "568/568 - 5s - loss: 0.0867 - val_loss: 0.1044 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "568/568 - 5s - loss: 0.0839 - val_loss: 0.1028 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "568/568 - 5s - loss: 0.0863 - val_loss: 0.0972 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "568/568 - 4s - loss: 0.0866 - val_loss: 0.1075 - 4s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "568/568 - 4s - loss: 0.0819 - val_loss: 0.0991 - 4s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "568/568 - 5s - loss: 0.0805 - val_loss: 0.1035 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "568/568 - 4s - loss: 0.0820 - val_loss: 0.0947 - 4s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "568/568 - 4s - loss: 0.0810 - val_loss: 0.1003 - 4s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "568/568 - 5s - loss: 0.0827 - val_loss: 0.1048 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "568/568 - 5s - loss: 0.0826 - val_loss: 0.1018 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "568/568 - 5s - loss: 0.0786 - val_loss: 0.0994 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "568/568 - 4s - loss: 0.0773 - val_loss: 0.0895 - 4s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "568/568 - 5s - loss: 0.0845 - val_loss: 0.0940 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "568/568 - 4s - loss: 0.0769 - val_loss: 0.0952 - 4s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "568/568 - 4s - loss: 0.0752 - val_loss: 0.0934 - 4s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "568/568 - 5s - loss: 0.0807 - val_loss: 0.0938 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "568/568 - 5s - loss: 0.0770 - val_loss: 0.0904 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "568/568 - 5s - loss: 0.0769 - val_loss: 0.0934 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "568/568 - 5s - loss: 0.0742 - val_loss: 0.1056 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "568/568 - 4s - loss: 0.0743 - val_loss: 0.0931 - 4s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "568/568 - 4s - loss: 0.0762 - val_loss: 0.0860 - 4s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "568/568 - 5s - loss: 0.0734 - val_loss: 0.0952 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "568/568 - 5s - loss: 0.0775 - val_loss: 0.0938 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "568/568 - 4s - loss: 0.0737 - val_loss: 0.1021 - 4s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "568/568 - 5s - loss: 0.0737 - val_loss: 0.0871 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "568/568 - 4s - loss: 0.0708 - val_loss: 0.0864 - 4s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "568/568 - 4s - loss: 0.0713 - val_loss: 0.0910 - 4s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "568/568 - 5s - loss: 0.0719 - val_loss: 0.0957 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "568/568 - 4s - loss: 0.0724 - val_loss: 0.0877 - 4s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "568/568 - 4s - loss: 0.0736 - val_loss: 0.0898 - 4s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "568/568 - 4s - loss: 0.0689 - val_loss: 0.0887 - 4s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_10_layer_call_fn, gru_cell_10_layer_call_and_return_conditional_losses, gru_cell_11_layer_call_fn, gru_cell_11_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_30_1_50_1_50_datt_seq2seq_gru_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_30_1_50_1_50_datt_seq2seq_gru_3\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002641EEA1F10> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002643D71C880> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.893861</td>\n",
       "      <td>0.89453</td>\n",
       "      <td>0.940431</td>\n",
       "      <td>0.909607</td>\n",
       "      <td>6.432377</td>\n",
       "      <td>5.684579</td>\n",
       "      <td>3.758578</td>\n",
       "      <td>5.291845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.800647</td>\n",
       "      <td>0.84452</td>\n",
       "      <td>0.897503</td>\n",
       "      <td>0.847557</td>\n",
       "      <td>8.815623</td>\n",
       "      <td>6.902397</td>\n",
       "      <td>4.930395</td>\n",
       "      <td>6.882805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.698215</td>\n",
       "      <td>0.807696</td>\n",
       "      <td>0.856815</td>\n",
       "      <td>0.787575</td>\n",
       "      <td>10.847805</td>\n",
       "      <td>7.677072</td>\n",
       "      <td>5.827663</td>\n",
       "      <td>8.117513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.596322</td>\n",
       "      <td>0.778815</td>\n",
       "      <td>0.827679</td>\n",
       "      <td>0.734272</td>\n",
       "      <td>12.547936</td>\n",
       "      <td>8.234432</td>\n",
       "      <td>6.393291</td>\n",
       "      <td>9.058553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.499703</td>\n",
       "      <td>0.757558</td>\n",
       "      <td>0.805292</td>\n",
       "      <td>0.687517</td>\n",
       "      <td>13.968681</td>\n",
       "      <td>8.62268</td>\n",
       "      <td>6.795858</td>\n",
       "      <td>9.79574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.409765</td>\n",
       "      <td>0.73671</td>\n",
       "      <td>0.784542</td>\n",
       "      <td>0.643672</td>\n",
       "      <td>15.16989</td>\n",
       "      <td>8.988416</td>\n",
       "      <td>7.148088</td>\n",
       "      <td>10.435465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.327862</td>\n",
       "      <td>0.71585</td>\n",
       "      <td>0.762406</td>\n",
       "      <td>0.602039</td>\n",
       "      <td>16.185361</td>\n",
       "      <td>9.340406</td>\n",
       "      <td>7.505278</td>\n",
       "      <td>11.010348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.253789</td>\n",
       "      <td>0.6957</td>\n",
       "      <td>0.737126</td>\n",
       "      <td>0.562205</td>\n",
       "      <td>17.050236</td>\n",
       "      <td>9.668822</td>\n",
       "      <td>7.892973</td>\n",
       "      <td>11.537344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.188993</td>\n",
       "      <td>0.67597</td>\n",
       "      <td>0.708077</td>\n",
       "      <td>0.524347</td>\n",
       "      <td>17.769993</td>\n",
       "      <td>9.981331</td>\n",
       "      <td>8.315694</td>\n",
       "      <td>12.022339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.132432</td>\n",
       "      <td>0.656212</td>\n",
       "      <td>0.675891</td>\n",
       "      <td>0.488178</td>\n",
       "      <td>18.375082</td>\n",
       "      <td>10.28461</td>\n",
       "      <td>8.760971</td>\n",
       "      <td>12.473555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.083841</td>\n",
       "      <td>0.636658</td>\n",
       "      <td>0.641764</td>\n",
       "      <td>0.454087</td>\n",
       "      <td>18.880221</td>\n",
       "      <td>10.575945</td>\n",
       "      <td>9.210248</td>\n",
       "      <td>12.888805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.042477</td>\n",
       "      <td>0.618706</td>\n",
       "      <td>0.60858</td>\n",
       "      <td>0.423254</td>\n",
       "      <td>18.945775</td>\n",
       "      <td>10.837356</td>\n",
       "      <td>9.627991</td>\n",
       "      <td>13.137041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00718</td>\n",
       "      <td>0.601238</td>\n",
       "      <td>0.577576</td>\n",
       "      <td>0.395331</td>\n",
       "      <td>18.444431</td>\n",
       "      <td>11.086882</td>\n",
       "      <td>10.003126</td>\n",
       "      <td>13.178146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.024146</td>\n",
       "      <td>0.583637</td>\n",
       "      <td>0.548138</td>\n",
       "      <td>0.36921</td>\n",
       "      <td>18.10381</td>\n",
       "      <td>11.332705</td>\n",
       "      <td>10.346808</td>\n",
       "      <td>13.261107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.054235</td>\n",
       "      <td>0.566192</td>\n",
       "      <td>0.51941</td>\n",
       "      <td>0.343789</td>\n",
       "      <td>18.07288</td>\n",
       "      <td>11.572782</td>\n",
       "      <td>10.671327</td>\n",
       "      <td>13.438996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.083529</td>\n",
       "      <td>0.548903</td>\n",
       "      <td>0.491373</td>\n",
       "      <td>0.318915</td>\n",
       "      <td>18.261968</td>\n",
       "      <td>11.806845</td>\n",
       "      <td>10.978005</td>\n",
       "      <td>13.682272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.113121</td>\n",
       "      <td>0.531759</td>\n",
       "      <td>0.463902</td>\n",
       "      <td>0.29418</td>\n",
       "      <td>18.14959</td>\n",
       "      <td>12.032621</td>\n",
       "      <td>11.270655</td>\n",
       "      <td>13.817622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.142588</td>\n",
       "      <td>0.514699</td>\n",
       "      <td>0.436909</td>\n",
       "      <td>0.269673</td>\n",
       "      <td>18.040452</td>\n",
       "      <td>12.253592</td>\n",
       "      <td>11.551192</td>\n",
       "      <td>13.948412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.169064</td>\n",
       "      <td>0.497686</td>\n",
       "      <td>0.409654</td>\n",
       "      <td>0.246092</td>\n",
       "      <td>17.982561</td>\n",
       "      <td>12.470307</td>\n",
       "      <td>11.828123</td>\n",
       "      <td>14.093664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.192615</td>\n",
       "      <td>0.479917</td>\n",
       "      <td>0.381641</td>\n",
       "      <td>0.222981</td>\n",
       "      <td>17.903848</td>\n",
       "      <td>12.69117</td>\n",
       "      <td>12.106221</td>\n",
       "      <td>14.233746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.213959</td>\n",
       "      <td>0.46295</td>\n",
       "      <td>0.354653</td>\n",
       "      <td>0.201215</td>\n",
       "      <td>17.880072</td>\n",
       "      <td>12.898549</td>\n",
       "      <td>12.368602</td>\n",
       "      <td>14.382408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.234033</td>\n",
       "      <td>0.446517</td>\n",
       "      <td>0.330362</td>\n",
       "      <td>0.180949</td>\n",
       "      <td>17.916203</td>\n",
       "      <td>13.095771</td>\n",
       "      <td>12.60083</td>\n",
       "      <td>14.537602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.253442</td>\n",
       "      <td>0.429167</td>\n",
       "      <td>0.30822</td>\n",
       "      <td>0.161315</td>\n",
       "      <td>17.812716</td>\n",
       "      <td>13.300578</td>\n",
       "      <td>12.809279</td>\n",
       "      <td>14.640858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.272006</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.288846</td>\n",
       "      <td>0.142614</td>\n",
       "      <td>17.706159</td>\n",
       "      <td>13.512497</td>\n",
       "      <td>12.989018</td>\n",
       "      <td>14.735891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.290786</td>\n",
       "      <td>0.392709</td>\n",
       "      <td>0.271855</td>\n",
       "      <td>0.124593</td>\n",
       "      <td>17.660255</td>\n",
       "      <td>13.722501</td>\n",
       "      <td>13.144162</td>\n",
       "      <td>14.842306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.309763</td>\n",
       "      <td>0.375008</td>\n",
       "      <td>0.257141</td>\n",
       "      <td>0.107462</td>\n",
       "      <td>17.615921</td>\n",
       "      <td>13.922485</td>\n",
       "      <td>13.277743</td>\n",
       "      <td>14.938716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.327495</td>\n",
       "      <td>0.358235</td>\n",
       "      <td>0.243516</td>\n",
       "      <td>0.091419</td>\n",
       "      <td>17.563984</td>\n",
       "      <td>14.107915</td>\n",
       "      <td>13.401319</td>\n",
       "      <td>15.024406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.34311</td>\n",
       "      <td>0.343427</td>\n",
       "      <td>0.229517</td>\n",
       "      <td>0.076611</td>\n",
       "      <td>17.624204</td>\n",
       "      <td>14.268678</td>\n",
       "      <td>13.528651</td>\n",
       "      <td>15.140511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.356993</td>\n",
       "      <td>0.32893</td>\n",
       "      <td>0.215406</td>\n",
       "      <td>0.062448</td>\n",
       "      <td>17.735772</td>\n",
       "      <td>14.423704</td>\n",
       "      <td>13.656311</td>\n",
       "      <td>15.271929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.366649</td>\n",
       "      <td>0.314906</td>\n",
       "      <td>0.19994</td>\n",
       "      <td>0.049399</td>\n",
       "      <td>17.819229</td>\n",
       "      <td>14.571187</td>\n",
       "      <td>13.795057</td>\n",
       "      <td>15.395158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.039585</td>\n",
       "      <td>0.56686</td>\n",
       "      <td>0.525805</td>\n",
       "      <td>0.377417</td>\n",
       "      <td>16.576101</td>\n",
       "      <td>11.328961</td>\n",
       "      <td>10.216449</td>\n",
       "      <td>12.70717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.893861   0.89453  0.940431  0.909607   6.432377   5.684579   \n",
       "1      0.800647   0.84452  0.897503  0.847557   8.815623   6.902397   \n",
       "2      0.698215  0.807696  0.856815  0.787575  10.847805   7.677072   \n",
       "3      0.596322  0.778815  0.827679  0.734272  12.547936   8.234432   \n",
       "4      0.499703  0.757558  0.805292  0.687517  13.968681    8.62268   \n",
       "5      0.409765   0.73671  0.784542  0.643672   15.16989   8.988416   \n",
       "6      0.327862   0.71585  0.762406  0.602039  16.185361   9.340406   \n",
       "7      0.253789    0.6957  0.737126  0.562205  17.050236   9.668822   \n",
       "8      0.188993   0.67597  0.708077  0.524347  17.769993   9.981331   \n",
       "9      0.132432  0.656212  0.675891  0.488178  18.375082   10.28461   \n",
       "10     0.083841  0.636658  0.641764  0.454087  18.880221  10.575945   \n",
       "11     0.042477  0.618706   0.60858  0.423254  18.945775  10.837356   \n",
       "12      0.00718  0.601238  0.577576  0.395331  18.444431  11.086882   \n",
       "13    -0.024146  0.583637  0.548138   0.36921   18.10381  11.332705   \n",
       "14    -0.054235  0.566192   0.51941  0.343789   18.07288  11.572782   \n",
       "15    -0.083529  0.548903  0.491373  0.318915  18.261968  11.806845   \n",
       "16    -0.113121  0.531759  0.463902   0.29418   18.14959  12.032621   \n",
       "17    -0.142588  0.514699  0.436909  0.269673  18.040452  12.253592   \n",
       "18    -0.169064  0.497686  0.409654  0.246092  17.982561  12.470307   \n",
       "19    -0.192615  0.479917  0.381641  0.222981  17.903848   12.69117   \n",
       "20    -0.213959   0.46295  0.354653  0.201215  17.880072  12.898549   \n",
       "21    -0.234033  0.446517  0.330362  0.180949  17.916203  13.095771   \n",
       "22    -0.253442  0.429167   0.30822  0.161315  17.812716  13.300578   \n",
       "23    -0.272006     0.411  0.288846  0.142614  17.706159  13.512497   \n",
       "24    -0.290786  0.392709  0.271855  0.124593  17.660255  13.722501   \n",
       "25    -0.309763  0.375008  0.257141  0.107462  17.615921  13.922485   \n",
       "26    -0.327495  0.358235  0.243516  0.091419  17.563984  14.107915   \n",
       "27     -0.34311  0.343427  0.229517  0.076611  17.624204  14.268678   \n",
       "28    -0.356993   0.32893  0.215406  0.062448  17.735772  14.423704   \n",
       "29    -0.366649  0.314906   0.19994  0.049399  17.819229  14.571187   \n",
       "mean   0.039585   0.56686  0.525805  0.377417  16.576101  11.328961   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       3.758578   5.291845  \n",
       "1       4.930395   6.882805  \n",
       "2       5.827663   8.117513  \n",
       "3       6.393291   9.058553  \n",
       "4       6.795858    9.79574  \n",
       "5       7.148088  10.435465  \n",
       "6       7.505278  11.010348  \n",
       "7       7.892973  11.537344  \n",
       "8       8.315694  12.022339  \n",
       "9       8.760971  12.473555  \n",
       "10      9.210248  12.888805  \n",
       "11      9.627991  13.137041  \n",
       "12     10.003126  13.178146  \n",
       "13     10.346808  13.261107  \n",
       "14     10.671327  13.438996  \n",
       "15     10.978005  13.682272  \n",
       "16     11.270655  13.817622  \n",
       "17     11.551192  13.948412  \n",
       "18     11.828123  14.093664  \n",
       "19     12.106221  14.233746  \n",
       "20     12.368602  14.382408  \n",
       "21      12.60083  14.537602  \n",
       "22     12.809279  14.640858  \n",
       "23     12.989018  14.735891  \n",
       "24     13.144162  14.842306  \n",
       "25     13.277743  14.938716  \n",
       "26     13.401319  15.024406  \n",
       "27     13.528651  15.140511  \n",
       "28     13.656311  15.271929  \n",
       "29     13.795057  15.395158  \n",
       "mean   10.216449   12.70717  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/20_30_1_50_1_50_datt_seq2seq_gru_3.csv\n",
      "\n",
      "\n",
      "3th iteration\n",
      "history size: 30\n",
      "future size: 10\n",
      "Epoch 1/10000\n",
      "572/572 - 8s - loss: 0.2247 - val_loss: 0.1504 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "572/572 - 5s - loss: 0.1491 - val_loss: 0.1367 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "572/572 - 5s - loss: 0.1367 - val_loss: 0.1293 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "572/572 - 5s - loss: 0.1264 - val_loss: 0.1181 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "572/572 - 5s - loss: 0.1171 - val_loss: 0.1102 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "572/572 - 5s - loss: 0.1087 - val_loss: 0.1056 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "572/572 - 5s - loss: 0.1036 - val_loss: 0.0999 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "572/572 - 5s - loss: 0.0981 - val_loss: 0.0967 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "572/572 - 5s - loss: 0.0942 - val_loss: 0.0919 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "572/572 - 5s - loss: 0.0912 - val_loss: 0.0899 - 5s/epoch - 9ms/step\n",
      "Epoch 11/10000\n",
      "572/572 - 5s - loss: 0.0891 - val_loss: 0.0874 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "572/572 - 5s - loss: 0.0857 - val_loss: 0.0947 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "572/572 - 5s - loss: 0.0831 - val_loss: 0.0886 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "572/572 - 5s - loss: 0.0797 - val_loss: 0.0832 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "572/572 - 5s - loss: 0.0805 - val_loss: 0.0805 - 5s/epoch - 9ms/step\n",
      "Epoch 16/10000\n",
      "572/572 - 5s - loss: 0.0758 - val_loss: 0.0808 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "572/572 - 5s - loss: 0.0732 - val_loss: 0.0776 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "572/572 - 5s - loss: 0.0707 - val_loss: 0.0833 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "572/572 - 5s - loss: 0.0701 - val_loss: 0.0790 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "572/572 - 5s - loss: 0.0694 - val_loss: 0.0758 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "572/572 - 5s - loss: 0.0677 - val_loss: 0.0730 - 5s/epoch - 9ms/step\n",
      "Epoch 22/10000\n",
      "572/572 - 5s - loss: 0.0648 - val_loss: 0.0735 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "572/572 - 5s - loss: 0.0640 - val_loss: 0.0720 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "572/572 - 5s - loss: 0.0638 - val_loss: 0.0750 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "572/572 - 5s - loss: 0.0616 - val_loss: 0.0682 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "572/572 - 5s - loss: 0.0606 - val_loss: 0.0696 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "572/572 - 5s - loss: 0.0603 - val_loss: 0.0730 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "572/572 - 5s - loss: 0.0586 - val_loss: 0.0668 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "572/572 - 5s - loss: 0.0578 - val_loss: 0.0685 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "572/572 - 5s - loss: 0.0569 - val_loss: 0.0640 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "572/572 - 5s - loss: 0.0556 - val_loss: 0.0684 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "572/572 - 5s - loss: 0.0568 - val_loss: 0.0672 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "572/572 - 5s - loss: 0.0561 - val_loss: 0.0662 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "572/572 - 5s - loss: 0.0541 - val_loss: 0.0644 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "572/572 - 5s - loss: 0.0528 - val_loss: 0.0631 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "572/572 - 5s - loss: 0.0529 - val_loss: 0.0650 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "572/572 - 5s - loss: 0.0523 - val_loss: 0.0604 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "572/572 - 5s - loss: 0.0508 - val_loss: 0.0613 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "572/572 - 5s - loss: 0.0514 - val_loss: 0.0595 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "572/572 - 5s - loss: 0.0494 - val_loss: 0.0621 - 5s/epoch - 9ms/step\n",
      "Epoch 41/10000\n",
      "572/572 - 5s - loss: 0.0491 - val_loss: 0.0605 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "572/572 - 5s - loss: 0.0499 - val_loss: 0.0589 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "572/572 - 5s - loss: 0.0494 - val_loss: 0.0602 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "572/572 - 5s - loss: 0.0494 - val_loss: 0.0593 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "572/572 - 5s - loss: 0.0469 - val_loss: 0.0576 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "572/572 - 5s - loss: 0.0478 - val_loss: 0.0591 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "572/572 - 5s - loss: 0.0464 - val_loss: 0.0559 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "572/572 - 5s - loss: 0.0454 - val_loss: 0.0571 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "572/572 - 5s - loss: 0.0476 - val_loss: 0.0590 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "572/572 - 5s - loss: 0.0456 - val_loss: 0.0550 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "572/572 - 5s - loss: 0.0445 - val_loss: 0.0553 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "572/572 - 5s - loss: 0.0456 - val_loss: 0.0550 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "572/572 - 5s - loss: 0.0447 - val_loss: 0.0528 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "572/572 - 5s - loss: 0.0432 - val_loss: 0.0565 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "572/572 - 5s - loss: 0.0443 - val_loss: 0.0591 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "572/572 - 5s - loss: 0.0438 - val_loss: 0.0546 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "572/572 - 5s - loss: 0.0427 - val_loss: 0.0520 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "572/572 - 5s - loss: 0.0456 - val_loss: 0.0536 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "572/572 - 5s - loss: 0.0412 - val_loss: 0.0522 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "572/572 - 5s - loss: 0.0436 - val_loss: 0.0551 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "572/572 - 5s - loss: 0.0440 - val_loss: 0.0519 - 5s/epoch - 9ms/step\n",
      "Epoch 62/10000\n",
      "572/572 - 5s - loss: 0.0406 - val_loss: 0.0504 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "572/572 - 5s - loss: 0.0456 - val_loss: 0.0563 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "572/572 - 5s - loss: 0.0416 - val_loss: 0.0501 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "572/572 - 5s - loss: 0.0407 - val_loss: 0.0506 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "572/572 - 5s - loss: 0.0400 - val_loss: 0.0512 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "572/572 - 5s - loss: 0.0400 - val_loss: 0.0533 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "572/572 - 5s - loss: 0.0400 - val_loss: 0.0529 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "572/572 - 5s - loss: 0.0405 - val_loss: 0.0516 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "572/572 - 5s - loss: 0.0397 - val_loss: 0.0526 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "572/572 - 5s - loss: 0.0396 - val_loss: 0.0541 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "572/572 - 5s - loss: 0.0395 - val_loss: 0.0516 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "572/572 - 5s - loss: 0.0386 - val_loss: 0.0500 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "572/572 - 5s - loss: 0.0387 - val_loss: 0.0509 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "572/572 - 5s - loss: 0.0397 - val_loss: 0.0560 - 5s/epoch - 9ms/step\n",
      "Epoch 76/10000\n",
      "572/572 - 5s - loss: 0.0392 - val_loss: 0.0537 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "572/572 - 5s - loss: 0.0386 - val_loss: 0.0485 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "572/572 - 5s - loss: 0.0369 - val_loss: 0.0488 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "572/572 - 5s - loss: 0.0373 - val_loss: 0.0506 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "572/572 - 5s - loss: 0.0379 - val_loss: 0.0487 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "572/572 - 5s - loss: 0.0383 - val_loss: 0.0482 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "572/572 - 5s - loss: 0.0377 - val_loss: 0.0481 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "572/572 - 5s - loss: 0.0371 - val_loss: 0.0473 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "572/572 - 5s - loss: 0.0360 - val_loss: 0.0498 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "572/572 - 5s - loss: 0.0362 - val_loss: 0.0503 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "572/572 - 5s - loss: 0.0363 - val_loss: 0.0468 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "572/572 - 5s - loss: 0.0355 - val_loss: 0.0477 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "572/572 - 5s - loss: 0.0370 - val_loss: 0.0512 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "572/572 - 5s - loss: 0.0361 - val_loss: 0.0466 - 5s/epoch - 9ms/step\n",
      "Epoch 90/10000\n",
      "572/572 - 5s - loss: 0.0354 - val_loss: 0.0482 - 5s/epoch - 9ms/step\n",
      "Epoch 91/10000\n",
      "572/572 - 5s - loss: 0.0357 - val_loss: 0.0518 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "572/572 - 5s - loss: 0.0360 - val_loss: 0.0458 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "572/572 - 5s - loss: 0.0366 - val_loss: 0.0512 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "572/572 - 5s - loss: 0.0362 - val_loss: 0.0460 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "572/572 - 5s - loss: 0.0337 - val_loss: 0.0456 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "572/572 - 5s - loss: 0.0340 - val_loss: 0.0494 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "572/572 - 5s - loss: 0.0352 - val_loss: 0.0481 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "572/572 - 5s - loss: 0.0356 - val_loss: 0.0480 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "572/572 - 5s - loss: 0.0362 - val_loss: 0.0451 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "572/572 - 5s - loss: 0.0328 - val_loss: 0.0455 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "572/572 - 5s - loss: 0.0335 - val_loss: 0.0462 - 5s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "572/572 - 5s - loss: 0.0327 - val_loss: 0.0454 - 5s/epoch - 9ms/step\n",
      "Epoch 103/10000\n",
      "572/572 - 5s - loss: 0.0334 - val_loss: 0.0475 - 5s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "572/572 - 5s - loss: 0.0336 - val_loss: 0.0487 - 5s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "572/572 - 5s - loss: 0.0343 - val_loss: 0.0450 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "572/572 - 5s - loss: 0.0324 - val_loss: 0.0453 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "572/572 - 5s - loss: 0.0359 - val_loss: 0.0446 - 5s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "572/572 - 5s - loss: 0.0321 - val_loss: 0.0448 - 5s/epoch - 9ms/step\n",
      "Epoch 109/10000\n",
      "572/572 - 5s - loss: 0.0319 - val_loss: 0.0441 - 5s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "572/572 - 5s - loss: 0.0314 - val_loss: 0.0440 - 5s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "572/572 - 5s - loss: 0.0320 - val_loss: 0.0458 - 5s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "572/572 - 5s - loss: 0.0330 - val_loss: 0.0433 - 5s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "572/572 - 5s - loss: 0.0311 - val_loss: 0.0438 - 5s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "572/572 - 5s - loss: 0.0315 - val_loss: 0.0428 - 5s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "572/572 - 5s - loss: 0.0311 - val_loss: 0.0430 - 5s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "572/572 - 5s - loss: 0.0326 - val_loss: 0.0445 - 5s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "572/572 - 5s - loss: 0.0319 - val_loss: 0.0452 - 5s/epoch - 8ms/step\n",
      "Epoch 118/10000\n",
      "572/572 - 5s - loss: 0.0318 - val_loss: 0.0439 - 5s/epoch - 8ms/step\n",
      "Epoch 119/10000\n",
      "572/572 - 5s - loss: 0.0310 - val_loss: 0.0445 - 5s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "572/572 - 5s - loss: 0.0302 - val_loss: 0.0436 - 5s/epoch - 8ms/step\n",
      "Epoch 121/10000\n",
      "572/572 - 5s - loss: 0.0303 - val_loss: 0.0444 - 5s/epoch - 8ms/step\n",
      "Epoch 122/10000\n",
      "572/572 - 5s - loss: 0.0311 - val_loss: 0.0506 - 5s/epoch - 8ms/step\n",
      "Epoch 123/10000\n",
      "572/572 - 5s - loss: 0.0319 - val_loss: 0.0449 - 5s/epoch - 8ms/step\n",
      "Epoch 124/10000\n",
      "572/572 - 5s - loss: 0.0309 - val_loss: 0.0422 - 5s/epoch - 9ms/step\n",
      "Epoch 125/10000\n",
      "572/572 - 5s - loss: 0.0297 - val_loss: 0.0410 - 5s/epoch - 8ms/step\n",
      "Epoch 126/10000\n",
      "572/572 - 5s - loss: 0.0305 - val_loss: 0.0491 - 5s/epoch - 8ms/step\n",
      "Epoch 127/10000\n",
      "572/572 - 5s - loss: 0.0311 - val_loss: 0.0429 - 5s/epoch - 9ms/step\n",
      "Epoch 128/10000\n",
      "572/572 - 5s - loss: 0.0294 - val_loss: 0.0427 - 5s/epoch - 8ms/step\n",
      "Epoch 129/10000\n",
      "572/572 - 5s - loss: 0.0300 - val_loss: 0.0416 - 5s/epoch - 8ms/step\n",
      "Epoch 130/10000\n",
      "572/572 - 5s - loss: 0.0296 - val_loss: 0.0425 - 5s/epoch - 8ms/step\n",
      "Epoch 131/10000\n",
      "572/572 - 5s - loss: 0.0296 - val_loss: 0.0409 - 5s/epoch - 8ms/step\n",
      "Epoch 132/10000\n",
      "572/572 - 5s - loss: 0.0292 - val_loss: 0.0415 - 5s/epoch - 8ms/step\n",
      "Epoch 133/10000\n",
      "572/572 - 5s - loss: 0.0279 - val_loss: 0.0418 - 5s/epoch - 8ms/step\n",
      "Epoch 134/10000\n",
      "572/572 - 5s - loss: 0.0289 - val_loss: 0.0410 - 5s/epoch - 8ms/step\n",
      "Epoch 135/10000\n",
      "572/572 - 5s - loss: 0.0289 - val_loss: 0.0403 - 5s/epoch - 9ms/step\n",
      "Epoch 136/10000\n",
      "572/572 - 5s - loss: 0.0296 - val_loss: 0.0407 - 5s/epoch - 8ms/step\n",
      "Epoch 137/10000\n",
      "572/572 - 5s - loss: 0.0294 - val_loss: 0.0404 - 5s/epoch - 9ms/step\n",
      "Epoch 138/10000\n",
      "572/572 - 5s - loss: 0.0284 - val_loss: 0.0412 - 5s/epoch - 8ms/step\n",
      "Epoch 139/10000\n",
      "572/572 - 5s - loss: 0.0282 - val_loss: 0.0392 - 5s/epoch - 8ms/step\n",
      "Epoch 140/10000\n",
      "572/572 - 5s - loss: 0.0274 - val_loss: 0.0389 - 5s/epoch - 9ms/step\n",
      "Epoch 141/10000\n",
      "572/572 - 5s - loss: 0.0315 - val_loss: 0.0435 - 5s/epoch - 8ms/step\n",
      "Epoch 142/10000\n",
      "572/572 - 5s - loss: 0.0309 - val_loss: 0.0416 - 5s/epoch - 8ms/step\n",
      "Epoch 143/10000\n",
      "572/572 - 5s - loss: 0.0287 - val_loss: 0.0407 - 5s/epoch - 8ms/step\n",
      "Epoch 144/10000\n",
      "572/572 - 5s - loss: 0.0270 - val_loss: 0.0426 - 5s/epoch - 8ms/step\n",
      "Epoch 145/10000\n",
      "572/572 - 5s - loss: 0.0282 - val_loss: 0.0416 - 5s/epoch - 8ms/step\n",
      "Epoch 146/10000\n",
      "572/572 - 5s - loss: 0.0281 - val_loss: 0.0404 - 5s/epoch - 8ms/step\n",
      "Epoch 147/10000\n",
      "572/572 - 5s - loss: 0.0307 - val_loss: 0.0436 - 5s/epoch - 8ms/step\n",
      "Epoch 148/10000\n",
      "572/572 - 5s - loss: 0.0276 - val_loss: 0.0391 - 5s/epoch - 9ms/step\n",
      "Epoch 149/10000\n",
      "572/572 - 5s - loss: 0.0270 - val_loss: 0.0387 - 5s/epoch - 8ms/step\n",
      "Epoch 150/10000\n",
      "572/572 - 5s - loss: 0.0273 - val_loss: 0.0401 - 5s/epoch - 8ms/step\n",
      "Epoch 151/10000\n",
      "572/572 - 5s - loss: 0.0276 - val_loss: 0.0456 - 5s/epoch - 9ms/step\n",
      "Epoch 152/10000\n",
      "572/572 - 5s - loss: 0.0279 - val_loss: 0.0377 - 5s/epoch - 8ms/step\n",
      "Epoch 153/10000\n",
      "572/572 - 5s - loss: 0.0272 - val_loss: 0.0393 - 5s/epoch - 8ms/step\n",
      "Epoch 154/10000\n",
      "572/572 - 5s - loss: 0.0267 - val_loss: 0.0392 - 5s/epoch - 8ms/step\n",
      "Epoch 155/10000\n",
      "572/572 - 5s - loss: 0.0267 - val_loss: 0.0384 - 5s/epoch - 8ms/step\n",
      "Epoch 156/10000\n",
      "572/572 - 5s - loss: 0.0267 - val_loss: 0.0381 - 5s/epoch - 8ms/step\n",
      "Epoch 157/10000\n",
      "572/572 - 5s - loss: 0.0264 - val_loss: 0.0386 - 5s/epoch - 8ms/step\n",
      "Epoch 158/10000\n",
      "572/572 - 5s - loss: 0.0268 - val_loss: 0.0424 - 5s/epoch - 8ms/step\n",
      "Epoch 159/10000\n",
      "572/572 - 5s - loss: 0.0281 - val_loss: 0.0376 - 5s/epoch - 8ms/step\n",
      "Epoch 160/10000\n",
      "572/572 - 5s - loss: 0.0261 - val_loss: 0.0395 - 5s/epoch - 8ms/step\n",
      "Epoch 161/10000\n",
      "572/572 - 5s - loss: 0.0263 - val_loss: 0.0382 - 5s/epoch - 8ms/step\n",
      "Epoch 162/10000\n",
      "572/572 - 5s - loss: 0.0266 - val_loss: 0.0395 - 5s/epoch - 8ms/step\n",
      "Epoch 163/10000\n",
      "572/572 - 5s - loss: 0.0261 - val_loss: 0.0371 - 5s/epoch - 8ms/step\n",
      "Epoch 164/10000\n",
      "572/572 - 5s - loss: 0.0254 - val_loss: 0.0378 - 5s/epoch - 8ms/step\n",
      "Epoch 165/10000\n",
      "572/572 - 5s - loss: 0.0266 - val_loss: 0.0372 - 5s/epoch - 8ms/step\n",
      "Epoch 166/10000\n",
      "572/572 - 5s - loss: 0.0250 - val_loss: 0.0368 - 5s/epoch - 8ms/step\n",
      "Epoch 167/10000\n",
      "572/572 - 5s - loss: 0.0257 - val_loss: 0.0376 - 5s/epoch - 8ms/step\n",
      "Epoch 168/10000\n",
      "572/572 - 5s - loss: 0.0263 - val_loss: 0.0419 - 5s/epoch - 8ms/step\n",
      "Epoch 169/10000\n",
      "572/572 - 5s - loss: 0.0262 - val_loss: 0.0386 - 5s/epoch - 8ms/step\n",
      "Epoch 170/10000\n",
      "572/572 - 5s - loss: 0.0263 - val_loss: 0.0446 - 5s/epoch - 8ms/step\n",
      "Epoch 171/10000\n",
      "572/572 - 5s - loss: 0.0270 - val_loss: 0.0359 - 5s/epoch - 8ms/step\n",
      "Epoch 172/10000\n",
      "572/572 - 5s - loss: 0.0249 - val_loss: 0.0363 - 5s/epoch - 8ms/step\n",
      "Epoch 173/10000\n",
      "572/572 - 5s - loss: 0.0248 - val_loss: 0.0360 - 5s/epoch - 8ms/step\n",
      "Epoch 174/10000\n",
      "572/572 - 5s - loss: 0.0255 - val_loss: 0.0418 - 5s/epoch - 9ms/step\n",
      "Epoch 175/10000\n",
      "572/572 - 5s - loss: 0.0266 - val_loss: 0.0457 - 5s/epoch - 8ms/step\n",
      "Epoch 176/10000\n",
      "572/572 - 5s - loss: 0.0277 - val_loss: 0.0432 - 5s/epoch - 8ms/step\n",
      "Epoch 177/10000\n",
      "572/572 - 5s - loss: 0.0260 - val_loss: 0.0385 - 5s/epoch - 8ms/step\n",
      "Epoch 178/10000\n",
      "572/572 - 5s - loss: 0.0255 - val_loss: 0.0377 - 5s/epoch - 8ms/step\n",
      "Epoch 179/10000\n",
      "572/572 - 5s - loss: 0.0246 - val_loss: 0.0382 - 5s/epoch - 8ms/step\n",
      "Epoch 180/10000\n",
      "572/572 - 5s - loss: 0.0253 - val_loss: 0.0367 - 5s/epoch - 8ms/step\n",
      "Epoch 181/10000\n",
      "572/572 - 5s - loss: 0.0250 - val_loss: 0.0362 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_12_layer_call_fn, gru_cell_12_layer_call_and_return_conditional_losses, gru_cell_13_layer_call_fn, gru_cell_13_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_10_1_50_1_50_datt_seq2seq_gru_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_10_1_50_1_50_datt_seq2seq_gru_3\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002641EC2A9D0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002646EC848E0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.912792</td>\n",
       "      <td>0.926941</td>\n",
       "      <td>0.953787</td>\n",
       "      <td>0.931173</td>\n",
       "      <td>4.792945</td>\n",
       "      <td>4.747927</td>\n",
       "      <td>3.319732</td>\n",
       "      <td>4.286868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.838698</td>\n",
       "      <td>0.856978</td>\n",
       "      <td>0.909511</td>\n",
       "      <td>0.868396</td>\n",
       "      <td>6.475166</td>\n",
       "      <td>6.643844</td>\n",
       "      <td>4.644555</td>\n",
       "      <td>5.921188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.726014</td>\n",
       "      <td>0.805581</td>\n",
       "      <td>0.858783</td>\n",
       "      <td>0.796793</td>\n",
       "      <td>8.32203</td>\n",
       "      <td>7.746906</td>\n",
       "      <td>5.801736</td>\n",
       "      <td>7.290224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.605282</td>\n",
       "      <td>0.76862</td>\n",
       "      <td>0.829261</td>\n",
       "      <td>0.734388</td>\n",
       "      <td>9.854118</td>\n",
       "      <td>8.452368</td>\n",
       "      <td>6.37947</td>\n",
       "      <td>8.228652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.491783</td>\n",
       "      <td>0.745054</td>\n",
       "      <td>0.820085</td>\n",
       "      <td>0.68564</td>\n",
       "      <td>11.070489</td>\n",
       "      <td>8.873541</td>\n",
       "      <td>6.54847</td>\n",
       "      <td>8.830833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.390888</td>\n",
       "      <td>0.730697</td>\n",
       "      <td>0.818639</td>\n",
       "      <td>0.646742</td>\n",
       "      <td>12.002389</td>\n",
       "      <td>9.121363</td>\n",
       "      <td>6.57477</td>\n",
       "      <td>9.232841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.303457</td>\n",
       "      <td>0.718479</td>\n",
       "      <td>0.816377</td>\n",
       "      <td>0.612771</td>\n",
       "      <td>12.713581</td>\n",
       "      <td>9.326643</td>\n",
       "      <td>6.615967</td>\n",
       "      <td>9.552064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.229588</td>\n",
       "      <td>0.704923</td>\n",
       "      <td>0.811573</td>\n",
       "      <td>0.582028</td>\n",
       "      <td>13.341343</td>\n",
       "      <td>9.548804</td>\n",
       "      <td>6.702547</td>\n",
       "      <td>9.864231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.16792</td>\n",
       "      <td>0.691169</td>\n",
       "      <td>0.806151</td>\n",
       "      <td>0.55508</td>\n",
       "      <td>13.884493</td>\n",
       "      <td>9.769095</td>\n",
       "      <td>6.798744</td>\n",
       "      <td>10.150777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.116146</td>\n",
       "      <td>0.682151</td>\n",
       "      <td>0.792188</td>\n",
       "      <td>0.530161</td>\n",
       "      <td>14.330515</td>\n",
       "      <td>9.910761</td>\n",
       "      <td>7.040152</td>\n",
       "      <td>10.427142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.478257</td>\n",
       "      <td>0.763059</td>\n",
       "      <td>0.841636</td>\n",
       "      <td>0.694317</td>\n",
       "      <td>10.678707</td>\n",
       "      <td>8.414125</td>\n",
       "      <td>6.042614</td>\n",
       "      <td>8.378482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3 TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE     nRMSE     nRMSE   \n",
       "0      0.912792  0.926941  0.953787  0.931173   4.792945  4.747927  3.319732   \n",
       "1      0.838698  0.856978  0.909511  0.868396   6.475166  6.643844  4.644555   \n",
       "2      0.726014  0.805581  0.858783  0.796793    8.32203  7.746906  5.801736   \n",
       "3      0.605282   0.76862  0.829261  0.734388   9.854118  8.452368   6.37947   \n",
       "4      0.491783  0.745054  0.820085   0.68564  11.070489  8.873541   6.54847   \n",
       "5      0.390888  0.730697  0.818639  0.646742  12.002389  9.121363   6.57477   \n",
       "6      0.303457  0.718479  0.816377  0.612771  12.713581  9.326643  6.615967   \n",
       "7      0.229588  0.704923  0.811573  0.582028  13.341343  9.548804  6.702547   \n",
       "8       0.16792  0.691169  0.806151   0.55508  13.884493  9.769095  6.798744   \n",
       "9      0.116146  0.682151  0.792188  0.530161  14.330515  9.910761  7.040152   \n",
       "mean   0.478257  0.763059  0.841636  0.694317  10.678707  8.414125  6.042614   \n",
       "\n",
       "            mean  \n",
       "index      nRMSE  \n",
       "0       4.286868  \n",
       "1       5.921188  \n",
       "2       7.290224  \n",
       "3       8.228652  \n",
       "4       8.830833  \n",
       "5       9.232841  \n",
       "6       9.552064  \n",
       "7       9.864231  \n",
       "8      10.150777  \n",
       "9      10.427142  \n",
       "mean    8.378482  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/30_10_1_50_1_50_datt_seq2seq_gru_3.csv\n",
      "\n",
      "\n",
      "3th iteration\n",
      "history size: 30\n",
      "future size: 20\n",
      "Epoch 1/10000\n",
      "568/568 - 8s - loss: 0.2990 - val_loss: 0.2597 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "568/568 - 5s - loss: 0.2177 - val_loss: 0.2408 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "568/568 - 4s - loss: 0.2026 - val_loss: 0.2198 - 4s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "568/568 - 4s - loss: 0.1912 - val_loss: 0.2141 - 4s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "568/568 - 4s - loss: 0.1829 - val_loss: 0.2034 - 4s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "568/568 - 5s - loss: 0.1725 - val_loss: 0.1965 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "568/568 - 4s - loss: 0.1675 - val_loss: 0.1883 - 4s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "568/568 - 5s - loss: 0.1598 - val_loss: 0.1841 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "568/568 - 4s - loss: 0.1536 - val_loss: 0.1723 - 4s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "568/568 - 5s - loss: 0.1480 - val_loss: 0.1710 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "568/568 - 4s - loss: 0.1438 - val_loss: 0.1686 - 4s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "568/568 - 4s - loss: 0.1382 - val_loss: 0.1675 - 4s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "568/568 - 4s - loss: 0.1334 - val_loss: 0.1640 - 4s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "568/568 - 4s - loss: 0.1288 - val_loss: 0.1549 - 4s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "568/568 - 5s - loss: 0.1251 - val_loss: 0.1508 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "568/568 - 5s - loss: 0.1245 - val_loss: 0.1532 - 5s/epoch - 9ms/step\n",
      "Epoch 17/10000\n",
      "568/568 - 5s - loss: 0.1195 - val_loss: 0.1458 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "568/568 - 4s - loss: 0.1156 - val_loss: 0.1394 - 4s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "568/568 - 4s - loss: 0.1108 - val_loss: 0.1419 - 4s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "568/568 - 4s - loss: 0.1111 - val_loss: 0.1316 - 4s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "568/568 - 4s - loss: 0.1075 - val_loss: 0.1376 - 4s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "568/568 - 5s - loss: 0.1034 - val_loss: 0.1308 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "568/568 - 4s - loss: 0.1017 - val_loss: 0.1258 - 4s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "568/568 - 4s - loss: 0.0995 - val_loss: 0.1176 - 4s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "568/568 - 4s - loss: 0.0964 - val_loss: 0.1208 - 4s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "568/568 - 4s - loss: 0.0964 - val_loss: 0.1316 - 4s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "568/568 - 4s - loss: 0.0965 - val_loss: 0.1138 - 4s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "568/568 - 4s - loss: 0.0913 - val_loss: 0.1123 - 4s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "568/568 - 4s - loss: 0.0907 - val_loss: 0.1195 - 4s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "568/568 - 4s - loss: 0.0896 - val_loss: 0.1210 - 4s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "568/568 - 5s - loss: 0.0896 - val_loss: 0.1104 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "568/568 - 5s - loss: 0.0863 - val_loss: 0.1085 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "568/568 - 4s - loss: 0.0849 - val_loss: 0.1075 - 4s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "568/568 - 4s - loss: 0.0841 - val_loss: 0.1107 - 4s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "568/568 - 4s - loss: 0.0835 - val_loss: 0.1069 - 4s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "568/568 - 4s - loss: 0.0830 - val_loss: 0.1089 - 4s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "568/568 - 4s - loss: 0.0815 - val_loss: 0.1022 - 4s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "568/568 - 4s - loss: 0.0805 - val_loss: 0.1039 - 4s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "568/568 - 4s - loss: 0.0805 - val_loss: 0.1033 - 4s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "568/568 - 4s - loss: 0.0835 - val_loss: 0.1076 - 4s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "568/568 - 5s - loss: 0.0780 - val_loss: 0.1019 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "568/568 - 4s - loss: 0.0762 - val_loss: 0.0961 - 4s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "568/568 - 4s - loss: 0.0746 - val_loss: 0.0973 - 4s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "568/568 - 4s - loss: 0.0752 - val_loss: 0.0952 - 4s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "568/568 - 5s - loss: 0.0728 - val_loss: 0.0993 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "568/568 - 5s - loss: 0.0727 - val_loss: 0.0919 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "568/568 - 4s - loss: 0.0739 - val_loss: 0.0921 - 4s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "568/568 - 4s - loss: 0.0746 - val_loss: 0.0982 - 4s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "568/568 - 5s - loss: 0.0695 - val_loss: 0.0954 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "568/568 - 4s - loss: 0.0715 - val_loss: 0.0874 - 4s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "568/568 - 4s - loss: 0.0692 - val_loss: 0.0890 - 4s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "568/568 - 4s - loss: 0.0690 - val_loss: 0.0968 - 4s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "568/568 - 4s - loss: 0.0673 - val_loss: 0.0826 - 4s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "568/568 - 4s - loss: 0.0657 - val_loss: 0.0931 - 4s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "568/568 - 4s - loss: 0.0676 - val_loss: 0.0898 - 4s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "568/568 - 4s - loss: 0.0682 - val_loss: 0.0892 - 4s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "568/568 - 4s - loss: 0.0671 - val_loss: 0.0878 - 4s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "568/568 - 4s - loss: 0.0636 - val_loss: 0.0847 - 4s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "568/568 - 4s - loss: 0.0633 - val_loss: 0.0884 - 4s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "568/568 - 4s - loss: 0.0645 - val_loss: 0.0932 - 4s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "568/568 - 4s - loss: 0.0683 - val_loss: 0.0816 - 4s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "568/568 - 5s - loss: 0.0610 - val_loss: 0.0870 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "568/568 - 4s - loss: 0.0626 - val_loss: 0.0841 - 4s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "568/568 - 4s - loss: 0.0617 - val_loss: 0.0781 - 4s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "568/568 - 4s - loss: 0.0613 - val_loss: 0.0805 - 4s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "568/568 - 4s - loss: 0.0612 - val_loss: 0.0800 - 4s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "568/568 - 5s - loss: 0.0601 - val_loss: 0.0843 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "568/568 - 4s - loss: 0.0637 - val_loss: 0.1221 - 4s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "568/568 - 5s - loss: 0.0660 - val_loss: 0.0826 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "568/568 - 4s - loss: 0.0598 - val_loss: 0.0777 - 4s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "568/568 - 4s - loss: 0.0596 - val_loss: 0.0758 - 4s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "568/568 - 4s - loss: 0.0558 - val_loss: 0.0757 - 4s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "568/568 - 4s - loss: 0.0557 - val_loss: 0.0747 - 4s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "568/568 - 5s - loss: 0.0586 - val_loss: 0.0746 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "568/568 - 4s - loss: 0.0561 - val_loss: 0.0818 - 4s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "568/568 - 4s - loss: 0.0565 - val_loss: 0.0741 - 4s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "568/568 - 4s - loss: 0.0550 - val_loss: 0.0772 - 4s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "568/568 - 5s - loss: 0.0574 - val_loss: 0.0802 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "568/568 - 4s - loss: 0.0567 - val_loss: 0.0694 - 4s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "568/568 - 4s - loss: 0.0547 - val_loss: 0.0758 - 4s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "568/568 - 4s - loss: 0.0545 - val_loss: 0.0807 - 4s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "568/568 - 4s - loss: 0.0534 - val_loss: 0.0699 - 4s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "568/568 - 4s - loss: 0.0583 - val_loss: 0.0732 - 4s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "568/568 - 5s - loss: 0.0523 - val_loss: 0.0660 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "568/568 - 4s - loss: 0.0531 - val_loss: 0.0683 - 4s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "568/568 - 4s - loss: 0.0516 - val_loss: 0.0765 - 4s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "568/568 - 4s - loss: 0.0523 - val_loss: 0.0681 - 4s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "568/568 - 4s - loss: 0.0528 - val_loss: 0.0642 - 4s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "568/568 - 4s - loss: 0.0528 - val_loss: 0.0674 - 4s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "568/568 - 4s - loss: 0.0511 - val_loss: 0.0746 - 4s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "568/568 - 4s - loss: 0.0509 - val_loss: 0.0687 - 4s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "568/568 - 4s - loss: 0.0503 - val_loss: 0.0708 - 4s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "568/568 - 4s - loss: 0.0649 - val_loss: 0.0731 - 4s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "568/568 - 4s - loss: 0.0532 - val_loss: 0.0676 - 4s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "568/568 - 4s - loss: 0.0507 - val_loss: 0.0645 - 4s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "568/568 - 5s - loss: 0.0498 - val_loss: 0.0723 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "568/568 - 4s - loss: 0.0514 - val_loss: 0.0653 - 4s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "568/568 - 5s - loss: 0.0500 - val_loss: 0.0709 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_14_layer_call_fn, gru_cell_14_layer_call_and_return_conditional_losses, gru_cell_15_layer_call_fn, gru_cell_15_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_20_1_50_1_50_datt_seq2seq_gru_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_20_1_50_1_50_datt_seq2seq_gru_3\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002646E998FA0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002641EE090A0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.851033</td>\n",
       "      <td>0.925251</td>\n",
       "      <td>0.933597</td>\n",
       "      <td>0.903294</td>\n",
       "      <td>7.613186</td>\n",
       "      <td>4.796939</td>\n",
       "      <td>3.965345</td>\n",
       "      <td>5.45849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.769398</td>\n",
       "      <td>0.869943</td>\n",
       "      <td>0.887584</td>\n",
       "      <td>0.842309</td>\n",
       "      <td>9.297563</td>\n",
       "      <td>6.32936</td>\n",
       "      <td>5.159738</td>\n",
       "      <td>6.928887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.681656</td>\n",
       "      <td>0.822587</td>\n",
       "      <td>0.833513</td>\n",
       "      <td>0.779252</td>\n",
       "      <td>10.444266</td>\n",
       "      <td>7.395124</td>\n",
       "      <td>6.279886</td>\n",
       "      <td>8.039758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.587427</td>\n",
       "      <td>0.780997</td>\n",
       "      <td>0.779901</td>\n",
       "      <td>0.716109</td>\n",
       "      <td>11.490514</td>\n",
       "      <td>8.219068</td>\n",
       "      <td>7.221242</td>\n",
       "      <td>8.976941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.49502</td>\n",
       "      <td>0.748647</td>\n",
       "      <td>0.732847</td>\n",
       "      <td>0.658838</td>\n",
       "      <td>12.508229</td>\n",
       "      <td>8.809101</td>\n",
       "      <td>7.956297</td>\n",
       "      <td>9.757876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.408736</td>\n",
       "      <td>0.723063</td>\n",
       "      <td>0.702106</td>\n",
       "      <td>0.611302</td>\n",
       "      <td>13.490183</td>\n",
       "      <td>9.251012</td>\n",
       "      <td>8.401459</td>\n",
       "      <td>10.380885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.333031</td>\n",
       "      <td>0.702201</td>\n",
       "      <td>0.681595</td>\n",
       "      <td>0.572276</td>\n",
       "      <td>14.049101</td>\n",
       "      <td>9.595936</td>\n",
       "      <td>8.685942</td>\n",
       "      <td>10.776993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.266619</td>\n",
       "      <td>0.683589</td>\n",
       "      <td>0.662905</td>\n",
       "      <td>0.537704</td>\n",
       "      <td>14.453314</td>\n",
       "      <td>9.894263</td>\n",
       "      <td>8.937448</td>\n",
       "      <td>11.095008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.20645</td>\n",
       "      <td>0.664621</td>\n",
       "      <td>0.637609</td>\n",
       "      <td>0.502893</td>\n",
       "      <td>14.81561</td>\n",
       "      <td>10.189607</td>\n",
       "      <td>9.267257</td>\n",
       "      <td>11.424158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.147585</td>\n",
       "      <td>0.643134</td>\n",
       "      <td>0.601964</td>\n",
       "      <td>0.464228</td>\n",
       "      <td>15.13637</td>\n",
       "      <td>10.51279</td>\n",
       "      <td>9.712907</td>\n",
       "      <td>11.787356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.088149</td>\n",
       "      <td>0.621223</td>\n",
       "      <td>0.559947</td>\n",
       "      <td>0.423107</td>\n",
       "      <td>15.496337</td>\n",
       "      <td>10.832415</td>\n",
       "      <td>10.213545</td>\n",
       "      <td>12.180766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.030294</td>\n",
       "      <td>0.600726</td>\n",
       "      <td>0.519325</td>\n",
       "      <td>0.383448</td>\n",
       "      <td>15.881906</td>\n",
       "      <td>11.122808</td>\n",
       "      <td>10.675915</td>\n",
       "      <td>12.56021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.024104</td>\n",
       "      <td>0.581172</td>\n",
       "      <td>0.486613</td>\n",
       "      <td>0.347894</td>\n",
       "      <td>16.100896</td>\n",
       "      <td>11.392892</td>\n",
       "      <td>11.034763</td>\n",
       "      <td>12.84285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.070435</td>\n",
       "      <td>0.561692</td>\n",
       "      <td>0.461403</td>\n",
       "      <td>0.317553</td>\n",
       "      <td>16.242769</td>\n",
       "      <td>11.656484</td>\n",
       "      <td>11.303851</td>\n",
       "      <td>13.067701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.108708</td>\n",
       "      <td>0.539638</td>\n",
       "      <td>0.441021</td>\n",
       "      <td>0.29065</td>\n",
       "      <td>16.367351</td>\n",
       "      <td>11.947714</td>\n",
       "      <td>11.516536</td>\n",
       "      <td>13.2772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.139891</td>\n",
       "      <td>0.516858</td>\n",
       "      <td>0.424812</td>\n",
       "      <td>0.26726</td>\n",
       "      <td>16.433898</td>\n",
       "      <td>12.241003</td>\n",
       "      <td>11.683584</td>\n",
       "      <td>13.452828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.163306</td>\n",
       "      <td>0.49246</td>\n",
       "      <td>0.410312</td>\n",
       "      <td>0.246489</td>\n",
       "      <td>16.441961</td>\n",
       "      <td>12.546137</td>\n",
       "      <td>11.832018</td>\n",
       "      <td>13.606705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.183506</td>\n",
       "      <td>0.466568</td>\n",
       "      <td>0.395139</td>\n",
       "      <td>0.226067</td>\n",
       "      <td>16.54394</td>\n",
       "      <td>12.861209</td>\n",
       "      <td>11.986737</td>\n",
       "      <td>13.797295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.205144</td>\n",
       "      <td>0.43811</td>\n",
       "      <td>0.379806</td>\n",
       "      <td>0.204257</td>\n",
       "      <td>16.714012</td>\n",
       "      <td>13.198313</td>\n",
       "      <td>12.141571</td>\n",
       "      <td>14.017965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.230845</td>\n",
       "      <td>0.409761</td>\n",
       "      <td>0.365675</td>\n",
       "      <td>0.18153</td>\n",
       "      <td>16.910722</td>\n",
       "      <td>13.524895</td>\n",
       "      <td>12.283387</td>\n",
       "      <td>14.239668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.186973</td>\n",
       "      <td>0.639612</td>\n",
       "      <td>0.594884</td>\n",
       "      <td>0.473823</td>\n",
       "      <td>14.321606</td>\n",
       "      <td>10.315854</td>\n",
       "      <td>9.512971</td>\n",
       "      <td>11.383477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.851033  0.925251  0.933597  0.903294   7.613186   4.796939   \n",
       "1      0.769398  0.869943  0.887584  0.842309   9.297563    6.32936   \n",
       "2      0.681656  0.822587  0.833513  0.779252  10.444266   7.395124   \n",
       "3      0.587427  0.780997  0.779901  0.716109  11.490514   8.219068   \n",
       "4       0.49502  0.748647  0.732847  0.658838  12.508229   8.809101   \n",
       "5      0.408736  0.723063  0.702106  0.611302  13.490183   9.251012   \n",
       "6      0.333031  0.702201  0.681595  0.572276  14.049101   9.595936   \n",
       "7      0.266619  0.683589  0.662905  0.537704  14.453314   9.894263   \n",
       "8       0.20645  0.664621  0.637609  0.502893   14.81561  10.189607   \n",
       "9      0.147585  0.643134  0.601964  0.464228   15.13637   10.51279   \n",
       "10     0.088149  0.621223  0.559947  0.423107  15.496337  10.832415   \n",
       "11     0.030294  0.600726  0.519325  0.383448  15.881906  11.122808   \n",
       "12    -0.024104  0.581172  0.486613  0.347894  16.100896  11.392892   \n",
       "13    -0.070435  0.561692  0.461403  0.317553  16.242769  11.656484   \n",
       "14    -0.108708  0.539638  0.441021   0.29065  16.367351  11.947714   \n",
       "15    -0.139891  0.516858  0.424812   0.26726  16.433898  12.241003   \n",
       "16    -0.163306   0.49246  0.410312  0.246489  16.441961  12.546137   \n",
       "17    -0.183506  0.466568  0.395139  0.226067   16.54394  12.861209   \n",
       "18    -0.205144   0.43811  0.379806  0.204257  16.714012  13.198313   \n",
       "19    -0.230845  0.409761  0.365675   0.18153  16.910722  13.524895   \n",
       "mean   0.186973  0.639612  0.594884  0.473823  14.321606  10.315854   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       3.965345    5.45849  \n",
       "1       5.159738   6.928887  \n",
       "2       6.279886   8.039758  \n",
       "3       7.221242   8.976941  \n",
       "4       7.956297   9.757876  \n",
       "5       8.401459  10.380885  \n",
       "6       8.685942  10.776993  \n",
       "7       8.937448  11.095008  \n",
       "8       9.267257  11.424158  \n",
       "9       9.712907  11.787356  \n",
       "10     10.213545  12.180766  \n",
       "11     10.675915   12.56021  \n",
       "12     11.034763   12.84285  \n",
       "13     11.303851  13.067701  \n",
       "14     11.516536    13.2772  \n",
       "15     11.683584  13.452828  \n",
       "16     11.832018  13.606705  \n",
       "17     11.986737  13.797295  \n",
       "18     12.141571  14.017965  \n",
       "19     12.283387  14.239668  \n",
       "mean    9.512971  11.383477  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/30_20_1_50_1_50_datt_seq2seq_gru_3.csv\n",
      "\n",
      "\n",
      "3th iteration\n",
      "history size: 30\n",
      "future size: 30\n",
      "Epoch 1/10000\n",
      "563/563 - 8s - loss: 0.3760 - val_loss: 0.3170 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.2874 - val_loss: 0.2956 - 5s/epoch - 9ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.2676 - val_loss: 0.2789 - 5s/epoch - 9ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 5s - loss: 0.2501 - val_loss: 0.2575 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.2332 - val_loss: 0.2419 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.2205 - val_loss: 0.2329 - 5s/epoch - 9ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.2081 - val_loss: 0.2164 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.1968 - val_loss: 0.2114 - 5s/epoch - 9ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.1872 - val_loss: 0.2036 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.1801 - val_loss: 0.2002 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.1741 - val_loss: 0.1913 - 5s/epoch - 9ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.1651 - val_loss: 0.1862 - 5s/epoch - 9ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.1647 - val_loss: 0.1825 - 5s/epoch - 9ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.1583 - val_loss: 0.1720 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.1508 - val_loss: 0.1672 - 5s/epoch - 9ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.1473 - val_loss: 0.1794 - 5s/epoch - 9ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.1476 - val_loss: 0.1629 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.1412 - val_loss: 0.1566 - 5s/epoch - 9ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.1404 - val_loss: 0.1766 - 5s/epoch - 9ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.1374 - val_loss: 0.1587 - 5s/epoch - 9ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.1328 - val_loss: 0.1511 - 5s/epoch - 9ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.1306 - val_loss: 0.1574 - 5s/epoch - 9ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.1317 - val_loss: 0.1513 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.1264 - val_loss: 0.1525 - 5s/epoch - 9ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.1244 - val_loss: 0.1443 - 5s/epoch - 9ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.1210 - val_loss: 0.1413 - 5s/epoch - 9ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.1207 - val_loss: 0.1402 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.1178 - val_loss: 0.1387 - 5s/epoch - 9ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.1193 - val_loss: 0.1407 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.1151 - val_loss: 0.1341 - 5s/epoch - 9ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.1138 - val_loss: 0.1360 - 5s/epoch - 9ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.1133 - val_loss: 0.1281 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.1105 - val_loss: 0.1290 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.1075 - val_loss: 0.1204 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.1051 - val_loss: 0.1288 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.1057 - val_loss: 0.1192 - 5s/epoch - 9ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.1016 - val_loss: 0.1184 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.1024 - val_loss: 0.1325 - 5s/epoch - 9ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.1015 - val_loss: 0.1108 - 5s/epoch - 9ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0995 - val_loss: 0.1223 - 5s/epoch - 9ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.1005 - val_loss: 0.1292 - 5s/epoch - 9ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0948 - val_loss: 0.1056 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0944 - val_loss: 0.1122 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0939 - val_loss: 0.1081 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0956 - val_loss: 0.1157 - 5s/epoch - 9ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0900 - val_loss: 0.1003 - 5s/epoch - 9ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0913 - val_loss: 0.1027 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0888 - val_loss: 0.1060 - 5s/epoch - 9ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0872 - val_loss: 0.1009 - 5s/epoch - 9ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0889 - val_loss: 0.1091 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0929 - val_loss: 0.0973 - 5s/epoch - 9ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0841 - val_loss: 0.1146 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0827 - val_loss: 0.1010 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0850 - val_loss: 0.0969 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0836 - val_loss: 0.0964 - 5s/epoch - 9ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0799 - val_loss: 0.0957 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0806 - val_loss: 0.0899 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0816 - val_loss: 0.0977 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0792 - val_loss: 0.0906 - 5s/epoch - 9ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0797 - val_loss: 0.0969 - 5s/epoch - 9ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0782 - val_loss: 0.1058 - 5s/epoch - 9ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0764 - val_loss: 0.0918 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 5s - loss: 0.0780 - val_loss: 0.0878 - 5s/epoch - 9ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0772 - val_loss: 0.0883 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0740 - val_loss: 0.0906 - 5s/epoch - 9ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0737 - val_loss: 0.0930 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0770 - val_loss: 0.0925 - 5s/epoch - 9ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 5s - loss: 0.0740 - val_loss: 0.0893 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 5s - loss: 0.0725 - val_loss: 0.0900 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 5s - loss: 0.0735 - val_loss: 0.0905 - 5s/epoch - 9ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 5s - loss: 0.0724 - val_loss: 0.0948 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 5s - loss: 0.0811 - val_loss: 0.0815 - 5s/epoch - 9ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 5s - loss: 0.0681 - val_loss: 0.0967 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 5s - loss: 0.0734 - val_loss: 0.0809 - 5s/epoch - 9ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 5s - loss: 0.0673 - val_loss: 0.0834 - 5s/epoch - 9ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 5s - loss: 0.0813 - val_loss: 0.0986 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 5s - loss: 0.0729 - val_loss: 0.0890 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 5s - loss: 0.0672 - val_loss: 0.0808 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 5s - loss: 0.0686 - val_loss: 0.0840 - 5s/epoch - 9ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 5s - loss: 0.0701 - val_loss: 0.0860 - 5s/epoch - 9ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 5s - loss: 0.0699 - val_loss: 0.0947 - 5s/epoch - 9ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 5s - loss: 0.0677 - val_loss: 0.0820 - 5s/epoch - 9ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 5s - loss: 0.0668 - val_loss: 0.0814 - 5s/epoch - 9ms/step\n",
      "Epoch 84/10000\n",
      "563/563 - 5s - loss: 0.0674 - val_loss: 0.0801 - 5s/epoch - 9ms/step\n",
      "Epoch 85/10000\n",
      "563/563 - 5s - loss: 0.0658 - val_loss: 0.0788 - 5s/epoch - 9ms/step\n",
      "Epoch 86/10000\n",
      "563/563 - 5s - loss: 0.0672 - val_loss: 0.0829 - 5s/epoch - 9ms/step\n",
      "Epoch 87/10000\n",
      "563/563 - 5s - loss: 0.0791 - val_loss: 0.1012 - 5s/epoch - 9ms/step\n",
      "Epoch 88/10000\n",
      "563/563 - 5s - loss: 0.0741 - val_loss: 0.0938 - 5s/epoch - 9ms/step\n",
      "Epoch 89/10000\n",
      "563/563 - 5s - loss: 0.0715 - val_loss: 0.0777 - 5s/epoch - 9ms/step\n",
      "Epoch 90/10000\n",
      "563/563 - 5s - loss: 0.0641 - val_loss: 0.0850 - 5s/epoch - 9ms/step\n",
      "Epoch 91/10000\n",
      "563/563 - 5s - loss: 0.0648 - val_loss: 0.0747 - 5s/epoch - 9ms/step\n",
      "Epoch 92/10000\n",
      "563/563 - 5s - loss: 0.0675 - val_loss: 0.0777 - 5s/epoch - 9ms/step\n",
      "Epoch 93/10000\n",
      "563/563 - 5s - loss: 0.0650 - val_loss: 0.0756 - 5s/epoch - 9ms/step\n",
      "Epoch 94/10000\n",
      "563/563 - 5s - loss: 0.0612 - val_loss: 0.0761 - 5s/epoch - 9ms/step\n",
      "Epoch 95/10000\n",
      "563/563 - 5s - loss: 0.0616 - val_loss: 0.0788 - 5s/epoch - 9ms/step\n",
      "Epoch 96/10000\n",
      "563/563 - 5s - loss: 0.0630 - val_loss: 0.0774 - 5s/epoch - 9ms/step\n",
      "Epoch 97/10000\n",
      "563/563 - 5s - loss: 0.0647 - val_loss: 0.0810 - 5s/epoch - 9ms/step\n",
      "Epoch 98/10000\n",
      "563/563 - 5s - loss: 0.0642 - val_loss: 0.0710 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "563/563 - 5s - loss: 0.0607 - val_loss: 0.0717 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "563/563 - 5s - loss: 0.0618 - val_loss: 0.0781 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "563/563 - 5s - loss: 0.0613 - val_loss: 0.0738 - 5s/epoch - 9ms/step\n",
      "Epoch 102/10000\n",
      "563/563 - 5s - loss: 0.0612 - val_loss: 0.0729 - 5s/epoch - 9ms/step\n",
      "Epoch 103/10000\n",
      "563/563 - 5s - loss: 0.0610 - val_loss: 0.0741 - 5s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "563/563 - 5s - loss: 0.0597 - val_loss: 0.0694 - 5s/epoch - 9ms/step\n",
      "Epoch 105/10000\n",
      "563/563 - 5s - loss: 0.0604 - val_loss: 0.0762 - 5s/epoch - 9ms/step\n",
      "Epoch 106/10000\n",
      "563/563 - 5s - loss: 0.0599 - val_loss: 0.0716 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "563/563 - 5s - loss: 0.0598 - val_loss: 0.0736 - 5s/epoch - 9ms/step\n",
      "Epoch 108/10000\n",
      "563/563 - 5s - loss: 0.0640 - val_loss: 0.1004 - 5s/epoch - 9ms/step\n",
      "Epoch 109/10000\n",
      "563/563 - 5s - loss: 0.0650 - val_loss: 0.0722 - 5s/epoch - 9ms/step\n",
      "Epoch 110/10000\n",
      "563/563 - 5s - loss: 0.0576 - val_loss: 0.0687 - 5s/epoch - 9ms/step\n",
      "Epoch 111/10000\n",
      "563/563 - 5s - loss: 0.0572 - val_loss: 0.0697 - 5s/epoch - 9ms/step\n",
      "Epoch 112/10000\n",
      "563/563 - 5s - loss: 0.0584 - val_loss: 0.0691 - 5s/epoch - 9ms/step\n",
      "Epoch 113/10000\n",
      "563/563 - 5s - loss: 0.0632 - val_loss: 0.0715 - 5s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "563/563 - 5s - loss: 0.0600 - val_loss: 0.0678 - 5s/epoch - 9ms/step\n",
      "Epoch 115/10000\n",
      "563/563 - 5s - loss: 0.0586 - val_loss: 0.0694 - 5s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "563/563 - 5s - loss: 0.0554 - val_loss: 0.0687 - 5s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "563/563 - 5s - loss: 0.0572 - val_loss: 0.0717 - 5s/epoch - 9ms/step\n",
      "Epoch 118/10000\n",
      "563/563 - 5s - loss: 0.0556 - val_loss: 0.0711 - 5s/epoch - 9ms/step\n",
      "Epoch 119/10000\n",
      "563/563 - 5s - loss: 0.0573 - val_loss: 0.0699 - 5s/epoch - 9ms/step\n",
      "Epoch 120/10000\n",
      "563/563 - 5s - loss: 0.0676 - val_loss: 0.0815 - 5s/epoch - 9ms/step\n",
      "Epoch 121/10000\n",
      "563/563 - 5s - loss: 0.0589 - val_loss: 0.0646 - 5s/epoch - 8ms/step\n",
      "Epoch 122/10000\n",
      "563/563 - 5s - loss: 0.0537 - val_loss: 0.0674 - 5s/epoch - 9ms/step\n",
      "Epoch 123/10000\n",
      "563/563 - 5s - loss: 0.0573 - val_loss: 0.0870 - 5s/epoch - 8ms/step\n",
      "Epoch 124/10000\n",
      "563/563 - 5s - loss: 0.0568 - val_loss: 0.0640 - 5s/epoch - 8ms/step\n",
      "Epoch 125/10000\n",
      "563/563 - 5s - loss: 0.0539 - val_loss: 0.0682 - 5s/epoch - 8ms/step\n",
      "Epoch 126/10000\n",
      "563/563 - 5s - loss: 0.0565 - val_loss: 0.0737 - 5s/epoch - 9ms/step\n",
      "Epoch 127/10000\n",
      "563/563 - 5s - loss: 0.0589 - val_loss: 0.0646 - 5s/epoch - 9ms/step\n",
      "Epoch 128/10000\n",
      "563/563 - 5s - loss: 0.0540 - val_loss: 0.0673 - 5s/epoch - 9ms/step\n",
      "Epoch 129/10000\n",
      "563/563 - 5s - loss: 0.0534 - val_loss: 0.0686 - 5s/epoch - 8ms/step\n",
      "Epoch 130/10000\n",
      "563/563 - 5s - loss: 0.0556 - val_loss: 0.0702 - 5s/epoch - 9ms/step\n",
      "Epoch 131/10000\n",
      "563/563 - 5s - loss: 0.0604 - val_loss: 0.1138 - 5s/epoch - 9ms/step\n",
      "Epoch 132/10000\n",
      "563/563 - 5s - loss: 0.0642 - val_loss: 0.0689 - 5s/epoch - 8ms/step\n",
      "Epoch 133/10000\n",
      "563/563 - 5s - loss: 0.0535 - val_loss: 0.0645 - 5s/epoch - 8ms/step\n",
      "Epoch 134/10000\n",
      "563/563 - 5s - loss: 0.0536 - val_loss: 0.0683 - 5s/epoch - 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_16_layer_call_fn, gru_cell_16_layer_call_and_return_conditional_losses, gru_cell_17_layer_call_fn, gru_cell_17_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_30_1_50_1_50_datt_seq2seq_gru_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_30_1_50_1_50_datt_seq2seq_gru_3\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002643CC586D0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000266104995E0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85546</td>\n",
       "      <td>0.864975</td>\n",
       "      <td>0.898531</td>\n",
       "      <td>0.872989</td>\n",
       "      <td>7.493959</td>\n",
       "      <td>6.445288</td>\n",
       "      <td>4.883878</td>\n",
       "      <td>6.274375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.789216</td>\n",
       "      <td>0.8132</td>\n",
       "      <td>0.856557</td>\n",
       "      <td>0.819658</td>\n",
       "      <td>9.04774</td>\n",
       "      <td>7.581502</td>\n",
       "      <td>5.808579</td>\n",
       "      <td>7.479274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6851</td>\n",
       "      <td>0.785095</td>\n",
       "      <td>0.812659</td>\n",
       "      <td>0.760951</td>\n",
       "      <td>11.057342</td>\n",
       "      <td>8.132608</td>\n",
       "      <td>6.640012</td>\n",
       "      <td>8.609988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.577979</td>\n",
       "      <td>0.762512</td>\n",
       "      <td>0.782934</td>\n",
       "      <td>0.707808</td>\n",
       "      <td>12.800317</td>\n",
       "      <td>8.55043</td>\n",
       "      <td>7.148979</td>\n",
       "      <td>9.499909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.479962</td>\n",
       "      <td>0.744187</td>\n",
       "      <td>0.764589</td>\n",
       "      <td>0.662912</td>\n",
       "      <td>14.209167</td>\n",
       "      <td>8.875972</td>\n",
       "      <td>7.445931</td>\n",
       "      <td>10.177023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.394253</td>\n",
       "      <td>0.727437</td>\n",
       "      <td>0.749971</td>\n",
       "      <td>0.623887</td>\n",
       "      <td>15.336265</td>\n",
       "      <td>9.164653</td>\n",
       "      <td>7.674034</td>\n",
       "      <td>10.724984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.318194</td>\n",
       "      <td>0.710052</td>\n",
       "      <td>0.735929</td>\n",
       "      <td>0.588058</td>\n",
       "      <td>16.272421</td>\n",
       "      <td>9.455398</td>\n",
       "      <td>7.887275</td>\n",
       "      <td>11.205031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.246976</td>\n",
       "      <td>0.692736</td>\n",
       "      <td>0.720987</td>\n",
       "      <td>0.553566</td>\n",
       "      <td>17.104349</td>\n",
       "      <td>9.73679</td>\n",
       "      <td>8.108596</td>\n",
       "      <td>11.649911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.180125</td>\n",
       "      <td>0.675186</td>\n",
       "      <td>0.700483</td>\n",
       "      <td>0.518598</td>\n",
       "      <td>17.851595</td>\n",
       "      <td>10.015109</td>\n",
       "      <td>8.402615</td>\n",
       "      <td>12.089773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.117419</td>\n",
       "      <td>0.657606</td>\n",
       "      <td>0.672504</td>\n",
       "      <td>0.482509</td>\n",
       "      <td>18.528226</td>\n",
       "      <td>10.285976</td>\n",
       "      <td>8.788251</td>\n",
       "      <td>12.534151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.05811</td>\n",
       "      <td>0.641158</td>\n",
       "      <td>0.638846</td>\n",
       "      <td>0.446038</td>\n",
       "      <td>19.149391</td>\n",
       "      <td>10.533025</td>\n",
       "      <td>9.231059</td>\n",
       "      <td>12.971158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.003861</td>\n",
       "      <td>0.626005</td>\n",
       "      <td>0.605624</td>\n",
       "      <td>0.41183</td>\n",
       "      <td>19.339382</td>\n",
       "      <td>10.756569</td>\n",
       "      <td>9.649135</td>\n",
       "      <td>13.248362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.044401</td>\n",
       "      <td>0.610299</td>\n",
       "      <td>0.580168</td>\n",
       "      <td>0.382022</td>\n",
       "      <td>18.939754</td>\n",
       "      <td>10.984341</td>\n",
       "      <td>9.958587</td>\n",
       "      <td>13.294227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.086307</td>\n",
       "      <td>0.594716</td>\n",
       "      <td>0.562592</td>\n",
       "      <td>0.357</td>\n",
       "      <td>18.670863</td>\n",
       "      <td>11.205766</td>\n",
       "      <td>10.16715</td>\n",
       "      <td>13.347926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.124105</td>\n",
       "      <td>0.579088</td>\n",
       "      <td>0.545949</td>\n",
       "      <td>0.333644</td>\n",
       "      <td>18.688855</td>\n",
       "      <td>11.424849</td>\n",
       "      <td>10.361122</td>\n",
       "      <td>13.491608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.159656</td>\n",
       "      <td>0.56363</td>\n",
       "      <td>0.528084</td>\n",
       "      <td>0.310686</td>\n",
       "      <td>18.91803</td>\n",
       "      <td>11.638004</td>\n",
       "      <td>10.56482</td>\n",
       "      <td>13.706952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.193536</td>\n",
       "      <td>0.548662</td>\n",
       "      <td>0.510677</td>\n",
       "      <td>0.288601</td>\n",
       "      <td>18.816023</td>\n",
       "      <td>11.838609</td>\n",
       "      <td>10.76009</td>\n",
       "      <td>13.804908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.223771</td>\n",
       "      <td>0.533313</td>\n",
       "      <td>0.494159</td>\n",
       "      <td>0.2679</td>\n",
       "      <td>18.688614</td>\n",
       "      <td>12.040931</td>\n",
       "      <td>10.942327</td>\n",
       "      <td>13.890624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.251952</td>\n",
       "      <td>0.517549</td>\n",
       "      <td>0.476946</td>\n",
       "      <td>0.247514</td>\n",
       "      <td>18.623278</td>\n",
       "      <td>12.244732</td>\n",
       "      <td>11.128924</td>\n",
       "      <td>13.998978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.278292</td>\n",
       "      <td>0.500738</td>\n",
       "      <td>0.458041</td>\n",
       "      <td>0.226829</td>\n",
       "      <td>18.54454</td>\n",
       "      <td>12.456405</td>\n",
       "      <td>11.329994</td>\n",
       "      <td>14.110313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.303878</td>\n",
       "      <td>0.484201</td>\n",
       "      <td>0.437937</td>\n",
       "      <td>0.206087</td>\n",
       "      <td>18.532641</td>\n",
       "      <td>12.661725</td>\n",
       "      <td>11.539922</td>\n",
       "      <td>14.244763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.329031</td>\n",
       "      <td>0.467283</td>\n",
       "      <td>0.41932</td>\n",
       "      <td>0.185858</td>\n",
       "      <td>18.588229</td>\n",
       "      <td>12.866937</td>\n",
       "      <td>11.731437</td>\n",
       "      <td>14.395534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.353847</td>\n",
       "      <td>0.450261</td>\n",
       "      <td>0.401706</td>\n",
       "      <td>0.16604</td>\n",
       "      <td>18.500721</td>\n",
       "      <td>13.069948</td>\n",
       "      <td>11.910834</td>\n",
       "      <td>14.493834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.377624</td>\n",
       "      <td>0.432496</td>\n",
       "      <td>0.383048</td>\n",
       "      <td>0.145973</td>\n",
       "      <td>18.408456</td>\n",
       "      <td>13.280499</td>\n",
       "      <td>12.098835</td>\n",
       "      <td>14.59593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.403427</td>\n",
       "      <td>0.414508</td>\n",
       "      <td>0.363329</td>\n",
       "      <td>0.124803</td>\n",
       "      <td>18.390409</td>\n",
       "      <td>13.489858</td>\n",
       "      <td>12.294344</td>\n",
       "      <td>14.724871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.430937</td>\n",
       "      <td>0.397926</td>\n",
       "      <td>0.344231</td>\n",
       "      <td>0.10374</td>\n",
       "      <td>18.384109</td>\n",
       "      <td>13.678847</td>\n",
       "      <td>12.48171</td>\n",
       "      <td>14.848222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.45982</td>\n",
       "      <td>0.379866</td>\n",
       "      <td>0.326809</td>\n",
       "      <td>0.082285</td>\n",
       "      <td>18.387961</td>\n",
       "      <td>13.880417</td>\n",
       "      <td>12.651223</td>\n",
       "      <td>14.9732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.489955</td>\n",
       "      <td>0.361338</td>\n",
       "      <td>0.310528</td>\n",
       "      <td>0.060637</td>\n",
       "      <td>18.531362</td>\n",
       "      <td>14.083409</td>\n",
       "      <td>12.808642</td>\n",
       "      <td>15.141138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.519747</td>\n",
       "      <td>0.342393</td>\n",
       "      <td>0.294666</td>\n",
       "      <td>0.039104</td>\n",
       "      <td>18.738374</td>\n",
       "      <td>14.288846</td>\n",
       "      <td>12.960561</td>\n",
       "      <td>15.32926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.547542</td>\n",
       "      <td>0.322855</td>\n",
       "      <td>0.279419</td>\n",
       "      <td>0.018244</td>\n",
       "      <td>18.935468</td>\n",
       "      <td>14.498089</td>\n",
       "      <td>13.105069</td>\n",
       "      <td>15.512875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.029039</td>\n",
       "      <td>0.573376</td>\n",
       "      <td>0.555241</td>\n",
       "      <td>0.366526</td>\n",
       "      <td>17.115928</td>\n",
       "      <td>11.305518</td>\n",
       "      <td>10.015465</td>\n",
       "      <td>12.812303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0       0.85546  0.864975  0.898531  0.872989   7.493959   6.445288   \n",
       "1      0.789216    0.8132  0.856557  0.819658    9.04774   7.581502   \n",
       "2        0.6851  0.785095  0.812659  0.760951  11.057342   8.132608   \n",
       "3      0.577979  0.762512  0.782934  0.707808  12.800317    8.55043   \n",
       "4      0.479962  0.744187  0.764589  0.662912  14.209167   8.875972   \n",
       "5      0.394253  0.727437  0.749971  0.623887  15.336265   9.164653   \n",
       "6      0.318194  0.710052  0.735929  0.588058  16.272421   9.455398   \n",
       "7      0.246976  0.692736  0.720987  0.553566  17.104349    9.73679   \n",
       "8      0.180125  0.675186  0.700483  0.518598  17.851595  10.015109   \n",
       "9      0.117419  0.657606  0.672504  0.482509  18.528226  10.285976   \n",
       "10      0.05811  0.641158  0.638846  0.446038  19.149391  10.533025   \n",
       "11     0.003861  0.626005  0.605624   0.41183  19.339382  10.756569   \n",
       "12    -0.044401  0.610299  0.580168  0.382022  18.939754  10.984341   \n",
       "13    -0.086307  0.594716  0.562592     0.357  18.670863  11.205766   \n",
       "14    -0.124105  0.579088  0.545949  0.333644  18.688855  11.424849   \n",
       "15    -0.159656   0.56363  0.528084  0.310686   18.91803  11.638004   \n",
       "16    -0.193536  0.548662  0.510677  0.288601  18.816023  11.838609   \n",
       "17    -0.223771  0.533313  0.494159    0.2679  18.688614  12.040931   \n",
       "18    -0.251952  0.517549  0.476946  0.247514  18.623278  12.244732   \n",
       "19    -0.278292  0.500738  0.458041  0.226829   18.54454  12.456405   \n",
       "20    -0.303878  0.484201  0.437937  0.206087  18.532641  12.661725   \n",
       "21    -0.329031  0.467283   0.41932  0.185858  18.588229  12.866937   \n",
       "22    -0.353847  0.450261  0.401706   0.16604  18.500721  13.069948   \n",
       "23    -0.377624  0.432496  0.383048  0.145973  18.408456  13.280499   \n",
       "24    -0.403427  0.414508  0.363329  0.124803  18.390409  13.489858   \n",
       "25    -0.430937  0.397926  0.344231   0.10374  18.384109  13.678847   \n",
       "26     -0.45982  0.379866  0.326809  0.082285  18.387961  13.880417   \n",
       "27    -0.489955  0.361338  0.310528  0.060637  18.531362  14.083409   \n",
       "28    -0.519747  0.342393  0.294666  0.039104  18.738374  14.288846   \n",
       "29    -0.547542  0.322855  0.279419  0.018244  18.935468  14.498089   \n",
       "mean  -0.029039  0.573376  0.555241  0.366526  17.115928  11.305518   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       4.883878   6.274375  \n",
       "1       5.808579   7.479274  \n",
       "2       6.640012   8.609988  \n",
       "3       7.148979   9.499909  \n",
       "4       7.445931  10.177023  \n",
       "5       7.674034  10.724984  \n",
       "6       7.887275  11.205031  \n",
       "7       8.108596  11.649911  \n",
       "8       8.402615  12.089773  \n",
       "9       8.788251  12.534151  \n",
       "10      9.231059  12.971158  \n",
       "11      9.649135  13.248362  \n",
       "12      9.958587  13.294227  \n",
       "13      10.16715  13.347926  \n",
       "14     10.361122  13.491608  \n",
       "15      10.56482  13.706952  \n",
       "16      10.76009  13.804908  \n",
       "17     10.942327  13.890624  \n",
       "18     11.128924  13.998978  \n",
       "19     11.329994  14.110313  \n",
       "20     11.539922  14.244763  \n",
       "21     11.731437  14.395534  \n",
       "22     11.910834  14.493834  \n",
       "23     12.098835   14.59593  \n",
       "24     12.294344  14.724871  \n",
       "25      12.48171  14.848222  \n",
       "26     12.651223    14.9732  \n",
       "27     12.808642  15.141138  \n",
       "28     12.960561   15.32926  \n",
       "29     13.105069  15.512875  \n",
       "mean   10.015465  12.812303  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/30_30_1_50_1_50_datt_seq2seq_gru_3.csv\n",
      "\n",
      "\n",
      "3th iteration\n",
      "history size: 40\n",
      "future size: 10\n",
      "Epoch 1/10000\n",
      "568/568 - 8s - loss: 0.2383 - val_loss: 0.1698 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "568/568 - 5s - loss: 0.1475 - val_loss: 0.1485 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "568/568 - 5s - loss: 0.1366 - val_loss: 0.1387 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "568/568 - 5s - loss: 0.1271 - val_loss: 0.1305 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "568/568 - 5s - loss: 0.1170 - val_loss: 0.1245 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "568/568 - 5s - loss: 0.1097 - val_loss: 0.1135 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "568/568 - 5s - loss: 0.1034 - val_loss: 0.1083 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "568/568 - 5s - loss: 0.0986 - val_loss: 0.1080 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "568/568 - 5s - loss: 0.0959 - val_loss: 0.1065 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "568/568 - 5s - loss: 0.0919 - val_loss: 0.0998 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "568/568 - 5s - loss: 0.0906 - val_loss: 0.0948 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "568/568 - 5s - loss: 0.0879 - val_loss: 0.0977 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "568/568 - 5s - loss: 0.0840 - val_loss: 0.0933 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "568/568 - 5s - loss: 0.0814 - val_loss: 0.0985 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "568/568 - 5s - loss: 0.0811 - val_loss: 0.0987 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "568/568 - 5s - loss: 0.0779 - val_loss: 0.0903 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "568/568 - 5s - loss: 0.0760 - val_loss: 0.0837 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "568/568 - 5s - loss: 0.0744 - val_loss: 0.0884 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "568/568 - 5s - loss: 0.0725 - val_loss: 0.0937 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "568/568 - 5s - loss: 0.0711 - val_loss: 0.0893 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "568/568 - 5s - loss: 0.0706 - val_loss: 0.0829 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "568/568 - 5s - loss: 0.0692 - val_loss: 0.0857 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "568/568 - 4s - loss: 0.0679 - val_loss: 0.0791 - 4s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "568/568 - 4s - loss: 0.0651 - val_loss: 0.0921 - 4s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "568/568 - 5s - loss: 0.0649 - val_loss: 0.0784 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "568/568 - 4s - loss: 0.0628 - val_loss: 0.0761 - 4s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "568/568 - 4s - loss: 0.0605 - val_loss: 0.0762 - 4s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "568/568 - 5s - loss: 0.0595 - val_loss: 0.0744 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "568/568 - 4s - loss: 0.0589 - val_loss: 0.0730 - 4s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "568/568 - 4s - loss: 0.0595 - val_loss: 0.0762 - 4s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "568/568 - 4s - loss: 0.0588 - val_loss: 0.0722 - 4s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "568/568 - 4s - loss: 0.0570 - val_loss: 0.0720 - 4s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "568/568 - 5s - loss: 0.0581 - val_loss: 0.0729 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "568/568 - 5s - loss: 0.0537 - val_loss: 0.0711 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "568/568 - 5s - loss: 0.0551 - val_loss: 0.0687 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "568/568 - 4s - loss: 0.0539 - val_loss: 0.0716 - 4s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "568/568 - 5s - loss: 0.0533 - val_loss: 0.0819 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "568/568 - 5s - loss: 0.0561 - val_loss: 0.0769 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "568/568 - 5s - loss: 0.0548 - val_loss: 0.0686 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "568/568 - 5s - loss: 0.0503 - val_loss: 0.0656 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "568/568 - 5s - loss: 0.0496 - val_loss: 0.0635 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "568/568 - 5s - loss: 0.0514 - val_loss: 0.0695 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "568/568 - 5s - loss: 0.0506 - val_loss: 0.0629 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "568/568 - 5s - loss: 0.0492 - val_loss: 0.0675 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "568/568 - 5s - loss: 0.0494 - val_loss: 0.0620 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "568/568 - 5s - loss: 0.0469 - val_loss: 0.0635 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "568/568 - 5s - loss: 0.0474 - val_loss: 0.0619 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "568/568 - 5s - loss: 0.0478 - val_loss: 0.0603 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "568/568 - 5s - loss: 0.0465 - val_loss: 0.0606 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "568/568 - 5s - loss: 0.0467 - val_loss: 0.0671 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "568/568 - 5s - loss: 0.0459 - val_loss: 0.0626 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "568/568 - 5s - loss: 0.0456 - val_loss: 0.0628 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "568/568 - 5s - loss: 0.0485 - val_loss: 0.0584 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "568/568 - 5s - loss: 0.0450 - val_loss: 0.0641 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "568/568 - 5s - loss: 0.0439 - val_loss: 0.0564 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "568/568 - 5s - loss: 0.0433 - val_loss: 0.0583 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "568/568 - 5s - loss: 0.0453 - val_loss: 0.0594 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "568/568 - 5s - loss: 0.0426 - val_loss: 0.0571 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "568/568 - 5s - loss: 0.0440 - val_loss: 0.0576 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "568/568 - 5s - loss: 0.0420 - val_loss: 0.0553 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "568/568 - 5s - loss: 0.0412 - val_loss: 0.0586 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "568/568 - 5s - loss: 0.0411 - val_loss: 0.0570 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "568/568 - 5s - loss: 0.0416 - val_loss: 0.0584 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "568/568 - 5s - loss: 0.0438 - val_loss: 0.0672 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "568/568 - 5s - loss: 0.0446 - val_loss: 0.0836 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "568/568 - 5s - loss: 0.0475 - val_loss: 0.0658 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "568/568 - 5s - loss: 0.0434 - val_loss: 0.0581 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "568/568 - 5s - loss: 0.0391 - val_loss: 0.0551 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "568/568 - 5s - loss: 0.0386 - val_loss: 0.0546 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "568/568 - 5s - loss: 0.0398 - val_loss: 0.0560 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "568/568 - 5s - loss: 0.0399 - val_loss: 0.0555 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "568/568 - 5s - loss: 0.0384 - val_loss: 0.0520 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "568/568 - 5s - loss: 0.0384 - val_loss: 0.0531 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "568/568 - 5s - loss: 0.0389 - val_loss: 0.0552 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "568/568 - 5s - loss: 0.0378 - val_loss: 0.0548 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "568/568 - 5s - loss: 0.0375 - val_loss: 0.0504 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "568/568 - 5s - loss: 0.0376 - val_loss: 0.0531 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "568/568 - 5s - loss: 0.0377 - val_loss: 0.0522 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "568/568 - 5s - loss: 0.0361 - val_loss: 0.0566 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "568/568 - 5s - loss: 0.0367 - val_loss: 0.0580 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "568/568 - 5s - loss: 0.0369 - val_loss: 0.0541 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "568/568 - 5s - loss: 0.0357 - val_loss: 0.0512 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "568/568 - 5s - loss: 0.0411 - val_loss: 0.0662 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "568/568 - 5s - loss: 0.0434 - val_loss: 0.0521 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "568/568 - 5s - loss: 0.0370 - val_loss: 0.0526 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "568/568 - 5s - loss: 0.0375 - val_loss: 0.0529 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_18_layer_call_fn, gru_cell_18_layer_call_and_return_conditional_losses, gru_cell_19_layer_call_fn, gru_cell_19_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_10_1_50_1_50_datt_seq2seq_gru_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_10_1_50_1_50_datt_seq2seq_gru_3\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002641EFB9C10> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002641886CC40> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.937027</td>\n",
       "      <td>0.945187</td>\n",
       "      <td>0.957435</td>\n",
       "      <td>0.94655</td>\n",
       "      <td>4.072344</td>\n",
       "      <td>4.120747</td>\n",
       "      <td>3.176498</td>\n",
       "      <td>3.789863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.864949</td>\n",
       "      <td>0.879094</td>\n",
       "      <td>0.910846</td>\n",
       "      <td>0.884963</td>\n",
       "      <td>5.926953</td>\n",
       "      <td>6.120718</td>\n",
       "      <td>4.59779</td>\n",
       "      <td>5.548487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.771093</td>\n",
       "      <td>0.823739</td>\n",
       "      <td>0.856892</td>\n",
       "      <td>0.817241</td>\n",
       "      <td>7.612152</td>\n",
       "      <td>7.390847</td>\n",
       "      <td>5.826038</td>\n",
       "      <td>6.943012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.672496</td>\n",
       "      <td>0.782091</td>\n",
       "      <td>0.820092</td>\n",
       "      <td>0.758226</td>\n",
       "      <td>8.984386</td>\n",
       "      <td>8.218939</td>\n",
       "      <td>6.533103</td>\n",
       "      <td>7.912143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.580225</td>\n",
       "      <td>0.760827</td>\n",
       "      <td>0.80179</td>\n",
       "      <td>0.714281</td>\n",
       "      <td>10.07113</td>\n",
       "      <td>8.611737</td>\n",
       "      <td>6.857835</td>\n",
       "      <td>8.513567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.49875</td>\n",
       "      <td>0.751166</td>\n",
       "      <td>0.793105</td>\n",
       "      <td>0.681007</td>\n",
       "      <td>10.897735</td>\n",
       "      <td>8.78485</td>\n",
       "      <td>7.007221</td>\n",
       "      <td>8.896602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.431799</td>\n",
       "      <td>0.741076</td>\n",
       "      <td>0.783106</td>\n",
       "      <td>0.651994</td>\n",
       "      <td>11.490996</td>\n",
       "      <td>8.961087</td>\n",
       "      <td>7.175813</td>\n",
       "      <td>9.209299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.377396</td>\n",
       "      <td>0.728083</td>\n",
       "      <td>0.767139</td>\n",
       "      <td>0.624206</td>\n",
       "      <td>11.9994</td>\n",
       "      <td>9.182491</td>\n",
       "      <td>7.43741</td>\n",
       "      <td>9.539767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.335504</td>\n",
       "      <td>0.711848</td>\n",
       "      <td>0.745822</td>\n",
       "      <td>0.597724</td>\n",
       "      <td>12.411018</td>\n",
       "      <td>9.451566</td>\n",
       "      <td>7.77285</td>\n",
       "      <td>9.878478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.299434</td>\n",
       "      <td>0.69115</td>\n",
       "      <td>0.720189</td>\n",
       "      <td>0.570258</td>\n",
       "      <td>12.758068</td>\n",
       "      <td>9.783493</td>\n",
       "      <td>8.158201</td>\n",
       "      <td>10.233254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.576867</td>\n",
       "      <td>0.781426</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.724645</td>\n",
       "      <td>9.622418</td>\n",
       "      <td>8.062648</td>\n",
       "      <td>6.454276</td>\n",
       "      <td>8.046447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3 TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE     nRMSE     nRMSE   \n",
       "0      0.937027  0.945187  0.957435   0.94655   4.072344  4.120747  3.176498   \n",
       "1      0.864949  0.879094  0.910846  0.884963   5.926953  6.120718   4.59779   \n",
       "2      0.771093  0.823739  0.856892  0.817241   7.612152  7.390847  5.826038   \n",
       "3      0.672496  0.782091  0.820092  0.758226   8.984386  8.218939  6.533103   \n",
       "4      0.580225  0.760827   0.80179  0.714281   10.07113  8.611737  6.857835   \n",
       "5       0.49875  0.751166  0.793105  0.681007  10.897735   8.78485  7.007221   \n",
       "6      0.431799  0.741076  0.783106  0.651994  11.490996  8.961087  7.175813   \n",
       "7      0.377396  0.728083  0.767139  0.624206    11.9994  9.182491   7.43741   \n",
       "8      0.335504  0.711848  0.745822  0.597724  12.411018  9.451566   7.77285   \n",
       "9      0.299434   0.69115  0.720189  0.570258  12.758068  9.783493  8.158201   \n",
       "mean   0.576867  0.781426  0.815642  0.724645   9.622418  8.062648  6.454276   \n",
       "\n",
       "            mean  \n",
       "index      nRMSE  \n",
       "0       3.789863  \n",
       "1       5.548487  \n",
       "2       6.943012  \n",
       "3       7.912143  \n",
       "4       8.513567  \n",
       "5       8.896602  \n",
       "6       9.209299  \n",
       "7       9.539767  \n",
       "8       9.878478  \n",
       "9      10.233254  \n",
       "mean    8.046447  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/40_10_1_50_1_50_datt_seq2seq_gru_3.csv\n",
      "\n",
      "\n",
      "3th iteration\n",
      "history size: 40\n",
      "future size: 20\n",
      "Epoch 1/10000\n",
      "563/563 - 9s - loss: 0.2991 - val_loss: 0.2495 - 9s/epoch - 16ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.2180 - val_loss: 0.2257 - 5s/epoch - 9ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.2027 - val_loss: 0.2221 - 5s/epoch - 9ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 5s - loss: 0.1905 - val_loss: 0.2016 - 5s/epoch - 9ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.1807 - val_loss: 0.2061 - 5s/epoch - 9ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.1711 - val_loss: 0.1962 - 5s/epoch - 9ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.1639 - val_loss: 0.1888 - 5s/epoch - 9ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.1578 - val_loss: 0.1694 - 5s/epoch - 9ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.1500 - val_loss: 0.1659 - 5s/epoch - 9ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.1450 - val_loss: 0.1576 - 5s/epoch - 9ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.1396 - val_loss: 0.1542 - 5s/epoch - 9ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.1328 - val_loss: 0.1546 - 5s/epoch - 9ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.1288 - val_loss: 0.1473 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.1234 - val_loss: 0.1366 - 5s/epoch - 9ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.1212 - val_loss: 0.1325 - 5s/epoch - 9ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.1179 - val_loss: 0.1292 - 5s/epoch - 9ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.1123 - val_loss: 0.1264 - 5s/epoch - 9ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.1091 - val_loss: 0.1339 - 5s/epoch - 9ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.1068 - val_loss: 0.1256 - 5s/epoch - 9ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.1024 - val_loss: 0.1215 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.1006 - val_loss: 0.1158 - 5s/epoch - 9ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.0967 - val_loss: 0.1224 - 5s/epoch - 9ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.0981 - val_loss: 0.1165 - 5s/epoch - 9ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.0927 - val_loss: 0.1151 - 5s/epoch - 9ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.0933 - val_loss: 0.1090 - 5s/epoch - 9ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 6s - loss: 0.0905 - val_loss: 0.1110 - 6s/epoch - 10ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.0879 - val_loss: 0.1098 - 5s/epoch - 9ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0882 - val_loss: 0.1091 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.0865 - val_loss: 0.1038 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.0887 - val_loss: 0.1093 - 5s/epoch - 9ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.0840 - val_loss: 0.1006 - 5s/epoch - 9ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.0811 - val_loss: 0.0983 - 5s/epoch - 9ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.0824 - val_loss: 0.1058 - 5s/epoch - 9ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.0788 - val_loss: 0.1041 - 5s/epoch - 9ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.0788 - val_loss: 0.0960 - 5s/epoch - 9ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0786 - val_loss: 0.0991 - 5s/epoch - 9ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0772 - val_loss: 0.1014 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0762 - val_loss: 0.1001 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.0789 - val_loss: 0.0974 - 5s/epoch - 9ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0742 - val_loss: 0.0921 - 5s/epoch - 9ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0726 - val_loss: 0.0891 - 5s/epoch - 9ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0709 - val_loss: 0.0963 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0732 - val_loss: 0.0910 - 5s/epoch - 9ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0711 - val_loss: 0.0908 - 5s/epoch - 9ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0690 - val_loss: 0.0880 - 5s/epoch - 9ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0689 - val_loss: 0.0851 - 5s/epoch - 9ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0670 - val_loss: 0.0861 - 5s/epoch - 9ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0666 - val_loss: 0.0833 - 5s/epoch - 9ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0674 - val_loss: 0.0832 - 5s/epoch - 9ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0659 - val_loss: 0.0854 - 5s/epoch - 9ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0659 - val_loss: 0.0805 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0661 - val_loss: 0.0815 - 5s/epoch - 9ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0637 - val_loss: 0.0793 - 5s/epoch - 9ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0620 - val_loss: 0.0802 - 5s/epoch - 9ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0619 - val_loss: 0.0844 - 5s/epoch - 9ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0641 - val_loss: 0.0837 - 5s/epoch - 9ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0597 - val_loss: 0.0782 - 5s/epoch - 9ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0616 - val_loss: 0.0786 - 5s/epoch - 9ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0598 - val_loss: 0.0800 - 5s/epoch - 9ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0585 - val_loss: 0.0781 - 5s/epoch - 9ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0591 - val_loss: 0.0784 - 5s/epoch - 9ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0607 - val_loss: 0.0767 - 5s/epoch - 9ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 5s - loss: 0.0568 - val_loss: 0.0819 - 5s/epoch - 9ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0608 - val_loss: 0.0739 - 5s/epoch - 9ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0574 - val_loss: 0.0807 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0555 - val_loss: 0.0745 - 5s/epoch - 9ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0553 - val_loss: 0.0730 - 5s/epoch - 9ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 5s - loss: 0.0574 - val_loss: 0.0785 - 5s/epoch - 9ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 5s - loss: 0.0548 - val_loss: 0.0833 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 5s - loss: 0.0562 - val_loss: 0.0789 - 5s/epoch - 9ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 5s - loss: 0.0561 - val_loss: 0.0712 - 5s/epoch - 9ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 5s - loss: 0.0546 - val_loss: 0.0733 - 5s/epoch - 9ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 5s - loss: 0.0528 - val_loss: 0.0786 - 5s/epoch - 9ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 5s - loss: 0.0599 - val_loss: 0.0766 - 5s/epoch - 9ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 5s - loss: 0.0530 - val_loss: 0.0735 - 5s/epoch - 9ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 5s - loss: 0.0513 - val_loss: 0.0700 - 5s/epoch - 9ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 5s - loss: 0.0515 - val_loss: 0.0715 - 5s/epoch - 9ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 5s - loss: 0.0516 - val_loss: 0.0670 - 5s/epoch - 9ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 5s - loss: 0.0517 - val_loss: 0.0716 - 5s/epoch - 9ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 5s - loss: 0.0549 - val_loss: 0.0665 - 5s/epoch - 9ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 5s - loss: 0.0545 - val_loss: 0.0676 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 5s - loss: 0.0492 - val_loss: 0.0639 - 5s/epoch - 9ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 5s - loss: 0.0501 - val_loss: 0.0719 - 5s/epoch - 9ms/step\n",
      "Epoch 84/10000\n",
      "563/563 - 5s - loss: 0.0494 - val_loss: 0.0702 - 5s/epoch - 9ms/step\n",
      "Epoch 85/10000\n",
      "563/563 - 5s - loss: 0.0496 - val_loss: 0.0638 - 5s/epoch - 9ms/step\n",
      "Epoch 86/10000\n",
      "563/563 - 5s - loss: 0.0474 - val_loss: 0.0665 - 5s/epoch - 9ms/step\n",
      "Epoch 87/10000\n",
      "563/563 - 5s - loss: 0.0524 - val_loss: 0.0755 - 5s/epoch - 9ms/step\n",
      "Epoch 88/10000\n",
      "563/563 - 5s - loss: 0.0518 - val_loss: 0.0682 - 5s/epoch - 9ms/step\n",
      "Epoch 89/10000\n",
      "563/563 - 5s - loss: 0.0495 - val_loss: 0.0835 - 5s/epoch - 9ms/step\n",
      "Epoch 90/10000\n",
      "563/563 - 5s - loss: 0.0483 - val_loss: 0.0723 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "563/563 - 5s - loss: 0.0473 - val_loss: 0.0633 - 5s/epoch - 9ms/step\n",
      "Epoch 92/10000\n",
      "563/563 - 5s - loss: 0.0475 - val_loss: 0.0708 - 5s/epoch - 9ms/step\n",
      "Epoch 93/10000\n",
      "563/563 - 5s - loss: 0.0476 - val_loss: 0.0636 - 5s/epoch - 9ms/step\n",
      "Epoch 94/10000\n",
      "563/563 - 5s - loss: 0.0477 - val_loss: 0.0614 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "563/563 - 5s - loss: 0.0462 - val_loss: 0.0630 - 5s/epoch - 9ms/step\n",
      "Epoch 96/10000\n",
      "563/563 - 5s - loss: 0.0472 - val_loss: 0.0640 - 5s/epoch - 9ms/step\n",
      "Epoch 97/10000\n",
      "563/563 - 5s - loss: 0.0464 - val_loss: 0.0634 - 5s/epoch - 9ms/step\n",
      "Epoch 98/10000\n",
      "563/563 - 5s - loss: 0.0471 - val_loss: 0.0651 - 5s/epoch - 9ms/step\n",
      "Epoch 99/10000\n",
      "563/563 - 5s - loss: 0.0457 - val_loss: 0.0591 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "563/563 - 5s - loss: 0.0467 - val_loss: 0.0601 - 5s/epoch - 9ms/step\n",
      "Epoch 101/10000\n",
      "563/563 - 5s - loss: 0.0465 - val_loss: 0.0626 - 5s/epoch - 9ms/step\n",
      "Epoch 102/10000\n",
      "563/563 - 5s - loss: 0.0436 - val_loss: 0.0584 - 5s/epoch - 9ms/step\n",
      "Epoch 103/10000\n",
      "563/563 - 5s - loss: 0.0501 - val_loss: 0.0655 - 5s/epoch - 9ms/step\n",
      "Epoch 104/10000\n",
      "563/563 - 5s - loss: 0.0458 - val_loss: 0.0667 - 5s/epoch - 9ms/step\n",
      "Epoch 105/10000\n",
      "563/563 - 5s - loss: 0.0445 - val_loss: 0.0588 - 5s/epoch - 9ms/step\n",
      "Epoch 106/10000\n",
      "563/563 - 5s - loss: 0.0433 - val_loss: 0.0576 - 5s/epoch - 9ms/step\n",
      "Epoch 107/10000\n",
      "563/563 - 5s - loss: 0.0442 - val_loss: 0.0568 - 5s/epoch - 9ms/step\n",
      "Epoch 108/10000\n",
      "563/563 - 5s - loss: 0.0438 - val_loss: 0.0615 - 5s/epoch - 8ms/step\n",
      "Epoch 109/10000\n",
      "563/563 - 5s - loss: 0.0448 - val_loss: 0.0606 - 5s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "563/563 - 5s - loss: 0.0439 - val_loss: 0.0568 - 5s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "563/563 - 5s - loss: 0.0422 - val_loss: 0.0581 - 5s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "563/563 - 5s - loss: 0.0437 - val_loss: 0.0625 - 5s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "563/563 - 5s - loss: 0.0454 - val_loss: 0.0736 - 5s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "563/563 - 5s - loss: 0.0455 - val_loss: 0.0590 - 5s/epoch - 9ms/step\n",
      "Epoch 115/10000\n",
      "563/563 - 5s - loss: 0.0460 - val_loss: 0.0621 - 5s/epoch - 9ms/step\n",
      "Epoch 116/10000\n",
      "563/563 - 5s - loss: 0.0420 - val_loss: 0.0584 - 5s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "563/563 - 5s - loss: 0.0412 - val_loss: 0.0576 - 5s/epoch - 9ms/step\n",
      "Epoch 118/10000\n",
      "563/563 - 5s - loss: 0.0426 - val_loss: 0.0578 - 5s/epoch - 9ms/step\n",
      "Epoch 119/10000\n",
      "563/563 - 5s - loss: 0.0416 - val_loss: 0.0591 - 5s/epoch - 9ms/step\n",
      "Epoch 120/10000\n",
      "563/563 - 5s - loss: 0.0406 - val_loss: 0.0599 - 5s/epoch - 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_20_layer_call_fn, gru_cell_20_layer_call_and_return_conditional_losses, gru_cell_21_layer_call_fn, gru_cell_21_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_20_1_50_1_50_datt_seq2seq_gru_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_20_1_50_1_50_datt_seq2seq_gru_3\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002643CE00BB0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000265F4825D00> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.848564</td>\n",
       "      <td>0.884945</td>\n",
       "      <td>0.940673</td>\n",
       "      <td>0.891394</td>\n",
       "      <td>7.678369</td>\n",
       "      <td>5.96423</td>\n",
       "      <td>3.741369</td>\n",
       "      <td>5.794656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.759927</td>\n",
       "      <td>0.819799</td>\n",
       "      <td>0.895445</td>\n",
       "      <td>0.825057</td>\n",
       "      <td>9.494114</td>\n",
       "      <td>7.466551</td>\n",
       "      <td>4.968264</td>\n",
       "      <td>7.309643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.619033</td>\n",
       "      <td>0.767386</td>\n",
       "      <td>0.852986</td>\n",
       "      <td>0.746468</td>\n",
       "      <td>11.438904</td>\n",
       "      <td>8.486464</td>\n",
       "      <td>5.893049</td>\n",
       "      <td>8.606139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.459897</td>\n",
       "      <td>0.726796</td>\n",
       "      <td>0.825471</td>\n",
       "      <td>0.670721</td>\n",
       "      <td>13.165168</td>\n",
       "      <td>9.200369</td>\n",
       "      <td>6.422294</td>\n",
       "      <td>9.595944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.318244</td>\n",
       "      <td>0.698154</td>\n",
       "      <td>0.812096</td>\n",
       "      <td>0.609498</td>\n",
       "      <td>14.554382</td>\n",
       "      <td>9.674926</td>\n",
       "      <td>6.665335</td>\n",
       "      <td>10.298215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.203404</td>\n",
       "      <td>0.675018</td>\n",
       "      <td>0.8063</td>\n",
       "      <td>0.561574</td>\n",
       "      <td>15.679439</td>\n",
       "      <td>10.043399</td>\n",
       "      <td>6.768543</td>\n",
       "      <td>10.83046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.113327</td>\n",
       "      <td>0.653127</td>\n",
       "      <td>0.803088</td>\n",
       "      <td>0.523181</td>\n",
       "      <td>16.2178</td>\n",
       "      <td>10.378511</td>\n",
       "      <td>6.825811</td>\n",
       "      <td>11.140707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.044904</td>\n",
       "      <td>0.628105</td>\n",
       "      <td>0.796255</td>\n",
       "      <td>0.489755</td>\n",
       "      <td>16.510133</td>\n",
       "      <td>10.748734</td>\n",
       "      <td>6.944575</td>\n",
       "      <td>11.401147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.004347</td>\n",
       "      <td>0.602323</td>\n",
       "      <td>0.779519</td>\n",
       "      <td>0.459165</td>\n",
       "      <td>16.680309</td>\n",
       "      <td>11.117013</td>\n",
       "      <td>7.225461</td>\n",
       "      <td>11.674261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.036932</td>\n",
       "      <td>0.576326</td>\n",
       "      <td>0.748333</td>\n",
       "      <td>0.429243</td>\n",
       "      <td>16.702294</td>\n",
       "      <td>11.474777</td>\n",
       "      <td>7.720745</td>\n",
       "      <td>11.965939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.063123</td>\n",
       "      <td>0.551349</td>\n",
       "      <td>0.706496</td>\n",
       "      <td>0.398241</td>\n",
       "      <td>16.734416</td>\n",
       "      <td>11.80883</td>\n",
       "      <td>8.339068</td>\n",
       "      <td>12.294105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.089754</td>\n",
       "      <td>0.528516</td>\n",
       "      <td>0.663281</td>\n",
       "      <td>0.367348</td>\n",
       "      <td>16.831963</td>\n",
       "      <td>12.104877</td>\n",
       "      <td>8.933398</td>\n",
       "      <td>12.623412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.116671</td>\n",
       "      <td>0.507171</td>\n",
       "      <td>0.622732</td>\n",
       "      <td>0.337744</td>\n",
       "      <td>16.802212</td>\n",
       "      <td>12.374949</td>\n",
       "      <td>9.458233</td>\n",
       "      <td>12.878465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.143943</td>\n",
       "      <td>0.487803</td>\n",
       "      <td>0.582061</td>\n",
       "      <td>0.308641</td>\n",
       "      <td>16.77468</td>\n",
       "      <td>12.616773</td>\n",
       "      <td>9.958045</td>\n",
       "      <td>13.116499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.172763</td>\n",
       "      <td>0.469508</td>\n",
       "      <td>0.540593</td>\n",
       "      <td>0.279113</td>\n",
       "      <td>16.811315</td>\n",
       "      <td>12.840624</td>\n",
       "      <td>10.443512</td>\n",
       "      <td>13.36515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.199395</td>\n",
       "      <td>0.449128</td>\n",
       "      <td>0.502984</td>\n",
       "      <td>0.250906</td>\n",
       "      <td>16.831137</td>\n",
       "      <td>13.084281</td>\n",
       "      <td>10.866357</td>\n",
       "      <td>13.593925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.219618</td>\n",
       "      <td>0.426829</td>\n",
       "      <td>0.469518</td>\n",
       "      <td>0.225576</td>\n",
       "      <td>16.807218</td>\n",
       "      <td>13.344493</td>\n",
       "      <td>11.230485</td>\n",
       "      <td>13.794065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.23169</td>\n",
       "      <td>0.405071</td>\n",
       "      <td>0.441862</td>\n",
       "      <td>0.205081</td>\n",
       "      <td>16.848898</td>\n",
       "      <td>13.592671</td>\n",
       "      <td>11.524326</td>\n",
       "      <td>13.988632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.2393</td>\n",
       "      <td>0.386135</td>\n",
       "      <td>0.42171</td>\n",
       "      <td>0.189515</td>\n",
       "      <td>16.921325</td>\n",
       "      <td>13.805445</td>\n",
       "      <td>11.735438</td>\n",
       "      <td>14.154069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.247292</td>\n",
       "      <td>0.369846</td>\n",
       "      <td>0.411193</td>\n",
       "      <td>0.177916</td>\n",
       "      <td>16.999606</td>\n",
       "      <td>13.985987</td>\n",
       "      <td>11.846342</td>\n",
       "      <td>14.277311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.080124</td>\n",
       "      <td>0.580667</td>\n",
       "      <td>0.68113</td>\n",
       "      <td>0.447307</td>\n",
       "      <td>15.324184</td>\n",
       "      <td>11.205695</td>\n",
       "      <td>8.375532</td>\n",
       "      <td>11.635137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.848564  0.884945  0.940673  0.891394   7.678369    5.96423   \n",
       "1      0.759927  0.819799  0.895445  0.825057   9.494114   7.466551   \n",
       "2      0.619033  0.767386  0.852986  0.746468  11.438904   8.486464   \n",
       "3      0.459897  0.726796  0.825471  0.670721  13.165168   9.200369   \n",
       "4      0.318244  0.698154  0.812096  0.609498  14.554382   9.674926   \n",
       "5      0.203404  0.675018    0.8063  0.561574  15.679439  10.043399   \n",
       "6      0.113327  0.653127  0.803088  0.523181    16.2178  10.378511   \n",
       "7      0.044904  0.628105  0.796255  0.489755  16.510133  10.748734   \n",
       "8     -0.004347  0.602323  0.779519  0.459165  16.680309  11.117013   \n",
       "9     -0.036932  0.576326  0.748333  0.429243  16.702294  11.474777   \n",
       "10    -0.063123  0.551349  0.706496  0.398241  16.734416   11.80883   \n",
       "11    -0.089754  0.528516  0.663281  0.367348  16.831963  12.104877   \n",
       "12    -0.116671  0.507171  0.622732  0.337744  16.802212  12.374949   \n",
       "13    -0.143943  0.487803  0.582061  0.308641   16.77468  12.616773   \n",
       "14    -0.172763  0.469508  0.540593  0.279113  16.811315  12.840624   \n",
       "15    -0.199395  0.449128  0.502984  0.250906  16.831137  13.084281   \n",
       "16    -0.219618  0.426829  0.469518  0.225576  16.807218  13.344493   \n",
       "17     -0.23169  0.405071  0.441862  0.205081  16.848898  13.592671   \n",
       "18      -0.2393  0.386135   0.42171  0.189515  16.921325  13.805445   \n",
       "19    -0.247292  0.369846  0.411193  0.177916  16.999606  13.985987   \n",
       "mean   0.080124  0.580667   0.68113  0.447307  15.324184  11.205695   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       3.741369   5.794656  \n",
       "1       4.968264   7.309643  \n",
       "2       5.893049   8.606139  \n",
       "3       6.422294   9.595944  \n",
       "4       6.665335  10.298215  \n",
       "5       6.768543   10.83046  \n",
       "6       6.825811  11.140707  \n",
       "7       6.944575  11.401147  \n",
       "8       7.225461  11.674261  \n",
       "9       7.720745  11.965939  \n",
       "10      8.339068  12.294105  \n",
       "11      8.933398  12.623412  \n",
       "12      9.458233  12.878465  \n",
       "13      9.958045  13.116499  \n",
       "14     10.443512   13.36515  \n",
       "15     10.866357  13.593925  \n",
       "16     11.230485  13.794065  \n",
       "17     11.524326  13.988632  \n",
       "18     11.735438  14.154069  \n",
       "19     11.846342  14.277311  \n",
       "mean    8.375532  11.635137  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/40_20_1_50_1_50_datt_seq2seq_gru_3.csv\n",
      "\n",
      "\n",
      "3th iteration\n",
      "history size: 40\n",
      "future size: 30\n",
      "Epoch 1/10000\n",
      "559/559 - 8s - loss: 0.3967 - val_loss: 0.3164 - 8s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "559/559 - 5s - loss: 0.2835 - val_loss: 0.2808 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "559/559 - 5s - loss: 0.2646 - val_loss: 0.2689 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "559/559 - 5s - loss: 0.2484 - val_loss: 0.2672 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "559/559 - 5s - loss: 0.2372 - val_loss: 0.2454 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "559/559 - 5s - loss: 0.2232 - val_loss: 0.2243 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "559/559 - 5s - loss: 0.2100 - val_loss: 0.2152 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "559/559 - 5s - loss: 0.1996 - val_loss: 0.2020 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "559/559 - 5s - loss: 0.1909 - val_loss: 0.1947 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "559/559 - 5s - loss: 0.1809 - val_loss: 0.1961 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "559/559 - 5s - loss: 0.1768 - val_loss: 0.1928 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "559/559 - 5s - loss: 0.1703 - val_loss: 0.1829 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "559/559 - 5s - loss: 0.1643 - val_loss: 0.1767 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "559/559 - 5s - loss: 0.1593 - val_loss: 0.1765 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "559/559 - 5s - loss: 0.1552 - val_loss: 0.1860 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "559/559 - 5s - loss: 0.1508 - val_loss: 0.1598 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "559/559 - 5s - loss: 0.1490 - val_loss: 0.1497 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "559/559 - 5s - loss: 0.1412 - val_loss: 0.1515 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "559/559 - 5s - loss: 0.1383 - val_loss: 0.1452 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "559/559 - 5s - loss: 0.1331 - val_loss: 0.1422 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "559/559 - 5s - loss: 0.1301 - val_loss: 0.1355 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "559/559 - 5s - loss: 0.1273 - val_loss: 0.1366 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "559/559 - 5s - loss: 0.1230 - val_loss: 0.1410 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "559/559 - 5s - loss: 0.1196 - val_loss: 0.1372 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "559/559 - 5s - loss: 0.1201 - val_loss: 0.1252 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "559/559 - 5s - loss: 0.1167 - val_loss: 0.1289 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "559/559 - 5s - loss: 0.1126 - val_loss: 0.1229 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "559/559 - 5s - loss: 0.1129 - val_loss: 0.1195 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "559/559 - 5s - loss: 0.1108 - val_loss: 0.1174 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "559/559 - 5s - loss: 0.1058 - val_loss: 0.1193 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "559/559 - 5s - loss: 0.1072 - val_loss: 0.1145 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "559/559 - 5s - loss: 0.1026 - val_loss: 0.1166 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "559/559 - 5s - loss: 0.1015 - val_loss: 0.1176 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "559/559 - 5s - loss: 0.1009 - val_loss: 0.1114 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "559/559 - 5s - loss: 0.1020 - val_loss: 0.1101 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "559/559 - 5s - loss: 0.0995 - val_loss: 0.1166 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "559/559 - 5s - loss: 0.0972 - val_loss: 0.1042 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "559/559 - 5s - loss: 0.0961 - val_loss: 0.1066 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "559/559 - 5s - loss: 0.0955 - val_loss: 0.1046 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "559/559 - 5s - loss: 0.0948 - val_loss: 0.1102 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "559/559 - 5s - loss: 0.0987 - val_loss: 0.1114 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "559/559 - 5s - loss: 0.0906 - val_loss: 0.1037 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "559/559 - 5s - loss: 0.0906 - val_loss: 0.0985 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "559/559 - 5s - loss: 0.0869 - val_loss: 0.0950 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "559/559 - 5s - loss: 0.0893 - val_loss: 0.0992 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "559/559 - 5s - loss: 0.0884 - val_loss: 0.1009 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "559/559 - 5s - loss: 0.0878 - val_loss: 0.1032 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "559/559 - 5s - loss: 0.0949 - val_loss: 0.0964 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "559/559 - 5s - loss: 0.0851 - val_loss: 0.0949 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "559/559 - 5s - loss: 0.0841 - val_loss: 0.0963 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "559/559 - 5s - loss: 0.0841 - val_loss: 0.0948 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "559/559 - 5s - loss: 0.0817 - val_loss: 0.0928 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "559/559 - 5s - loss: 0.0839 - val_loss: 0.0912 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "559/559 - 5s - loss: 0.0813 - val_loss: 0.0939 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "559/559 - 5s - loss: 0.0815 - val_loss: 0.0900 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "559/559 - 5s - loss: 0.0804 - val_loss: 0.0945 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "559/559 - 5s - loss: 0.0822 - val_loss: 0.0880 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "559/559 - 5s - loss: 0.0780 - val_loss: 0.0883 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "559/559 - 5s - loss: 0.0781 - val_loss: 0.0900 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "559/559 - 5s - loss: 0.0973 - val_loss: 0.1065 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "559/559 - 5s - loss: 0.0830 - val_loss: 0.0883 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "559/559 - 5s - loss: 0.0765 - val_loss: 0.0849 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "559/559 - 5s - loss: 0.0760 - val_loss: 0.0895 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "559/559 - 5s - loss: 0.0762 - val_loss: 0.0886 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "559/559 - 5s - loss: 0.0753 - val_loss: 0.0896 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "559/559 - 5s - loss: 0.0759 - val_loss: 0.0936 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "559/559 - 5s - loss: 0.0747 - val_loss: 0.0903 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "559/559 - 5s - loss: 0.0732 - val_loss: 0.0835 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "559/559 - 5s - loss: 0.0746 - val_loss: 0.0856 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "559/559 - 5s - loss: 0.0718 - val_loss: 0.0900 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "559/559 - 5s - loss: 0.0724 - val_loss: 0.0819 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "559/559 - 5s - loss: 0.0737 - val_loss: 0.0808 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "559/559 - 5s - loss: 0.0703 - val_loss: 0.0854 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "559/559 - 5s - loss: 0.0705 - val_loss: 0.0825 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "559/559 - 5s - loss: 0.0691 - val_loss: 0.0816 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "559/559 - 5s - loss: 0.0698 - val_loss: 0.0788 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "559/559 - 5s - loss: 0.0700 - val_loss: 0.0837 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "559/559 - 5s - loss: 0.0694 - val_loss: 0.0792 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "559/559 - 5s - loss: 0.0689 - val_loss: 0.0800 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "559/559 - 5s - loss: 0.0673 - val_loss: 0.0789 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "559/559 - 5s - loss: 0.0692 - val_loss: 0.0774 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "559/559 - 5s - loss: 0.0669 - val_loss: 0.0832 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "559/559 - 5s - loss: 0.0677 - val_loss: 0.0780 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "559/559 - 5s - loss: 0.0663 - val_loss: 0.0758 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "559/559 - 5s - loss: 0.0658 - val_loss: 0.0766 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "559/559 - 5s - loss: 0.0658 - val_loss: 0.0808 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "559/559 - 5s - loss: 0.0652 - val_loss: 0.0768 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "559/559 - 5s - loss: 0.0648 - val_loss: 0.0770 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "559/559 - 5s - loss: 0.0644 - val_loss: 0.0736 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "559/559 - 5s - loss: 0.0651 - val_loss: 0.0813 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "559/559 - 5s - loss: 0.0632 - val_loss: 0.0756 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "559/559 - 5s - loss: 0.0631 - val_loss: 0.0720 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "559/559 - 5s - loss: 0.0646 - val_loss: 0.0726 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "559/559 - 5s - loss: 0.0639 - val_loss: 0.0819 - 5s/epoch - 9ms/step\n",
      "Epoch 95/10000\n",
      "559/559 - 5s - loss: 0.0626 - val_loss: 0.0739 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "559/559 - 5s - loss: 0.0625 - val_loss: 0.0706 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "559/559 - 5s - loss: 0.0612 - val_loss: 0.0751 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "559/559 - 5s - loss: 0.0632 - val_loss: 0.0703 - 5s/epoch - 9ms/step\n",
      "Epoch 99/10000\n",
      "559/559 - 5s - loss: 0.0633 - val_loss: 0.0668 - 5s/epoch - 9ms/step\n",
      "Epoch 100/10000\n",
      "559/559 - 5s - loss: 0.0607 - val_loss: 0.0751 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "559/559 - 5s - loss: 0.0611 - val_loss: 0.0826 - 5s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "559/559 - 5s - loss: 0.0621 - val_loss: 0.0709 - 5s/epoch - 9ms/step\n",
      "Epoch 103/10000\n",
      "559/559 - 5s - loss: 0.0614 - val_loss: 0.0697 - 5s/epoch - 9ms/step\n",
      "Epoch 104/10000\n",
      "559/559 - 5s - loss: 0.0597 - val_loss: 0.0745 - 5s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "559/559 - 5s - loss: 0.0596 - val_loss: 0.0693 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "559/559 - 5s - loss: 0.0589 - val_loss: 0.0731 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "559/559 - 5s - loss: 0.0597 - val_loss: 0.0693 - 5s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "559/559 - 5s - loss: 0.0598 - val_loss: 0.0718 - 5s/epoch - 8ms/step\n",
      "Epoch 109/10000\n",
      "559/559 - 5s - loss: 0.0620 - val_loss: 0.0672 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_22_layer_call_fn, gru_cell_22_layer_call_and_return_conditional_losses, gru_cell_23_layer_call_fn, gru_cell_23_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_30_1_50_1_50_datt_seq2seq_gru_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_30_1_50_1_50_datt_seq2seq_gru_3\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002643D7BFE20> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002643CC202E0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.809986</td>\n",
       "      <td>0.858661</td>\n",
       "      <td>0.920271</td>\n",
       "      <td>0.862973</td>\n",
       "      <td>8.599178</td>\n",
       "      <td>6.610451</td>\n",
       "      <td>4.325114</td>\n",
       "      <td>6.511581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.713528</td>\n",
       "      <td>0.802197</td>\n",
       "      <td>0.883731</td>\n",
       "      <td>0.799819</td>\n",
       "      <td>10.56088</td>\n",
       "      <td>7.820689</td>\n",
       "      <td>5.225606</td>\n",
       "      <td>7.869058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.598569</td>\n",
       "      <td>0.744418</td>\n",
       "      <td>0.845187</td>\n",
       "      <td>0.729391</td>\n",
       "      <td>12.504205</td>\n",
       "      <td>8.890648</td>\n",
       "      <td>6.032727</td>\n",
       "      <td>9.142527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500985</td>\n",
       "      <td>0.697889</td>\n",
       "      <td>0.817344</td>\n",
       "      <td>0.672073</td>\n",
       "      <td>13.943769</td>\n",
       "      <td>9.66755</td>\n",
       "      <td>6.555347</td>\n",
       "      <td>10.055555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.41673</td>\n",
       "      <td>0.665142</td>\n",
       "      <td>0.792854</td>\n",
       "      <td>0.624909</td>\n",
       "      <td>15.075598</td>\n",
       "      <td>10.180035</td>\n",
       "      <td>6.983384</td>\n",
       "      <td>10.746339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.343306</td>\n",
       "      <td>0.641356</td>\n",
       "      <td>0.765965</td>\n",
       "      <td>0.583542</td>\n",
       "      <td>15.995531</td>\n",
       "      <td>10.538047</td>\n",
       "      <td>7.42465</td>\n",
       "      <td>11.319409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.279011</td>\n",
       "      <td>0.62284</td>\n",
       "      <td>0.737156</td>\n",
       "      <td>0.546336</td>\n",
       "      <td>16.758794</td>\n",
       "      <td>10.809238</td>\n",
       "      <td>7.87022</td>\n",
       "      <td>11.812751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.225007</td>\n",
       "      <td>0.606821</td>\n",
       "      <td>0.707253</td>\n",
       "      <td>0.513027</td>\n",
       "      <td>17.373736</td>\n",
       "      <td>11.038859</td>\n",
       "      <td>8.307956</td>\n",
       "      <td>12.240184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.181393</td>\n",
       "      <td>0.591987</td>\n",
       "      <td>0.674506</td>\n",
       "      <td>0.482629</td>\n",
       "      <td>17.854818</td>\n",
       "      <td>11.248163</td>\n",
       "      <td>8.762296</td>\n",
       "      <td>12.621759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.144403</td>\n",
       "      <td>0.577165</td>\n",
       "      <td>0.637314</td>\n",
       "      <td>0.452961</td>\n",
       "      <td>18.253563</td>\n",
       "      <td>11.452524</td>\n",
       "      <td>9.252015</td>\n",
       "      <td>12.986034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.109649</td>\n",
       "      <td>0.561283</td>\n",
       "      <td>0.596913</td>\n",
       "      <td>0.422615</td>\n",
       "      <td>18.621614</td>\n",
       "      <td>11.667552</td>\n",
       "      <td>9.756562</td>\n",
       "      <td>13.348576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.076674</td>\n",
       "      <td>0.544733</td>\n",
       "      <td>0.557647</td>\n",
       "      <td>0.393018</td>\n",
       "      <td>18.614964</td>\n",
       "      <td>11.887413</td>\n",
       "      <td>10.224153</td>\n",
       "      <td>13.57551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.04568</td>\n",
       "      <td>0.526511</td>\n",
       "      <td>0.52354</td>\n",
       "      <td>0.365244</td>\n",
       "      <td>18.093116</td>\n",
       "      <td>12.125933</td>\n",
       "      <td>10.615014</td>\n",
       "      <td>13.611354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.019059</td>\n",
       "      <td>0.506541</td>\n",
       "      <td>0.493224</td>\n",
       "      <td>0.339608</td>\n",
       "      <td>17.724562</td>\n",
       "      <td>12.382498</td>\n",
       "      <td>10.951608</td>\n",
       "      <td>13.686222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.004933</td>\n",
       "      <td>0.486074</td>\n",
       "      <td>0.465495</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>17.646348</td>\n",
       "      <td>12.641227</td>\n",
       "      <td>11.251562</td>\n",
       "      <td>13.846379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.023937</td>\n",
       "      <td>0.465275</td>\n",
       "      <td>0.43836</td>\n",
       "      <td>0.293233</td>\n",
       "      <td>17.747366</td>\n",
       "      <td>12.898392</td>\n",
       "      <td>11.537443</td>\n",
       "      <td>14.061067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.03736</td>\n",
       "      <td>0.44451</td>\n",
       "      <td>0.41097</td>\n",
       "      <td>0.272706</td>\n",
       "      <td>17.509961</td>\n",
       "      <td>13.147548</td>\n",
       "      <td>11.819361</td>\n",
       "      <td>14.158957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.046297</td>\n",
       "      <td>0.422802</td>\n",
       "      <td>0.383503</td>\n",
       "      <td>0.253336</td>\n",
       "      <td>17.247362</td>\n",
       "      <td>13.403163</td>\n",
       "      <td>12.095212</td>\n",
       "      <td>14.248579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.053478</td>\n",
       "      <td>0.401449</td>\n",
       "      <td>0.356513</td>\n",
       "      <td>0.234828</td>\n",
       "      <td>17.050478</td>\n",
       "      <td>13.65075</td>\n",
       "      <td>12.360297</td>\n",
       "      <td>14.353842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.059975</td>\n",
       "      <td>0.379092</td>\n",
       "      <td>0.330751</td>\n",
       "      <td>0.216623</td>\n",
       "      <td>16.858175</td>\n",
       "      <td>13.904344</td>\n",
       "      <td>12.607853</td>\n",
       "      <td>14.456791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.066266</td>\n",
       "      <td>0.35643</td>\n",
       "      <td>0.307113</td>\n",
       "      <td>0.199092</td>\n",
       "      <td>16.737272</td>\n",
       "      <td>14.157866</td>\n",
       "      <td>12.831231</td>\n",
       "      <td>14.575456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.072434</td>\n",
       "      <td>0.333984</td>\n",
       "      <td>0.286617</td>\n",
       "      <td>0.182722</td>\n",
       "      <td>16.683493</td>\n",
       "      <td>14.403439</td>\n",
       "      <td>13.022613</td>\n",
       "      <td>14.703182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.079455</td>\n",
       "      <td>0.312752</td>\n",
       "      <td>0.269227</td>\n",
       "      <td>0.167508</td>\n",
       "      <td>16.512558</td>\n",
       "      <td>14.631057</td>\n",
       "      <td>13.183695</td>\n",
       "      <td>14.77577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.087086</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>0.255161</td>\n",
       "      <td>0.153859</td>\n",
       "      <td>16.351529</td>\n",
       "      <td>14.836342</td>\n",
       "      <td>13.313434</td>\n",
       "      <td>14.833768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.097224</td>\n",
       "      <td>0.274127</td>\n",
       "      <td>0.244911</td>\n",
       "      <td>0.140605</td>\n",
       "      <td>16.266027</td>\n",
       "      <td>15.040027</td>\n",
       "      <td>13.407603</td>\n",
       "      <td>14.904552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.107515</td>\n",
       "      <td>0.253416</td>\n",
       "      <td>0.237547</td>\n",
       "      <td>0.127816</td>\n",
       "      <td>16.183293</td>\n",
       "      <td>15.254017</td>\n",
       "      <td>13.476</td>\n",
       "      <td>14.971103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.116142</td>\n",
       "      <td>0.232017</td>\n",
       "      <td>0.231451</td>\n",
       "      <td>0.115775</td>\n",
       "      <td>16.090462</td>\n",
       "      <td>15.469591</td>\n",
       "      <td>13.53337</td>\n",
       "      <td>15.031141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.122598</td>\n",
       "      <td>0.2091</td>\n",
       "      <td>0.224372</td>\n",
       "      <td>0.103625</td>\n",
       "      <td>16.098633</td>\n",
       "      <td>15.697134</td>\n",
       "      <td>13.599776</td>\n",
       "      <td>15.131848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.126408</td>\n",
       "      <td>0.185567</td>\n",
       "      <td>0.216805</td>\n",
       "      <td>0.091988</td>\n",
       "      <td>16.14697</td>\n",
       "      <td>15.92834</td>\n",
       "      <td>13.66978</td>\n",
       "      <td>15.248364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.130557</td>\n",
       "      <td>0.162256</td>\n",
       "      <td>0.209809</td>\n",
       "      <td>0.080503</td>\n",
       "      <td>16.20004</td>\n",
       "      <td>16.154624</td>\n",
       "      <td>13.734224</td>\n",
       "      <td>15.362963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.107744</td>\n",
       "      <td>0.471997</td>\n",
       "      <td>0.49405</td>\n",
       "      <td>0.35793</td>\n",
       "      <td>16.243477</td>\n",
       "      <td>12.584582</td>\n",
       "      <td>10.591004</td>\n",
       "      <td>13.139687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.809986  0.858661  0.920271  0.862973   8.599178   6.610451   \n",
       "1      0.713528  0.802197  0.883731  0.799819   10.56088   7.820689   \n",
       "2      0.598569  0.744418  0.845187  0.729391  12.504205   8.890648   \n",
       "3      0.500985  0.697889  0.817344  0.672073  13.943769    9.66755   \n",
       "4       0.41673  0.665142  0.792854  0.624909  15.075598  10.180035   \n",
       "5      0.343306  0.641356  0.765965  0.583542  15.995531  10.538047   \n",
       "6      0.279011   0.62284  0.737156  0.546336  16.758794  10.809238   \n",
       "7      0.225007  0.606821  0.707253  0.513027  17.373736  11.038859   \n",
       "8      0.181393  0.591987  0.674506  0.482629  17.854818  11.248163   \n",
       "9      0.144403  0.577165  0.637314  0.452961  18.253563  11.452524   \n",
       "10     0.109649  0.561283  0.596913  0.422615  18.621614  11.667552   \n",
       "11     0.076674  0.544733  0.557647  0.393018  18.614964  11.887413   \n",
       "12      0.04568  0.526511   0.52354  0.365244  18.093116  12.125933   \n",
       "13     0.019059  0.506541  0.493224  0.339608  17.724562  12.382498   \n",
       "14    -0.004933  0.486074  0.465495  0.315545  17.646348  12.641227   \n",
       "15    -0.023937  0.465275   0.43836  0.293233  17.747366  12.898392   \n",
       "16     -0.03736   0.44451   0.41097  0.272706  17.509961  13.147548   \n",
       "17    -0.046297  0.422802  0.383503  0.253336  17.247362  13.403163   \n",
       "18    -0.053478  0.401449  0.356513  0.234828  17.050478   13.65075   \n",
       "19    -0.059975  0.379092  0.330751  0.216623  16.858175  13.904344   \n",
       "20    -0.066266   0.35643  0.307113  0.199092  16.737272  14.157866   \n",
       "21    -0.072434  0.333984  0.286617  0.182722  16.683493  14.403439   \n",
       "22    -0.079455  0.312752  0.269227  0.167508  16.512558  14.631057   \n",
       "23    -0.087086  0.293503  0.255161  0.153859  16.351529  14.836342   \n",
       "24    -0.097224  0.274127  0.244911  0.140605  16.266027  15.040027   \n",
       "25    -0.107515  0.253416  0.237547  0.127816  16.183293  15.254017   \n",
       "26    -0.116142  0.232017  0.231451  0.115775  16.090462  15.469591   \n",
       "27    -0.122598    0.2091  0.224372  0.103625  16.098633  15.697134   \n",
       "28    -0.126408  0.185567  0.216805  0.091988   16.14697   15.92834   \n",
       "29    -0.130557  0.162256  0.209809  0.080503   16.20004  16.154624   \n",
       "mean   0.107744  0.471997   0.49405   0.35793  16.243477  12.584582   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       4.325114   6.511581  \n",
       "1       5.225606   7.869058  \n",
       "2       6.032727   9.142527  \n",
       "3       6.555347  10.055555  \n",
       "4       6.983384  10.746339  \n",
       "5        7.42465  11.319409  \n",
       "6        7.87022  11.812751  \n",
       "7       8.307956  12.240184  \n",
       "8       8.762296  12.621759  \n",
       "9       9.252015  12.986034  \n",
       "10      9.756562  13.348576  \n",
       "11     10.224153   13.57551  \n",
       "12     10.615014  13.611354  \n",
       "13     10.951608  13.686222  \n",
       "14     11.251562  13.846379  \n",
       "15     11.537443  14.061067  \n",
       "16     11.819361  14.158957  \n",
       "17     12.095212  14.248579  \n",
       "18     12.360297  14.353842  \n",
       "19     12.607853  14.456791  \n",
       "20     12.831231  14.575456  \n",
       "21     13.022613  14.703182  \n",
       "22     13.183695   14.77577  \n",
       "23     13.313434  14.833768  \n",
       "24     13.407603  14.904552  \n",
       "25        13.476  14.971103  \n",
       "26      13.53337  15.031141  \n",
       "27     13.599776  15.131848  \n",
       "28      13.66978  15.248364  \n",
       "29     13.734224  15.362963  \n",
       "mean   10.591004  13.139687  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/40_30_1_50_1_50_datt_seq2seq_gru_3.csv\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4th iteration\n",
      "history size: 10\n",
      "future size: 10\n",
      "Epoch 1/10000\n",
      "581/581 - 8s - loss: 0.2314 - val_loss: 0.1500 - 8s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "581/581 - 5s - loss: 0.1513 - val_loss: 0.1340 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "581/581 - 5s - loss: 0.1407 - val_loss: 0.1304 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "581/581 - 5s - loss: 0.1315 - val_loss: 0.1162 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "581/581 - 5s - loss: 0.1239 - val_loss: 0.1314 - 5s/epoch - 9ms/step\n",
      "Epoch 6/10000\n",
      "581/581 - 5s - loss: 0.1173 - val_loss: 0.1061 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "581/581 - 5s - loss: 0.1113 - val_loss: 0.1010 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "581/581 - 5s - loss: 0.1069 - val_loss: 0.0971 - 5s/epoch - 9ms/step\n",
      "Epoch 9/10000\n",
      "581/581 - 5s - loss: 0.1015 - val_loss: 0.1010 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "581/581 - 5s - loss: 0.0997 - val_loss: 0.0930 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "581/581 - 5s - loss: 0.0964 - val_loss: 0.0929 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "581/581 - 5s - loss: 0.0923 - val_loss: 0.0910 - 5s/epoch - 9ms/step\n",
      "Epoch 13/10000\n",
      "581/581 - 5s - loss: 0.0918 - val_loss: 0.0882 - 5s/epoch - 9ms/step\n",
      "Epoch 14/10000\n",
      "581/581 - 5s - loss: 0.0896 - val_loss: 0.0869 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "581/581 - 5s - loss: 0.0870 - val_loss: 0.0835 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "581/581 - 5s - loss: 0.0847 - val_loss: 0.0836 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "581/581 - 5s - loss: 0.0818 - val_loss: 0.0825 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "581/581 - 5s - loss: 0.0837 - val_loss: 0.0851 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "581/581 - 5s - loss: 0.0793 - val_loss: 0.0873 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "581/581 - 5s - loss: 0.0779 - val_loss: 0.0820 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "581/581 - 5s - loss: 0.0769 - val_loss: 0.0802 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "581/581 - 5s - loss: 0.0751 - val_loss: 0.0809 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "581/581 - 5s - loss: 0.0734 - val_loss: 0.0795 - 5s/epoch - 9ms/step\n",
      "Epoch 24/10000\n",
      "581/581 - 5s - loss: 0.0732 - val_loss: 0.0767 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "581/581 - 5s - loss: 0.0720 - val_loss: 0.0772 - 5s/epoch - 9ms/step\n",
      "Epoch 26/10000\n",
      "581/581 - 5s - loss: 0.0717 - val_loss: 0.0755 - 5s/epoch - 9ms/step\n",
      "Epoch 27/10000\n",
      "581/581 - 5s - loss: 0.0695 - val_loss: 0.0776 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "581/581 - 5s - loss: 0.0695 - val_loss: 0.0853 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "581/581 - 5s - loss: 0.0686 - val_loss: 0.0724 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "581/581 - 5s - loss: 0.0678 - val_loss: 0.0716 - 5s/epoch - 9ms/step\n",
      "Epoch 31/10000\n",
      "581/581 - 5s - loss: 0.0670 - val_loss: 0.0702 - 5s/epoch - 9ms/step\n",
      "Epoch 32/10000\n",
      "581/581 - 5s - loss: 0.0642 - val_loss: 0.0749 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "581/581 - 5s - loss: 0.0646 - val_loss: 0.0708 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "581/581 - 5s - loss: 0.0636 - val_loss: 0.0714 - 5s/epoch - 9ms/step\n",
      "Epoch 35/10000\n",
      "581/581 - 5s - loss: 0.0645 - val_loss: 0.0708 - 5s/epoch - 9ms/step\n",
      "Epoch 36/10000\n",
      "581/581 - 5s - loss: 0.0658 - val_loss: 0.0713 - 5s/epoch - 9ms/step\n",
      "Epoch 37/10000\n",
      "581/581 - 5s - loss: 0.0619 - val_loss: 0.0664 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "581/581 - 5s - loss: 0.0602 - val_loss: 0.0684 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "581/581 - 5s - loss: 0.0613 - val_loss: 0.0687 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "581/581 - 5s - loss: 0.0600 - val_loss: 0.0677 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "581/581 - 5s - loss: 0.0588 - val_loss: 0.0684 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "581/581 - 5s - loss: 0.0584 - val_loss: 0.0684 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "581/581 - 5s - loss: 0.0570 - val_loss: 0.0670 - 5s/epoch - 9ms/step\n",
      "Epoch 44/10000\n",
      "581/581 - 5s - loss: 0.0587 - val_loss: 0.0697 - 5s/epoch - 9ms/step\n",
      "Epoch 45/10000\n",
      "581/581 - 5s - loss: 0.0556 - val_loss: 0.0685 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "581/581 - 5s - loss: 0.0553 - val_loss: 0.0658 - 5s/epoch - 9ms/step\n",
      "Epoch 47/10000\n",
      "581/581 - 5s - loss: 0.0561 - val_loss: 0.0660 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "581/581 - 5s - loss: 0.0568 - val_loss: 0.0658 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "581/581 - 5s - loss: 0.0553 - val_loss: 0.0643 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "581/581 - 5s - loss: 0.0540 - val_loss: 0.0650 - 5s/epoch - 9ms/step\n",
      "Epoch 51/10000\n",
      "581/581 - 5s - loss: 0.0523 - val_loss: 0.0668 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "581/581 - 5s - loss: 0.0535 - val_loss: 0.0648 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "581/581 - 5s - loss: 0.0546 - val_loss: 0.0712 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "581/581 - 5s - loss: 0.0518 - val_loss: 0.0618 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "581/581 - 5s - loss: 0.0520 - val_loss: 0.0627 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "581/581 - 5s - loss: 0.0511 - val_loss: 0.0738 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "581/581 - 5s - loss: 0.0522 - val_loss: 0.0632 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "581/581 - 5s - loss: 0.0502 - val_loss: 0.0659 - 5s/epoch - 9ms/step\n",
      "Epoch 59/10000\n",
      "581/581 - 5s - loss: 0.0523 - val_loss: 0.0667 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "581/581 - 5s - loss: 0.0511 - val_loss: 0.0618 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "581/581 - 5s - loss: 0.0480 - val_loss: 0.0631 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "581/581 - 5s - loss: 0.0484 - val_loss: 0.0734 - 5s/epoch - 9ms/step\n",
      "Epoch 63/10000\n",
      "581/581 - 5s - loss: 0.0513 - val_loss: 0.0624 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "581/581 - 5s - loss: 0.0475 - val_loss: 0.0627 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "581/581 - 5s - loss: 0.0475 - val_loss: 0.0630 - 5s/epoch - 9ms/step\n",
      "Epoch 66/10000\n",
      "581/581 - 5s - loss: 0.0470 - val_loss: 0.0615 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "581/581 - 5s - loss: 0.0467 - val_loss: 0.0616 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "581/581 - 5s - loss: 0.0507 - val_loss: 0.0657 - 5s/epoch - 9ms/step\n",
      "Epoch 69/10000\n",
      "581/581 - 5s - loss: 0.0485 - val_loss: 0.0619 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "581/581 - 5s - loss: 0.0457 - val_loss: 0.0625 - 5s/epoch - 9ms/step\n",
      "Epoch 71/10000\n",
      "581/581 - 5s - loss: 0.0459 - val_loss: 0.0615 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "581/581 - 5s - loss: 0.0453 - val_loss: 0.0620 - 5s/epoch - 9ms/step\n",
      "Epoch 73/10000\n",
      "581/581 - 5s - loss: 0.0471 - val_loss: 0.0653 - 5s/epoch - 9ms/step\n",
      "Epoch 74/10000\n",
      "581/581 - 5s - loss: 0.0445 - val_loss: 0.0596 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "581/581 - 5s - loss: 0.0480 - val_loss: 0.0675 - 5s/epoch - 9ms/step\n",
      "Epoch 76/10000\n",
      "581/581 - 5s - loss: 0.0475 - val_loss: 0.0586 - 5s/epoch - 9ms/step\n",
      "Epoch 77/10000\n",
      "581/581 - 5s - loss: 0.0437 - val_loss: 0.0567 - 5s/epoch - 9ms/step\n",
      "Epoch 78/10000\n",
      "581/581 - 5s - loss: 0.0442 - val_loss: 0.0603 - 5s/epoch - 9ms/step\n",
      "Epoch 79/10000\n",
      "581/581 - 5s - loss: 0.0473 - val_loss: 0.0601 - 5s/epoch - 9ms/step\n",
      "Epoch 80/10000\n",
      "581/581 - 5s - loss: 0.0444 - val_loss: 0.0602 - 5s/epoch - 9ms/step\n",
      "Epoch 81/10000\n",
      "581/581 - 5s - loss: 0.0424 - val_loss: 0.0597 - 5s/epoch - 9ms/step\n",
      "Epoch 82/10000\n",
      "581/581 - 5s - loss: 0.0470 - val_loss: 0.0607 - 5s/epoch - 9ms/step\n",
      "Epoch 83/10000\n",
      "581/581 - 5s - loss: 0.0438 - val_loss: 0.0610 - 5s/epoch - 9ms/step\n",
      "Epoch 84/10000\n",
      "581/581 - 5s - loss: 0.0421 - val_loss: 0.0595 - 5s/epoch - 9ms/step\n",
      "Epoch 85/10000\n",
      "581/581 - 5s - loss: 0.0417 - val_loss: 0.0580 - 5s/epoch - 9ms/step\n",
      "Epoch 86/10000\n",
      "581/581 - 5s - loss: 0.0440 - val_loss: 0.0578 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "581/581 - 5s - loss: 0.0419 - val_loss: 0.0626 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_24_layer_call_fn, gru_cell_24_layer_call_and_return_conditional_losses, gru_cell_25_layer_call_fn, gru_cell_25_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_10_1_50_1_50_datt_seq2seq_gru_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_10_1_50_1_50_datt_seq2seq_gru_4\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002643CAE1EE0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002643CB335B0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.942859</td>\n",
       "      <td>0.948061</td>\n",
       "      <td>0.966412</td>\n",
       "      <td>0.952444</td>\n",
       "      <td>3.88124</td>\n",
       "      <td>3.985366</td>\n",
       "      <td>2.857462</td>\n",
       "      <td>3.574689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.861988</td>\n",
       "      <td>0.901767</td>\n",
       "      <td>0.932465</td>\n",
       "      <td>0.89874</td>\n",
       "      <td>5.99326</td>\n",
       "      <td>5.481499</td>\n",
       "      <td>4.050819</td>\n",
       "      <td>5.175192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.754878</td>\n",
       "      <td>0.858826</td>\n",
       "      <td>0.884269</td>\n",
       "      <td>0.832658</td>\n",
       "      <td>7.878535</td>\n",
       "      <td>6.571826</td>\n",
       "      <td>5.301479</td>\n",
       "      <td>6.583947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.638922</td>\n",
       "      <td>0.827545</td>\n",
       "      <td>0.833672</td>\n",
       "      <td>0.766713</td>\n",
       "      <td>9.435344</td>\n",
       "      <td>7.264515</td>\n",
       "      <td>6.353937</td>\n",
       "      <td>7.684599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.525575</td>\n",
       "      <td>0.804106</td>\n",
       "      <td>0.795831</td>\n",
       "      <td>0.708504</td>\n",
       "      <td>10.709665</td>\n",
       "      <td>7.743547</td>\n",
       "      <td>7.037462</td>\n",
       "      <td>8.496891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.421047</td>\n",
       "      <td>0.784077</td>\n",
       "      <td>0.776569</td>\n",
       "      <td>0.660564</td>\n",
       "      <td>11.716914</td>\n",
       "      <td>8.131158</td>\n",
       "      <td>7.360119</td>\n",
       "      <td>9.069397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.328147</td>\n",
       "      <td>0.765192</td>\n",
       "      <td>0.768267</td>\n",
       "      <td>0.620535</td>\n",
       "      <td>12.502409</td>\n",
       "      <td>8.480019</td>\n",
       "      <td>7.494252</td>\n",
       "      <td>9.492227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.247644</td>\n",
       "      <td>0.746951</td>\n",
       "      <td>0.761378</td>\n",
       "      <td>0.585324</td>\n",
       "      <td>13.198842</td>\n",
       "      <td>8.803567</td>\n",
       "      <td>7.604083</td>\n",
       "      <td>9.868831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.183071</td>\n",
       "      <td>0.728109</td>\n",
       "      <td>0.748633</td>\n",
       "      <td>0.553271</td>\n",
       "      <td>13.769521</td>\n",
       "      <td>9.125866</td>\n",
       "      <td>7.803923</td>\n",
       "      <td>10.233104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.131125</td>\n",
       "      <td>0.710081</td>\n",
       "      <td>0.727607</td>\n",
       "      <td>0.522938</td>\n",
       "      <td>14.217225</td>\n",
       "      <td>9.423861</td>\n",
       "      <td>8.123161</td>\n",
       "      <td>10.588083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.503526</td>\n",
       "      <td>0.807471</td>\n",
       "      <td>0.81951</td>\n",
       "      <td>0.710169</td>\n",
       "      <td>10.330296</td>\n",
       "      <td>7.501122</td>\n",
       "      <td>6.39867</td>\n",
       "      <td>8.076696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3 TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE     nRMSE     nRMSE   \n",
       "0      0.942859  0.948061  0.966412  0.952444    3.88124  3.985366  2.857462   \n",
       "1      0.861988  0.901767  0.932465   0.89874    5.99326  5.481499  4.050819   \n",
       "2      0.754878  0.858826  0.884269  0.832658   7.878535  6.571826  5.301479   \n",
       "3      0.638922  0.827545  0.833672  0.766713   9.435344  7.264515  6.353937   \n",
       "4      0.525575  0.804106  0.795831  0.708504  10.709665  7.743547  7.037462   \n",
       "5      0.421047  0.784077  0.776569  0.660564  11.716914  8.131158  7.360119   \n",
       "6      0.328147  0.765192  0.768267  0.620535  12.502409  8.480019  7.494252   \n",
       "7      0.247644  0.746951  0.761378  0.585324  13.198842  8.803567  7.604083   \n",
       "8      0.183071  0.728109  0.748633  0.553271  13.769521  9.125866  7.803923   \n",
       "9      0.131125  0.710081  0.727607  0.522938  14.217225  9.423861  8.123161   \n",
       "mean   0.503526  0.807471   0.81951  0.710169  10.330296  7.501122   6.39867   \n",
       "\n",
       "            mean  \n",
       "index      nRMSE  \n",
       "0       3.574689  \n",
       "1       5.175192  \n",
       "2       6.583947  \n",
       "3       7.684599  \n",
       "4       8.496891  \n",
       "5       9.069397  \n",
       "6       9.492227  \n",
       "7       9.868831  \n",
       "8      10.233104  \n",
       "9      10.588083  \n",
       "mean    8.076696  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/10_10_1_50_1_50_datt_seq2seq_gru_4.csv\n",
      "\n",
      "\n",
      "4th iteration\n",
      "history size: 10\n",
      "future size: 20\n",
      "Epoch 1/10000\n",
      "577/577 - 8s - loss: 0.3284 - val_loss: 0.2709 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "577/577 - 4s - loss: 0.2271 - val_loss: 0.2474 - 4s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "577/577 - 4s - loss: 0.2138 - val_loss: 0.2265 - 4s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "577/577 - 4s - loss: 0.2033 - val_loss: 0.2251 - 4s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "577/577 - 4s - loss: 0.1943 - val_loss: 0.2162 - 4s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "577/577 - 5s - loss: 0.1867 - val_loss: 0.1991 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "577/577 - 5s - loss: 0.1787 - val_loss: 0.1994 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "577/577 - 4s - loss: 0.1712 - val_loss: 0.1990 - 4s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "577/577 - 4s - loss: 0.1663 - val_loss: 0.1784 - 4s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "577/577 - 4s - loss: 0.1616 - val_loss: 0.1749 - 4s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "577/577 - 5s - loss: 0.1565 - val_loss: 0.1824 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "577/577 - 4s - loss: 0.1534 - val_loss: 0.1658 - 4s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "577/577 - 4s - loss: 0.1480 - val_loss: 0.1695 - 4s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "577/577 - 4s - loss: 0.1428 - val_loss: 0.1679 - 4s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "577/577 - 5s - loss: 0.1380 - val_loss: 0.1577 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "577/577 - 5s - loss: 0.1336 - val_loss: 0.1463 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "577/577 - 4s - loss: 0.1297 - val_loss: 0.1449 - 4s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "577/577 - 4s - loss: 0.1271 - val_loss: 0.1407 - 4s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "577/577 - 5s - loss: 0.1259 - val_loss: 0.1452 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "577/577 - 5s - loss: 0.1211 - val_loss: 0.1362 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "577/577 - 4s - loss: 0.1191 - val_loss: 0.1354 - 4s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "577/577 - 4s - loss: 0.1146 - val_loss: 0.1509 - 4s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "577/577 - 5s - loss: 0.1152 - val_loss: 0.1280 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "577/577 - 4s - loss: 0.1112 - val_loss: 0.1373 - 4s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "577/577 - 4s - loss: 0.1138 - val_loss: 0.1289 - 4s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "577/577 - 4s - loss: 0.1080 - val_loss: 0.1375 - 4s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "577/577 - 4s - loss: 0.1075 - val_loss: 0.1210 - 4s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "577/577 - 5s - loss: 0.1071 - val_loss: 0.1375 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "577/577 - 4s - loss: 0.1061 - val_loss: 0.1430 - 4s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "577/577 - 4s - loss: 0.1032 - val_loss: 0.1185 - 4s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "577/577 - 4s - loss: 0.1002 - val_loss: 0.1213 - 4s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "577/577 - 5s - loss: 0.0997 - val_loss: 0.1185 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "577/577 - 4s - loss: 0.1034 - val_loss: 0.1214 - 4s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "577/577 - 4s - loss: 0.1020 - val_loss: 0.1172 - 4s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "577/577 - 4s - loss: 0.0978 - val_loss: 0.1100 - 4s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "577/577 - 4s - loss: 0.0956 - val_loss: 0.1140 - 4s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "577/577 - 4s - loss: 0.0942 - val_loss: 0.1212 - 4s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "577/577 - 4s - loss: 0.0960 - val_loss: 0.1109 - 4s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "577/577 - 4s - loss: 0.0976 - val_loss: 0.1246 - 4s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "577/577 - 4s - loss: 0.0928 - val_loss: 0.1077 - 4s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "577/577 - 4s - loss: 0.0900 - val_loss: 0.1082 - 4s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "577/577 - 4s - loss: 0.0927 - val_loss: 0.1080 - 4s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "577/577 - 4s - loss: 0.0915 - val_loss: 0.1122 - 4s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "577/577 - 5s - loss: 0.0890 - val_loss: 0.1064 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "577/577 - 4s - loss: 0.0874 - val_loss: 0.1082 - 4s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "577/577 - 4s - loss: 0.0865 - val_loss: 0.1108 - 4s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "577/577 - 4s - loss: 0.0868 - val_loss: 0.1077 - 4s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "577/577 - 4s - loss: 0.0882 - val_loss: 0.1024 - 4s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "577/577 - 4s - loss: 0.0834 - val_loss: 0.1070 - 4s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "577/577 - 4s - loss: 0.0847 - val_loss: 0.1050 - 4s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "577/577 - 4s - loss: 0.0838 - val_loss: 0.1050 - 4s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "577/577 - 4s - loss: 0.0814 - val_loss: 0.1021 - 4s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "577/577 - 4s - loss: 0.0815 - val_loss: 0.1018 - 4s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "577/577 - 4s - loss: 0.0816 - val_loss: 0.1022 - 4s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "577/577 - 4s - loss: 0.0829 - val_loss: 0.1013 - 4s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "577/577 - 5s - loss: 0.0809 - val_loss: 0.0991 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "577/577 - 4s - loss: 0.0797 - val_loss: 0.1003 - 4s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "577/577 - 4s - loss: 0.0777 - val_loss: 0.1033 - 4s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "577/577 - 5s - loss: 0.0778 - val_loss: 0.1031 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "577/577 - 4s - loss: 0.0778 - val_loss: 0.1009 - 4s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "577/577 - 5s - loss: 0.0771 - val_loss: 0.0973 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "577/577 - 5s - loss: 0.0773 - val_loss: 0.0973 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "577/577 - 4s - loss: 0.0817 - val_loss: 0.1012 - 4s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "577/577 - 5s - loss: 0.0749 - val_loss: 0.0988 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "577/577 - 4s - loss: 0.0744 - val_loss: 0.1005 - 4s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "577/577 - 4s - loss: 0.0752 - val_loss: 0.0986 - 4s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "577/577 - 5s - loss: 0.0797 - val_loss: 0.1149 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "577/577 - 4s - loss: 0.0761 - val_loss: 0.0938 - 4s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "577/577 - 4s - loss: 0.0731 - val_loss: 0.1035 - 4s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "577/577 - 4s - loss: 0.0730 - val_loss: 0.0931 - 4s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "577/577 - 4s - loss: 0.0756 - val_loss: 0.0934 - 4s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "577/577 - 5s - loss: 0.0697 - val_loss: 0.1014 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "577/577 - 5s - loss: 0.0707 - val_loss: 0.1307 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "577/577 - 5s - loss: 0.0774 - val_loss: 0.0967 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "577/577 - 4s - loss: 0.0727 - val_loss: 0.0960 - 4s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "577/577 - 4s - loss: 0.0736 - val_loss: 0.0920 - 4s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "577/577 - 4s - loss: 0.0673 - val_loss: 0.0894 - 4s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "577/577 - 4s - loss: 0.0700 - val_loss: 0.0999 - 4s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "577/577 - 4s - loss: 0.0696 - val_loss: 0.0916 - 4s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "577/577 - 4s - loss: 0.0691 - val_loss: 0.0987 - 4s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "577/577 - 4s - loss: 0.0703 - val_loss: 0.0895 - 4s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "577/577 - 5s - loss: 0.0705 - val_loss: 0.0911 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "577/577 - 4s - loss: 0.0676 - val_loss: 0.0970 - 4s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "577/577 - 5s - loss: 0.0671 - val_loss: 0.0908 - 5s/epoch - 10ms/step\n",
      "Epoch 85/10000\n",
      "577/577 - 5s - loss: 0.0729 - val_loss: 0.0934 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "577/577 - 4s - loss: 0.0701 - val_loss: 0.0907 - 4s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "577/577 - 4s - loss: 0.0684 - val_loss: 0.0882 - 4s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "577/577 - 4s - loss: 0.0656 - val_loss: 0.0883 - 4s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "577/577 - 5s - loss: 0.0667 - val_loss: 0.0876 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "577/577 - 5s - loss: 0.0659 - val_loss: 0.0849 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "577/577 - 4s - loss: 0.0705 - val_loss: 0.0969 - 4s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "577/577 - 4s - loss: 0.0656 - val_loss: 0.0898 - 4s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "577/577 - 4s - loss: 0.0647 - val_loss: 0.0861 - 4s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "577/577 - 4s - loss: 0.0655 - val_loss: 0.0966 - 4s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "577/577 - 4s - loss: 0.0718 - val_loss: 0.0923 - 4s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "577/577 - 4s - loss: 0.0631 - val_loss: 0.0879 - 4s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "577/577 - 4s - loss: 0.0646 - val_loss: 0.0892 - 4s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "577/577 - 4s - loss: 0.0678 - val_loss: 0.0892 - 4s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "577/577 - 4s - loss: 0.0629 - val_loss: 0.0851 - 4s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "577/577 - 5s - loss: 0.0634 - val_loss: 0.0849 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_26_layer_call_fn, gru_cell_26_layer_call_and_return_conditional_losses, gru_cell_27_layer_call_fn, gru_cell_27_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_20_1_50_1_50_datt_seq2seq_gru_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_20_1_50_1_50_datt_seq2seq_gru_4\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002643D6882B0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002641EFAA0D0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.921968</td>\n",
       "      <td>0.91794</td>\n",
       "      <td>0.942551</td>\n",
       "      <td>0.927486</td>\n",
       "      <td>5.517457</td>\n",
       "      <td>5.00472</td>\n",
       "      <td>3.729257</td>\n",
       "      <td>4.750478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.844353</td>\n",
       "      <td>0.871918</td>\n",
       "      <td>0.906007</td>\n",
       "      <td>0.874093</td>\n",
       "      <td>7.650614</td>\n",
       "      <td>6.254468</td>\n",
       "      <td>4.769596</td>\n",
       "      <td>6.224893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.74756</td>\n",
       "      <td>0.842848</td>\n",
       "      <td>0.868207</td>\n",
       "      <td>0.819538</td>\n",
       "      <td>9.317855</td>\n",
       "      <td>6.930555</td>\n",
       "      <td>5.647034</td>\n",
       "      <td>7.298482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.64641</td>\n",
       "      <td>0.817814</td>\n",
       "      <td>0.846514</td>\n",
       "      <td>0.770246</td>\n",
       "      <td>10.659541</td>\n",
       "      <td>7.464759</td>\n",
       "      <td>6.092651</td>\n",
       "      <td>8.072317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.54889</td>\n",
       "      <td>0.793021</td>\n",
       "      <td>0.831578</td>\n",
       "      <td>0.724497</td>\n",
       "      <td>11.848206</td>\n",
       "      <td>7.960041</td>\n",
       "      <td>6.38025</td>\n",
       "      <td>8.729499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.455926</td>\n",
       "      <td>0.765761</td>\n",
       "      <td>0.814635</td>\n",
       "      <td>0.678774</td>\n",
       "      <td>12.969376</td>\n",
       "      <td>8.472173</td>\n",
       "      <td>6.691063</td>\n",
       "      <td>9.377537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.369308</td>\n",
       "      <td>0.738235</td>\n",
       "      <td>0.793828</td>\n",
       "      <td>0.63379</td>\n",
       "      <td>13.691208</td>\n",
       "      <td>8.958873</td>\n",
       "      <td>7.054668</td>\n",
       "      <td>9.901583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.289673</td>\n",
       "      <td>0.711268</td>\n",
       "      <td>0.769978</td>\n",
       "      <td>0.590306</td>\n",
       "      <td>14.251965</td>\n",
       "      <td>9.412029</td>\n",
       "      <td>7.449872</td>\n",
       "      <td>10.371289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.216318</td>\n",
       "      <td>0.686653</td>\n",
       "      <td>0.743765</td>\n",
       "      <td>0.548912</td>\n",
       "      <td>14.747702</td>\n",
       "      <td>9.80826</td>\n",
       "      <td>7.861205</td>\n",
       "      <td>10.805722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.151619</td>\n",
       "      <td>0.663512</td>\n",
       "      <td>0.713855</td>\n",
       "      <td>0.509662</td>\n",
       "      <td>15.121326</td>\n",
       "      <td>10.165931</td>\n",
       "      <td>8.305033</td>\n",
       "      <td>11.19743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.088631</td>\n",
       "      <td>0.639981</td>\n",
       "      <td>0.680298</td>\n",
       "      <td>0.469636</td>\n",
       "      <td>15.509737</td>\n",
       "      <td>10.517131</td>\n",
       "      <td>8.776545</td>\n",
       "      <td>11.601138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.02719</td>\n",
       "      <td>0.616073</td>\n",
       "      <td>0.645446</td>\n",
       "      <td>0.42957</td>\n",
       "      <td>15.922183</td>\n",
       "      <td>10.862073</td>\n",
       "      <td>9.240862</td>\n",
       "      <td>12.008372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.034408</td>\n",
       "      <td>0.591238</td>\n",
       "      <td>0.6127</td>\n",
       "      <td>0.389843</td>\n",
       "      <td>16.195309</td>\n",
       "      <td>11.209041</td>\n",
       "      <td>9.656182</td>\n",
       "      <td>12.353511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.09388</td>\n",
       "      <td>0.565834</td>\n",
       "      <td>0.586573</td>\n",
       "      <td>0.352842</td>\n",
       "      <td>16.432899</td>\n",
       "      <td>11.55388</td>\n",
       "      <td>9.974407</td>\n",
       "      <td>12.653729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.15203</td>\n",
       "      <td>0.539493</td>\n",
       "      <td>0.565901</td>\n",
       "      <td>0.317788</td>\n",
       "      <td>16.696548</td>\n",
       "      <td>11.900863</td>\n",
       "      <td>10.218266</td>\n",
       "      <td>12.938559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.208247</td>\n",
       "      <td>0.515092</td>\n",
       "      <td>0.549224</td>\n",
       "      <td>0.285356</td>\n",
       "      <td>16.930475</td>\n",
       "      <td>12.213904</td>\n",
       "      <td>10.410769</td>\n",
       "      <td>13.185049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.262018</td>\n",
       "      <td>0.490231</td>\n",
       "      <td>0.532106</td>\n",
       "      <td>0.25344</td>\n",
       "      <td>17.135422</td>\n",
       "      <td>12.52374</td>\n",
       "      <td>10.605399</td>\n",
       "      <td>13.42152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.314328</td>\n",
       "      <td>0.465691</td>\n",
       "      <td>0.513258</td>\n",
       "      <td>0.22154</td>\n",
       "      <td>17.442838</td>\n",
       "      <td>12.821906</td>\n",
       "      <td>10.816402</td>\n",
       "      <td>13.693715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.365191</td>\n",
       "      <td>0.439114</td>\n",
       "      <td>0.492232</td>\n",
       "      <td>0.188719</td>\n",
       "      <td>17.794756</td>\n",
       "      <td>13.137414</td>\n",
       "      <td>11.047122</td>\n",
       "      <td>13.993097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.415531</td>\n",
       "      <td>0.412457</td>\n",
       "      <td>0.468655</td>\n",
       "      <td>0.155193</td>\n",
       "      <td>18.137576</td>\n",
       "      <td>13.446199</td>\n",
       "      <td>11.300406</td>\n",
       "      <td>14.294727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.173111</td>\n",
       "      <td>0.654209</td>\n",
       "      <td>0.693866</td>\n",
       "      <td>0.507062</td>\n",
       "      <td>14.19865</td>\n",
       "      <td>10.030898</td>\n",
       "      <td>8.301349</td>\n",
       "      <td>10.843632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.921968   0.91794  0.942551  0.927486   5.517457    5.00472   \n",
       "1      0.844353  0.871918  0.906007  0.874093   7.650614   6.254468   \n",
       "2       0.74756  0.842848  0.868207  0.819538   9.317855   6.930555   \n",
       "3       0.64641  0.817814  0.846514  0.770246  10.659541   7.464759   \n",
       "4       0.54889  0.793021  0.831578  0.724497  11.848206   7.960041   \n",
       "5      0.455926  0.765761  0.814635  0.678774  12.969376   8.472173   \n",
       "6      0.369308  0.738235  0.793828   0.63379  13.691208   8.958873   \n",
       "7      0.289673  0.711268  0.769978  0.590306  14.251965   9.412029   \n",
       "8      0.216318  0.686653  0.743765  0.548912  14.747702    9.80826   \n",
       "9      0.151619  0.663512  0.713855  0.509662  15.121326  10.165931   \n",
       "10     0.088631  0.639981  0.680298  0.469636  15.509737  10.517131   \n",
       "11      0.02719  0.616073  0.645446   0.42957  15.922183  10.862073   \n",
       "12    -0.034408  0.591238    0.6127  0.389843  16.195309  11.209041   \n",
       "13     -0.09388  0.565834  0.586573  0.352842  16.432899   11.55388   \n",
       "14     -0.15203  0.539493  0.565901  0.317788  16.696548  11.900863   \n",
       "15    -0.208247  0.515092  0.549224  0.285356  16.930475  12.213904   \n",
       "16    -0.262018  0.490231  0.532106   0.25344  17.135422   12.52374   \n",
       "17    -0.314328  0.465691  0.513258   0.22154  17.442838  12.821906   \n",
       "18    -0.365191  0.439114  0.492232  0.188719  17.794756  13.137414   \n",
       "19    -0.415531  0.412457  0.468655  0.155193  18.137576  13.446199   \n",
       "mean   0.173111  0.654209  0.693866  0.507062   14.19865  10.030898   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       3.729257   4.750478  \n",
       "1       4.769596   6.224893  \n",
       "2       5.647034   7.298482  \n",
       "3       6.092651   8.072317  \n",
       "4        6.38025   8.729499  \n",
       "5       6.691063   9.377537  \n",
       "6       7.054668   9.901583  \n",
       "7       7.449872  10.371289  \n",
       "8       7.861205  10.805722  \n",
       "9       8.305033   11.19743  \n",
       "10      8.776545  11.601138  \n",
       "11      9.240862  12.008372  \n",
       "12      9.656182  12.353511  \n",
       "13      9.974407  12.653729  \n",
       "14     10.218266  12.938559  \n",
       "15     10.410769  13.185049  \n",
       "16     10.605399   13.42152  \n",
       "17     10.816402  13.693715  \n",
       "18     11.047122  13.993097  \n",
       "19     11.300406  14.294727  \n",
       "mean    8.301349  10.843632  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/10_20_1_50_1_50_datt_seq2seq_gru_4.csv\n",
      "\n",
      "\n",
      "4th iteration\n",
      "history size: 10\n",
      "future size: 30\n",
      "Epoch 1/10000\n",
      "572/572 - 8s - loss: 0.3628 - val_loss: 0.3091 - 8s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "572/572 - 5s - loss: 0.2947 - val_loss: 0.2772 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "572/572 - 5s - loss: 0.2757 - val_loss: 0.2735 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "572/572 - 5s - loss: 0.2615 - val_loss: 0.2499 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "572/572 - 5s - loss: 0.2490 - val_loss: 0.2438 - 5s/epoch - 9ms/step\n",
      "Epoch 6/10000\n",
      "572/572 - 5s - loss: 0.2356 - val_loss: 0.2335 - 5s/epoch - 9ms/step\n",
      "Epoch 7/10000\n",
      "572/572 - 5s - loss: 0.2255 - val_loss: 0.2220 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "572/572 - 5s - loss: 0.2167 - val_loss: 0.2105 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "572/572 - 5s - loss: 0.2074 - val_loss: 0.2137 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "572/572 - 5s - loss: 0.2010 - val_loss: 0.2018 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "572/572 - 5s - loss: 0.1955 - val_loss: 0.1947 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "572/572 - 5s - loss: 0.1891 - val_loss: 0.1970 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "572/572 - 5s - loss: 0.1840 - val_loss: 0.1891 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "572/572 - 5s - loss: 0.1790 - val_loss: 0.1799 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "572/572 - 5s - loss: 0.1723 - val_loss: 0.1756 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "572/572 - 5s - loss: 0.1735 - val_loss: 0.1729 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "572/572 - 5s - loss: 0.1672 - val_loss: 0.1690 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "572/572 - 5s - loss: 0.1604 - val_loss: 0.1650 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "572/572 - 5s - loss: 0.1580 - val_loss: 0.1657 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "572/572 - 5s - loss: 0.1571 - val_loss: 0.1629 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "572/572 - 5s - loss: 0.1513 - val_loss: 0.1581 - 5s/epoch - 9ms/step\n",
      "Epoch 22/10000\n",
      "572/572 - 5s - loss: 0.1509 - val_loss: 0.1545 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "572/572 - 5s - loss: 0.1489 - val_loss: 0.1552 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "572/572 - 5s - loss: 0.1427 - val_loss: 0.1531 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "572/572 - 5s - loss: 0.1440 - val_loss: 0.1498 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "572/572 - 5s - loss: 0.1450 - val_loss: 0.1595 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "572/572 - 5s - loss: 0.1383 - val_loss: 0.1424 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "572/572 - 5s - loss: 0.1386 - val_loss: 0.1464 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "572/572 - 5s - loss: 0.1348 - val_loss: 0.1478 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "572/572 - 5s - loss: 0.1334 - val_loss: 0.1415 - 5s/epoch - 9ms/step\n",
      "Epoch 31/10000\n",
      "572/572 - 5s - loss: 0.1302 - val_loss: 0.1360 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "572/572 - 5s - loss: 0.1310 - val_loss: 0.1378 - 5s/epoch - 9ms/step\n",
      "Epoch 33/10000\n",
      "572/572 - 5s - loss: 0.1292 - val_loss: 0.1335 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "572/572 - 5s - loss: 0.1283 - val_loss: 0.1407 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "572/572 - 5s - loss: 0.1301 - val_loss: 0.1402 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "572/572 - 5s - loss: 0.1246 - val_loss: 0.1359 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "572/572 - 5s - loss: 0.1212 - val_loss: 0.1319 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "572/572 - 5s - loss: 0.1196 - val_loss: 0.1359 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "572/572 - 5s - loss: 0.1190 - val_loss: 0.1349 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "572/572 - 5s - loss: 0.1182 - val_loss: 0.1311 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "572/572 - 5s - loss: 0.1194 - val_loss: 0.1285 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "572/572 - 5s - loss: 0.1142 - val_loss: 0.1233 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "572/572 - 5s - loss: 0.1165 - val_loss: 0.1304 - 5s/epoch - 9ms/step\n",
      "Epoch 44/10000\n",
      "572/572 - 5s - loss: 0.1137 - val_loss: 0.1232 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "572/572 - 5s - loss: 0.1109 - val_loss: 0.1269 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "572/572 - 5s - loss: 0.1125 - val_loss: 0.1219 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "572/572 - 5s - loss: 0.1092 - val_loss: 0.1243 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "572/572 - 5s - loss: 0.1085 - val_loss: 0.1199 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "572/572 - 5s - loss: 0.1077 - val_loss: 0.1202 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "572/572 - 5s - loss: 0.1080 - val_loss: 0.1263 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "572/572 - 5s - loss: 0.1052 - val_loss: 0.1223 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "572/572 - 5s - loss: 0.1030 - val_loss: 0.1170 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "572/572 - 5s - loss: 0.1038 - val_loss: 0.1211 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "572/572 - 5s - loss: 0.1026 - val_loss: 0.1126 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "572/572 - 5s - loss: 0.1003 - val_loss: 0.1124 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "572/572 - 5s - loss: 0.1008 - val_loss: 0.1116 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "572/572 - 5s - loss: 0.0993 - val_loss: 0.1108 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "572/572 - 5s - loss: 0.0993 - val_loss: 0.1080 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "572/572 - 5s - loss: 0.1044 - val_loss: 0.1207 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "572/572 - 5s - loss: 0.0999 - val_loss: 0.1070 - 5s/epoch - 9ms/step\n",
      "Epoch 61/10000\n",
      "572/572 - 5s - loss: 0.0943 - val_loss: 0.1062 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "572/572 - 5s - loss: 0.0957 - val_loss: 0.1106 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "572/572 - 5s - loss: 0.1008 - val_loss: 0.1127 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "572/572 - 5s - loss: 0.0980 - val_loss: 0.1180 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "572/572 - 5s - loss: 0.0979 - val_loss: 0.1077 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "572/572 - 5s - loss: 0.0919 - val_loss: 0.1042 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "572/572 - 5s - loss: 0.0912 - val_loss: 0.1065 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "572/572 - 5s - loss: 0.0913 - val_loss: 0.1054 - 5s/epoch - 9ms/step\n",
      "Epoch 69/10000\n",
      "572/572 - 5s - loss: 0.0919 - val_loss: 0.1020 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "572/572 - 5s - loss: 0.0918 - val_loss: 0.1045 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "572/572 - 5s - loss: 0.0933 - val_loss: 0.1017 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "572/572 - 5s - loss: 0.0900 - val_loss: 0.1137 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "572/572 - 5s - loss: 0.0923 - val_loss: 0.1036 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "572/572 - 5s - loss: 0.0896 - val_loss: 0.1021 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "572/572 - 5s - loss: 0.0933 - val_loss: 0.1012 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "572/572 - 5s - loss: 0.0881 - val_loss: 0.1106 - 5s/epoch - 9ms/step\n",
      "Epoch 77/10000\n",
      "572/572 - 5s - loss: 0.0892 - val_loss: 0.1074 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "572/572 - 5s - loss: 0.0904 - val_loss: 0.0973 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "572/572 - 5s - loss: 0.0860 - val_loss: 0.1082 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "572/572 - 5s - loss: 0.0956 - val_loss: 0.1093 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "572/572 - 5s - loss: 0.0871 - val_loss: 0.1028 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "572/572 - 5s - loss: 0.0865 - val_loss: 0.1087 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "572/572 - 5s - loss: 0.0921 - val_loss: 0.0994 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "572/572 - 5s - loss: 0.0842 - val_loss: 0.0993 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "572/572 - 5s - loss: 0.0828 - val_loss: 0.0977 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "572/572 - 5s - loss: 0.0847 - val_loss: 0.0955 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "572/572 - 5s - loss: 0.0849 - val_loss: 0.1146 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "572/572 - 5s - loss: 0.0868 - val_loss: 0.1055 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "572/572 - 5s - loss: 0.0817 - val_loss: 0.0946 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "572/572 - 5s - loss: 0.0830 - val_loss: 0.1104 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "572/572 - 5s - loss: 0.0861 - val_loss: 0.0994 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "572/572 - 5s - loss: 0.0812 - val_loss: 0.0941 - 5s/epoch - 9ms/step\n",
      "Epoch 93/10000\n",
      "572/572 - 5s - loss: 0.0793 - val_loss: 0.0948 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "572/572 - 5s - loss: 0.0812 - val_loss: 0.0969 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "572/572 - 5s - loss: 0.0826 - val_loss: 0.0932 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "572/572 - 5s - loss: 0.0816 - val_loss: 0.1006 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "572/572 - 5s - loss: 0.0834 - val_loss: 0.0925 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "572/572 - 5s - loss: 0.0809 - val_loss: 0.0924 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "572/572 - 5s - loss: 0.0768 - val_loss: 0.0935 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "572/572 - 5s - loss: 0.0772 - val_loss: 0.0931 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "572/572 - 5s - loss: 0.0787 - val_loss: 0.0973 - 5s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "572/572 - 5s - loss: 0.0808 - val_loss: 0.1058 - 5s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "572/572 - 5s - loss: 0.0872 - val_loss: 0.1046 - 5s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "572/572 - 5s - loss: 0.0779 - val_loss: 0.0907 - 5s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "572/572 - 5s - loss: 0.0754 - val_loss: 0.0933 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "572/572 - 5s - loss: 0.0780 - val_loss: 0.0905 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "572/572 - 5s - loss: 0.0805 - val_loss: 0.0945 - 5s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "572/572 - 5s - loss: 0.0764 - val_loss: 0.0907 - 5s/epoch - 8ms/step\n",
      "Epoch 109/10000\n",
      "572/572 - 5s - loss: 0.0757 - val_loss: 0.0886 - 5s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "572/572 - 5s - loss: 0.0793 - val_loss: 0.0926 - 5s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "572/572 - 5s - loss: 0.0755 - val_loss: 0.0896 - 5s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "572/572 - 5s - loss: 0.0799 - val_loss: 0.0901 - 5s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "572/572 - 5s - loss: 0.0770 - val_loss: 0.0863 - 5s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "572/572 - 5s - loss: 0.0935 - val_loss: 0.1053 - 5s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "572/572 - 5s - loss: 0.0821 - val_loss: 0.0912 - 5s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "572/572 - 5s - loss: 0.0747 - val_loss: 0.0897 - 5s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "572/572 - 5s - loss: 0.0736 - val_loss: 0.0907 - 5s/epoch - 8ms/step\n",
      "Epoch 118/10000\n",
      "572/572 - 5s - loss: 0.0738 - val_loss: 0.0846 - 5s/epoch - 8ms/step\n",
      "Epoch 119/10000\n",
      "572/572 - 5s - loss: 0.0751 - val_loss: 0.0930 - 5s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "572/572 - 5s - loss: 0.0763 - val_loss: 0.0861 - 5s/epoch - 9ms/step\n",
      "Epoch 121/10000\n",
      "572/572 - 5s - loss: 0.0719 - val_loss: 0.0876 - 5s/epoch - 8ms/step\n",
      "Epoch 122/10000\n",
      "572/572 - 5s - loss: 0.0724 - val_loss: 0.0867 - 5s/epoch - 8ms/step\n",
      "Epoch 123/10000\n",
      "572/572 - 5s - loss: 0.0741 - val_loss: 0.0892 - 5s/epoch - 9ms/step\n",
      "Epoch 124/10000\n",
      "572/572 - 5s - loss: 0.0762 - val_loss: 0.0991 - 5s/epoch - 8ms/step\n",
      "Epoch 125/10000\n",
      "572/572 - 5s - loss: 0.0805 - val_loss: 0.0863 - 5s/epoch - 8ms/step\n",
      "Epoch 126/10000\n",
      "572/572 - 5s - loss: 0.0704 - val_loss: 0.0872 - 5s/epoch - 8ms/step\n",
      "Epoch 127/10000\n",
      "572/572 - 5s - loss: 0.0709 - val_loss: 0.0892 - 5s/epoch - 8ms/step\n",
      "Epoch 128/10000\n",
      "572/572 - 5s - loss: 0.0718 - val_loss: 0.0868 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_28_layer_call_fn, gru_cell_28_layer_call_and_return_conditional_losses, gru_cell_29_layer_call_fn, gru_cell_29_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_30_1_50_1_50_datt_seq2seq_gru_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_30_1_50_1_50_datt_seq2seq_gru_4\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002646F759FA0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002643CC68190> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.895122</td>\n",
       "      <td>0.918984</td>\n",
       "      <td>0.927879</td>\n",
       "      <td>0.913995</td>\n",
       "      <td>6.393283</td>\n",
       "      <td>4.971823</td>\n",
       "      <td>4.162619</td>\n",
       "      <td>5.175908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.846462</td>\n",
       "      <td>0.877583</td>\n",
       "      <td>0.894503</td>\n",
       "      <td>0.872849</td>\n",
       "      <td>7.735622</td>\n",
       "      <td>6.11183</td>\n",
       "      <td>5.034571</td>\n",
       "      <td>6.294008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.776583</td>\n",
       "      <td>0.839377</td>\n",
       "      <td>0.858092</td>\n",
       "      <td>0.824684</td>\n",
       "      <td>9.332783</td>\n",
       "      <td>7.001401</td>\n",
       "      <td>5.838534</td>\n",
       "      <td>7.390906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.696261</td>\n",
       "      <td>0.806907</td>\n",
       "      <td>0.827948</td>\n",
       "      <td>0.777039</td>\n",
       "      <td>10.883932</td>\n",
       "      <td>7.677488</td>\n",
       "      <td>6.427244</td>\n",
       "      <td>8.329555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.609948</td>\n",
       "      <td>0.778402</td>\n",
       "      <td>0.794668</td>\n",
       "      <td>0.727673</td>\n",
       "      <td>12.335373</td>\n",
       "      <td>8.226193</td>\n",
       "      <td>7.019195</td>\n",
       "      <td>9.193587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.52138</td>\n",
       "      <td>0.753537</td>\n",
       "      <td>0.754611</td>\n",
       "      <td>0.676509</td>\n",
       "      <td>13.665463</td>\n",
       "      <td>8.677884</td>\n",
       "      <td>7.671028</td>\n",
       "      <td>10.004791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.43755</td>\n",
       "      <td>0.730835</td>\n",
       "      <td>0.713009</td>\n",
       "      <td>0.627131</td>\n",
       "      <td>14.815178</td>\n",
       "      <td>9.071424</td>\n",
       "      <td>8.293921</td>\n",
       "      <td>10.726841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.360188</td>\n",
       "      <td>0.710282</td>\n",
       "      <td>0.673209</td>\n",
       "      <td>0.581226</td>\n",
       "      <td>15.801096</td>\n",
       "      <td>9.41423</td>\n",
       "      <td>8.84889</td>\n",
       "      <td>11.354738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.293058</td>\n",
       "      <td>0.691995</td>\n",
       "      <td>0.632319</td>\n",
       "      <td>0.539124</td>\n",
       "      <td>16.60852</td>\n",
       "      <td>9.710712</td>\n",
       "      <td>9.384429</td>\n",
       "      <td>11.90122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.237982</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.587058</td>\n",
       "      <td>0.500014</td>\n",
       "      <td>17.244267</td>\n",
       "      <td>9.97842</td>\n",
       "      <td>9.943322</td>\n",
       "      <td>12.38867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.190565</td>\n",
       "      <td>0.657285</td>\n",
       "      <td>0.537353</td>\n",
       "      <td>0.461735</td>\n",
       "      <td>17.776611</td>\n",
       "      <td>10.249512</td>\n",
       "      <td>10.523111</td>\n",
       "      <td>12.849745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.148185</td>\n",
       "      <td>0.636389</td>\n",
       "      <td>0.485644</td>\n",
       "      <td>0.423406</td>\n",
       "      <td>17.905226</td>\n",
       "      <td>10.560745</td>\n",
       "      <td>11.094095</td>\n",
       "      <td>13.186689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.110655</td>\n",
       "      <td>0.612176</td>\n",
       "      <td>0.438093</td>\n",
       "      <td>0.386975</td>\n",
       "      <td>17.497138</td>\n",
       "      <td>10.91078</td>\n",
       "      <td>11.593953</td>\n",
       "      <td>13.333957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.078008</td>\n",
       "      <td>0.585359</td>\n",
       "      <td>0.397409</td>\n",
       "      <td>0.353592</td>\n",
       "      <td>17.220245</td>\n",
       "      <td>11.285668</td>\n",
       "      <td>12.004718</td>\n",
       "      <td>13.503544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.045135</td>\n",
       "      <td>0.558249</td>\n",
       "      <td>0.362704</td>\n",
       "      <td>0.322029</td>\n",
       "      <td>17.243275</td>\n",
       "      <td>11.654003</td>\n",
       "      <td>12.344365</td>\n",
       "      <td>13.747214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.015712</td>\n",
       "      <td>0.529825</td>\n",
       "      <td>0.330591</td>\n",
       "      <td>0.292043</td>\n",
       "      <td>17.446021</td>\n",
       "      <td>12.029076</td>\n",
       "      <td>12.649619</td>\n",
       "      <td>14.041572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.010082</td>\n",
       "      <td>0.501463</td>\n",
       "      <td>0.297362</td>\n",
       "      <td>0.262914</td>\n",
       "      <td>17.324728</td>\n",
       "      <td>12.390135</td>\n",
       "      <td>12.958043</td>\n",
       "      <td>14.224302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.03396</td>\n",
       "      <td>0.47276</td>\n",
       "      <td>0.260688</td>\n",
       "      <td>0.233163</td>\n",
       "      <td>17.190186</td>\n",
       "      <td>12.745856</td>\n",
       "      <td>13.289216</td>\n",
       "      <td>14.408419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.056164</td>\n",
       "      <td>0.44485</td>\n",
       "      <td>0.220668</td>\n",
       "      <td>0.203118</td>\n",
       "      <td>17.112749</td>\n",
       "      <td>13.083209</td>\n",
       "      <td>13.640728</td>\n",
       "      <td>14.612229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.079591</td>\n",
       "      <td>0.417811</td>\n",
       "      <td>0.179928</td>\n",
       "      <td>0.172716</td>\n",
       "      <td>17.046086</td>\n",
       "      <td>13.400598</td>\n",
       "      <td>13.988322</td>\n",
       "      <td>14.811669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.104897</td>\n",
       "      <td>0.3924</td>\n",
       "      <td>0.139384</td>\n",
       "      <td>0.142295</td>\n",
       "      <td>17.060193</td>\n",
       "      <td>13.692249</td>\n",
       "      <td>14.326054</td>\n",
       "      <td>15.026165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.130749</td>\n",
       "      <td>0.366042</td>\n",
       "      <td>0.100814</td>\n",
       "      <td>0.112036</td>\n",
       "      <td>17.144049</td>\n",
       "      <td>13.987777</td>\n",
       "      <td>14.641047</td>\n",
       "      <td>15.257624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.158823</td>\n",
       "      <td>0.339181</td>\n",
       "      <td>0.06527</td>\n",
       "      <td>0.081876</td>\n",
       "      <td>17.114882</td>\n",
       "      <td>14.282372</td>\n",
       "      <td>14.92646</td>\n",
       "      <td>15.441238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.18711</td>\n",
       "      <td>0.312521</td>\n",
       "      <td>0.03383</td>\n",
       "      <td>0.05308</td>\n",
       "      <td>17.089124</td>\n",
       "      <td>14.569527</td>\n",
       "      <td>15.175552</td>\n",
       "      <td>15.611401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.214004</td>\n",
       "      <td>0.286913</td>\n",
       "      <td>0.006829</td>\n",
       "      <td>0.026579</td>\n",
       "      <td>17.110081</td>\n",
       "      <td>14.840354</td>\n",
       "      <td>15.385715</td>\n",
       "      <td>15.778716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.24111</td>\n",
       "      <td>0.261924</td>\n",
       "      <td>-0.015086</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>17.132642</td>\n",
       "      <td>15.10044</td>\n",
       "      <td>15.554658</td>\n",
       "      <td>15.929247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.266012</td>\n",
       "      <td>0.238234</td>\n",
       "      <td>-0.032657</td>\n",
       "      <td>-0.020145</td>\n",
       "      <td>17.140082</td>\n",
       "      <td>15.341957</td>\n",
       "      <td>15.68945</td>\n",
       "      <td>16.057163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.287583</td>\n",
       "      <td>0.217263</td>\n",
       "      <td>-0.048296</td>\n",
       "      <td>-0.039539</td>\n",
       "      <td>17.247482</td>\n",
       "      <td>15.552115</td>\n",
       "      <td>15.80921</td>\n",
       "      <td>16.202936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.303273</td>\n",
       "      <td>0.197381</td>\n",
       "      <td>-0.064157</td>\n",
       "      <td>-0.056683</td>\n",
       "      <td>17.376618</td>\n",
       "      <td>15.748829</td>\n",
       "      <td>15.929429</td>\n",
       "      <td>16.351625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.315477</td>\n",
       "      <td>0.180387</td>\n",
       "      <td>-0.082626</td>\n",
       "      <td>-0.072572</td>\n",
       "      <td>17.482881</td>\n",
       "      <td>15.914784</td>\n",
       "      <td>16.068894</td>\n",
       "      <td>16.488853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.129132</td>\n",
       "      <td>0.533044</td>\n",
       "      <td>0.375901</td>\n",
       "      <td>0.346026</td>\n",
       "      <td>15.682527</td>\n",
       "      <td>11.606046</td>\n",
       "      <td>11.67388</td>\n",
       "      <td>12.987484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.895122  0.918984  0.927879  0.913995   6.393283   4.971823   \n",
       "1      0.846462  0.877583  0.894503  0.872849   7.735622    6.11183   \n",
       "2      0.776583  0.839377  0.858092  0.824684   9.332783   7.001401   \n",
       "3      0.696261  0.806907  0.827948  0.777039  10.883932   7.677488   \n",
       "4      0.609948  0.778402  0.794668  0.727673  12.335373   8.226193   \n",
       "5       0.52138  0.753537  0.754611  0.676509  13.665463   8.677884   \n",
       "6       0.43755  0.730835  0.713009  0.627131  14.815178   9.071424   \n",
       "7      0.360188  0.710282  0.673209  0.581226  15.801096    9.41423   \n",
       "8      0.293058  0.691995  0.632319  0.539124   16.60852   9.710712   \n",
       "9      0.237982     0.675  0.587058  0.500014  17.244267    9.97842   \n",
       "10     0.190565  0.657285  0.537353  0.461735  17.776611  10.249512   \n",
       "11     0.148185  0.636389  0.485644  0.423406  17.905226  10.560745   \n",
       "12     0.110655  0.612176  0.438093  0.386975  17.497138   10.91078   \n",
       "13     0.078008  0.585359  0.397409  0.353592  17.220245  11.285668   \n",
       "14     0.045135  0.558249  0.362704  0.322029  17.243275  11.654003   \n",
       "15     0.015712  0.529825  0.330591  0.292043  17.446021  12.029076   \n",
       "16    -0.010082  0.501463  0.297362  0.262914  17.324728  12.390135   \n",
       "17     -0.03396   0.47276  0.260688  0.233163  17.190186  12.745856   \n",
       "18    -0.056164   0.44485  0.220668  0.203118  17.112749  13.083209   \n",
       "19    -0.079591  0.417811  0.179928  0.172716  17.046086  13.400598   \n",
       "20    -0.104897    0.3924  0.139384  0.142295  17.060193  13.692249   \n",
       "21    -0.130749  0.366042  0.100814  0.112036  17.144049  13.987777   \n",
       "22    -0.158823  0.339181   0.06527  0.081876  17.114882  14.282372   \n",
       "23     -0.18711  0.312521   0.03383   0.05308  17.089124  14.569527   \n",
       "24    -0.214004  0.286913  0.006829  0.026579  17.110081  14.840354   \n",
       "25     -0.24111  0.261924 -0.015086  0.001909  17.132642   15.10044   \n",
       "26    -0.266012  0.238234 -0.032657 -0.020145  17.140082  15.341957   \n",
       "27    -0.287583  0.217263 -0.048296 -0.039539  17.247482  15.552115   \n",
       "28    -0.303273  0.197381 -0.064157 -0.056683  17.376618  15.748829   \n",
       "29    -0.315477  0.180387 -0.082626 -0.072572  17.482881  15.914784   \n",
       "mean   0.129132  0.533044  0.375901  0.346026  15.682527  11.606046   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       4.162619   5.175908  \n",
       "1       5.034571   6.294008  \n",
       "2       5.838534   7.390906  \n",
       "3       6.427244   8.329555  \n",
       "4       7.019195   9.193587  \n",
       "5       7.671028  10.004791  \n",
       "6       8.293921  10.726841  \n",
       "7        8.84889  11.354738  \n",
       "8       9.384429   11.90122  \n",
       "9       9.943322   12.38867  \n",
       "10     10.523111  12.849745  \n",
       "11     11.094095  13.186689  \n",
       "12     11.593953  13.333957  \n",
       "13     12.004718  13.503544  \n",
       "14     12.344365  13.747214  \n",
       "15     12.649619  14.041572  \n",
       "16     12.958043  14.224302  \n",
       "17     13.289216  14.408419  \n",
       "18     13.640728  14.612229  \n",
       "19     13.988322  14.811669  \n",
       "20     14.326054  15.026165  \n",
       "21     14.641047  15.257624  \n",
       "22      14.92646  15.441238  \n",
       "23     15.175552  15.611401  \n",
       "24     15.385715  15.778716  \n",
       "25     15.554658  15.929247  \n",
       "26      15.68945  16.057163  \n",
       "27      15.80921  16.202936  \n",
       "28     15.929429  16.351625  \n",
       "29     16.068894  16.488853  \n",
       "mean    11.67388  12.987484  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/10_30_1_50_1_50_datt_seq2seq_gru_4.csv\n",
      "\n",
      "\n",
      "4th iteration\n",
      "history size: 20\n",
      "future size: 10\n",
      "Epoch 1/10000\n",
      "577/577 - 8s - loss: 0.2467 - val_loss: 0.1591 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "577/577 - 4s - loss: 0.1497 - val_loss: 0.1422 - 4s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "577/577 - 4s - loss: 0.1378 - val_loss: 0.1347 - 4s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "577/577 - 4s - loss: 0.1290 - val_loss: 0.1284 - 4s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "577/577 - 4s - loss: 0.1198 - val_loss: 0.1197 - 4s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "577/577 - 4s - loss: 0.1111 - val_loss: 0.1126 - 4s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "577/577 - 4s - loss: 0.1055 - val_loss: 0.1062 - 4s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "577/577 - 4s - loss: 0.1006 - val_loss: 0.1007 - 4s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "577/577 - 4s - loss: 0.0979 - val_loss: 0.1032 - 4s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "577/577 - 4s - loss: 0.0925 - val_loss: 0.0955 - 4s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "577/577 - 4s - loss: 0.0912 - val_loss: 0.1053 - 4s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "577/577 - 4s - loss: 0.0875 - val_loss: 0.0930 - 4s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "577/577 - 4s - loss: 0.0857 - val_loss: 0.0915 - 4s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "577/577 - 4s - loss: 0.0818 - val_loss: 0.0912 - 4s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "577/577 - 4s - loss: 0.0796 - val_loss: 0.0888 - 4s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "577/577 - 4s - loss: 0.0781 - val_loss: 0.0971 - 4s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "577/577 - 5s - loss: 0.0771 - val_loss: 0.0847 - 5s/epoch - 9ms/step\n",
      "Epoch 18/10000\n",
      "577/577 - 5s - loss: 0.0754 - val_loss: 0.0843 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "577/577 - 5s - loss: 0.0742 - val_loss: 0.0818 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "577/577 - 5s - loss: 0.0730 - val_loss: 0.0812 - 5s/epoch - 9ms/step\n",
      "Epoch 21/10000\n",
      "577/577 - 5s - loss: 0.0712 - val_loss: 0.0800 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "577/577 - 5s - loss: 0.0686 - val_loss: 0.0820 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "577/577 - 4s - loss: 0.0663 - val_loss: 0.0793 - 4s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "577/577 - 4s - loss: 0.0667 - val_loss: 0.0760 - 4s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "577/577 - 4s - loss: 0.0649 - val_loss: 0.0857 - 4s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "577/577 - 4s - loss: 0.0659 - val_loss: 0.0769 - 4s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "577/577 - 4s - loss: 0.0625 - val_loss: 0.0738 - 4s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "577/577 - 4s - loss: 0.0617 - val_loss: 0.0708 - 4s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "577/577 - 4s - loss: 0.0608 - val_loss: 0.0699 - 4s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "577/577 - 4s - loss: 0.0602 - val_loss: 0.0722 - 4s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "577/577 - 5s - loss: 0.0600 - val_loss: 0.0702 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "577/577 - 4s - loss: 0.0578 - val_loss: 0.0694 - 4s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "577/577 - 4s - loss: 0.0572 - val_loss: 0.0655 - 4s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "577/577 - 4s - loss: 0.0553 - val_loss: 0.0694 - 4s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "577/577 - 4s - loss: 0.0566 - val_loss: 0.0646 - 4s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "577/577 - 4s - loss: 0.0541 - val_loss: 0.0648 - 4s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "577/577 - 4s - loss: 0.0545 - val_loss: 0.0651 - 4s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "577/577 - 4s - loss: 0.0532 - val_loss: 0.0636 - 4s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "577/577 - 4s - loss: 0.0545 - val_loss: 0.0706 - 4s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "577/577 - 4s - loss: 0.0523 - val_loss: 0.0654 - 4s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "577/577 - 4s - loss: 0.0519 - val_loss: 0.0618 - 4s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "577/577 - 5s - loss: 0.0509 - val_loss: 0.0617 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "577/577 - 4s - loss: 0.0499 - val_loss: 0.0618 - 4s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "577/577 - 4s - loss: 0.0487 - val_loss: 0.0653 - 4s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "577/577 - 4s - loss: 0.0509 - val_loss: 0.0628 - 4s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "577/577 - 4s - loss: 0.0506 - val_loss: 0.0628 - 4s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "577/577 - 4s - loss: 0.0476 - val_loss: 0.0602 - 4s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "577/577 - 4s - loss: 0.0476 - val_loss: 0.0585 - 4s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "577/577 - 4s - loss: 0.0475 - val_loss: 0.0603 - 4s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "577/577 - 4s - loss: 0.0491 - val_loss: 0.0629 - 4s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "577/577 - 4s - loss: 0.0467 - val_loss: 0.0583 - 4s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "577/577 - 4s - loss: 0.0449 - val_loss: 0.0578 - 4s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "577/577 - 4s - loss: 0.0454 - val_loss: 0.0600 - 4s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "577/577 - 4s - loss: 0.0450 - val_loss: 0.0605 - 4s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "577/577 - 4s - loss: 0.0439 - val_loss: 0.0563 - 4s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "577/577 - 4s - loss: 0.0468 - val_loss: 0.0698 - 4s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "577/577 - 4s - loss: 0.0456 - val_loss: 0.0557 - 4s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "577/577 - 4s - loss: 0.0425 - val_loss: 0.0572 - 4s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "577/577 - 4s - loss: 0.0478 - val_loss: 0.0574 - 4s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "577/577 - 4s - loss: 0.0450 - val_loss: 0.0556 - 4s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "577/577 - 4s - loss: 0.0416 - val_loss: 0.0551 - 4s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "577/577 - 4s - loss: 0.0419 - val_loss: 0.0547 - 4s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "577/577 - 4s - loss: 0.0425 - val_loss: 0.0635 - 4s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "577/577 - 5s - loss: 0.0429 - val_loss: 0.0546 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "577/577 - 4s - loss: 0.0410 - val_loss: 0.0540 - 4s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "577/577 - 4s - loss: 0.0405 - val_loss: 0.0540 - 4s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "577/577 - 4s - loss: 0.0406 - val_loss: 0.0545 - 4s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "577/577 - 4s - loss: 0.0420 - val_loss: 0.0568 - 4s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "577/577 - 4s - loss: 0.0420 - val_loss: 0.0575 - 4s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "577/577 - 4s - loss: 0.0404 - val_loss: 0.0529 - 4s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "577/577 - 4s - loss: 0.0393 - val_loss: 0.0544 - 4s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "577/577 - 4s - loss: 0.0387 - val_loss: 0.0516 - 4s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "577/577 - 4s - loss: 0.0386 - val_loss: 0.0537 - 4s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "577/577 - 4s - loss: 0.0401 - val_loss: 0.0560 - 4s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "577/577 - 4s - loss: 0.0401 - val_loss: 0.0528 - 4s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "577/577 - 4s - loss: 0.0384 - val_loss: 0.0511 - 4s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "577/577 - 4s - loss: 0.0416 - val_loss: 0.0523 - 4s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "577/577 - 4s - loss: 0.0381 - val_loss: 0.0504 - 4s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "577/577 - 4s - loss: 0.0376 - val_loss: 0.0501 - 4s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "577/577 - 4s - loss: 0.0365 - val_loss: 0.0515 - 4s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "577/577 - 4s - loss: 0.0387 - val_loss: 0.0541 - 4s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "577/577 - 4s - loss: 0.0379 - val_loss: 0.0514 - 4s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "577/577 - 4s - loss: 0.0372 - val_loss: 0.0487 - 4s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "577/577 - 4s - loss: 0.0359 - val_loss: 0.0498 - 4s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "577/577 - 4s - loss: 0.0372 - val_loss: 0.0658 - 4s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "577/577 - 4s - loss: 0.0376 - val_loss: 0.0511 - 4s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "577/577 - 4s - loss: 0.0358 - val_loss: 0.0482 - 4s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "577/577 - 4s - loss: 0.0386 - val_loss: 0.0492 - 4s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "577/577 - 4s - loss: 0.0363 - val_loss: 0.0541 - 4s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "577/577 - 4s - loss: 0.0373 - val_loss: 0.0491 - 4s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "577/577 - 4s - loss: 0.0348 - val_loss: 0.0477 - 4s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "577/577 - 4s - loss: 0.0346 - val_loss: 0.0520 - 4s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "577/577 - 4s - loss: 0.0348 - val_loss: 0.0485 - 4s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "577/577 - 4s - loss: 0.0343 - val_loss: 0.0514 - 4s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "577/577 - 4s - loss: 0.0365 - val_loss: 0.0473 - 4s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "577/577 - 5s - loss: 0.0341 - val_loss: 0.0462 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "577/577 - 4s - loss: 0.0330 - val_loss: 0.0504 - 4s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "577/577 - 4s - loss: 0.0346 - val_loss: 0.0489 - 4s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "577/577 - 4s - loss: 0.0331 - val_loss: 0.0487 - 4s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "577/577 - 4s - loss: 0.0331 - val_loss: 0.0489 - 4s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "577/577 - 4s - loss: 0.0339 - val_loss: 0.0486 - 4s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "577/577 - 4s - loss: 0.0355 - val_loss: 0.0478 - 4s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "577/577 - 4s - loss: 0.0323 - val_loss: 0.0480 - 4s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "577/577 - 4s - loss: 0.0327 - val_loss: 0.0490 - 4s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "577/577 - 4s - loss: 0.0320 - val_loss: 0.0453 - 4s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "577/577 - 4s - loss: 0.0331 - val_loss: 0.0491 - 4s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "577/577 - 4s - loss: 0.0326 - val_loss: 0.0477 - 4s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "577/577 - 4s - loss: 0.0365 - val_loss: 0.0498 - 4s/epoch - 8ms/step\n",
      "Epoch 109/10000\n",
      "577/577 - 4s - loss: 0.0334 - val_loss: 0.0497 - 4s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "577/577 - 5s - loss: 0.0328 - val_loss: 0.0467 - 5s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "577/577 - 4s - loss: 0.0319 - val_loss: 0.0462 - 4s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "577/577 - 4s - loss: 0.0322 - val_loss: 0.0489 - 4s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "577/577 - 4s - loss: 0.0318 - val_loss: 0.0451 - 4s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "577/577 - 4s - loss: 0.0304 - val_loss: 0.0470 - 4s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "577/577 - 4s - loss: 0.0321 - val_loss: 0.0513 - 4s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "577/577 - 4s - loss: 0.0315 - val_loss: 0.0456 - 4s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "577/577 - 4s - loss: 0.0310 - val_loss: 0.0469 - 4s/epoch - 8ms/step\n",
      "Epoch 118/10000\n",
      "577/577 - 4s - loss: 0.0317 - val_loss: 0.0452 - 4s/epoch - 8ms/step\n",
      "Epoch 119/10000\n",
      "577/577 - 4s - loss: 0.0311 - val_loss: 0.0441 - 4s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "577/577 - 4s - loss: 0.0298 - val_loss: 0.0456 - 4s/epoch - 8ms/step\n",
      "Epoch 121/10000\n",
      "577/577 - 4s - loss: 0.0295 - val_loss: 0.0445 - 4s/epoch - 8ms/step\n",
      "Epoch 122/10000\n",
      "577/577 - 4s - loss: 0.0299 - val_loss: 0.0466 - 4s/epoch - 8ms/step\n",
      "Epoch 123/10000\n",
      "577/577 - 4s - loss: 0.0309 - val_loss: 0.0506 - 4s/epoch - 8ms/step\n",
      "Epoch 124/10000\n",
      "577/577 - 4s - loss: 0.0305 - val_loss: 0.0472 - 4s/epoch - 8ms/step\n",
      "Epoch 125/10000\n",
      "577/577 - 4s - loss: 0.0304 - val_loss: 0.0465 - 4s/epoch - 8ms/step\n",
      "Epoch 126/10000\n",
      "577/577 - 4s - loss: 0.0308 - val_loss: 0.0467 - 4s/epoch - 8ms/step\n",
      "Epoch 127/10000\n",
      "577/577 - 5s - loss: 0.0295 - val_loss: 0.0435 - 5s/epoch - 8ms/step\n",
      "Epoch 128/10000\n",
      "577/577 - 4s - loss: 0.0309 - val_loss: 0.0487 - 4s/epoch - 8ms/step\n",
      "Epoch 129/10000\n",
      "577/577 - 4s - loss: 0.0305 - val_loss: 0.0426 - 4s/epoch - 8ms/step\n",
      "Epoch 130/10000\n",
      "577/577 - 4s - loss: 0.0283 - val_loss: 0.0440 - 4s/epoch - 8ms/step\n",
      "Epoch 131/10000\n",
      "577/577 - 4s - loss: 0.0288 - val_loss: 0.0465 - 4s/epoch - 8ms/step\n",
      "Epoch 132/10000\n",
      "577/577 - 4s - loss: 0.0294 - val_loss: 0.0458 - 4s/epoch - 8ms/step\n",
      "Epoch 133/10000\n",
      "577/577 - 4s - loss: 0.0292 - val_loss: 0.0439 - 4s/epoch - 8ms/step\n",
      "Epoch 134/10000\n",
      "577/577 - 4s - loss: 0.0302 - val_loss: 0.0443 - 4s/epoch - 8ms/step\n",
      "Epoch 135/10000\n",
      "577/577 - 4s - loss: 0.0285 - val_loss: 0.0439 - 4s/epoch - 8ms/step\n",
      "Epoch 136/10000\n",
      "577/577 - 4s - loss: 0.0278 - val_loss: 0.0446 - 4s/epoch - 8ms/step\n",
      "Epoch 137/10000\n",
      "577/577 - 4s - loss: 0.0287 - val_loss: 0.0456 - 4s/epoch - 8ms/step\n",
      "Epoch 138/10000\n",
      "577/577 - 4s - loss: 0.0294 - val_loss: 0.0440 - 4s/epoch - 8ms/step\n",
      "Epoch 139/10000\n",
      "577/577 - 4s - loss: 0.0285 - val_loss: 0.0455 - 4s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_30_layer_call_fn, gru_cell_30_layer_call_and_return_conditional_losses, gru_cell_31_layer_call_fn, gru_cell_31_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_10_1_50_1_50_datt_seq2seq_gru_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_10_1_50_1_50_datt_seq2seq_gru_4\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002643D394850> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000026600274D60> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.924426</td>\n",
       "      <td>0.923585</td>\n",
       "      <td>0.957766</td>\n",
       "      <td>0.935259</td>\n",
       "      <td>4.466253</td>\n",
       "      <td>4.845346</td>\n",
       "      <td>3.18993</td>\n",
       "      <td>4.167176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.842972</td>\n",
       "      <td>0.86424</td>\n",
       "      <td>0.918655</td>\n",
       "      <td>0.875289</td>\n",
       "      <td>6.397007</td>\n",
       "      <td>6.459126</td>\n",
       "      <td>4.426271</td>\n",
       "      <td>5.760801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.747989</td>\n",
       "      <td>0.815884</td>\n",
       "      <td>0.876318</td>\n",
       "      <td>0.813397</td>\n",
       "      <td>7.993798</td>\n",
       "      <td>7.522799</td>\n",
       "      <td>5.456765</td>\n",
       "      <td>6.991121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.65479</td>\n",
       "      <td>0.782952</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.760342</td>\n",
       "      <td>9.231468</td>\n",
       "      <td>8.169178</td>\n",
       "      <td>6.141076</td>\n",
       "      <td>7.847241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.56902</td>\n",
       "      <td>0.759656</td>\n",
       "      <td>0.825673</td>\n",
       "      <td>0.718117</td>\n",
       "      <td>10.21229</td>\n",
       "      <td>8.597593</td>\n",
       "      <td>6.475377</td>\n",
       "      <td>8.42842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.490843</td>\n",
       "      <td>0.742939</td>\n",
       "      <td>0.819134</td>\n",
       "      <td>0.684305</td>\n",
       "      <td>10.990498</td>\n",
       "      <td>8.892895</td>\n",
       "      <td>6.594473</td>\n",
       "      <td>8.825955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.423002</td>\n",
       "      <td>0.729528</td>\n",
       "      <td>0.816302</td>\n",
       "      <td>0.656277</td>\n",
       "      <td>11.586419</td>\n",
       "      <td>9.122379</td>\n",
       "      <td>6.645167</td>\n",
       "      <td>9.117988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.365848</td>\n",
       "      <td>0.71771</td>\n",
       "      <td>0.810508</td>\n",
       "      <td>0.631355</td>\n",
       "      <td>12.116073</td>\n",
       "      <td>9.319743</td>\n",
       "      <td>6.748843</td>\n",
       "      <td>9.394887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.316976</td>\n",
       "      <td>0.705415</td>\n",
       "      <td>0.799992</td>\n",
       "      <td>0.607461</td>\n",
       "      <td>12.586746</td>\n",
       "      <td>9.520904</td>\n",
       "      <td>6.933305</td>\n",
       "      <td>9.680318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.271931</td>\n",
       "      <td>0.687565</td>\n",
       "      <td>0.783607</td>\n",
       "      <td>0.581034</td>\n",
       "      <td>13.007857</td>\n",
       "      <td>9.805272</td>\n",
       "      <td>7.211528</td>\n",
       "      <td>10.008219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.56078</td>\n",
       "      <td>0.772947</td>\n",
       "      <td>0.845124</td>\n",
       "      <td>0.726284</td>\n",
       "      <td>9.858841</td>\n",
       "      <td>8.225524</td>\n",
       "      <td>5.982273</td>\n",
       "      <td>8.022213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3 TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE     nRMSE     nRMSE   \n",
       "0      0.924426  0.923585  0.957766  0.935259   4.466253  4.845346   3.18993   \n",
       "1      0.842972   0.86424  0.918655  0.875289   6.397007  6.459126  4.426271   \n",
       "2      0.747989  0.815884  0.876318  0.813397   7.993798  7.522799  5.456765   \n",
       "3       0.65479  0.782952  0.843284  0.760342   9.231468  8.169178  6.141076   \n",
       "4       0.56902  0.759656  0.825673  0.718117   10.21229  8.597593  6.475377   \n",
       "5      0.490843  0.742939  0.819134  0.684305  10.990498  8.892895  6.594473   \n",
       "6      0.423002  0.729528  0.816302  0.656277  11.586419  9.122379  6.645167   \n",
       "7      0.365848   0.71771  0.810508  0.631355  12.116073  9.319743  6.748843   \n",
       "8      0.316976  0.705415  0.799992  0.607461  12.586746  9.520904  6.933305   \n",
       "9      0.271931  0.687565  0.783607  0.581034  13.007857  9.805272  7.211528   \n",
       "mean    0.56078  0.772947  0.845124  0.726284   9.858841  8.225524  5.982273   \n",
       "\n",
       "            mean  \n",
       "index      nRMSE  \n",
       "0       4.167176  \n",
       "1       5.760801  \n",
       "2       6.991121  \n",
       "3       7.847241  \n",
       "4        8.42842  \n",
       "5       8.825955  \n",
       "6       9.117988  \n",
       "7       9.394887  \n",
       "8       9.680318  \n",
       "9      10.008219  \n",
       "mean    8.022213  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/20_10_1_50_1_50_datt_seq2seq_gru_4.csv\n",
      "\n",
      "\n",
      "4th iteration\n",
      "history size: 20\n",
      "future size: 20\n",
      "Epoch 1/10000\n",
      "572/572 - 8s - loss: 0.3381 - val_loss: 0.2432 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "572/572 - 5s - loss: 0.2311 - val_loss: 0.2393 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "572/572 - 5s - loss: 0.2147 - val_loss: 0.2109 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "572/572 - 5s - loss: 0.2014 - val_loss: 0.2011 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "572/572 - 5s - loss: 0.1908 - val_loss: 0.1920 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "572/572 - 5s - loss: 0.1815 - val_loss: 0.1855 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "572/572 - 5s - loss: 0.1803 - val_loss: 0.1888 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "572/572 - 5s - loss: 0.1679 - val_loss: 0.1733 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "572/572 - 5s - loss: 0.1600 - val_loss: 0.1701 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "572/572 - 5s - loss: 0.1554 - val_loss: 0.1625 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "572/572 - 5s - loss: 0.1484 - val_loss: 0.1604 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "572/572 - 5s - loss: 0.1437 - val_loss: 0.1499 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "572/572 - 5s - loss: 0.1380 - val_loss: 0.1463 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "572/572 - 5s - loss: 0.1326 - val_loss: 0.1508 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "572/572 - 5s - loss: 0.1296 - val_loss: 0.1412 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "572/572 - 5s - loss: 0.1268 - val_loss: 0.1412 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "572/572 - 5s - loss: 0.1195 - val_loss: 0.1321 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "572/572 - 5s - loss: 0.1148 - val_loss: 0.1321 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "572/572 - 5s - loss: 0.1134 - val_loss: 0.1257 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "572/572 - 5s - loss: 0.1096 - val_loss: 0.1260 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "572/572 - 5s - loss: 0.1075 - val_loss: 0.1203 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "572/572 - 5s - loss: 0.1053 - val_loss: 0.1176 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "572/572 - 5s - loss: 0.1047 - val_loss: 0.1163 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "572/572 - 5s - loss: 0.1024 - val_loss: 0.1195 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "572/572 - 5s - loss: 0.0996 - val_loss: 0.1134 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "572/572 - 5s - loss: 0.0963 - val_loss: 0.1121 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "572/572 - 5s - loss: 0.0968 - val_loss: 0.1136 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "572/572 - 5s - loss: 0.0949 - val_loss: 0.1088 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "572/572 - 5s - loss: 0.0929 - val_loss: 0.1083 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "572/572 - 5s - loss: 0.0916 - val_loss: 0.1058 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "572/572 - 5s - loss: 0.0922 - val_loss: 0.1038 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "572/572 - 5s - loss: 0.0884 - val_loss: 0.1037 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "572/572 - 5s - loss: 0.0886 - val_loss: 0.1045 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "572/572 - 5s - loss: 0.0888 - val_loss: 0.1075 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "572/572 - 5s - loss: 0.0861 - val_loss: 0.0985 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "572/572 - 5s - loss: 0.0851 - val_loss: 0.1017 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "572/572 - 5s - loss: 0.0847 - val_loss: 0.0990 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "572/572 - 5s - loss: 0.0823 - val_loss: 0.0990 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "572/572 - 5s - loss: 0.0823 - val_loss: 0.1074 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "572/572 - 5s - loss: 0.0829 - val_loss: 0.0976 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "572/572 - 5s - loss: 0.0820 - val_loss: 0.0983 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "572/572 - 5s - loss: 0.0802 - val_loss: 0.1004 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "572/572 - 5s - loss: 0.0791 - val_loss: 0.0994 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "572/572 - 5s - loss: 0.0796 - val_loss: 0.0945 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "572/572 - 5s - loss: 0.0777 - val_loss: 0.0931 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "572/572 - 5s - loss: 0.0815 - val_loss: 0.0923 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "572/572 - 5s - loss: 0.0752 - val_loss: 0.0906 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "572/572 - 5s - loss: 0.0751 - val_loss: 0.0927 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "572/572 - 5s - loss: 0.0743 - val_loss: 0.0889 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "572/572 - 5s - loss: 0.0742 - val_loss: 0.0914 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "572/572 - 5s - loss: 0.0771 - val_loss: 0.0896 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "572/572 - 5s - loss: 0.0706 - val_loss: 0.0890 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "572/572 - 5s - loss: 0.0720 - val_loss: 0.0852 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "572/572 - 5s - loss: 0.0700 - val_loss: 0.0841 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "572/572 - 5s - loss: 0.0703 - val_loss: 0.0841 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "572/572 - 5s - loss: 0.0700 - val_loss: 0.0873 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "572/572 - 5s - loss: 0.0694 - val_loss: 0.0900 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "572/572 - 5s - loss: 0.0690 - val_loss: 0.1163 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "572/572 - 5s - loss: 0.0679 - val_loss: 0.0839 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "572/572 - 5s - loss: 0.0685 - val_loss: 0.0817 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "572/572 - 5s - loss: 0.0713 - val_loss: 0.0903 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "572/572 - 5s - loss: 0.0645 - val_loss: 0.0806 - 5s/epoch - 9ms/step\n",
      "Epoch 63/10000\n",
      "572/572 - 5s - loss: 0.0684 - val_loss: 0.0798 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "572/572 - 5s - loss: 0.0631 - val_loss: 0.0754 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "572/572 - 5s - loss: 0.0634 - val_loss: 0.0783 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "572/572 - 5s - loss: 0.0656 - val_loss: 0.0799 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "572/572 - 5s - loss: 0.0631 - val_loss: 0.0835 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "572/572 - 5s - loss: 0.0620 - val_loss: 0.0770 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "572/572 - 5s - loss: 0.0620 - val_loss: 0.0773 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "572/572 - 5s - loss: 0.0621 - val_loss: 0.0768 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "572/572 - 5s - loss: 0.0638 - val_loss: 0.0788 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "572/572 - 5s - loss: 0.0652 - val_loss: 0.0752 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "572/572 - 5s - loss: 0.0599 - val_loss: 0.0849 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "572/572 - 5s - loss: 0.0644 - val_loss: 0.0877 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "572/572 - 5s - loss: 0.0665 - val_loss: 0.0810 - 5s/epoch - 9ms/step\n",
      "Epoch 76/10000\n",
      "572/572 - 5s - loss: 0.0606 - val_loss: 0.0709 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "572/572 - 5s - loss: 0.0585 - val_loss: 0.0721 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "572/572 - 5s - loss: 0.0615 - val_loss: 0.0721 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "572/572 - 5s - loss: 0.0588 - val_loss: 0.0864 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "572/572 - 5s - loss: 0.0647 - val_loss: 0.0822 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "572/572 - 5s - loss: 0.0587 - val_loss: 0.0692 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "572/572 - 5s - loss: 0.0557 - val_loss: 0.0715 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "572/572 - 5s - loss: 0.0553 - val_loss: 0.0679 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "572/572 - 5s - loss: 0.0584 - val_loss: 0.2009 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "572/572 - 5s - loss: 0.0653 - val_loss: 0.0694 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "572/572 - 5s - loss: 0.0536 - val_loss: 0.0665 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "572/572 - 5s - loss: 0.0550 - val_loss: 0.0664 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "572/572 - 5s - loss: 0.0546 - val_loss: 0.0696 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "572/572 - 5s - loss: 0.0559 - val_loss: 0.0733 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "572/572 - 5s - loss: 0.0557 - val_loss: 0.0686 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "572/572 - 5s - loss: 0.0550 - val_loss: 0.0736 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "572/572 - 5s - loss: 0.0631 - val_loss: 0.0705 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "572/572 - 5s - loss: 0.0582 - val_loss: 0.0734 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "572/572 - 5s - loss: 0.0542 - val_loss: 0.0695 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "572/572 - 5s - loss: 0.0532 - val_loss: 0.0691 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "572/572 - 5s - loss: 0.0529 - val_loss: 0.0655 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "572/572 - 6s - loss: 0.0516 - val_loss: 0.0663 - 6s/epoch - 10ms/step\n",
      "Epoch 98/10000\n",
      "572/572 - 6s - loss: 0.0518 - val_loss: 0.0667 - 6s/epoch - 10ms/step\n",
      "Epoch 99/10000\n",
      "572/572 - 5s - loss: 0.0540 - val_loss: 0.0716 - 5s/epoch - 9ms/step\n",
      "Epoch 100/10000\n",
      "572/572 - 5s - loss: 0.0529 - val_loss: 0.0653 - 5s/epoch - 9ms/step\n",
      "Epoch 101/10000\n",
      "572/572 - 5s - loss: 0.0519 - val_loss: 0.0742 - 5s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "572/572 - 5s - loss: 0.0571 - val_loss: 0.0670 - 5s/epoch - 9ms/step\n",
      "Epoch 103/10000\n",
      "572/572 - 5s - loss: 0.0497 - val_loss: 0.0638 - 5s/epoch - 9ms/step\n",
      "Epoch 104/10000\n",
      "572/572 - 5s - loss: 0.0492 - val_loss: 0.0644 - 5s/epoch - 9ms/step\n",
      "Epoch 105/10000\n",
      "572/572 - 6s - loss: 0.0509 - val_loss: 0.0621 - 6s/epoch - 11ms/step\n",
      "Epoch 106/10000\n",
      "572/572 - 6s - loss: 0.0488 - val_loss: 0.0642 - 6s/epoch - 10ms/step\n",
      "Epoch 107/10000\n",
      "572/572 - 5s - loss: 0.0511 - val_loss: 0.0644 - 5s/epoch - 10ms/step\n",
      "Epoch 108/10000\n",
      "572/572 - 5s - loss: 0.0524 - val_loss: 0.0906 - 5s/epoch - 9ms/step\n",
      "Epoch 109/10000\n",
      "572/572 - 5s - loss: 0.0529 - val_loss: 0.0629 - 5s/epoch - 9ms/step\n",
      "Epoch 110/10000\n",
      "572/572 - 5s - loss: 0.0482 - val_loss: 0.0625 - 5s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "572/572 - 5s - loss: 0.0475 - val_loss: 0.0609 - 5s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "572/572 - 5s - loss: 0.0498 - val_loss: 0.0632 - 5s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "572/572 - 5s - loss: 0.0491 - val_loss: 0.0637 - 5s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "572/572 - 5s - loss: 0.0478 - val_loss: 0.0623 - 5s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "572/572 - 5s - loss: 0.0481 - val_loss: 0.0652 - 5s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "572/572 - 5s - loss: 0.0476 - val_loss: 0.0620 - 5s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "572/572 - 5s - loss: 0.0474 - val_loss: 0.0622 - 5s/epoch - 9ms/step\n",
      "Epoch 118/10000\n",
      "572/572 - 5s - loss: 0.0484 - val_loss: 0.0610 - 5s/epoch - 8ms/step\n",
      "Epoch 119/10000\n",
      "572/572 - 5s - loss: 0.0581 - val_loss: 0.0801 - 5s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "572/572 - 5s - loss: 0.0548 - val_loss: 0.0683 - 5s/epoch - 8ms/step\n",
      "Epoch 121/10000\n",
      "572/572 - 5s - loss: 0.0513 - val_loss: 0.0620 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_32_layer_call_fn, gru_cell_32_layer_call_and_return_conditional_losses, gru_cell_33_layer_call_fn, gru_cell_33_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_20_1_50_1_50_datt_seq2seq_gru_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_20_1_50_1_50_datt_seq2seq_gru_4\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002643D5286A0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000264793D3D60> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.900626</td>\n",
       "      <td>0.90041</td>\n",
       "      <td>0.941954</td>\n",
       "      <td>0.91433</td>\n",
       "      <td>6.228671</td>\n",
       "      <td>5.525166</td>\n",
       "      <td>3.727405</td>\n",
       "      <td>5.160414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80955</td>\n",
       "      <td>0.837642</td>\n",
       "      <td>0.89706</td>\n",
       "      <td>0.848084</td>\n",
       "      <td>8.466391</td>\n",
       "      <td>7.056892</td>\n",
       "      <td>4.963086</td>\n",
       "      <td>6.82879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.702976</td>\n",
       "      <td>0.779802</td>\n",
       "      <td>0.833405</td>\n",
       "      <td>0.772061</td>\n",
       "      <td>10.111782</td>\n",
       "      <td>8.221392</td>\n",
       "      <td>6.312929</td>\n",
       "      <td>8.215368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.597296</td>\n",
       "      <td>0.727256</td>\n",
       "      <td>0.790397</td>\n",
       "      <td>0.704983</td>\n",
       "      <td>11.380698</td>\n",
       "      <td>9.153121</td>\n",
       "      <td>7.080099</td>\n",
       "      <td>9.204639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.496775</td>\n",
       "      <td>0.684401</td>\n",
       "      <td>0.772873</td>\n",
       "      <td>0.65135</td>\n",
       "      <td>12.51785</td>\n",
       "      <td>9.850401</td>\n",
       "      <td>7.369401</td>\n",
       "      <td>9.91255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.403751</td>\n",
       "      <td>0.64978</td>\n",
       "      <td>0.766376</td>\n",
       "      <td>0.606636</td>\n",
       "      <td>13.578418</td>\n",
       "      <td>10.381802</td>\n",
       "      <td>7.472926</td>\n",
       "      <td>10.477715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.316838</td>\n",
       "      <td>0.622684</td>\n",
       "      <td>0.757312</td>\n",
       "      <td>0.565611</td>\n",
       "      <td>14.247869</td>\n",
       "      <td>10.77903</td>\n",
       "      <td>7.615492</td>\n",
       "      <td>10.880797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.234709</td>\n",
       "      <td>0.600561</td>\n",
       "      <td>0.73925</td>\n",
       "      <td>0.52484</td>\n",
       "      <td>14.789107</td>\n",
       "      <td>11.094055</td>\n",
       "      <td>7.892196</td>\n",
       "      <td>11.258453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.15756</td>\n",
       "      <td>0.576108</td>\n",
       "      <td>0.708364</td>\n",
       "      <td>0.480677</td>\n",
       "      <td>15.28353</td>\n",
       "      <td>11.432383</td>\n",
       "      <td>8.344442</td>\n",
       "      <td>11.686785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.085965</td>\n",
       "      <td>0.547843</td>\n",
       "      <td>0.659021</td>\n",
       "      <td>0.430943</td>\n",
       "      <td>15.684713</td>\n",
       "      <td>11.80964</td>\n",
       "      <td>9.019938</td>\n",
       "      <td>12.17143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.020138</td>\n",
       "      <td>0.518715</td>\n",
       "      <td>0.595307</td>\n",
       "      <td>0.378053</td>\n",
       "      <td>16.065912</td>\n",
       "      <td>12.186166</td>\n",
       "      <td>9.823919</td>\n",
       "      <td>12.691999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.040825</td>\n",
       "      <td>0.488435</td>\n",
       "      <td>0.535632</td>\n",
       "      <td>0.327747</td>\n",
       "      <td>16.448233</td>\n",
       "      <td>12.565185</td>\n",
       "      <td>10.52152</td>\n",
       "      <td>13.178313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.097512</td>\n",
       "      <td>0.458423</td>\n",
       "      <td>0.490348</td>\n",
       "      <td>0.283753</td>\n",
       "      <td>16.655975</td>\n",
       "      <td>12.929726</td>\n",
       "      <td>11.021749</td>\n",
       "      <td>13.535817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.148686</td>\n",
       "      <td>0.429468</td>\n",
       "      <td>0.451224</td>\n",
       "      <td>0.244002</td>\n",
       "      <td>16.810283</td>\n",
       "      <td>13.272596</td>\n",
       "      <td>11.43709</td>\n",
       "      <td>13.83999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.196969</td>\n",
       "      <td>0.399009</td>\n",
       "      <td>0.407151</td>\n",
       "      <td>0.203064</td>\n",
       "      <td>16.989613</td>\n",
       "      <td>13.624068</td>\n",
       "      <td>11.88715</td>\n",
       "      <td>14.166944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.240503</td>\n",
       "      <td>0.365242</td>\n",
       "      <td>0.35515</td>\n",
       "      <td>0.159963</td>\n",
       "      <td>17.128455</td>\n",
       "      <td>14.003711</td>\n",
       "      <td>12.39762</td>\n",
       "      <td>14.509929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.278474</td>\n",
       "      <td>0.326637</td>\n",
       "      <td>0.303347</td>\n",
       "      <td>0.11717</td>\n",
       "      <td>17.224238</td>\n",
       "      <td>14.424302</td>\n",
       "      <td>12.886592</td>\n",
       "      <td>14.845044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.308327</td>\n",
       "      <td>0.280202</td>\n",
       "      <td>0.25429</td>\n",
       "      <td>0.075389</td>\n",
       "      <td>17.385861</td>\n",
       "      <td>14.913741</td>\n",
       "      <td>13.333773</td>\n",
       "      <td>15.211125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.330331</td>\n",
       "      <td>0.226187</td>\n",
       "      <td>0.210416</td>\n",
       "      <td>0.035424</td>\n",
       "      <td>17.556072</td>\n",
       "      <td>15.463636</td>\n",
       "      <td>13.721342</td>\n",
       "      <td>15.58035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.345251</td>\n",
       "      <td>0.168613</td>\n",
       "      <td>0.176958</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>17.679628</td>\n",
       "      <td>16.028682</td>\n",
       "      <td>14.010632</td>\n",
       "      <td>15.906314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.136965</td>\n",
       "      <td>0.529371</td>\n",
       "      <td>0.582292</td>\n",
       "      <td>0.416209</td>\n",
       "      <td>14.611665</td>\n",
       "      <td>11.735785</td>\n",
       "      <td>9.541965</td>\n",
       "      <td>11.963138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.900626   0.90041  0.941954   0.91433   6.228671   5.525166   \n",
       "1       0.80955  0.837642   0.89706  0.848084   8.466391   7.056892   \n",
       "2      0.702976  0.779802  0.833405  0.772061  10.111782   8.221392   \n",
       "3      0.597296  0.727256  0.790397  0.704983  11.380698   9.153121   \n",
       "4      0.496775  0.684401  0.772873   0.65135   12.51785   9.850401   \n",
       "5      0.403751   0.64978  0.766376  0.606636  13.578418  10.381802   \n",
       "6      0.316838  0.622684  0.757312  0.565611  14.247869   10.77903   \n",
       "7      0.234709  0.600561   0.73925   0.52484  14.789107  11.094055   \n",
       "8       0.15756  0.576108  0.708364  0.480677   15.28353  11.432383   \n",
       "9      0.085965  0.547843  0.659021  0.430943  15.684713   11.80964   \n",
       "10     0.020138  0.518715  0.595307  0.378053  16.065912  12.186166   \n",
       "11    -0.040825  0.488435  0.535632  0.327747  16.448233  12.565185   \n",
       "12    -0.097512  0.458423  0.490348  0.283753  16.655975  12.929726   \n",
       "13    -0.148686  0.429468  0.451224  0.244002  16.810283  13.272596   \n",
       "14    -0.196969  0.399009  0.407151  0.203064  16.989613  13.624068   \n",
       "15    -0.240503  0.365242   0.35515  0.159963  17.128455  14.003711   \n",
       "16    -0.278474  0.326637  0.303347   0.11717  17.224238  14.424302   \n",
       "17    -0.308327  0.280202   0.25429  0.075389  17.385861  14.913741   \n",
       "18    -0.330331  0.226187  0.210416  0.035424  17.556072  15.463636   \n",
       "19    -0.345251  0.168613  0.176958  0.000107  17.679628  16.028682   \n",
       "mean   0.136965  0.529371  0.582292  0.416209  14.611665  11.735785   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       3.727405   5.160414  \n",
       "1       4.963086    6.82879  \n",
       "2       6.312929   8.215368  \n",
       "3       7.080099   9.204639  \n",
       "4       7.369401    9.91255  \n",
       "5       7.472926  10.477715  \n",
       "6       7.615492  10.880797  \n",
       "7       7.892196  11.258453  \n",
       "8       8.344442  11.686785  \n",
       "9       9.019938   12.17143  \n",
       "10      9.823919  12.691999  \n",
       "11      10.52152  13.178313  \n",
       "12     11.021749  13.535817  \n",
       "13      11.43709   13.83999  \n",
       "14      11.88715  14.166944  \n",
       "15      12.39762  14.509929  \n",
       "16     12.886592  14.845044  \n",
       "17     13.333773  15.211125  \n",
       "18     13.721342   15.58035  \n",
       "19     14.010632  15.906314  \n",
       "mean    9.541965  11.963138  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/20_20_1_50_1_50_datt_seq2seq_gru_4.csv\n",
      "\n",
      "\n",
      "4th iteration\n",
      "history size: 20\n",
      "future size: 30\n",
      "Epoch 1/10000\n",
      "568/568 - 8s - loss: 0.3870 - val_loss: 0.3238 - 8s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "568/568 - 5s - loss: 0.2889 - val_loss: 0.3000 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "568/568 - 5s - loss: 0.2702 - val_loss: 0.2864 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "568/568 - 5s - loss: 0.2530 - val_loss: 0.2778 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "568/568 - 4s - loss: 0.2372 - val_loss: 0.2512 - 4s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "568/568 - 5s - loss: 0.2218 - val_loss: 0.2440 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "568/568 - 5s - loss: 0.2105 - val_loss: 0.2275 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "568/568 - 5s - loss: 0.2000 - val_loss: 0.2287 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "568/568 - 5s - loss: 0.1924 - val_loss: 0.2183 - 5s/epoch - 9ms/step\n",
      "Epoch 10/10000\n",
      "568/568 - 5s - loss: 0.1868 - val_loss: 0.2081 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "568/568 - 5s - loss: 0.1783 - val_loss: 0.2007 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "568/568 - 5s - loss: 0.1733 - val_loss: 0.1989 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "568/568 - 5s - loss: 0.1677 - val_loss: 0.2032 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "568/568 - 5s - loss: 0.1628 - val_loss: 0.1817 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "568/568 - 5s - loss: 0.1576 - val_loss: 0.1781 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "568/568 - 5s - loss: 0.1573 - val_loss: 0.1779 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "568/568 - 5s - loss: 0.1503 - val_loss: 0.1705 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "568/568 - 5s - loss: 0.1462 - val_loss: 0.1691 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "568/568 - 5s - loss: 0.1421 - val_loss: 0.1658 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "568/568 - 5s - loss: 0.1397 - val_loss: 0.1628 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "568/568 - 5s - loss: 0.1378 - val_loss: 0.1635 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "568/568 - 5s - loss: 0.1345 - val_loss: 0.1561 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "568/568 - 5s - loss: 0.1306 - val_loss: 0.1548 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "568/568 - 5s - loss: 0.1278 - val_loss: 0.1510 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "568/568 - 5s - loss: 0.1238 - val_loss: 0.1416 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "568/568 - 5s - loss: 0.1233 - val_loss: 0.1467 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "568/568 - 5s - loss: 0.1190 - val_loss: 0.1456 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "568/568 - 5s - loss: 0.1177 - val_loss: 0.1400 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "568/568 - 5s - loss: 0.1151 - val_loss: 0.1404 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "568/568 - 5s - loss: 0.1160 - val_loss: 0.1354 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "568/568 - 5s - loss: 0.1105 - val_loss: 0.1333 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "568/568 - 5s - loss: 0.1100 - val_loss: 0.1485 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "568/568 - 4s - loss: 0.1080 - val_loss: 0.1325 - 4s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "568/568 - 5s - loss: 0.1068 - val_loss: 0.1243 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "568/568 - 4s - loss: 0.1065 - val_loss: 0.1293 - 4s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "568/568 - 5s - loss: 0.1044 - val_loss: 0.1296 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "568/568 - 5s - loss: 0.1037 - val_loss: 0.1172 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "568/568 - 4s - loss: 0.0997 - val_loss: 0.1172 - 4s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "568/568 - 5s - loss: 0.1007 - val_loss: 0.1228 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "568/568 - 4s - loss: 0.0974 - val_loss: 0.1444 - 4s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "568/568 - 5s - loss: 0.0970 - val_loss: 0.1106 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "568/568 - 5s - loss: 0.0950 - val_loss: 0.1169 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "568/568 - 5s - loss: 0.0942 - val_loss: 0.1161 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "568/568 - 5s - loss: 0.0925 - val_loss: 0.1221 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "568/568 - 5s - loss: 0.0904 - val_loss: 0.1165 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "568/568 - 5s - loss: 0.0969 - val_loss: 0.1102 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "568/568 - 5s - loss: 0.0901 - val_loss: 0.1050 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "568/568 - 4s - loss: 0.0880 - val_loss: 0.1043 - 4s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "568/568 - 5s - loss: 0.0915 - val_loss: 0.1117 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "568/568 - 4s - loss: 0.0898 - val_loss: 0.1061 - 4s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "568/568 - 4s - loss: 0.0859 - val_loss: 0.1029 - 4s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "568/568 - 5s - loss: 0.0847 - val_loss: 0.1029 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "568/568 - 5s - loss: 0.0848 - val_loss: 0.1223 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "568/568 - 5s - loss: 0.0853 - val_loss: 0.1065 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "568/568 - 5s - loss: 0.0840 - val_loss: 0.1048 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "568/568 - 4s - loss: 0.0833 - val_loss: 0.1062 - 4s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "568/568 - 4s - loss: 0.0861 - val_loss: 0.0991 - 4s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "568/568 - 4s - loss: 0.0789 - val_loss: 0.0928 - 4s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "568/568 - 5s - loss: 0.0799 - val_loss: 0.0991 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "568/568 - 5s - loss: 0.0848 - val_loss: 0.0975 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "568/568 - 5s - loss: 0.0822 - val_loss: 0.0977 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "568/568 - 4s - loss: 0.0776 - val_loss: 0.0949 - 4s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "568/568 - 5s - loss: 0.0799 - val_loss: 0.0973 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "568/568 - 5s - loss: 0.0799 - val_loss: 0.1629 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "568/568 - 5s - loss: 0.0839 - val_loss: 0.0968 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "568/568 - 5s - loss: 0.0767 - val_loss: 0.0958 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "568/568 - 5s - loss: 0.0755 - val_loss: 0.0913 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "568/568 - 4s - loss: 0.0747 - val_loss: 0.1011 - 4s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "568/568 - 5s - loss: 0.0839 - val_loss: 0.0977 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "568/568 - 4s - loss: 0.0741 - val_loss: 0.0933 - 4s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "568/568 - 5s - loss: 0.0766 - val_loss: 0.0878 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "568/568 - 5s - loss: 0.0717 - val_loss: 0.0925 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "568/568 - 5s - loss: 0.0729 - val_loss: 0.0902 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "568/568 - 5s - loss: 0.0790 - val_loss: 0.0901 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "568/568 - 4s - loss: 0.0717 - val_loss: 0.0905 - 4s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "568/568 - 5s - loss: 0.0728 - val_loss: 0.0985 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "568/568 - 4s - loss: 0.0759 - val_loss: 0.0937 - 4s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "568/568 - 5s - loss: 0.0727 - val_loss: 0.0900 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "568/568 - 5s - loss: 0.0726 - val_loss: 0.0894 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "568/568 - 5s - loss: 0.0722 - val_loss: 0.0887 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "568/568 - 5s - loss: 0.0694 - val_loss: 0.0968 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_34_layer_call_fn, gru_cell_34_layer_call_and_return_conditional_losses, gru_cell_35_layer_call_fn, gru_cell_35_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_30_1_50_1_50_datt_seq2seq_gru_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_30_1_50_1_50_datt_seq2seq_gru_4\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002646E3B1AF0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002643D3CD1C0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.867822</td>\n",
       "      <td>0.916434</td>\n",
       "      <td>0.933174</td>\n",
       "      <td>0.90581</td>\n",
       "      <td>7.178162</td>\n",
       "      <td>5.059983</td>\n",
       "      <td>3.98094</td>\n",
       "      <td>5.406361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.805784</td>\n",
       "      <td>0.863514</td>\n",
       "      <td>0.886634</td>\n",
       "      <td>0.851977</td>\n",
       "      <td>8.701293</td>\n",
       "      <td>6.467043</td>\n",
       "      <td>5.18524</td>\n",
       "      <td>6.784525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.709689</td>\n",
       "      <td>0.819114</td>\n",
       "      <td>0.81903</td>\n",
       "      <td>0.782611</td>\n",
       "      <td>10.639582</td>\n",
       "      <td>7.445677</td>\n",
       "      <td>6.551626</td>\n",
       "      <td>8.212295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.604335</td>\n",
       "      <td>0.786165</td>\n",
       "      <td>0.758187</td>\n",
       "      <td>0.716229</td>\n",
       "      <td>12.422776</td>\n",
       "      <td>8.096457</td>\n",
       "      <td>7.573476</td>\n",
       "      <td>9.364236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.499106</td>\n",
       "      <td>0.759072</td>\n",
       "      <td>0.713469</td>\n",
       "      <td>0.657215</td>\n",
       "      <td>13.977018</td>\n",
       "      <td>8.595715</td>\n",
       "      <td>8.243995</td>\n",
       "      <td>10.272243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.399461</td>\n",
       "      <td>0.735992</td>\n",
       "      <td>0.682615</td>\n",
       "      <td>0.606022</td>\n",
       "      <td>15.301729</td>\n",
       "      <td>9.000661</td>\n",
       "      <td>8.67565</td>\n",
       "      <td>10.99268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.307671</td>\n",
       "      <td>0.713962</td>\n",
       "      <td>0.661057</td>\n",
       "      <td>0.560896</td>\n",
       "      <td>16.426665</td>\n",
       "      <td>9.371382</td>\n",
       "      <td>8.96421</td>\n",
       "      <td>11.587419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.222474</td>\n",
       "      <td>0.691627</td>\n",
       "      <td>0.644281</td>\n",
       "      <td>0.519461</td>\n",
       "      <td>17.404319</td>\n",
       "      <td>9.733307</td>\n",
       "      <td>9.181636</td>\n",
       "      <td>12.106421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.144626</td>\n",
       "      <td>0.667285</td>\n",
       "      <td>0.626714</td>\n",
       "      <td>0.479542</td>\n",
       "      <td>18.24959</td>\n",
       "      <td>10.114201</td>\n",
       "      <td>9.403406</td>\n",
       "      <td>12.589066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.072847</td>\n",
       "      <td>0.64356</td>\n",
       "      <td>0.603036</td>\n",
       "      <td>0.439814</td>\n",
       "      <td>18.995619</td>\n",
       "      <td>10.472152</td>\n",
       "      <td>9.695762</td>\n",
       "      <td>13.054511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.006222</td>\n",
       "      <td>0.620356</td>\n",
       "      <td>0.569838</td>\n",
       "      <td>0.398805</td>\n",
       "      <td>19.663749</td>\n",
       "      <td>10.810589</td>\n",
       "      <td>10.092585</td>\n",
       "      <td>13.522308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.055983</td>\n",
       "      <td>0.597809</td>\n",
       "      <td>0.532668</td>\n",
       "      <td>0.358164</td>\n",
       "      <td>19.896026</td>\n",
       "      <td>11.130374</td>\n",
       "      <td>10.520275</td>\n",
       "      <td>13.848891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.113427</td>\n",
       "      <td>0.577706</td>\n",
       "      <td>0.501937</td>\n",
       "      <td>0.322072</td>\n",
       "      <td>19.532635</td>\n",
       "      <td>11.40932</td>\n",
       "      <td>10.861845</td>\n",
       "      <td>13.9346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.165278</td>\n",
       "      <td>0.559669</td>\n",
       "      <td>0.481706</td>\n",
       "      <td>0.292033</td>\n",
       "      <td>19.310953</td>\n",
       "      <td>11.654325</td>\n",
       "      <td>11.081315</td>\n",
       "      <td>14.015531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.211054</td>\n",
       "      <td>0.542841</td>\n",
       "      <td>0.468105</td>\n",
       "      <td>0.266631</td>\n",
       "      <td>19.370485</td>\n",
       "      <td>11.880171</td>\n",
       "      <td>11.226486</td>\n",
       "      <td>14.159048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.252293</td>\n",
       "      <td>0.52793</td>\n",
       "      <td>0.455494</td>\n",
       "      <td>0.24371</td>\n",
       "      <td>19.632705</td>\n",
       "      <td>12.078198</td>\n",
       "      <td>11.358606</td>\n",
       "      <td>14.356503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.288458</td>\n",
       "      <td>0.513518</td>\n",
       "      <td>0.441497</td>\n",
       "      <td>0.222186</td>\n",
       "      <td>19.526792</td>\n",
       "      <td>12.26475</td>\n",
       "      <td>11.503766</td>\n",
       "      <td>14.431769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.321874</td>\n",
       "      <td>0.499091</td>\n",
       "      <td>0.426676</td>\n",
       "      <td>0.201297</td>\n",
       "      <td>19.404284</td>\n",
       "      <td>12.449077</td>\n",
       "      <td>11.655682</td>\n",
       "      <td>14.503014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.353146</td>\n",
       "      <td>0.484271</td>\n",
       "      <td>0.410917</td>\n",
       "      <td>0.180681</td>\n",
       "      <td>19.346602</td>\n",
       "      <td>12.635735</td>\n",
       "      <td>11.815464</td>\n",
       "      <td>14.599267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.382605</td>\n",
       "      <td>0.468438</td>\n",
       "      <td>0.394348</td>\n",
       "      <td>0.16006</td>\n",
       "      <td>19.277257</td>\n",
       "      <td>12.830462</td>\n",
       "      <td>11.981191</td>\n",
       "      <td>14.696303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.410383</td>\n",
       "      <td>0.45233</td>\n",
       "      <td>0.378285</td>\n",
       "      <td>0.140078</td>\n",
       "      <td>19.272396</td>\n",
       "      <td>13.025461</td>\n",
       "      <td>12.140023</td>\n",
       "      <td>14.812627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.435913</td>\n",
       "      <td>0.436302</td>\n",
       "      <td>0.364834</td>\n",
       "      <td>0.121741</td>\n",
       "      <td>19.32621</td>\n",
       "      <td>13.216067</td>\n",
       "      <td>12.272207</td>\n",
       "      <td>14.938161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.4598</td>\n",
       "      <td>0.420516</td>\n",
       "      <td>0.353884</td>\n",
       "      <td>0.104867</td>\n",
       "      <td>19.223156</td>\n",
       "      <td>13.400987</td>\n",
       "      <td>12.379297</td>\n",
       "      <td>15.001146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.48233</td>\n",
       "      <td>0.403777</td>\n",
       "      <td>0.342763</td>\n",
       "      <td>0.08807</td>\n",
       "      <td>19.114032</td>\n",
       "      <td>13.595094</td>\n",
       "      <td>12.486933</td>\n",
       "      <td>15.065353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.504343</td>\n",
       "      <td>0.386404</td>\n",
       "      <td>0.329305</td>\n",
       "      <td>0.070455</td>\n",
       "      <td>19.065284</td>\n",
       "      <td>13.793555</td>\n",
       "      <td>12.614979</td>\n",
       "      <td>15.157939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.524794</td>\n",
       "      <td>0.368866</td>\n",
       "      <td>0.314326</td>\n",
       "      <td>0.052799</td>\n",
       "      <td>19.007041</td>\n",
       "      <td>13.990723</td>\n",
       "      <td>12.756457</td>\n",
       "      <td>15.251407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.54577</td>\n",
       "      <td>0.35204</td>\n",
       "      <td>0.30077</td>\n",
       "      <td>0.03568</td>\n",
       "      <td>18.953047</td>\n",
       "      <td>14.175841</td>\n",
       "      <td>12.884204</td>\n",
       "      <td>15.337697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.564794</td>\n",
       "      <td>0.335923</td>\n",
       "      <td>0.290187</td>\n",
       "      <td>0.020439</td>\n",
       "      <td>19.023144</td>\n",
       "      <td>14.349985</td>\n",
       "      <td>12.985091</td>\n",
       "      <td>15.45274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.580142</td>\n",
       "      <td>0.321239</td>\n",
       "      <td>0.283725</td>\n",
       "      <td>0.008274</td>\n",
       "      <td>19.138562</td>\n",
       "      <td>14.506115</td>\n",
       "      <td>13.048211</td>\n",
       "      <td>15.564296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.594364</td>\n",
       "      <td>0.308624</td>\n",
       "      <td>0.280383</td>\n",
       "      <td>-0.001786</td>\n",
       "      <td>19.246607</td>\n",
       "      <td>14.637844</td>\n",
       "      <td>13.08317</td>\n",
       "      <td>15.655874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.086891</td>\n",
       "      <td>0.559146</td>\n",
       "      <td>0.508328</td>\n",
       "      <td>0.326861</td>\n",
       "      <td>17.520924</td>\n",
       "      <td>11.406375</td>\n",
       "      <td>10.540124</td>\n",
       "      <td>13.155808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.867822  0.916434  0.933174   0.90581   7.178162   5.059983   \n",
       "1      0.805784  0.863514  0.886634  0.851977   8.701293   6.467043   \n",
       "2      0.709689  0.819114   0.81903  0.782611  10.639582   7.445677   \n",
       "3      0.604335  0.786165  0.758187  0.716229  12.422776   8.096457   \n",
       "4      0.499106  0.759072  0.713469  0.657215  13.977018   8.595715   \n",
       "5      0.399461  0.735992  0.682615  0.606022  15.301729   9.000661   \n",
       "6      0.307671  0.713962  0.661057  0.560896  16.426665   9.371382   \n",
       "7      0.222474  0.691627  0.644281  0.519461  17.404319   9.733307   \n",
       "8      0.144626  0.667285  0.626714  0.479542   18.24959  10.114201   \n",
       "9      0.072847   0.64356  0.603036  0.439814  18.995619  10.472152   \n",
       "10     0.006222  0.620356  0.569838  0.398805  19.663749  10.810589   \n",
       "11    -0.055983  0.597809  0.532668  0.358164  19.896026  11.130374   \n",
       "12    -0.113427  0.577706  0.501937  0.322072  19.532635   11.40932   \n",
       "13    -0.165278  0.559669  0.481706  0.292033  19.310953  11.654325   \n",
       "14    -0.211054  0.542841  0.468105  0.266631  19.370485  11.880171   \n",
       "15    -0.252293   0.52793  0.455494   0.24371  19.632705  12.078198   \n",
       "16    -0.288458  0.513518  0.441497  0.222186  19.526792   12.26475   \n",
       "17    -0.321874  0.499091  0.426676  0.201297  19.404284  12.449077   \n",
       "18    -0.353146  0.484271  0.410917  0.180681  19.346602  12.635735   \n",
       "19    -0.382605  0.468438  0.394348   0.16006  19.277257  12.830462   \n",
       "20    -0.410383   0.45233  0.378285  0.140078  19.272396  13.025461   \n",
       "21    -0.435913  0.436302  0.364834  0.121741   19.32621  13.216067   \n",
       "22      -0.4598  0.420516  0.353884  0.104867  19.223156  13.400987   \n",
       "23     -0.48233  0.403777  0.342763   0.08807  19.114032  13.595094   \n",
       "24    -0.504343  0.386404  0.329305  0.070455  19.065284  13.793555   \n",
       "25    -0.524794  0.368866  0.314326  0.052799  19.007041  13.990723   \n",
       "26     -0.54577   0.35204   0.30077   0.03568  18.953047  14.175841   \n",
       "27    -0.564794  0.335923  0.290187  0.020439  19.023144  14.349985   \n",
       "28    -0.580142  0.321239  0.283725  0.008274  19.138562  14.506115   \n",
       "29    -0.594364  0.308624  0.280383 -0.001786  19.246607  14.637844   \n",
       "mean  -0.086891  0.559146  0.508328  0.326861  17.520924  11.406375   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0        3.98094   5.406361  \n",
       "1        5.18524   6.784525  \n",
       "2       6.551626   8.212295  \n",
       "3       7.573476   9.364236  \n",
       "4       8.243995  10.272243  \n",
       "5        8.67565   10.99268  \n",
       "6        8.96421  11.587419  \n",
       "7       9.181636  12.106421  \n",
       "8       9.403406  12.589066  \n",
       "9       9.695762  13.054511  \n",
       "10     10.092585  13.522308  \n",
       "11     10.520275  13.848891  \n",
       "12     10.861845    13.9346  \n",
       "13     11.081315  14.015531  \n",
       "14     11.226486  14.159048  \n",
       "15     11.358606  14.356503  \n",
       "16     11.503766  14.431769  \n",
       "17     11.655682  14.503014  \n",
       "18     11.815464  14.599267  \n",
       "19     11.981191  14.696303  \n",
       "20     12.140023  14.812627  \n",
       "21     12.272207  14.938161  \n",
       "22     12.379297  15.001146  \n",
       "23     12.486933  15.065353  \n",
       "24     12.614979  15.157939  \n",
       "25     12.756457  15.251407  \n",
       "26     12.884204  15.337697  \n",
       "27     12.985091   15.45274  \n",
       "28     13.048211  15.564296  \n",
       "29      13.08317  15.655874  \n",
       "mean   10.540124  13.155808  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/20_30_1_50_1_50_datt_seq2seq_gru_4.csv\n",
      "\n",
      "\n",
      "4th iteration\n",
      "history size: 30\n",
      "future size: 10\n",
      "Epoch 1/10000\n",
      "572/572 - 8s - loss: 0.2442 - val_loss: 0.1545 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "572/572 - 5s - loss: 0.1485 - val_loss: 0.1390 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "572/572 - 5s - loss: 0.1362 - val_loss: 0.1274 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "572/572 - 5s - loss: 0.1263 - val_loss: 0.1161 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "572/572 - 5s - loss: 0.1163 - val_loss: 0.1072 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "572/572 - 5s - loss: 0.1080 - val_loss: 0.1052 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "572/572 - 5s - loss: 0.1041 - val_loss: 0.0986 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "572/572 - 5s - loss: 0.0984 - val_loss: 0.0958 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "572/572 - 5s - loss: 0.0943 - val_loss: 0.1045 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "572/572 - 5s - loss: 0.0919 - val_loss: 0.0932 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "572/572 - 5s - loss: 0.0877 - val_loss: 0.0958 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "572/572 - 5s - loss: 0.0856 - val_loss: 0.0882 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "572/572 - 5s - loss: 0.0820 - val_loss: 0.0860 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "572/572 - 5s - loss: 0.0796 - val_loss: 0.0903 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "572/572 - 5s - loss: 0.0786 - val_loss: 0.0881 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "572/572 - 5s - loss: 0.0763 - val_loss: 0.0819 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "572/572 - 5s - loss: 0.0728 - val_loss: 0.0826 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "572/572 - 5s - loss: 0.0725 - val_loss: 0.0836 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "572/572 - 5s - loss: 0.0722 - val_loss: 0.0781 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "572/572 - 5s - loss: 0.0714 - val_loss: 0.0804 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "572/572 - 5s - loss: 0.0738 - val_loss: 0.0780 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "572/572 - 5s - loss: 0.0655 - val_loss: 0.0811 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "572/572 - 5s - loss: 0.0663 - val_loss: 0.0736 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "572/572 - 5s - loss: 0.0635 - val_loss: 0.0712 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "572/572 - 5s - loss: 0.0626 - val_loss: 0.0693 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "572/572 - 5s - loss: 0.0613 - val_loss: 0.0753 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "572/572 - 5s - loss: 0.0613 - val_loss: 0.0713 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "572/572 - 5s - loss: 0.0597 - val_loss: 0.0723 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "572/572 - 5s - loss: 0.0599 - val_loss: 0.0694 - 5s/epoch - 9ms/step\n",
      "Epoch 30/10000\n",
      "572/572 - 5s - loss: 0.0590 - val_loss: 0.0707 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "572/572 - 5s - loss: 0.0577 - val_loss: 0.0666 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "572/572 - 5s - loss: 0.0554 - val_loss: 0.0634 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "572/572 - 5s - loss: 0.0551 - val_loss: 0.0639 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "572/572 - 5s - loss: 0.0535 - val_loss: 0.0629 - 5s/epoch - 9ms/step\n",
      "Epoch 35/10000\n",
      "572/572 - 5s - loss: 0.0539 - val_loss: 0.0665 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "572/572 - 5s - loss: 0.0530 - val_loss: 0.0644 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "572/572 - 5s - loss: 0.0528 - val_loss: 0.0602 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "572/572 - 5s - loss: 0.0507 - val_loss: 0.0612 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "572/572 - 5s - loss: 0.0498 - val_loss: 0.0659 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "572/572 - 5s - loss: 0.0525 - val_loss: 0.0721 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "572/572 - 5s - loss: 0.0542 - val_loss: 0.0687 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "572/572 - 5s - loss: 0.0495 - val_loss: 0.0586 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "572/572 - 5s - loss: 0.0530 - val_loss: 0.0596 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "572/572 - 5s - loss: 0.0515 - val_loss: 0.0577 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "572/572 - 5s - loss: 0.0466 - val_loss: 0.0574 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "572/572 - 5s - loss: 0.0466 - val_loss: 0.0558 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "572/572 - 5s - loss: 0.0459 - val_loss: 0.0596 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "572/572 - 5s - loss: 0.0469 - val_loss: 0.0574 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "572/572 - 5s - loss: 0.0469 - val_loss: 0.0554 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "572/572 - 5s - loss: 0.0462 - val_loss: 0.0592 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "572/572 - 5s - loss: 0.0464 - val_loss: 0.0564 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "572/572 - 5s - loss: 0.0450 - val_loss: 0.0560 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "572/572 - 5s - loss: 0.0462 - val_loss: 0.0606 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "572/572 - 5s - loss: 0.0441 - val_loss: 0.0526 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "572/572 - 5s - loss: 0.0427 - val_loss: 0.0557 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "572/572 - 5s - loss: 0.0441 - val_loss: 0.0559 - 5s/epoch - 9ms/step\n",
      "Epoch 57/10000\n",
      "572/572 - 5s - loss: 0.0436 - val_loss: 0.0571 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "572/572 - 5s - loss: 0.0442 - val_loss: 0.0598 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "572/572 - 5s - loss: 0.0435 - val_loss: 0.0540 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "572/572 - 5s - loss: 0.0426 - val_loss: 0.0525 - 5s/epoch - 9ms/step\n",
      "Epoch 61/10000\n",
      "572/572 - 5s - loss: 0.0414 - val_loss: 0.0506 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "572/572 - 5s - loss: 0.0413 - val_loss: 0.0517 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "572/572 - 5s - loss: 0.0419 - val_loss: 0.0515 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "572/572 - 5s - loss: 0.0415 - val_loss: 0.0543 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "572/572 - 5s - loss: 0.0405 - val_loss: 0.0529 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "572/572 - 5s - loss: 0.0399 - val_loss: 0.0520 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "572/572 - 5s - loss: 0.0393 - val_loss: 0.0522 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "572/572 - 5s - loss: 0.0397 - val_loss: 0.0495 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "572/572 - 5s - loss: 0.0394 - val_loss: 0.0520 - 5s/epoch - 9ms/step\n",
      "Epoch 70/10000\n",
      "572/572 - 5s - loss: 0.0401 - val_loss: 0.0504 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "572/572 - 5s - loss: 0.0390 - val_loss: 0.0487 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "572/572 - 5s - loss: 0.0386 - val_loss: 0.0479 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "572/572 - 5s - loss: 0.0378 - val_loss: 0.0530 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "572/572 - 5s - loss: 0.0397 - val_loss: 0.0488 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "572/572 - 5s - loss: 0.0379 - val_loss: 0.0486 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "572/572 - 5s - loss: 0.0375 - val_loss: 0.0518 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "572/572 - 5s - loss: 0.0373 - val_loss: 0.0483 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "572/572 - 5s - loss: 0.0368 - val_loss: 0.0476 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "572/572 - 5s - loss: 0.0371 - val_loss: 0.0492 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "572/572 - 5s - loss: 0.0368 - val_loss: 0.0477 - 5s/epoch - 9ms/step\n",
      "Epoch 81/10000\n",
      "572/572 - 5s - loss: 0.0371 - val_loss: 0.0488 - 5s/epoch - 9ms/step\n",
      "Epoch 82/10000\n",
      "572/572 - 5s - loss: 0.0362 - val_loss: 0.0477 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "572/572 - 5s - loss: 0.0359 - val_loss: 0.0479 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "572/572 - 5s - loss: 0.0357 - val_loss: 0.0483 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "572/572 - 5s - loss: 0.0356 - val_loss: 0.0503 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "572/572 - 5s - loss: 0.0351 - val_loss: 0.0461 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "572/572 - 5s - loss: 0.0346 - val_loss: 0.0453 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "572/572 - 5s - loss: 0.0344 - val_loss: 0.0468 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "572/572 - 5s - loss: 0.0353 - val_loss: 0.0469 - 5s/epoch - 9ms/step\n",
      "Epoch 90/10000\n",
      "572/572 - 5s - loss: 0.0360 - val_loss: 0.0474 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "572/572 - 5s - loss: 0.0340 - val_loss: 0.0479 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "572/572 - 5s - loss: 0.0338 - val_loss: 0.0465 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "572/572 - 5s - loss: 0.0335 - val_loss: 0.0468 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "572/572 - 5s - loss: 0.0338 - val_loss: 0.0467 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "572/572 - 5s - loss: 0.0337 - val_loss: 0.0446 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "572/572 - 5s - loss: 0.0368 - val_loss: 0.0460 - 5s/epoch - 9ms/step\n",
      "Epoch 97/10000\n",
      "572/572 - 5s - loss: 0.0318 - val_loss: 0.0445 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "572/572 - 5s - loss: 0.0322 - val_loss: 0.0441 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "572/572 - 5s - loss: 0.0334 - val_loss: 0.0452 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "572/572 - 5s - loss: 0.0321 - val_loss: 0.0459 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "572/572 - 5s - loss: 0.0318 - val_loss: 0.0444 - 5s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "572/572 - 5s - loss: 0.0313 - val_loss: 0.0429 - 5s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "572/572 - 5s - loss: 0.0343 - val_loss: 0.0460 - 5s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "572/572 - 5s - loss: 0.0312 - val_loss: 0.0432 - 5s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "572/572 - 5s - loss: 0.0306 - val_loss: 0.0419 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "572/572 - 5s - loss: 0.0307 - val_loss: 0.0426 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "572/572 - 5s - loss: 0.0308 - val_loss: 0.0412 - 5s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "572/572 - 5s - loss: 0.0306 - val_loss: 0.0421 - 5s/epoch - 8ms/step\n",
      "Epoch 109/10000\n",
      "572/572 - 5s - loss: 0.0310 - val_loss: 0.0432 - 5s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "572/572 - 5s - loss: 0.0292 - val_loss: 0.0418 - 5s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "572/572 - 5s - loss: 0.0294 - val_loss: 0.0422 - 5s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "572/572 - 5s - loss: 0.0345 - val_loss: 0.0430 - 5s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "572/572 - 5s - loss: 0.0301 - val_loss: 0.0413 - 5s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "572/572 - 5s - loss: 0.0296 - val_loss: 0.0428 - 5s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "572/572 - 5s - loss: 0.0287 - val_loss: 0.0396 - 5s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "572/572 - 5s - loss: 0.0290 - val_loss: 0.0438 - 5s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "572/572 - 5s - loss: 0.0291 - val_loss: 0.0394 - 5s/epoch - 8ms/step\n",
      "Epoch 118/10000\n",
      "572/572 - 5s - loss: 0.0289 - val_loss: 0.0415 - 5s/epoch - 8ms/step\n",
      "Epoch 119/10000\n",
      "572/572 - 5s - loss: 0.0311 - val_loss: 0.0410 - 5s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "572/572 - 5s - loss: 0.0281 - val_loss: 0.0404 - 5s/epoch - 8ms/step\n",
      "Epoch 121/10000\n",
      "572/572 - 5s - loss: 0.0288 - val_loss: 0.0390 - 5s/epoch - 9ms/step\n",
      "Epoch 122/10000\n",
      "572/572 - 5s - loss: 0.0276 - val_loss: 0.0397 - 5s/epoch - 8ms/step\n",
      "Epoch 123/10000\n",
      "572/572 - 5s - loss: 0.0280 - val_loss: 0.0403 - 5s/epoch - 8ms/step\n",
      "Epoch 124/10000\n",
      "572/572 - 5s - loss: 0.0294 - val_loss: 0.0388 - 5s/epoch - 8ms/step\n",
      "Epoch 125/10000\n",
      "572/572 - 5s - loss: 0.0281 - val_loss: 0.0397 - 5s/epoch - 8ms/step\n",
      "Epoch 126/10000\n",
      "572/572 - 5s - loss: 0.0292 - val_loss: 0.0394 - 5s/epoch - 8ms/step\n",
      "Epoch 127/10000\n",
      "572/572 - 5s - loss: 0.0270 - val_loss: 0.0380 - 5s/epoch - 8ms/step\n",
      "Epoch 128/10000\n",
      "572/572 - 5s - loss: 0.0281 - val_loss: 0.0430 - 5s/epoch - 8ms/step\n",
      "Epoch 129/10000\n",
      "572/572 - 5s - loss: 0.0286 - val_loss: 0.0381 - 5s/epoch - 8ms/step\n",
      "Epoch 130/10000\n",
      "572/572 - 5s - loss: 0.0312 - val_loss: 0.0414 - 5s/epoch - 8ms/step\n",
      "Epoch 131/10000\n",
      "572/572 - 5s - loss: 0.0275 - val_loss: 0.0386 - 5s/epoch - 9ms/step\n",
      "Epoch 132/10000\n",
      "572/572 - 5s - loss: 0.0266 - val_loss: 0.0377 - 5s/epoch - 9ms/step\n",
      "Epoch 133/10000\n",
      "572/572 - 5s - loss: 0.0273 - val_loss: 0.0396 - 5s/epoch - 8ms/step\n",
      "Epoch 134/10000\n",
      "572/572 - 5s - loss: 0.0277 - val_loss: 0.0388 - 5s/epoch - 9ms/step\n",
      "Epoch 135/10000\n",
      "572/572 - 5s - loss: 0.0271 - val_loss: 0.0379 - 5s/epoch - 8ms/step\n",
      "Epoch 136/10000\n",
      "572/572 - 5s - loss: 0.0265 - val_loss: 0.0398 - 5s/epoch - 8ms/step\n",
      "Epoch 137/10000\n",
      "572/572 - 5s - loss: 0.0269 - val_loss: 0.0387 - 5s/epoch - 8ms/step\n",
      "Epoch 138/10000\n",
      "572/572 - 5s - loss: 0.0264 - val_loss: 0.0397 - 5s/epoch - 8ms/step\n",
      "Epoch 139/10000\n",
      "572/572 - 5s - loss: 0.0262 - val_loss: 0.0378 - 5s/epoch - 8ms/step\n",
      "Epoch 140/10000\n",
      "572/572 - 5s - loss: 0.0278 - val_loss: 0.0404 - 5s/epoch - 8ms/step\n",
      "Epoch 141/10000\n",
      "572/572 - 5s - loss: 0.0273 - val_loss: 0.0373 - 5s/epoch - 8ms/step\n",
      "Epoch 142/10000\n",
      "572/572 - 5s - loss: 0.0273 - val_loss: 0.0395 - 5s/epoch - 8ms/step\n",
      "Epoch 143/10000\n",
      "572/572 - 5s - loss: 0.0266 - val_loss: 0.0421 - 5s/epoch - 8ms/step\n",
      "Epoch 144/10000\n",
      "572/572 - 5s - loss: 0.0265 - val_loss: 0.0362 - 5s/epoch - 8ms/step\n",
      "Epoch 145/10000\n",
      "572/572 - 5s - loss: 0.0254 - val_loss: 0.0387 - 5s/epoch - 8ms/step\n",
      "Epoch 146/10000\n",
      "572/572 - 5s - loss: 0.0270 - val_loss: 0.0393 - 5s/epoch - 8ms/step\n",
      "Epoch 147/10000\n",
      "572/572 - 5s - loss: 0.0263 - val_loss: 0.0370 - 5s/epoch - 8ms/step\n",
      "Epoch 148/10000\n",
      "572/572 - 5s - loss: 0.0251 - val_loss: 0.0362 - 5s/epoch - 9ms/step\n",
      "Epoch 149/10000\n",
      "572/572 - 5s - loss: 0.0255 - val_loss: 0.0377 - 5s/epoch - 8ms/step\n",
      "Epoch 150/10000\n",
      "572/572 - 5s - loss: 0.0253 - val_loss: 0.0381 - 5s/epoch - 8ms/step\n",
      "Epoch 151/10000\n",
      "572/572 - 5s - loss: 0.0259 - val_loss: 0.0390 - 5s/epoch - 8ms/step\n",
      "Epoch 152/10000\n",
      "572/572 - 5s - loss: 0.0261 - val_loss: 0.0383 - 5s/epoch - 8ms/step\n",
      "Epoch 153/10000\n",
      "572/572 - 5s - loss: 0.0250 - val_loss: 0.0362 - 5s/epoch - 8ms/step\n",
      "Epoch 154/10000\n",
      "572/572 - 5s - loss: 0.0252 - val_loss: 0.0359 - 5s/epoch - 8ms/step\n",
      "Epoch 155/10000\n",
      "572/572 - 5s - loss: 0.0245 - val_loss: 0.0355 - 5s/epoch - 8ms/step\n",
      "Epoch 156/10000\n",
      "572/572 - 5s - loss: 0.0252 - val_loss: 0.0362 - 5s/epoch - 8ms/step\n",
      "Epoch 157/10000\n",
      "572/572 - 5s - loss: 0.0256 - val_loss: 0.0396 - 5s/epoch - 8ms/step\n",
      "Epoch 158/10000\n",
      "572/572 - 5s - loss: 0.0270 - val_loss: 0.0354 - 5s/epoch - 8ms/step\n",
      "Epoch 159/10000\n",
      "572/572 - 5s - loss: 0.0243 - val_loss: 0.0365 - 5s/epoch - 8ms/step\n",
      "Epoch 160/10000\n",
      "572/572 - 5s - loss: 0.0239 - val_loss: 0.0357 - 5s/epoch - 8ms/step\n",
      "Epoch 161/10000\n",
      "572/572 - 5s - loss: 0.0244 - val_loss: 0.0371 - 5s/epoch - 8ms/step\n",
      "Epoch 162/10000\n",
      "572/572 - 5s - loss: 0.0245 - val_loss: 0.0357 - 5s/epoch - 9ms/step\n",
      "Epoch 163/10000\n",
      "572/572 - 5s - loss: 0.0245 - val_loss: 0.0419 - 5s/epoch - 8ms/step\n",
      "Epoch 164/10000\n",
      "572/572 - 5s - loss: 0.0292 - val_loss: 0.0405 - 5s/epoch - 8ms/step\n",
      "Epoch 165/10000\n",
      "572/572 - 5s - loss: 0.0241 - val_loss: 0.0354 - 5s/epoch - 8ms/step\n",
      "Epoch 166/10000\n",
      "572/572 - 5s - loss: 0.0232 - val_loss: 0.0358 - 5s/epoch - 8ms/step\n",
      "Epoch 167/10000\n",
      "572/572 - 5s - loss: 0.0241 - val_loss: 0.0362 - 5s/epoch - 8ms/step\n",
      "Epoch 168/10000\n",
      "572/572 - 5s - loss: 0.0249 - val_loss: 0.0378 - 5s/epoch - 8ms/step\n",
      "Epoch 169/10000\n",
      "572/572 - 5s - loss: 0.0253 - val_loss: 0.0381 - 5s/epoch - 8ms/step\n",
      "Epoch 170/10000\n",
      "572/572 - 5s - loss: 0.0235 - val_loss: 0.0341 - 5s/epoch - 8ms/step\n",
      "Epoch 171/10000\n",
      "572/572 - 5s - loss: 0.0235 - val_loss: 0.0347 - 5s/epoch - 8ms/step\n",
      "Epoch 172/10000\n",
      "572/572 - 5s - loss: 0.0234 - val_loss: 0.0353 - 5s/epoch - 8ms/step\n",
      "Epoch 173/10000\n",
      "572/572 - 5s - loss: 0.0241 - val_loss: 0.0380 - 5s/epoch - 8ms/step\n",
      "Epoch 174/10000\n",
      "572/572 - 5s - loss: 0.0244 - val_loss: 0.0357 - 5s/epoch - 8ms/step\n",
      "Epoch 175/10000\n",
      "572/572 - 5s - loss: 0.0231 - val_loss: 0.0373 - 5s/epoch - 8ms/step\n",
      "Epoch 176/10000\n",
      "572/572 - 5s - loss: 0.0240 - val_loss: 0.0372 - 5s/epoch - 8ms/step\n",
      "Epoch 177/10000\n",
      "572/572 - 5s - loss: 0.0238 - val_loss: 0.0352 - 5s/epoch - 8ms/step\n",
      "Epoch 178/10000\n",
      "572/572 - 5s - loss: 0.0231 - val_loss: 0.0349 - 5s/epoch - 8ms/step\n",
      "Epoch 179/10000\n",
      "572/572 - 5s - loss: 0.0239 - val_loss: 0.0356 - 5s/epoch - 8ms/step\n",
      "Epoch 180/10000\n",
      "572/572 - 5s - loss: 0.0234 - val_loss: 0.0363 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_36_layer_call_fn, gru_cell_36_layer_call_and_return_conditional_losses, gru_cell_37_layer_call_fn, gru_cell_37_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_10_1_50_1_50_datt_seq2seq_gru_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_10_1_50_1_50_datt_seq2seq_gru_4\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002643D261B80> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000266024392E0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.922841</td>\n",
       "      <td>0.92342</td>\n",
       "      <td>0.950476</td>\n",
       "      <td>0.932246</td>\n",
       "      <td>4.508323</td>\n",
       "      <td>4.860986</td>\n",
       "      <td>3.436602</td>\n",
       "      <td>4.268637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.834567</td>\n",
       "      <td>0.862151</td>\n",
       "      <td>0.890801</td>\n",
       "      <td>0.862506</td>\n",
       "      <td>6.557553</td>\n",
       "      <td>6.522585</td>\n",
       "      <td>5.102195</td>\n",
       "      <td>6.060778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.716344</td>\n",
       "      <td>0.820608</td>\n",
       "      <td>0.819701</td>\n",
       "      <td>0.785551</td>\n",
       "      <td>8.467624</td>\n",
       "      <td>7.441496</td>\n",
       "      <td>6.555566</td>\n",
       "      <td>7.488229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.587567</td>\n",
       "      <td>0.793969</td>\n",
       "      <td>0.768911</td>\n",
       "      <td>0.716816</td>\n",
       "      <td>10.072818</td>\n",
       "      <td>7.975941</td>\n",
       "      <td>7.421766</td>\n",
       "      <td>8.490175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.463046</td>\n",
       "      <td>0.774499</td>\n",
       "      <td>0.74489</td>\n",
       "      <td>0.660812</td>\n",
       "      <td>11.379173</td>\n",
       "      <td>8.345404</td>\n",
       "      <td>7.797745</td>\n",
       "      <td>9.174107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.349427</td>\n",
       "      <td>0.759458</td>\n",
       "      <td>0.736787</td>\n",
       "      <td>0.615224</td>\n",
       "      <td>12.404152</td>\n",
       "      <td>8.62054</td>\n",
       "      <td>7.920689</td>\n",
       "      <td>9.64846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.25295</td>\n",
       "      <td>0.750255</td>\n",
       "      <td>0.733411</td>\n",
       "      <td>0.578872</td>\n",
       "      <td>13.166455</td>\n",
       "      <td>8.784527</td>\n",
       "      <td>7.971701</td>\n",
       "      <td>9.974227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.176784</td>\n",
       "      <td>0.742868</td>\n",
       "      <td>0.728338</td>\n",
       "      <td>0.54933</td>\n",
       "      <td>13.790976</td>\n",
       "      <td>8.913729</td>\n",
       "      <td>8.047903</td>\n",
       "      <td>10.250869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.118022</td>\n",
       "      <td>0.729377</td>\n",
       "      <td>0.716878</td>\n",
       "      <td>0.521426</td>\n",
       "      <td>14.294744</td>\n",
       "      <td>9.144832</td>\n",
       "      <td>8.216446</td>\n",
       "      <td>10.552007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.074011</td>\n",
       "      <td>0.703121</td>\n",
       "      <td>0.68314</td>\n",
       "      <td>0.486757</td>\n",
       "      <td>14.668113</td>\n",
       "      <td>9.578253</td>\n",
       "      <td>8.693222</td>\n",
       "      <td>10.979863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.449556</td>\n",
       "      <td>0.785973</td>\n",
       "      <td>0.777333</td>\n",
       "      <td>0.670954</td>\n",
       "      <td>10.930993</td>\n",
       "      <td>8.018829</td>\n",
       "      <td>7.116384</td>\n",
       "      <td>8.688735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3 TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE     nRMSE     nRMSE   \n",
       "0      0.922841   0.92342  0.950476  0.932246   4.508323  4.860986  3.436602   \n",
       "1      0.834567  0.862151  0.890801  0.862506   6.557553  6.522585  5.102195   \n",
       "2      0.716344  0.820608  0.819701  0.785551   8.467624  7.441496  6.555566   \n",
       "3      0.587567  0.793969  0.768911  0.716816  10.072818  7.975941  7.421766   \n",
       "4      0.463046  0.774499   0.74489  0.660812  11.379173  8.345404  7.797745   \n",
       "5      0.349427  0.759458  0.736787  0.615224  12.404152   8.62054  7.920689   \n",
       "6       0.25295  0.750255  0.733411  0.578872  13.166455  8.784527  7.971701   \n",
       "7      0.176784  0.742868  0.728338   0.54933  13.790976  8.913729  8.047903   \n",
       "8      0.118022  0.729377  0.716878  0.521426  14.294744  9.144832  8.216446   \n",
       "9      0.074011  0.703121   0.68314  0.486757  14.668113  9.578253  8.693222   \n",
       "mean   0.449556  0.785973  0.777333  0.670954  10.930993  8.018829  7.116384   \n",
       "\n",
       "            mean  \n",
       "index      nRMSE  \n",
       "0       4.268637  \n",
       "1       6.060778  \n",
       "2       7.488229  \n",
       "3       8.490175  \n",
       "4       9.174107  \n",
       "5        9.64846  \n",
       "6       9.974227  \n",
       "7      10.250869  \n",
       "8      10.552007  \n",
       "9      10.979863  \n",
       "mean    8.688735  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/30_10_1_50_1_50_datt_seq2seq_gru_4.csv\n",
      "\n",
      "\n",
      "4th iteration\n",
      "history size: 30\n",
      "future size: 20\n",
      "Epoch 1/10000\n",
      "568/568 - 8s - loss: 0.3087 - val_loss: 0.2541 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "568/568 - 5s - loss: 0.2166 - val_loss: 0.2391 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "568/568 - 5s - loss: 0.2021 - val_loss: 0.2271 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "568/568 - 5s - loss: 0.1917 - val_loss: 0.2150 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "568/568 - 5s - loss: 0.1832 - val_loss: 0.2020 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "568/568 - 5s - loss: 0.1735 - val_loss: 0.2002 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "568/568 - 5s - loss: 0.1668 - val_loss: 0.1932 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "568/568 - 5s - loss: 0.1585 - val_loss: 0.1822 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "568/568 - 5s - loss: 0.1520 - val_loss: 0.1796 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "568/568 - 5s - loss: 0.1447 - val_loss: 0.1757 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "568/568 - 5s - loss: 0.1391 - val_loss: 0.1674 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "568/568 - 5s - loss: 0.1349 - val_loss: 0.1523 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "568/568 - 5s - loss: 0.1270 - val_loss: 0.1507 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "568/568 - 5s - loss: 0.1232 - val_loss: 0.1451 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "568/568 - 5s - loss: 0.1186 - val_loss: 0.1438 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "568/568 - 5s - loss: 0.1122 - val_loss: 0.1376 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "568/568 - 5s - loss: 0.1096 - val_loss: 0.1345 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "568/568 - 5s - loss: 0.1051 - val_loss: 0.1311 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "568/568 - 5s - loss: 0.1037 - val_loss: 0.1375 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "568/568 - 5s - loss: 0.1027 - val_loss: 0.1229 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "568/568 - 5s - loss: 0.0983 - val_loss: 0.1191 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "568/568 - 5s - loss: 0.0973 - val_loss: 0.1191 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "568/568 - 5s - loss: 0.0960 - val_loss: 0.1179 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "568/568 - 5s - loss: 0.0941 - val_loss: 0.1153 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "568/568 - 5s - loss: 0.0916 - val_loss: 0.1107 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "568/568 - 5s - loss: 0.0903 - val_loss: 0.1125 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "568/568 - 5s - loss: 0.0898 - val_loss: 0.1123 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "568/568 - 5s - loss: 0.0878 - val_loss: 0.1073 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "568/568 - 4s - loss: 0.0858 - val_loss: 0.1118 - 4s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "568/568 - 5s - loss: 0.0867 - val_loss: 0.1101 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "568/568 - 5s - loss: 0.0847 - val_loss: 0.1090 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "568/568 - 5s - loss: 0.0832 - val_loss: 0.1091 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "568/568 - 4s - loss: 0.0821 - val_loss: 0.1064 - 4s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "568/568 - 5s - loss: 0.0799 - val_loss: 0.1055 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "568/568 - 5s - loss: 0.0829 - val_loss: 0.1050 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "568/568 - 5s - loss: 0.0780 - val_loss: 0.1012 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "568/568 - 5s - loss: 0.0785 - val_loss: 0.0989 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "568/568 - 5s - loss: 0.0776 - val_loss: 0.1000 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "568/568 - 4s - loss: 0.0765 - val_loss: 0.1022 - 4s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "568/568 - 5s - loss: 0.0761 - val_loss: 0.1012 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "568/568 - 5s - loss: 0.0751 - val_loss: 0.0979 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "568/568 - 5s - loss: 0.0739 - val_loss: 0.1022 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "568/568 - 5s - loss: 0.0738 - val_loss: 0.0943 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "568/568 - 4s - loss: 0.0743 - val_loss: 0.0990 - 4s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "568/568 - 5s - loss: 0.0707 - val_loss: 0.0972 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "568/568 - 5s - loss: 0.0714 - val_loss: 0.0987 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "568/568 - 5s - loss: 0.0703 - val_loss: 0.0917 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "568/568 - 4s - loss: 0.0706 - val_loss: 0.0917 - 4s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "568/568 - 5s - loss: 0.0701 - val_loss: 0.0963 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "568/568 - 5s - loss: 0.0685 - val_loss: 0.0898 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "568/568 - 5s - loss: 0.0774 - val_loss: 0.0971 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "568/568 - 5s - loss: 0.0691 - val_loss: 0.0906 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "568/568 - 5s - loss: 0.0674 - val_loss: 0.0897 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "568/568 - 4s - loss: 0.0684 - val_loss: 0.0906 - 4s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "568/568 - 5s - loss: 0.0669 - val_loss: 0.0902 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "568/568 - 5s - loss: 0.0651 - val_loss: 0.0838 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "568/568 - 5s - loss: 0.0638 - val_loss: 0.0874 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "568/568 - 5s - loss: 0.0667 - val_loss: 0.0896 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "568/568 - 5s - loss: 0.0627 - val_loss: 0.0839 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "568/568 - 5s - loss: 0.0623 - val_loss: 0.0834 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "568/568 - 5s - loss: 0.0628 - val_loss: 0.0855 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "568/568 - 5s - loss: 0.0613 - val_loss: 0.0827 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "568/568 - 5s - loss: 0.0610 - val_loss: 0.0843 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "568/568 - 5s - loss: 0.0600 - val_loss: 0.0829 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "568/568 - 5s - loss: 0.0609 - val_loss: 0.0947 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "568/568 - 4s - loss: 0.0617 - val_loss: 0.0861 - 4s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "568/568 - 5s - loss: 0.0654 - val_loss: 0.0831 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "568/568 - 5s - loss: 0.0584 - val_loss: 0.0783 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "568/568 - 5s - loss: 0.0563 - val_loss: 0.0814 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "568/568 - 5s - loss: 0.0568 - val_loss: 0.0815 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "568/568 - 5s - loss: 0.0641 - val_loss: 0.0984 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "568/568 - 5s - loss: 0.0602 - val_loss: 0.0833 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "568/568 - 5s - loss: 0.0566 - val_loss: 0.0772 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "568/568 - 5s - loss: 0.0555 - val_loss: 0.0802 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "568/568 - 5s - loss: 0.0564 - val_loss: 0.0802 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "568/568 - 5s - loss: 0.0639 - val_loss: 0.0834 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "568/568 - 5s - loss: 0.0567 - val_loss: 0.0796 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "568/568 - 5s - loss: 0.0538 - val_loss: 0.0764 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "568/568 - 5s - loss: 0.0525 - val_loss: 0.0780 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "568/568 - 5s - loss: 0.0537 - val_loss: 0.0777 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "568/568 - 5s - loss: 0.0518 - val_loss: 0.0734 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "568/568 - 5s - loss: 0.0551 - val_loss: 0.0794 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "568/568 - 4s - loss: 0.0529 - val_loss: 0.0754 - 4s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "568/568 - 5s - loss: 0.0519 - val_loss: 0.0751 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "568/568 - 5s - loss: 0.0510 - val_loss: 0.0706 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "568/568 - 5s - loss: 0.0523 - val_loss: 0.0842 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "568/568 - 4s - loss: 0.0533 - val_loss: 0.0720 - 4s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "568/568 - 5s - loss: 0.0502 - val_loss: 0.0719 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "568/568 - 5s - loss: 0.0502 - val_loss: 0.0883 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "568/568 - 5s - loss: 0.0515 - val_loss: 0.0739 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "568/568 - 5s - loss: 0.0505 - val_loss: 0.0690 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "568/568 - 5s - loss: 0.0487 - val_loss: 0.0732 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "568/568 - 4s - loss: 0.0511 - val_loss: 0.0714 - 4s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "568/568 - 5s - loss: 0.0496 - val_loss: 0.0711 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "568/568 - 5s - loss: 0.0492 - val_loss: 0.0688 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "568/568 - 5s - loss: 0.0488 - val_loss: 0.0716 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "568/568 - 5s - loss: 0.0474 - val_loss: 0.0703 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "568/568 - 5s - loss: 0.0491 - val_loss: 0.0722 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "568/568 - 5s - loss: 0.0522 - val_loss: 0.0894 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "568/568 - 4s - loss: 0.0490 - val_loss: 0.0651 - 4s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "568/568 - 5s - loss: 0.0471 - val_loss: 0.0708 - 5s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "568/568 - 5s - loss: 0.0516 - val_loss: 0.0738 - 5s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "568/568 - 5s - loss: 0.0467 - val_loss: 0.0693 - 5s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "568/568 - 5s - loss: 0.0457 - val_loss: 0.0664 - 5s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "568/568 - 5s - loss: 0.0462 - val_loss: 0.0669 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "568/568 - 5s - loss: 0.0460 - val_loss: 0.0706 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "568/568 - 5s - loss: 0.0476 - val_loss: 0.0694 - 5s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "568/568 - 5s - loss: 0.0443 - val_loss: 0.0693 - 5s/epoch - 8ms/step\n",
      "Epoch 109/10000\n",
      "568/568 - 5s - loss: 0.0455 - val_loss: 0.0626 - 5s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "568/568 - 5s - loss: 0.0451 - val_loss: 0.0702 - 5s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "568/568 - 4s - loss: 0.0449 - val_loss: 0.0634 - 4s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "568/568 - 5s - loss: 0.0450 - val_loss: 0.0676 - 5s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "568/568 - 4s - loss: 0.0446 - val_loss: 0.0684 - 4s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "568/568 - 5s - loss: 0.0465 - val_loss: 0.0678 - 5s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "568/568 - 5s - loss: 0.0443 - val_loss: 0.0674 - 5s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "568/568 - 5s - loss: 0.0438 - val_loss: 0.0607 - 5s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "568/568 - 4s - loss: 0.0440 - val_loss: 0.0686 - 4s/epoch - 8ms/step\n",
      "Epoch 118/10000\n",
      "568/568 - 5s - loss: 0.0432 - val_loss: 0.0662 - 5s/epoch - 8ms/step\n",
      "Epoch 119/10000\n",
      "568/568 - 5s - loss: 0.0437 - val_loss: 0.0651 - 5s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "568/568 - 5s - loss: 0.0435 - val_loss: 0.0612 - 5s/epoch - 8ms/step\n",
      "Epoch 121/10000\n",
      "568/568 - 4s - loss: 0.0432 - val_loss: 0.0611 - 4s/epoch - 8ms/step\n",
      "Epoch 122/10000\n",
      "568/568 - 5s - loss: 0.0422 - val_loss: 0.0659 - 5s/epoch - 8ms/step\n",
      "Epoch 123/10000\n",
      "568/568 - 5s - loss: 0.0448 - val_loss: 0.0640 - 5s/epoch - 8ms/step\n",
      "Epoch 124/10000\n",
      "568/568 - 5s - loss: 0.0577 - val_loss: 0.0740 - 5s/epoch - 8ms/step\n",
      "Epoch 125/10000\n",
      "568/568 - 5s - loss: 0.0467 - val_loss: 0.0758 - 5s/epoch - 8ms/step\n",
      "Epoch 126/10000\n",
      "568/568 - 5s - loss: 0.0457 - val_loss: 0.0640 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_38_layer_call_fn, gru_cell_38_layer_call_and_return_conditional_losses, gru_cell_39_layer_call_fn, gru_cell_39_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_20_1_50_1_50_datt_seq2seq_gru_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_20_1_50_1_50_datt_seq2seq_gru_4\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002646EEDB1C0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002643EAB3760> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.882259</td>\n",
       "      <td>0.901629</td>\n",
       "      <td>0.932232</td>\n",
       "      <td>0.905374</td>\n",
       "      <td>6.768378</td>\n",
       "      <td>5.50293</td>\n",
       "      <td>4.005896</td>\n",
       "      <td>5.425735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.798864</td>\n",
       "      <td>0.842588</td>\n",
       "      <td>0.88959</td>\n",
       "      <td>0.843681</td>\n",
       "      <td>8.683251</td>\n",
       "      <td>6.963261</td>\n",
       "      <td>5.113503</td>\n",
       "      <td>6.920005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.703826</td>\n",
       "      <td>0.788233</td>\n",
       "      <td>0.823493</td>\n",
       "      <td>0.771851</td>\n",
       "      <td>10.074035</td>\n",
       "      <td>8.07945</td>\n",
       "      <td>6.466099</td>\n",
       "      <td>8.206528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.608104</td>\n",
       "      <td>0.751258</td>\n",
       "      <td>0.780869</td>\n",
       "      <td>0.71341</td>\n",
       "      <td>11.198878</td>\n",
       "      <td>8.75935</td>\n",
       "      <td>7.205357</td>\n",
       "      <td>9.054528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.515255</td>\n",
       "      <td>0.727996</td>\n",
       "      <td>0.766857</td>\n",
       "      <td>0.670036</td>\n",
       "      <td>12.255054</td>\n",
       "      <td>9.163829</td>\n",
       "      <td>7.43263</td>\n",
       "      <td>9.617171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.427443</td>\n",
       "      <td>0.710826</td>\n",
       "      <td>0.762511</td>\n",
       "      <td>0.633593</td>\n",
       "      <td>13.275065</td>\n",
       "      <td>9.45319</td>\n",
       "      <td>7.501458</td>\n",
       "      <td>10.076571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.343371</td>\n",
       "      <td>0.691612</td>\n",
       "      <td>0.752515</td>\n",
       "      <td>0.595832</td>\n",
       "      <td>13.939778</td>\n",
       "      <td>9.765048</td>\n",
       "      <td>7.657763</td>\n",
       "      <td>10.454196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.263541</td>\n",
       "      <td>0.669424</td>\n",
       "      <td>0.732065</td>\n",
       "      <td>0.55501</td>\n",
       "      <td>14.483611</td>\n",
       "      <td>10.113316</td>\n",
       "      <td>7.968059</td>\n",
       "      <td>10.854995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.191008</td>\n",
       "      <td>0.643698</td>\n",
       "      <td>0.701504</td>\n",
       "      <td>0.51207</td>\n",
       "      <td>14.959068</td>\n",
       "      <td>10.502648</td>\n",
       "      <td>8.410693</td>\n",
       "      <td>11.290803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.123235</td>\n",
       "      <td>0.614733</td>\n",
       "      <td>0.660091</td>\n",
       "      <td>0.46602</td>\n",
       "      <td>15.35104</td>\n",
       "      <td>10.923115</td>\n",
       "      <td>8.975725</td>\n",
       "      <td>11.74996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.056727</td>\n",
       "      <td>0.585164</td>\n",
       "      <td>0.616869</td>\n",
       "      <td>0.419587</td>\n",
       "      <td>15.761074</td>\n",
       "      <td>11.336312</td>\n",
       "      <td>9.530098</td>\n",
       "      <td>12.209161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.006136</td>\n",
       "      <td>0.552747</td>\n",
       "      <td>0.581714</td>\n",
       "      <td>0.376109</td>\n",
       "      <td>16.177477</td>\n",
       "      <td>11.772141</td>\n",
       "      <td>9.959002</td>\n",
       "      <td>12.636207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.064139</td>\n",
       "      <td>0.515255</td>\n",
       "      <td>0.55414</td>\n",
       "      <td>0.335086</td>\n",
       "      <td>16.412591</td>\n",
       "      <td>12.256672</td>\n",
       "      <td>10.283473</td>\n",
       "      <td>12.984245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.113856</td>\n",
       "      <td>0.476695</td>\n",
       "      <td>0.530089</td>\n",
       "      <td>0.297643</td>\n",
       "      <td>16.568923</td>\n",
       "      <td>12.736659</td>\n",
       "      <td>10.558502</td>\n",
       "      <td>13.288028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.157302</td>\n",
       "      <td>0.44082</td>\n",
       "      <td>0.505512</td>\n",
       "      <td>0.26301</td>\n",
       "      <td>16.722187</td>\n",
       "      <td>13.167727</td>\n",
       "      <td>10.831832</td>\n",
       "      <td>13.573915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.193734</td>\n",
       "      <td>0.406364</td>\n",
       "      <td>0.479594</td>\n",
       "      <td>0.230741</td>\n",
       "      <td>16.817548</td>\n",
       "      <td>13.568739</td>\n",
       "      <td>11.113286</td>\n",
       "      <td>13.833191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.221517</td>\n",
       "      <td>0.374569</td>\n",
       "      <td>0.455741</td>\n",
       "      <td>0.202931</td>\n",
       "      <td>16.848314</td>\n",
       "      <td>13.927219</td>\n",
       "      <td>11.367116</td>\n",
       "      <td>14.04755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.245335</td>\n",
       "      <td>0.341414</td>\n",
       "      <td>0.437738</td>\n",
       "      <td>0.177939</td>\n",
       "      <td>16.97058</td>\n",
       "      <td>14.290529</td>\n",
       "      <td>11.556933</td>\n",
       "      <td>14.272681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.266955</td>\n",
       "      <td>0.309648</td>\n",
       "      <td>0.426651</td>\n",
       "      <td>0.156448</td>\n",
       "      <td>17.137277</td>\n",
       "      <td>14.629457</td>\n",
       "      <td>11.674024</td>\n",
       "      <td>14.480253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.287466</td>\n",
       "      <td>0.281946</td>\n",
       "      <td>0.421732</td>\n",
       "      <td>0.138737</td>\n",
       "      <td>17.295309</td>\n",
       "      <td>14.917586</td>\n",
       "      <td>11.728073</td>\n",
       "      <td>14.646989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.16786</td>\n",
       "      <td>0.581331</td>\n",
       "      <td>0.640575</td>\n",
       "      <td>0.463255</td>\n",
       "      <td>14.384972</td>\n",
       "      <td>11.091459</td>\n",
       "      <td>8.966976</td>\n",
       "      <td>11.481136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.882259  0.901629  0.932232  0.905374   6.768378    5.50293   \n",
       "1      0.798864  0.842588   0.88959  0.843681   8.683251   6.963261   \n",
       "2      0.703826  0.788233  0.823493  0.771851  10.074035    8.07945   \n",
       "3      0.608104  0.751258  0.780869   0.71341  11.198878    8.75935   \n",
       "4      0.515255  0.727996  0.766857  0.670036  12.255054   9.163829   \n",
       "5      0.427443  0.710826  0.762511  0.633593  13.275065    9.45319   \n",
       "6      0.343371  0.691612  0.752515  0.595832  13.939778   9.765048   \n",
       "7      0.263541  0.669424  0.732065   0.55501  14.483611  10.113316   \n",
       "8      0.191008  0.643698  0.701504   0.51207  14.959068  10.502648   \n",
       "9      0.123235  0.614733  0.660091   0.46602   15.35104  10.923115   \n",
       "10     0.056727  0.585164  0.616869  0.419587  15.761074  11.336312   \n",
       "11    -0.006136  0.552747  0.581714  0.376109  16.177477  11.772141   \n",
       "12    -0.064139  0.515255   0.55414  0.335086  16.412591  12.256672   \n",
       "13    -0.113856  0.476695  0.530089  0.297643  16.568923  12.736659   \n",
       "14    -0.157302   0.44082  0.505512   0.26301  16.722187  13.167727   \n",
       "15    -0.193734  0.406364  0.479594  0.230741  16.817548  13.568739   \n",
       "16    -0.221517  0.374569  0.455741  0.202931  16.848314  13.927219   \n",
       "17    -0.245335  0.341414  0.437738  0.177939   16.97058  14.290529   \n",
       "18    -0.266955  0.309648  0.426651  0.156448  17.137277  14.629457   \n",
       "19    -0.287466  0.281946  0.421732  0.138737  17.295309  14.917586   \n",
       "mean    0.16786  0.581331  0.640575  0.463255  14.384972  11.091459   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       4.005896   5.425735  \n",
       "1       5.113503   6.920005  \n",
       "2       6.466099   8.206528  \n",
       "3       7.205357   9.054528  \n",
       "4        7.43263   9.617171  \n",
       "5       7.501458  10.076571  \n",
       "6       7.657763  10.454196  \n",
       "7       7.968059  10.854995  \n",
       "8       8.410693  11.290803  \n",
       "9       8.975725   11.74996  \n",
       "10      9.530098  12.209161  \n",
       "11      9.959002  12.636207  \n",
       "12     10.283473  12.984245  \n",
       "13     10.558502  13.288028  \n",
       "14     10.831832  13.573915  \n",
       "15     11.113286  13.833191  \n",
       "16     11.367116   14.04755  \n",
       "17     11.556933  14.272681  \n",
       "18     11.674024  14.480253  \n",
       "19     11.728073  14.646989  \n",
       "mean    8.966976  11.481136  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/30_20_1_50_1_50_datt_seq2seq_gru_4.csv\n",
      "\n",
      "\n",
      "4th iteration\n",
      "history size: 30\n",
      "future size: 30\n",
      "Epoch 1/10000\n",
      "563/563 - 9s - loss: 0.3866 - val_loss: 0.3176 - 9s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.2880 - val_loss: 0.2948 - 5s/epoch - 9ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.2681 - val_loss: 0.2723 - 5s/epoch - 9ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 5s - loss: 0.2518 - val_loss: 0.2606 - 5s/epoch - 9ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.2386 - val_loss: 0.2509 - 5s/epoch - 9ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.2241 - val_loss: 0.2342 - 5s/epoch - 9ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.2148 - val_loss: 0.2266 - 5s/epoch - 9ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.2058 - val_loss: 0.2263 - 5s/epoch - 9ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.1964 - val_loss: 0.2131 - 5s/epoch - 9ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.1874 - val_loss: 0.1998 - 5s/epoch - 9ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.1796 - val_loss: 0.1965 - 5s/epoch - 9ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.1718 - val_loss: 0.1885 - 5s/epoch - 9ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.1664 - val_loss: 0.1846 - 5s/epoch - 9ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.1607 - val_loss: 0.1873 - 5s/epoch - 9ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.1555 - val_loss: 0.1708 - 5s/epoch - 9ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.1521 - val_loss: 0.1689 - 5s/epoch - 9ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.1478 - val_loss: 0.1650 - 5s/epoch - 9ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.1453 - val_loss: 0.1605 - 5s/epoch - 9ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.1401 - val_loss: 0.1648 - 5s/epoch - 9ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.1356 - val_loss: 0.1559 - 5s/epoch - 9ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.1336 - val_loss: 0.1530 - 5s/epoch - 9ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.1339 - val_loss: 0.1472 - 5s/epoch - 9ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.1294 - val_loss: 0.1498 - 5s/epoch - 9ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.1284 - val_loss: 0.1421 - 5s/epoch - 9ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.1249 - val_loss: 0.1399 - 5s/epoch - 9ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.1230 - val_loss: 0.1377 - 5s/epoch - 9ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.1226 - val_loss: 0.1441 - 5s/epoch - 9ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.1170 - val_loss: 0.1297 - 5s/epoch - 9ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.1159 - val_loss: 0.1316 - 5s/epoch - 9ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.1228 - val_loss: 0.1435 - 5s/epoch - 9ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.1136 - val_loss: 0.1264 - 5s/epoch - 9ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.1104 - val_loss: 0.1209 - 5s/epoch - 9ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.1204 - val_loss: 0.1428 - 5s/epoch - 9ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.1147 - val_loss: 0.1237 - 5s/epoch - 9ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.1060 - val_loss: 0.1243 - 5s/epoch - 9ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.1061 - val_loss: 0.1135 - 5s/epoch - 9ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.1035 - val_loss: 0.1198 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.1040 - val_loss: 0.1171 - 5s/epoch - 9ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.1006 - val_loss: 0.1184 - 5s/epoch - 9ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.1009 - val_loss: 0.1241 - 5s/epoch - 9ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0986 - val_loss: 0.1234 - 5s/epoch - 9ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.1006 - val_loss: 0.1227 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.1029 - val_loss: 0.1252 - 5s/epoch - 9ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.1022 - val_loss: 0.1119 - 5s/epoch - 9ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0951 - val_loss: 0.1080 - 5s/epoch - 9ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0928 - val_loss: 0.1099 - 5s/epoch - 9ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0945 - val_loss: 0.1090 - 5s/epoch - 9ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0930 - val_loss: 0.1043 - 5s/epoch - 9ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0925 - val_loss: 0.1066 - 5s/epoch - 9ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0890 - val_loss: 0.0987 - 5s/epoch - 9ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0910 - val_loss: 0.1133 - 5s/epoch - 9ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0899 - val_loss: 0.1010 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0901 - val_loss: 0.1101 - 5s/epoch - 9ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0869 - val_loss: 0.1077 - 5s/epoch - 9ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0910 - val_loss: 0.1036 - 5s/epoch - 9ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0856 - val_loss: 0.1011 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0859 - val_loss: 0.1029 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0847 - val_loss: 0.0964 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0832 - val_loss: 0.1008 - 5s/epoch - 9ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0838 - val_loss: 0.1022 - 5s/epoch - 9ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0854 - val_loss: 0.0969 - 5s/epoch - 9ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0818 - val_loss: 0.0925 - 5s/epoch - 9ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 5s - loss: 0.0838 - val_loss: 0.0963 - 5s/epoch - 9ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0824 - val_loss: 0.0955 - 5s/epoch - 9ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0809 - val_loss: 0.1035 - 5s/epoch - 9ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0818 - val_loss: 0.0882 - 5s/epoch - 9ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0811 - val_loss: 0.0971 - 5s/epoch - 9ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 5s - loss: 0.0800 - val_loss: 0.0949 - 5s/epoch - 9ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 5s - loss: 0.0810 - val_loss: 0.0907 - 5s/epoch - 9ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 5s - loss: 0.0787 - val_loss: 0.0932 - 5s/epoch - 9ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 5s - loss: 0.0767 - val_loss: 0.0920 - 5s/epoch - 9ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 5s - loss: 0.0793 - val_loss: 0.0909 - 5s/epoch - 9ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 5s - loss: 0.0756 - val_loss: 0.0963 - 5s/epoch - 9ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 5s - loss: 0.0774 - val_loss: 0.0858 - 5s/epoch - 9ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 5s - loss: 0.0767 - val_loss: 0.0896 - 5s/epoch - 9ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 5s - loss: 0.0767 - val_loss: 0.0902 - 5s/epoch - 9ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 5s - loss: 0.0754 - val_loss: 0.0906 - 5s/epoch - 9ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 5s - loss: 0.0751 - val_loss: 0.1021 - 5s/epoch - 9ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 5s - loss: 0.0747 - val_loss: 0.0855 - 5s/epoch - 9ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 5s - loss: 0.0748 - val_loss: 0.0886 - 5s/epoch - 9ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 5s - loss: 0.0740 - val_loss: 0.0970 - 5s/epoch - 9ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 5s - loss: 0.0746 - val_loss: 0.0859 - 5s/epoch - 9ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 5s - loss: 0.0716 - val_loss: 0.0861 - 5s/epoch - 9ms/step\n",
      "Epoch 84/10000\n",
      "563/563 - 5s - loss: 0.0718 - val_loss: 0.0813 - 5s/epoch - 9ms/step\n",
      "Epoch 85/10000\n",
      "563/563 - 5s - loss: 0.0712 - val_loss: 0.0828 - 5s/epoch - 9ms/step\n",
      "Epoch 86/10000\n",
      "563/563 - 5s - loss: 0.0713 - val_loss: 0.0889 - 5s/epoch - 9ms/step\n",
      "Epoch 87/10000\n",
      "563/563 - 5s - loss: 0.0716 - val_loss: 0.0847 - 5s/epoch - 9ms/step\n",
      "Epoch 88/10000\n",
      "563/563 - 5s - loss: 0.0710 - val_loss: 0.0834 - 5s/epoch - 9ms/step\n",
      "Epoch 89/10000\n",
      "563/563 - 5s - loss: 0.0700 - val_loss: 0.0803 - 5s/epoch - 9ms/step\n",
      "Epoch 90/10000\n",
      "563/563 - 5s - loss: 0.0698 - val_loss: 0.0814 - 5s/epoch - 9ms/step\n",
      "Epoch 91/10000\n",
      "563/563 - 6s - loss: 0.0696 - val_loss: 0.0846 - 6s/epoch - 10ms/step\n",
      "Epoch 92/10000\n",
      "563/563 - 5s - loss: 0.0695 - val_loss: 0.0779 - 5s/epoch - 9ms/step\n",
      "Epoch 93/10000\n",
      "563/563 - 5s - loss: 0.0686 - val_loss: 0.1002 - 5s/epoch - 9ms/step\n",
      "Epoch 94/10000\n",
      "563/563 - 5s - loss: 0.0711 - val_loss: 0.0877 - 5s/epoch - 9ms/step\n",
      "Epoch 95/10000\n",
      "563/563 - 5s - loss: 0.0687 - val_loss: 0.0798 - 5s/epoch - 9ms/step\n",
      "Epoch 96/10000\n",
      "563/563 - 5s - loss: 0.0668 - val_loss: 0.0774 - 5s/epoch - 9ms/step\n",
      "Epoch 97/10000\n",
      "563/563 - 5s - loss: 0.0661 - val_loss: 0.0784 - 5s/epoch - 9ms/step\n",
      "Epoch 98/10000\n",
      "563/563 - 5s - loss: 0.0677 - val_loss: 0.0827 - 5s/epoch - 9ms/step\n",
      "Epoch 99/10000\n",
      "563/563 - 5s - loss: 0.0685 - val_loss: 0.0859 - 5s/epoch - 9ms/step\n",
      "Epoch 100/10000\n",
      "563/563 - 5s - loss: 0.0654 - val_loss: 0.0784 - 5s/epoch - 9ms/step\n",
      "Epoch 101/10000\n",
      "563/563 - 5s - loss: 0.0672 - val_loss: 0.0870 - 5s/epoch - 9ms/step\n",
      "Epoch 102/10000\n",
      "563/563 - 5s - loss: 0.0670 - val_loss: 0.0744 - 5s/epoch - 9ms/step\n",
      "Epoch 103/10000\n",
      "563/563 - 5s - loss: 0.0648 - val_loss: 0.0805 - 5s/epoch - 9ms/step\n",
      "Epoch 104/10000\n",
      "563/563 - 5s - loss: 0.0664 - val_loss: 0.0775 - 5s/epoch - 9ms/step\n",
      "Epoch 105/10000\n",
      "563/563 - 5s - loss: 0.0644 - val_loss: 0.0705 - 5s/epoch - 9ms/step\n",
      "Epoch 106/10000\n",
      "563/563 - 5s - loss: 0.0653 - val_loss: 0.0774 - 5s/epoch - 9ms/step\n",
      "Epoch 107/10000\n",
      "563/563 - 5s - loss: 0.0658 - val_loss: 0.0763 - 5s/epoch - 9ms/step\n",
      "Epoch 108/10000\n",
      "563/563 - 5s - loss: 0.0645 - val_loss: 0.0765 - 5s/epoch - 9ms/step\n",
      "Epoch 109/10000\n",
      "563/563 - 5s - loss: 0.0628 - val_loss: 0.0728 - 5s/epoch - 9ms/step\n",
      "Epoch 110/10000\n",
      "563/563 - 5s - loss: 0.0642 - val_loss: 0.0765 - 5s/epoch - 9ms/step\n",
      "Epoch 111/10000\n",
      "563/563 - 5s - loss: 0.0620 - val_loss: 0.0731 - 5s/epoch - 9ms/step\n",
      "Epoch 112/10000\n",
      "563/563 - 5s - loss: 0.0618 - val_loss: 0.0705 - 5s/epoch - 9ms/step\n",
      "Epoch 113/10000\n",
      "563/563 - 5s - loss: 0.0647 - val_loss: 0.0813 - 5s/epoch - 9ms/step\n",
      "Epoch 114/10000\n",
      "563/563 - 5s - loss: 0.0611 - val_loss: 0.0750 - 5s/epoch - 9ms/step\n",
      "Epoch 115/10000\n",
      "563/563 - 5s - loss: 0.0623 - val_loss: 0.0703 - 5s/epoch - 9ms/step\n",
      "Epoch 116/10000\n",
      "563/563 - 5s - loss: 0.0626 - val_loss: 0.0782 - 5s/epoch - 9ms/step\n",
      "Epoch 117/10000\n",
      "563/563 - 5s - loss: 0.0635 - val_loss: 0.0933 - 5s/epoch - 9ms/step\n",
      "Epoch 118/10000\n",
      "563/563 - 5s - loss: 0.0603 - val_loss: 0.0715 - 5s/epoch - 9ms/step\n",
      "Epoch 119/10000\n",
      "563/563 - 5s - loss: 0.0603 - val_loss: 0.0724 - 5s/epoch - 9ms/step\n",
      "Epoch 120/10000\n",
      "563/563 - 5s - loss: 0.0616 - val_loss: 0.0787 - 5s/epoch - 9ms/step\n",
      "Epoch 121/10000\n",
      "563/563 - 5s - loss: 0.0602 - val_loss: 0.0719 - 5s/epoch - 9ms/step\n",
      "Epoch 122/10000\n",
      "563/563 - 5s - loss: 0.0605 - val_loss: 0.0852 - 5s/epoch - 9ms/step\n",
      "Epoch 123/10000\n",
      "563/563 - 5s - loss: 0.0664 - val_loss: 0.0676 - 5s/epoch - 9ms/step\n",
      "Epoch 124/10000\n",
      "563/563 - 5s - loss: 0.0586 - val_loss: 0.0720 - 5s/epoch - 9ms/step\n",
      "Epoch 125/10000\n",
      "563/563 - 5s - loss: 0.0581 - val_loss: 0.0716 - 5s/epoch - 9ms/step\n",
      "Epoch 126/10000\n",
      "563/563 - 5s - loss: 0.0601 - val_loss: 0.0817 - 5s/epoch - 9ms/step\n",
      "Epoch 127/10000\n",
      "563/563 - 5s - loss: 0.0585 - val_loss: 0.0692 - 5s/epoch - 9ms/step\n",
      "Epoch 128/10000\n",
      "563/563 - 5s - loss: 0.0580 - val_loss: 0.0698 - 5s/epoch - 9ms/step\n",
      "Epoch 129/10000\n",
      "563/563 - 5s - loss: 0.0586 - val_loss: 0.0731 - 5s/epoch - 9ms/step\n",
      "Epoch 130/10000\n",
      "563/563 - 5s - loss: 0.0577 - val_loss: 0.0658 - 5s/epoch - 9ms/step\n",
      "Epoch 131/10000\n",
      "563/563 - 5s - loss: 0.0584 - val_loss: 0.0752 - 5s/epoch - 9ms/step\n",
      "Epoch 132/10000\n",
      "563/563 - 5s - loss: 0.0594 - val_loss: 0.0697 - 5s/epoch - 9ms/step\n",
      "Epoch 133/10000\n",
      "563/563 - 5s - loss: 0.0574 - val_loss: 0.0722 - 5s/epoch - 9ms/step\n",
      "Epoch 134/10000\n",
      "563/563 - 5s - loss: 0.0567 - val_loss: 0.0722 - 5s/epoch - 9ms/step\n",
      "Epoch 135/10000\n",
      "563/563 - 5s - loss: 0.0572 - val_loss: 0.0688 - 5s/epoch - 9ms/step\n",
      "Epoch 136/10000\n",
      "563/563 - 5s - loss: 0.0568 - val_loss: 0.0684 - 5s/epoch - 9ms/step\n",
      "Epoch 137/10000\n",
      "563/563 - 5s - loss: 0.0560 - val_loss: 0.0672 - 5s/epoch - 9ms/step\n",
      "Epoch 138/10000\n",
      "563/563 - 5s - loss: 0.0572 - val_loss: 0.0728 - 5s/epoch - 9ms/step\n",
      "Epoch 139/10000\n",
      "563/563 - 5s - loss: 0.0568 - val_loss: 0.0703 - 5s/epoch - 9ms/step\n",
      "Epoch 140/10000\n",
      "563/563 - 5s - loss: 0.0572 - val_loss: 0.0663 - 5s/epoch - 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_40_layer_call_fn, gru_cell_40_layer_call_and_return_conditional_losses, gru_cell_41_layer_call_fn, gru_cell_41_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_30_1_50_1_50_datt_seq2seq_gru_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_30_1_50_1_50_datt_seq2seq_gru_4\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002643D69F640> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002646E50D340> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.765959</td>\n",
       "      <td>0.871084</td>\n",
       "      <td>0.920127</td>\n",
       "      <td>0.85239</td>\n",
       "      <td>9.535929</td>\n",
       "      <td>6.297802</td>\n",
       "      <td>4.333103</td>\n",
       "      <td>6.722278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.634981</td>\n",
       "      <td>0.827652</td>\n",
       "      <td>0.856516</td>\n",
       "      <td>0.77305</td>\n",
       "      <td>11.906366</td>\n",
       "      <td>7.282338</td>\n",
       "      <td>5.809411</td>\n",
       "      <td>8.332705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.532882</td>\n",
       "      <td>0.797346</td>\n",
       "      <td>0.790426</td>\n",
       "      <td>0.706885</td>\n",
       "      <td>13.46722</td>\n",
       "      <td>7.897391</td>\n",
       "      <td>7.022971</td>\n",
       "      <td>9.462527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.446547</td>\n",
       "      <td>0.775221</td>\n",
       "      <td>0.74277</td>\n",
       "      <td>0.654846</td>\n",
       "      <td>14.658661</td>\n",
       "      <td>8.318508</td>\n",
       "      <td>7.782309</td>\n",
       "      <td>10.253159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.367112</td>\n",
       "      <td>0.753616</td>\n",
       "      <td>0.708517</td>\n",
       "      <td>0.609748</td>\n",
       "      <td>15.675243</td>\n",
       "      <td>8.710845</td>\n",
       "      <td>8.285381</td>\n",
       "      <td>10.89049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.290919</td>\n",
       "      <td>0.731115</td>\n",
       "      <td>0.682108</td>\n",
       "      <td>0.568047</td>\n",
       "      <td>16.592874</td>\n",
       "      <td>9.102608</td>\n",
       "      <td>8.653033</td>\n",
       "      <td>11.449505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.220483</td>\n",
       "      <td>0.7065</td>\n",
       "      <td>0.658254</td>\n",
       "      <td>0.528412</td>\n",
       "      <td>17.399408</td>\n",
       "      <td>9.513137</td>\n",
       "      <td>8.972604</td>\n",
       "      <td>11.961716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.156858</td>\n",
       "      <td>0.681053</td>\n",
       "      <td>0.634516</td>\n",
       "      <td>0.490809</td>\n",
       "      <td>18.098913</td>\n",
       "      <td>9.920164</td>\n",
       "      <td>9.280415</td>\n",
       "      <td>12.433164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.098897</td>\n",
       "      <td>0.655069</td>\n",
       "      <td>0.609802</td>\n",
       "      <td>0.454589</td>\n",
       "      <td>18.715029</td>\n",
       "      <td>10.320589</td>\n",
       "      <td>9.590609</td>\n",
       "      <td>12.875409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.045723</td>\n",
       "      <td>0.629307</td>\n",
       "      <td>0.58479</td>\n",
       "      <td>0.41994</td>\n",
       "      <td>19.266094</td>\n",
       "      <td>10.702614</td>\n",
       "      <td>9.895389</td>\n",
       "      <td>13.288032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.003053</td>\n",
       "      <td>0.60357</td>\n",
       "      <td>0.559112</td>\n",
       "      <td>0.386543</td>\n",
       "      <td>19.761351</td>\n",
       "      <td>11.070949</td>\n",
       "      <td>10.199287</td>\n",
       "      <td>13.677196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.047261</td>\n",
       "      <td>0.5771</td>\n",
       "      <td>0.535201</td>\n",
       "      <td>0.355013</td>\n",
       "      <td>19.829414</td>\n",
       "      <td>11.438263</td>\n",
       "      <td>10.47528</td>\n",
       "      <td>13.914319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.086907</td>\n",
       "      <td>0.549306</td>\n",
       "      <td>0.515646</td>\n",
       "      <td>0.326015</td>\n",
       "      <td>19.321323</td>\n",
       "      <td>11.812707</td>\n",
       "      <td>10.696504</td>\n",
       "      <td>13.943511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.121329</td>\n",
       "      <td>0.521436</td>\n",
       "      <td>0.500369</td>\n",
       "      <td>0.300159</td>\n",
       "      <td>18.969452</td>\n",
       "      <td>12.176753</td>\n",
       "      <td>10.866275</td>\n",
       "      <td>14.00416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.152701</td>\n",
       "      <td>0.493463</td>\n",
       "      <td>0.485241</td>\n",
       "      <td>0.275334</td>\n",
       "      <td>18.925077</td>\n",
       "      <td>12.533157</td>\n",
       "      <td>11.03205</td>\n",
       "      <td>14.163428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.180621</td>\n",
       "      <td>0.46687</td>\n",
       "      <td>0.467138</td>\n",
       "      <td>0.251129</td>\n",
       "      <td>19.088272</td>\n",
       "      <td>12.863758</td>\n",
       "      <td>11.226321</td>\n",
       "      <td>14.392784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.204335</td>\n",
       "      <td>0.441908</td>\n",
       "      <td>0.447458</td>\n",
       "      <td>0.228344</td>\n",
       "      <td>18.900957</td>\n",
       "      <td>13.164452</td>\n",
       "      <td>11.434065</td>\n",
       "      <td>14.499824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.223796</td>\n",
       "      <td>0.420305</td>\n",
       "      <td>0.427741</td>\n",
       "      <td>0.208083</td>\n",
       "      <td>18.688809</td>\n",
       "      <td>13.419826</td>\n",
       "      <td>11.638548</td>\n",
       "      <td>14.582394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.239804</td>\n",
       "      <td>0.40183</td>\n",
       "      <td>0.40715</td>\n",
       "      <td>0.189725</td>\n",
       "      <td>18.532704</td>\n",
       "      <td>13.634365</td>\n",
       "      <td>11.848199</td>\n",
       "      <td>14.671756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.252944</td>\n",
       "      <td>0.384906</td>\n",
       "      <td>0.387036</td>\n",
       "      <td>0.172999</td>\n",
       "      <td>18.359752</td>\n",
       "      <td>13.826088</td>\n",
       "      <td>12.049363</td>\n",
       "      <td>14.745068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.264692</td>\n",
       "      <td>0.368218</td>\n",
       "      <td>0.367663</td>\n",
       "      <td>0.157063</td>\n",
       "      <td>18.252034</td>\n",
       "      <td>14.013169</td>\n",
       "      <td>12.240088</td>\n",
       "      <td>14.835097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.274561</td>\n",
       "      <td>0.352549</td>\n",
       "      <td>0.349189</td>\n",
       "      <td>0.142392</td>\n",
       "      <td>18.20333</td>\n",
       "      <td>14.185035</td>\n",
       "      <td>12.419675</td>\n",
       "      <td>14.936013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.281993</td>\n",
       "      <td>0.337295</td>\n",
       "      <td>0.329383</td>\n",
       "      <td>0.128228</td>\n",
       "      <td>18.003078</td>\n",
       "      <td>14.350117</td>\n",
       "      <td>12.610208</td>\n",
       "      <td>14.987801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.288747</td>\n",
       "      <td>0.323502</td>\n",
       "      <td>0.308244</td>\n",
       "      <td>0.114333</td>\n",
       "      <td>17.804748</td>\n",
       "      <td>14.499839</td>\n",
       "      <td>12.811331</td>\n",
       "      <td>15.038639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.295788</td>\n",
       "      <td>0.310105</td>\n",
       "      <td>0.28806</td>\n",
       "      <td>0.100792</td>\n",
       "      <td>17.671096</td>\n",
       "      <td>14.643273</td>\n",
       "      <td>13.000787</td>\n",
       "      <td>15.105052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.304649</td>\n",
       "      <td>0.296872</td>\n",
       "      <td>0.270749</td>\n",
       "      <td>0.087658</td>\n",
       "      <td>17.554121</td>\n",
       "      <td>14.782289</td>\n",
       "      <td>13.162462</td>\n",
       "      <td>15.166291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.314632</td>\n",
       "      <td>0.283056</td>\n",
       "      <td>0.255851</td>\n",
       "      <td>0.074758</td>\n",
       "      <td>17.449624</td>\n",
       "      <td>14.924591</td>\n",
       "      <td>13.301274</td>\n",
       "      <td>15.225163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.324298</td>\n",
       "      <td>0.268465</td>\n",
       "      <td>0.242473</td>\n",
       "      <td>0.062213</td>\n",
       "      <td>17.470832</td>\n",
       "      <td>15.072656</td>\n",
       "      <td>13.425914</td>\n",
       "      <td>15.323134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.33324</td>\n",
       "      <td>0.25451</td>\n",
       "      <td>0.229328</td>\n",
       "      <td>0.050199</td>\n",
       "      <td>17.55094</td>\n",
       "      <td>15.213697</td>\n",
       "      <td>13.547566</td>\n",
       "      <td>15.437401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.341993</td>\n",
       "      <td>0.243951</td>\n",
       "      <td>0.215919</td>\n",
       "      <td>0.039292</td>\n",
       "      <td>17.633147</td>\n",
       "      <td>15.319508</td>\n",
       "      <td>13.670312</td>\n",
       "      <td>15.540989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.032566</td>\n",
       "      <td>0.510906</td>\n",
       "      <td>0.492559</td>\n",
       "      <td>0.323633</td>\n",
       "      <td>17.44286</td>\n",
       "      <td>12.033683</td>\n",
       "      <td>10.709358</td>\n",
       "      <td>13.3953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.765959  0.871084  0.920127   0.85239   9.535929   6.297802   \n",
       "1      0.634981  0.827652  0.856516   0.77305  11.906366   7.282338   \n",
       "2      0.532882  0.797346  0.790426  0.706885   13.46722   7.897391   \n",
       "3      0.446547  0.775221   0.74277  0.654846  14.658661   8.318508   \n",
       "4      0.367112  0.753616  0.708517  0.609748  15.675243   8.710845   \n",
       "5      0.290919  0.731115  0.682108  0.568047  16.592874   9.102608   \n",
       "6      0.220483    0.7065  0.658254  0.528412  17.399408   9.513137   \n",
       "7      0.156858  0.681053  0.634516  0.490809  18.098913   9.920164   \n",
       "8      0.098897  0.655069  0.609802  0.454589  18.715029  10.320589   \n",
       "9      0.045723  0.629307   0.58479   0.41994  19.266094  10.702614   \n",
       "10    -0.003053   0.60357  0.559112  0.386543  19.761351  11.070949   \n",
       "11    -0.047261    0.5771  0.535201  0.355013  19.829414  11.438263   \n",
       "12    -0.086907  0.549306  0.515646  0.326015  19.321323  11.812707   \n",
       "13    -0.121329  0.521436  0.500369  0.300159  18.969452  12.176753   \n",
       "14    -0.152701  0.493463  0.485241  0.275334  18.925077  12.533157   \n",
       "15    -0.180621   0.46687  0.467138  0.251129  19.088272  12.863758   \n",
       "16    -0.204335  0.441908  0.447458  0.228344  18.900957  13.164452   \n",
       "17    -0.223796  0.420305  0.427741  0.208083  18.688809  13.419826   \n",
       "18    -0.239804   0.40183   0.40715  0.189725  18.532704  13.634365   \n",
       "19    -0.252944  0.384906  0.387036  0.172999  18.359752  13.826088   \n",
       "20    -0.264692  0.368218  0.367663  0.157063  18.252034  14.013169   \n",
       "21    -0.274561  0.352549  0.349189  0.142392   18.20333  14.185035   \n",
       "22    -0.281993  0.337295  0.329383  0.128228  18.003078  14.350117   \n",
       "23    -0.288747  0.323502  0.308244  0.114333  17.804748  14.499839   \n",
       "24    -0.295788  0.310105   0.28806  0.100792  17.671096  14.643273   \n",
       "25    -0.304649  0.296872  0.270749  0.087658  17.554121  14.782289   \n",
       "26    -0.314632  0.283056  0.255851  0.074758  17.449624  14.924591   \n",
       "27    -0.324298  0.268465  0.242473  0.062213  17.470832  15.072656   \n",
       "28     -0.33324   0.25451  0.229328  0.050199   17.55094  15.213697   \n",
       "29    -0.341993  0.243951  0.215919  0.039292  17.633147  15.319508   \n",
       "mean  -0.032566  0.510906  0.492559  0.323633   17.44286  12.033683   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       4.333103   6.722278  \n",
       "1       5.809411   8.332705  \n",
       "2       7.022971   9.462527  \n",
       "3       7.782309  10.253159  \n",
       "4       8.285381   10.89049  \n",
       "5       8.653033  11.449505  \n",
       "6       8.972604  11.961716  \n",
       "7       9.280415  12.433164  \n",
       "8       9.590609  12.875409  \n",
       "9       9.895389  13.288032  \n",
       "10     10.199287  13.677196  \n",
       "11      10.47528  13.914319  \n",
       "12     10.696504  13.943511  \n",
       "13     10.866275   14.00416  \n",
       "14      11.03205  14.163428  \n",
       "15     11.226321  14.392784  \n",
       "16     11.434065  14.499824  \n",
       "17     11.638548  14.582394  \n",
       "18     11.848199  14.671756  \n",
       "19     12.049363  14.745068  \n",
       "20     12.240088  14.835097  \n",
       "21     12.419675  14.936013  \n",
       "22     12.610208  14.987801  \n",
       "23     12.811331  15.038639  \n",
       "24     13.000787  15.105052  \n",
       "25     13.162462  15.166291  \n",
       "26     13.301274  15.225163  \n",
       "27     13.425914  15.323134  \n",
       "28     13.547566  15.437401  \n",
       "29     13.670312  15.540989  \n",
       "mean   10.709358    13.3953  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/30_30_1_50_1_50_datt_seq2seq_gru_4.csv\n",
      "\n",
      "\n",
      "4th iteration\n",
      "history size: 40\n",
      "future size: 10\n",
      "Epoch 1/10000\n",
      "568/568 - 8s - loss: 0.2255 - val_loss: 0.1716 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "568/568 - 5s - loss: 0.1458 - val_loss: 0.1613 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "568/568 - 5s - loss: 0.1332 - val_loss: 0.1349 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "568/568 - 5s - loss: 0.1244 - val_loss: 0.1308 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "568/568 - 5s - loss: 0.1151 - val_loss: 0.1208 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "568/568 - 5s - loss: 0.1092 - val_loss: 0.1134 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "568/568 - 5s - loss: 0.1040 - val_loss: 0.1071 - 5s/epoch - 9ms/step\n",
      "Epoch 8/10000\n",
      "568/568 - 5s - loss: 0.0992 - val_loss: 0.1061 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "568/568 - 5s - loss: 0.0953 - val_loss: 0.1009 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "568/568 - 5s - loss: 0.0919 - val_loss: 0.0997 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "568/568 - 5s - loss: 0.0893 - val_loss: 0.0971 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "568/568 - 5s - loss: 0.0850 - val_loss: 0.0954 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "568/568 - 5s - loss: 0.0827 - val_loss: 0.0911 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "568/568 - 5s - loss: 0.0828 - val_loss: 0.0932 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "568/568 - 5s - loss: 0.0782 - val_loss: 0.0889 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "568/568 - 5s - loss: 0.0757 - val_loss: 0.0875 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "568/568 - 5s - loss: 0.0727 - val_loss: 0.0820 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "568/568 - 5s - loss: 0.0727 - val_loss: 0.0832 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "568/568 - 5s - loss: 0.0706 - val_loss: 0.0862 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "568/568 - 5s - loss: 0.0735 - val_loss: 0.0883 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "568/568 - 5s - loss: 0.0685 - val_loss: 0.0817 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "568/568 - 5s - loss: 0.0649 - val_loss: 0.0779 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "568/568 - 5s - loss: 0.0641 - val_loss: 0.0886 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "568/568 - 5s - loss: 0.0628 - val_loss: 0.0777 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "568/568 - 5s - loss: 0.0613 - val_loss: 0.0798 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "568/568 - 5s - loss: 0.0606 - val_loss: 0.0736 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "568/568 - 5s - loss: 0.0622 - val_loss: 0.0737 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "568/568 - 5s - loss: 0.0588 - val_loss: 0.0815 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "568/568 - 5s - loss: 0.0603 - val_loss: 0.0742 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "568/568 - 5s - loss: 0.0579 - val_loss: 0.0715 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "568/568 - 5s - loss: 0.0600 - val_loss: 0.0775 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "568/568 - 5s - loss: 0.0566 - val_loss: 0.0694 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "568/568 - 5s - loss: 0.0537 - val_loss: 0.0693 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "568/568 - 5s - loss: 0.0547 - val_loss: 0.0701 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "568/568 - 5s - loss: 0.0532 - val_loss: 0.0660 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "568/568 - 5s - loss: 0.0542 - val_loss: 0.0701 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "568/568 - 5s - loss: 0.0518 - val_loss: 0.0687 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "568/568 - 5s - loss: 0.0507 - val_loss: 0.0659 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "568/568 - 5s - loss: 0.0520 - val_loss: 0.0661 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "568/568 - 5s - loss: 0.0501 - val_loss: 0.0628 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "568/568 - 5s - loss: 0.0490 - val_loss: 0.0605 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "568/568 - 5s - loss: 0.0482 - val_loss: 0.0666 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "568/568 - 5s - loss: 0.0519 - val_loss: 0.0698 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "568/568 - 5s - loss: 0.0497 - val_loss: 0.0644 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "568/568 - 5s - loss: 0.0476 - val_loss: 0.0631 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "568/568 - 5s - loss: 0.0509 - val_loss: 0.0760 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "568/568 - 5s - loss: 0.0545 - val_loss: 0.0634 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "568/568 - 5s - loss: 0.0464 - val_loss: 0.0607 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "568/568 - 5s - loss: 0.0470 - val_loss: 0.0619 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "568/568 - 5s - loss: 0.0462 - val_loss: 0.0603 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "568/568 - 5s - loss: 0.0448 - val_loss: 0.0602 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "568/568 - 5s - loss: 0.0553 - val_loss: 0.0706 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "568/568 - 5s - loss: 0.0466 - val_loss: 0.0613 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "568/568 - 5s - loss: 0.0444 - val_loss: 0.0625 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "568/568 - 5s - loss: 0.0437 - val_loss: 0.0618 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "568/568 - 5s - loss: 0.0435 - val_loss: 0.0560 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "568/568 - 5s - loss: 0.0431 - val_loss: 0.0578 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "568/568 - 5s - loss: 0.0433 - val_loss: 0.0595 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "568/568 - 5s - loss: 0.0448 - val_loss: 0.0612 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "568/568 - 5s - loss: 0.0418 - val_loss: 0.0575 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "568/568 - 5s - loss: 0.0415 - val_loss: 0.0564 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "568/568 - 5s - loss: 0.0417 - val_loss: 0.0586 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "568/568 - 5s - loss: 0.0413 - val_loss: 0.0583 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "568/568 - 5s - loss: 0.0405 - val_loss: 0.0571 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "568/568 - 5s - loss: 0.0418 - val_loss: 0.0560 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "568/568 - 5s - loss: 0.0404 - val_loss: 0.0535 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "568/568 - 5s - loss: 0.0401 - val_loss: 0.0568 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "568/568 - 5s - loss: 0.0394 - val_loss: 0.0684 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "568/568 - 5s - loss: 0.0404 - val_loss: 0.0527 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "568/568 - 5s - loss: 0.0391 - val_loss: 0.0556 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "568/568 - 5s - loss: 0.0381 - val_loss: 0.0543 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "568/568 - 5s - loss: 0.0393 - val_loss: 0.0520 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "568/568 - 5s - loss: 0.0383 - val_loss: 0.0642 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "568/568 - 5s - loss: 0.0384 - val_loss: 0.0507 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "568/568 - 5s - loss: 0.0365 - val_loss: 0.0545 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "568/568 - 5s - loss: 0.0375 - val_loss: 0.0508 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "568/568 - 5s - loss: 0.0373 - val_loss: 0.0526 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "568/568 - 5s - loss: 0.0400 - val_loss: 0.0506 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "568/568 - 5s - loss: 0.0358 - val_loss: 0.0539 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "568/568 - 5s - loss: 0.0372 - val_loss: 0.0508 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "568/568 - 5s - loss: 0.0376 - val_loss: 0.0652 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "568/568 - 5s - loss: 0.0365 - val_loss: 0.0519 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "568/568 - 5s - loss: 0.0367 - val_loss: 0.0507 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "568/568 - 5s - loss: 0.0344 - val_loss: 0.0494 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "568/568 - 5s - loss: 0.0343 - val_loss: 0.0551 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "568/568 - 5s - loss: 0.0343 - val_loss: 0.0501 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "568/568 - 5s - loss: 0.0352 - val_loss: 0.0567 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "568/568 - 5s - loss: 0.0381 - val_loss: 0.0500 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "568/568 - 5s - loss: 0.0338 - val_loss: 0.0496 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "568/568 - 5s - loss: 0.0331 - val_loss: 0.0500 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "568/568 - 5s - loss: 0.0336 - val_loss: 0.0484 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "568/568 - 5s - loss: 0.0331 - val_loss: 0.0478 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "568/568 - 5s - loss: 0.0328 - val_loss: 0.0574 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "568/568 - 5s - loss: 0.0338 - val_loss: 0.0483 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "568/568 - 5s - loss: 0.0335 - val_loss: 0.0504 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "568/568 - 5s - loss: 0.0335 - val_loss: 0.0471 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "568/568 - 5s - loss: 0.0314 - val_loss: 0.0497 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "568/568 - 5s - loss: 0.0333 - val_loss: 0.0545 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "568/568 - 5s - loss: 0.0376 - val_loss: 0.0530 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "568/568 - 5s - loss: 0.0310 - val_loss: 0.0470 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "568/568 - 5s - loss: 0.0303 - val_loss: 0.0471 - 5s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "568/568 - 5s - loss: 0.0307 - val_loss: 0.0450 - 5s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "568/568 - 5s - loss: 0.0321 - val_loss: 0.0475 - 5s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "568/568 - 5s - loss: 0.0315 - val_loss: 0.0466 - 5s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "568/568 - 5s - loss: 0.0302 - val_loss: 0.0455 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "568/568 - 5s - loss: 0.0300 - val_loss: 0.0456 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "568/568 - 5s - loss: 0.0317 - val_loss: 0.0486 - 5s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "568/568 - 5s - loss: 0.0313 - val_loss: 0.0529 - 5s/epoch - 8ms/step\n",
      "Epoch 109/10000\n",
      "568/568 - 5s - loss: 0.0352 - val_loss: 0.0477 - 5s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "568/568 - 5s - loss: 0.0303 - val_loss: 0.0450 - 5s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "568/568 - 5s - loss: 0.0291 - val_loss: 0.0478 - 5s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "568/568 - 5s - loss: 0.0316 - val_loss: 0.0482 - 5s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "568/568 - 5s - loss: 0.0309 - val_loss: 0.0456 - 5s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "568/568 - 5s - loss: 0.0294 - val_loss: 0.0452 - 5s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "568/568 - 5s - loss: 0.0301 - val_loss: 0.0467 - 5s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "568/568 - 5s - loss: 0.0297 - val_loss: 0.0454 - 5s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "568/568 - 5s - loss: 0.0291 - val_loss: 0.0462 - 5s/epoch - 8ms/step\n",
      "Epoch 118/10000\n",
      "568/568 - 5s - loss: 0.0291 - val_loss: 0.0433 - 5s/epoch - 8ms/step\n",
      "Epoch 119/10000\n",
      "568/568 - 5s - loss: 0.0291 - val_loss: 0.0452 - 5s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "568/568 - 5s - loss: 0.0292 - val_loss: 0.0515 - 5s/epoch - 8ms/step\n",
      "Epoch 121/10000\n",
      "568/568 - 5s - loss: 0.0285 - val_loss: 0.0457 - 5s/epoch - 8ms/step\n",
      "Epoch 122/10000\n",
      "568/568 - 5s - loss: 0.0285 - val_loss: 0.0512 - 5s/epoch - 8ms/step\n",
      "Epoch 123/10000\n",
      "568/568 - 5s - loss: 0.0300 - val_loss: 0.0453 - 5s/epoch - 8ms/step\n",
      "Epoch 124/10000\n",
      "568/568 - 5s - loss: 0.0290 - val_loss: 0.0510 - 5s/epoch - 8ms/step\n",
      "Epoch 125/10000\n",
      "568/568 - 5s - loss: 0.0290 - val_loss: 0.0452 - 5s/epoch - 8ms/step\n",
      "Epoch 126/10000\n",
      "568/568 - 5s - loss: 0.0286 - val_loss: 0.0428 - 5s/epoch - 8ms/step\n",
      "Epoch 127/10000\n",
      "568/568 - 5s - loss: 0.0284 - val_loss: 0.0439 - 5s/epoch - 8ms/step\n",
      "Epoch 128/10000\n",
      "568/568 - 5s - loss: 0.0274 - val_loss: 0.0449 - 5s/epoch - 8ms/step\n",
      "Epoch 129/10000\n",
      "568/568 - 5s - loss: 0.0276 - val_loss: 0.0417 - 5s/epoch - 8ms/step\n",
      "Epoch 130/10000\n",
      "568/568 - 5s - loss: 0.0288 - val_loss: 0.0444 - 5s/epoch - 8ms/step\n",
      "Epoch 131/10000\n",
      "568/568 - 5s - loss: 0.0285 - val_loss: 0.0450 - 5s/epoch - 8ms/step\n",
      "Epoch 132/10000\n",
      "568/568 - 5s - loss: 0.0281 - val_loss: 0.0465 - 5s/epoch - 8ms/step\n",
      "Epoch 133/10000\n",
      "568/568 - 5s - loss: 0.0276 - val_loss: 0.0424 - 5s/epoch - 8ms/step\n",
      "Epoch 134/10000\n",
      "568/568 - 5s - loss: 0.0282 - val_loss: 0.0424 - 5s/epoch - 8ms/step\n",
      "Epoch 135/10000\n",
      "568/568 - 5s - loss: 0.0285 - val_loss: 0.0444 - 5s/epoch - 8ms/step\n",
      "Epoch 136/10000\n",
      "568/568 - 5s - loss: 0.0286 - val_loss: 0.0423 - 5s/epoch - 8ms/step\n",
      "Epoch 137/10000\n",
      "568/568 - 5s - loss: 0.0266 - val_loss: 0.0430 - 5s/epoch - 8ms/step\n",
      "Epoch 138/10000\n",
      "568/568 - 5s - loss: 0.0269 - val_loss: 0.0435 - 5s/epoch - 8ms/step\n",
      "Epoch 139/10000\n",
      "568/568 - 5s - loss: 0.0267 - val_loss: 0.0430 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_42_layer_call_fn, gru_cell_42_layer_call_and_return_conditional_losses, gru_cell_43_layer_call_fn, gru_cell_43_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_10_1_50_1_50_datt_seq2seq_gru_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_10_1_50_1_50_datt_seq2seq_gru_4\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002641F0797F0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002646F0FCC70> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.918275</td>\n",
       "      <td>0.924654</td>\n",
       "      <td>0.947537</td>\n",
       "      <td>0.930155</td>\n",
       "      <td>4.639218</td>\n",
       "      <td>4.831303</td>\n",
       "      <td>3.526543</td>\n",
       "      <td>4.332355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.846902</td>\n",
       "      <td>0.857702</td>\n",
       "      <td>0.91299</td>\n",
       "      <td>0.872531</td>\n",
       "      <td>6.310549</td>\n",
       "      <td>6.640151</td>\n",
       "      <td>4.54218</td>\n",
       "      <td>5.83096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.754404</td>\n",
       "      <td>0.798958</td>\n",
       "      <td>0.867163</td>\n",
       "      <td>0.806842</td>\n",
       "      <td>7.884756</td>\n",
       "      <td>7.893309</td>\n",
       "      <td>5.61306</td>\n",
       "      <td>7.130375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.652927</td>\n",
       "      <td>0.760635</td>\n",
       "      <td>0.832964</td>\n",
       "      <td>0.748842</td>\n",
       "      <td>9.248897</td>\n",
       "      <td>8.614067</td>\n",
       "      <td>6.295063</td>\n",
       "      <td>8.052676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.550971</td>\n",
       "      <td>0.738689</td>\n",
       "      <td>0.817991</td>\n",
       "      <td>0.70255</td>\n",
       "      <td>10.416149</td>\n",
       "      <td>9.001472</td>\n",
       "      <td>6.57159</td>\n",
       "      <td>8.66307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.455549</td>\n",
       "      <td>0.724459</td>\n",
       "      <td>0.814748</td>\n",
       "      <td>0.664919</td>\n",
       "      <td>11.357652</td>\n",
       "      <td>9.244273</td>\n",
       "      <td>6.630589</td>\n",
       "      <td>9.077505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.370754</td>\n",
       "      <td>0.712898</td>\n",
       "      <td>0.815189</td>\n",
       "      <td>0.632947</td>\n",
       "      <td>12.092518</td>\n",
       "      <td>9.436101</td>\n",
       "      <td>6.623873</td>\n",
       "      <td>9.384164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.29789</td>\n",
       "      <td>0.698771</td>\n",
       "      <td>0.816202</td>\n",
       "      <td>0.604288</td>\n",
       "      <td>12.742545</td>\n",
       "      <td>9.664749</td>\n",
       "      <td>6.607594</td>\n",
       "      <td>9.671629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.235412</td>\n",
       "      <td>0.68281</td>\n",
       "      <td>0.809991</td>\n",
       "      <td>0.576071</td>\n",
       "      <td>13.312967</td>\n",
       "      <td>9.916362</td>\n",
       "      <td>6.720448</td>\n",
       "      <td>9.983259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.178654</td>\n",
       "      <td>0.665749</td>\n",
       "      <td>0.787627</td>\n",
       "      <td>0.54401</td>\n",
       "      <td>13.814129</td>\n",
       "      <td>10.177856</td>\n",
       "      <td>7.107418</td>\n",
       "      <td>10.366468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.526174</td>\n",
       "      <td>0.756533</td>\n",
       "      <td>0.84224</td>\n",
       "      <td>0.708316</td>\n",
       "      <td>10.181938</td>\n",
       "      <td>8.541964</td>\n",
       "      <td>6.023836</td>\n",
       "      <td>8.249246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE     nRMSE   \n",
       "0      0.918275  0.924654  0.947537  0.930155   4.639218   4.831303  3.526543   \n",
       "1      0.846902  0.857702   0.91299  0.872531   6.310549   6.640151   4.54218   \n",
       "2      0.754404  0.798958  0.867163  0.806842   7.884756   7.893309   5.61306   \n",
       "3      0.652927  0.760635  0.832964  0.748842   9.248897   8.614067  6.295063   \n",
       "4      0.550971  0.738689  0.817991   0.70255  10.416149   9.001472   6.57159   \n",
       "5      0.455549  0.724459  0.814748  0.664919  11.357652   9.244273  6.630589   \n",
       "6      0.370754  0.712898  0.815189  0.632947  12.092518   9.436101  6.623873   \n",
       "7       0.29789  0.698771  0.816202  0.604288  12.742545   9.664749  6.607594   \n",
       "8      0.235412   0.68281  0.809991  0.576071  13.312967   9.916362  6.720448   \n",
       "9      0.178654  0.665749  0.787627   0.54401  13.814129  10.177856  7.107418   \n",
       "mean   0.526174  0.756533   0.84224  0.708316  10.181938   8.541964  6.023836   \n",
       "\n",
       "            mean  \n",
       "index      nRMSE  \n",
       "0       4.332355  \n",
       "1        5.83096  \n",
       "2       7.130375  \n",
       "3       8.052676  \n",
       "4        8.66307  \n",
       "5       9.077505  \n",
       "6       9.384164  \n",
       "7       9.671629  \n",
       "8       9.983259  \n",
       "9      10.366468  \n",
       "mean    8.249246  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/40_10_1_50_1_50_datt_seq2seq_gru_4.csv\n",
      "\n",
      "\n",
      "4th iteration\n",
      "history size: 40\n",
      "future size: 20\n",
      "Epoch 1/10000\n",
      "563/563 - 8s - loss: 0.3396 - val_loss: 0.2568 - 8s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.2230 - val_loss: 0.2314 - 5s/epoch - 9ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.2063 - val_loss: 0.2166 - 5s/epoch - 9ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 5s - loss: 0.1950 - val_loss: 0.2151 - 5s/epoch - 9ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.1868 - val_loss: 0.2027 - 5s/epoch - 9ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.1781 - val_loss: 0.1905 - 5s/epoch - 9ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.1697 - val_loss: 0.1849 - 5s/epoch - 9ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.1634 - val_loss: 0.1947 - 5s/epoch - 9ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.1572 - val_loss: 0.1801 - 5s/epoch - 9ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.1489 - val_loss: 0.1686 - 5s/epoch - 9ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.1437 - val_loss: 0.1567 - 5s/epoch - 9ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.1378 - val_loss: 0.1549 - 5s/epoch - 9ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.1314 - val_loss: 0.1452 - 5s/epoch - 9ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.1244 - val_loss: 0.1409 - 5s/epoch - 9ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.1200 - val_loss: 0.1380 - 5s/epoch - 9ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.1175 - val_loss: 0.1337 - 5s/epoch - 9ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.1132 - val_loss: 0.1374 - 5s/epoch - 9ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.1107 - val_loss: 0.1275 - 5s/epoch - 9ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.1049 - val_loss: 0.1235 - 5s/epoch - 9ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.1032 - val_loss: 0.1264 - 5s/epoch - 9ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.1011 - val_loss: 0.1229 - 5s/epoch - 9ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.1004 - val_loss: 0.1188 - 5s/epoch - 9ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.0986 - val_loss: 0.1170 - 5s/epoch - 9ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.0965 - val_loss: 0.1181 - 5s/epoch - 9ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.0934 - val_loss: 0.1121 - 5s/epoch - 9ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.0914 - val_loss: 0.1109 - 5s/epoch - 9ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.0907 - val_loss: 0.1144 - 5s/epoch - 9ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0892 - val_loss: 0.1100 - 5s/epoch - 9ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.0881 - val_loss: 0.1076 - 5s/epoch - 9ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.0858 - val_loss: 0.1319 - 5s/epoch - 9ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.0879 - val_loss: 0.1039 - 5s/epoch - 9ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.0829 - val_loss: 0.1036 - 5s/epoch - 9ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.0808 - val_loss: 0.1052 - 5s/epoch - 9ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.0800 - val_loss: 0.1030 - 5s/epoch - 9ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.0807 - val_loss: 0.1069 - 5s/epoch - 9ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0796 - val_loss: 0.1070 - 5s/epoch - 9ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0793 - val_loss: 0.1016 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0778 - val_loss: 0.0986 - 5s/epoch - 9ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.0749 - val_loss: 0.0956 - 5s/epoch - 9ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0751 - val_loss: 0.1058 - 5s/epoch - 9ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0738 - val_loss: 0.0971 - 5s/epoch - 9ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0734 - val_loss: 0.0999 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0753 - val_loss: 0.0951 - 5s/epoch - 9ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0723 - val_loss: 0.1002 - 5s/epoch - 9ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0706 - val_loss: 0.0917 - 5s/epoch - 9ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0695 - val_loss: 0.0927 - 5s/epoch - 9ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0734 - val_loss: 0.0952 - 5s/epoch - 9ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0683 - val_loss: 0.0916 - 5s/epoch - 9ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0684 - val_loss: 0.0940 - 5s/epoch - 9ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0735 - val_loss: 0.1081 - 5s/epoch - 9ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0714 - val_loss: 0.0897 - 5s/epoch - 9ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0666 - val_loss: 0.0947 - 5s/epoch - 9ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0674 - val_loss: 0.0878 - 5s/epoch - 9ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0649 - val_loss: 0.0893 - 5s/epoch - 9ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0663 - val_loss: 0.0898 - 5s/epoch - 9ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0648 - val_loss: 0.0887 - 5s/epoch - 9ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0633 - val_loss: 0.0901 - 5s/epoch - 9ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0631 - val_loss: 0.0874 - 5s/epoch - 10ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0634 - val_loss: 0.0851 - 5s/epoch - 9ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0628 - val_loss: 0.0864 - 5s/epoch - 9ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0632 - val_loss: 0.0894 - 5s/epoch - 9ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0604 - val_loss: 0.0880 - 5s/epoch - 9ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 5s - loss: 0.0650 - val_loss: 0.0812 - 5s/epoch - 9ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0598 - val_loss: 0.0839 - 5s/epoch - 9ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0601 - val_loss: 0.0850 - 5s/epoch - 9ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0608 - val_loss: 0.0791 - 5s/epoch - 9ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0589 - val_loss: 0.0827 - 5s/epoch - 9ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 5s - loss: 0.0594 - val_loss: 0.0804 - 5s/epoch - 9ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 5s - loss: 0.0702 - val_loss: 0.0939 - 5s/epoch - 9ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 5s - loss: 0.0611 - val_loss: 0.0802 - 5s/epoch - 9ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 5s - loss: 0.0564 - val_loss: 0.0763 - 5s/epoch - 9ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 5s - loss: 0.0560 - val_loss: 0.0784 - 5s/epoch - 9ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 5s - loss: 0.0567 - val_loss: 0.0774 - 5s/epoch - 9ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 5s - loss: 0.0562 - val_loss: 0.0794 - 5s/epoch - 9ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 5s - loss: 0.0583 - val_loss: 0.0822 - 5s/epoch - 9ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 5s - loss: 0.0577 - val_loss: 0.0783 - 5s/epoch - 9ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 5s - loss: 0.0552 - val_loss: 0.0783 - 5s/epoch - 9ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 5s - loss: 0.0557 - val_loss: 0.0804 - 5s/epoch - 9ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 5s - loss: 0.0557 - val_loss: 0.0788 - 5s/epoch - 9ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 5s - loss: 0.0554 - val_loss: 0.0791 - 5s/epoch - 9ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 5s - loss: 0.0545 - val_loss: 0.0760 - 5s/epoch - 9ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 5s - loss: 0.0547 - val_loss: 0.0787 - 5s/epoch - 9ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 5s - loss: 0.0564 - val_loss: 0.0747 - 5s/epoch - 9ms/step\n",
      "Epoch 84/10000\n",
      "563/563 - 5s - loss: 0.0558 - val_loss: 0.0773 - 5s/epoch - 9ms/step\n",
      "Epoch 85/10000\n",
      "563/563 - 5s - loss: 0.0537 - val_loss: 0.0886 - 5s/epoch - 9ms/step\n",
      "Epoch 86/10000\n",
      "563/563 - 5s - loss: 0.0524 - val_loss: 0.0754 - 5s/epoch - 9ms/step\n",
      "Epoch 87/10000\n",
      "563/563 - 5s - loss: 0.0543 - val_loss: 0.0734 - 5s/epoch - 9ms/step\n",
      "Epoch 88/10000\n",
      "563/563 - 5s - loss: 0.0535 - val_loss: 0.0745 - 5s/epoch - 9ms/step\n",
      "Epoch 89/10000\n",
      "563/563 - 5s - loss: 0.0541 - val_loss: 0.0739 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "563/563 - 5s - loss: 0.0528 - val_loss: 0.0737 - 5s/epoch - 9ms/step\n",
      "Epoch 91/10000\n",
      "563/563 - 5s - loss: 0.0500 - val_loss: 0.0688 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "563/563 - 5s - loss: 0.0541 - val_loss: 0.0835 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "563/563 - 5s - loss: 0.0523 - val_loss: 0.0711 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "563/563 - 5s - loss: 0.0525 - val_loss: 0.0763 - 5s/epoch - 9ms/step\n",
      "Epoch 95/10000\n",
      "563/563 - 5s - loss: 0.0505 - val_loss: 0.0698 - 5s/epoch - 9ms/step\n",
      "Epoch 96/10000\n",
      "563/563 - 5s - loss: 0.0502 - val_loss: 0.0699 - 5s/epoch - 9ms/step\n",
      "Epoch 97/10000\n",
      "563/563 - 5s - loss: 0.0479 - val_loss: 0.0737 - 5s/epoch - 9ms/step\n",
      "Epoch 98/10000\n",
      "563/563 - 5s - loss: 0.0495 - val_loss: 0.0778 - 5s/epoch - 9ms/step\n",
      "Epoch 99/10000\n",
      "563/563 - 5s - loss: 0.0498 - val_loss: 0.0701 - 5s/epoch - 9ms/step\n",
      "Epoch 100/10000\n",
      "563/563 - 5s - loss: 0.0513 - val_loss: 0.0674 - 5s/epoch - 9ms/step\n",
      "Epoch 101/10000\n",
      "563/563 - 5s - loss: 0.0512 - val_loss: 0.0747 - 5s/epoch - 9ms/step\n",
      "Epoch 102/10000\n",
      "563/563 - 5s - loss: 0.0485 - val_loss: 0.0687 - 5s/epoch - 9ms/step\n",
      "Epoch 103/10000\n",
      "563/563 - 5s - loss: 0.0479 - val_loss: 0.0672 - 5s/epoch - 9ms/step\n",
      "Epoch 104/10000\n",
      "563/563 - 5s - loss: 0.0488 - val_loss: 0.0750 - 5s/epoch - 9ms/step\n",
      "Epoch 105/10000\n",
      "563/563 - 5s - loss: 0.0486 - val_loss: 0.0654 - 5s/epoch - 9ms/step\n",
      "Epoch 106/10000\n",
      "563/563 - 5s - loss: 0.0494 - val_loss: 0.0709 - 5s/epoch - 9ms/step\n",
      "Epoch 107/10000\n",
      "563/563 - 5s - loss: 0.0475 - val_loss: 0.0722 - 5s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "563/563 - 5s - loss: 0.0499 - val_loss: 0.0668 - 5s/epoch - 9ms/step\n",
      "Epoch 109/10000\n",
      "563/563 - 5s - loss: 0.0484 - val_loss: 0.0698 - 5s/epoch - 9ms/step\n",
      "Epoch 110/10000\n",
      "563/563 - 5s - loss: 0.0453 - val_loss: 0.0708 - 5s/epoch - 9ms/step\n",
      "Epoch 111/10000\n",
      "563/563 - 5s - loss: 0.0463 - val_loss: 0.0700 - 5s/epoch - 9ms/step\n",
      "Epoch 112/10000\n",
      "563/563 - 5s - loss: 0.0515 - val_loss: 0.0708 - 5s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "563/563 - 5s - loss: 0.0459 - val_loss: 0.0693 - 5s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "563/563 - 5s - loss: 0.0466 - val_loss: 0.0667 - 5s/epoch - 9ms/step\n",
      "Epoch 115/10000\n",
      "563/563 - 5s - loss: 0.0450 - val_loss: 0.0626 - 5s/epoch - 9ms/step\n",
      "Epoch 116/10000\n",
      "563/563 - 5s - loss: 0.0446 - val_loss: 0.0655 - 5s/epoch - 9ms/step\n",
      "Epoch 117/10000\n",
      "563/563 - 5s - loss: 0.0457 - val_loss: 0.0668 - 5s/epoch - 9ms/step\n",
      "Epoch 118/10000\n",
      "563/563 - 5s - loss: 0.0457 - val_loss: 0.0604 - 5s/epoch - 9ms/step\n",
      "Epoch 119/10000\n",
      "563/563 - 5s - loss: 0.0491 - val_loss: 0.0994 - 5s/epoch - 9ms/step\n",
      "Epoch 120/10000\n",
      "563/563 - 5s - loss: 0.0535 - val_loss: 0.0647 - 5s/epoch - 9ms/step\n",
      "Epoch 121/10000\n",
      "563/563 - 5s - loss: 0.0439 - val_loss: 0.0683 - 5s/epoch - 9ms/step\n",
      "Epoch 122/10000\n",
      "563/563 - 5s - loss: 0.0504 - val_loss: 0.0725 - 5s/epoch - 9ms/step\n",
      "Epoch 123/10000\n",
      "563/563 - 5s - loss: 0.0442 - val_loss: 0.0621 - 5s/epoch - 9ms/step\n",
      "Epoch 124/10000\n",
      "563/563 - 5s - loss: 0.0441 - val_loss: 0.0610 - 5s/epoch - 9ms/step\n",
      "Epoch 125/10000\n",
      "563/563 - 5s - loss: 0.0420 - val_loss: 0.0620 - 5s/epoch - 9ms/step\n",
      "Epoch 126/10000\n",
      "563/563 - 5s - loss: 0.0435 - val_loss: 0.0618 - 5s/epoch - 9ms/step\n",
      "Epoch 127/10000\n",
      "563/563 - 5s - loss: 0.0438 - val_loss: 0.0633 - 5s/epoch - 9ms/step\n",
      "Epoch 128/10000\n",
      "563/563 - 5s - loss: 0.0437 - val_loss: 0.0628 - 5s/epoch - 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_44_layer_call_fn, gru_cell_44_layer_call_and_return_conditional_losses, gru_cell_45_layer_call_fn, gru_cell_45_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_20_1_50_1_50_datt_seq2seq_gru_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_20_1_50_1_50_datt_seq2seq_gru_4\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002646F6C7CA0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002641F019490> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.863711</td>\n",
       "      <td>0.903602</td>\n",
       "      <td>0.928917</td>\n",
       "      <td>0.898744</td>\n",
       "      <td>7.284245</td>\n",
       "      <td>5.459274</td>\n",
       "      <td>4.095333</td>\n",
       "      <td>5.612951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.781809</td>\n",
       "      <td>0.83321</td>\n",
       "      <td>0.908396</td>\n",
       "      <td>0.841138</td>\n",
       "      <td>9.051098</td>\n",
       "      <td>7.183327</td>\n",
       "      <td>4.650403</td>\n",
       "      <td>6.961609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.677142</td>\n",
       "      <td>0.776316</td>\n",
       "      <td>0.877554</td>\n",
       "      <td>0.777004</td>\n",
       "      <td>10.530435</td>\n",
       "      <td>8.321967</td>\n",
       "      <td>5.378134</td>\n",
       "      <td>8.076845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.562348</td>\n",
       "      <td>0.738627</td>\n",
       "      <td>0.849858</td>\n",
       "      <td>0.716944</td>\n",
       "      <td>11.850936</td>\n",
       "      <td>8.998953</td>\n",
       "      <td>5.956727</td>\n",
       "      <td>8.935539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.71382</td>\n",
       "      <td>0.82661</td>\n",
       "      <td>0.667798</td>\n",
       "      <td>12.917585</td>\n",
       "      <td>9.420506</td>\n",
       "      <td>6.40275</td>\n",
       "      <td>9.58028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.380647</td>\n",
       "      <td>0.695724</td>\n",
       "      <td>0.807665</td>\n",
       "      <td>0.628012</td>\n",
       "      <td>13.82549</td>\n",
       "      <td>9.718182</td>\n",
       "      <td>6.744652</td>\n",
       "      <td>10.096108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.311365</td>\n",
       "      <td>0.679813</td>\n",
       "      <td>0.792637</td>\n",
       "      <td>0.594605</td>\n",
       "      <td>14.292385</td>\n",
       "      <td>9.971303</td>\n",
       "      <td>7.004612</td>\n",
       "      <td>10.422767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.252892</td>\n",
       "      <td>0.663811</td>\n",
       "      <td>0.779046</td>\n",
       "      <td>0.565249</td>\n",
       "      <td>14.602218</td>\n",
       "      <td>10.219718</td>\n",
       "      <td>7.231926</td>\n",
       "      <td>10.684621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.203636</td>\n",
       "      <td>0.647704</td>\n",
       "      <td>0.760796</td>\n",
       "      <td>0.537379</td>\n",
       "      <td>14.853128</td>\n",
       "      <td>10.463482</td>\n",
       "      <td>7.526004</td>\n",
       "      <td>10.947538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.159652</td>\n",
       "      <td>0.631369</td>\n",
       "      <td>0.733769</td>\n",
       "      <td>0.508263</td>\n",
       "      <td>15.035939</td>\n",
       "      <td>10.703462</td>\n",
       "      <td>7.941012</td>\n",
       "      <td>11.226805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.119092</td>\n",
       "      <td>0.612665</td>\n",
       "      <td>0.694897</td>\n",
       "      <td>0.475551</td>\n",
       "      <td>15.232953</td>\n",
       "      <td>10.972265</td>\n",
       "      <td>8.502239</td>\n",
       "      <td>11.569152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.080342</td>\n",
       "      <td>0.595088</td>\n",
       "      <td>0.647897</td>\n",
       "      <td>0.441109</td>\n",
       "      <td>15.46264</td>\n",
       "      <td>11.217798</td>\n",
       "      <td>9.135192</td>\n",
       "      <td>11.938543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.043096</td>\n",
       "      <td>0.577288</td>\n",
       "      <td>0.604922</td>\n",
       "      <td>0.408436</td>\n",
       "      <td>15.553853</td>\n",
       "      <td>11.460868</td>\n",
       "      <td>9.678906</td>\n",
       "      <td>12.231209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.008655</td>\n",
       "      <td>0.558773</td>\n",
       "      <td>0.575101</td>\n",
       "      <td>0.380843</td>\n",
       "      <td>15.615808</td>\n",
       "      <td>11.710115</td>\n",
       "      <td>10.04063</td>\n",
       "      <td>12.455518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.023207</td>\n",
       "      <td>0.540435</td>\n",
       "      <td>0.55518</td>\n",
       "      <td>0.357469</td>\n",
       "      <td>15.702843</td>\n",
       "      <td>11.951441</td>\n",
       "      <td>10.276372</td>\n",
       "      <td>12.643552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.049998</td>\n",
       "      <td>0.521921</td>\n",
       "      <td>0.535543</td>\n",
       "      <td>0.335822</td>\n",
       "      <td>15.748045</td>\n",
       "      <td>12.18918</td>\n",
       "      <td>10.5044</td>\n",
       "      <td>12.813875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.071847</td>\n",
       "      <td>0.502482</td>\n",
       "      <td>0.511148</td>\n",
       "      <td>0.313928</td>\n",
       "      <td>15.756157</td>\n",
       "      <td>12.432661</td>\n",
       "      <td>10.780825</td>\n",
       "      <td>12.989881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.087535</td>\n",
       "      <td>0.48243</td>\n",
       "      <td>0.483186</td>\n",
       "      <td>0.292694</td>\n",
       "      <td>15.832242</td>\n",
       "      <td>12.678171</td>\n",
       "      <td>11.089503</td>\n",
       "      <td>13.199972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.101512</td>\n",
       "      <td>0.462031</td>\n",
       "      <td>0.454262</td>\n",
       "      <td>0.271594</td>\n",
       "      <td>15.952943</td>\n",
       "      <td>12.92387</td>\n",
       "      <td>11.400357</td>\n",
       "      <td>13.425723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.113275</td>\n",
       "      <td>0.439875</td>\n",
       "      <td>0.424103</td>\n",
       "      <td>0.250234</td>\n",
       "      <td>16.060387</td>\n",
       "      <td>13.185972</td>\n",
       "      <td>11.715762</td>\n",
       "      <td>13.65404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.222999</td>\n",
       "      <td>0.628849</td>\n",
       "      <td>0.687574</td>\n",
       "      <td>0.513141</td>\n",
       "      <td>14.058066</td>\n",
       "      <td>10.559126</td>\n",
       "      <td>8.302787</td>\n",
       "      <td>10.973326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.863711  0.903602  0.928917  0.898744   7.284245   5.459274   \n",
       "1      0.781809   0.83321  0.908396  0.841138   9.051098   7.183327   \n",
       "2      0.677142  0.776316  0.877554  0.777004  10.530435   8.321967   \n",
       "3      0.562348  0.738627  0.849858  0.716944  11.850936   8.998953   \n",
       "4      0.462963   0.71382   0.82661  0.667798  12.917585   9.420506   \n",
       "5      0.380647  0.695724  0.807665  0.628012   13.82549   9.718182   \n",
       "6      0.311365  0.679813  0.792637  0.594605  14.292385   9.971303   \n",
       "7      0.252892  0.663811  0.779046  0.565249  14.602218  10.219718   \n",
       "8      0.203636  0.647704  0.760796  0.537379  14.853128  10.463482   \n",
       "9      0.159652  0.631369  0.733769  0.508263  15.035939  10.703462   \n",
       "10     0.119092  0.612665  0.694897  0.475551  15.232953  10.972265   \n",
       "11     0.080342  0.595088  0.647897  0.441109   15.46264  11.217798   \n",
       "12     0.043096  0.577288  0.604922  0.408436  15.553853  11.460868   \n",
       "13     0.008655  0.558773  0.575101  0.380843  15.615808  11.710115   \n",
       "14    -0.023207  0.540435   0.55518  0.357469  15.702843  11.951441   \n",
       "15    -0.049998  0.521921  0.535543  0.335822  15.748045   12.18918   \n",
       "16    -0.071847  0.502482  0.511148  0.313928  15.756157  12.432661   \n",
       "17    -0.087535   0.48243  0.483186  0.292694  15.832242  12.678171   \n",
       "18    -0.101512  0.462031  0.454262  0.271594  15.952943   12.92387   \n",
       "19    -0.113275  0.439875  0.424103  0.250234  16.060387  13.185972   \n",
       "mean   0.222999  0.628849  0.687574  0.513141  14.058066  10.559126   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       4.095333   5.612951  \n",
       "1       4.650403   6.961609  \n",
       "2       5.378134   8.076845  \n",
       "3       5.956727   8.935539  \n",
       "4        6.40275    9.58028  \n",
       "5       6.744652  10.096108  \n",
       "6       7.004612  10.422767  \n",
       "7       7.231926  10.684621  \n",
       "8       7.526004  10.947538  \n",
       "9       7.941012  11.226805  \n",
       "10      8.502239  11.569152  \n",
       "11      9.135192  11.938543  \n",
       "12      9.678906  12.231209  \n",
       "13      10.04063  12.455518  \n",
       "14     10.276372  12.643552  \n",
       "15       10.5044  12.813875  \n",
       "16     10.780825  12.989881  \n",
       "17     11.089503  13.199972  \n",
       "18     11.400357  13.425723  \n",
       "19     11.715762   13.65404  \n",
       "mean    8.302787  10.973326  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/40_20_1_50_1_50_datt_seq2seq_gru_4.csv\n",
      "\n",
      "\n",
      "4th iteration\n",
      "history size: 40\n",
      "future size: 30\n",
      "Epoch 1/10000\n",
      "559/559 - 8s - loss: 0.3840 - val_loss: 0.3122 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "559/559 - 5s - loss: 0.2865 - val_loss: 0.2910 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "559/559 - 5s - loss: 0.2650 - val_loss: 0.2648 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "559/559 - 5s - loss: 0.2496 - val_loss: 0.2535 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "559/559 - 5s - loss: 0.2348 - val_loss: 0.2440 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "559/559 - 5s - loss: 0.2185 - val_loss: 0.2235 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "559/559 - 5s - loss: 0.2065 - val_loss: 0.2155 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "559/559 - 5s - loss: 0.1958 - val_loss: 0.1998 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "559/559 - 5s - loss: 0.1854 - val_loss: 0.1893 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "559/559 - 5s - loss: 0.1767 - val_loss: 0.1793 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "559/559 - 5s - loss: 0.1695 - val_loss: 0.1805 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "559/559 - 5s - loss: 0.1644 - val_loss: 0.1798 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "559/559 - 5s - loss: 0.1592 - val_loss: 0.1638 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "559/559 - 5s - loss: 0.1553 - val_loss: 0.1612 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "559/559 - 5s - loss: 0.1489 - val_loss: 0.1523 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "559/559 - 5s - loss: 0.1458 - val_loss: 0.1483 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "559/559 - 5s - loss: 0.1459 - val_loss: 0.1600 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "559/559 - 5s - loss: 0.1389 - val_loss: 0.1412 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "559/559 - 5s - loss: 0.1353 - val_loss: 0.1420 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "559/559 - 5s - loss: 0.1324 - val_loss: 0.1523 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "559/559 - 5s - loss: 0.1316 - val_loss: 0.1359 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "559/559 - 5s - loss: 0.1237 - val_loss: 0.1315 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "559/559 - 5s - loss: 0.1265 - val_loss: 0.1309 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "559/559 - 5s - loss: 0.1188 - val_loss: 0.1230 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "559/559 - 5s - loss: 0.1286 - val_loss: 0.1279 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "559/559 - 5s - loss: 0.1141 - val_loss: 0.1223 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "559/559 - 5s - loss: 0.1113 - val_loss: 0.1245 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "559/559 - 5s - loss: 0.1103 - val_loss: 0.1197 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "559/559 - 5s - loss: 0.1101 - val_loss: 0.1175 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "559/559 - 5s - loss: 0.1056 - val_loss: 0.1152 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "559/559 - 5s - loss: 0.1059 - val_loss: 0.1159 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "559/559 - 5s - loss: 0.1049 - val_loss: 0.1200 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "559/559 - 5s - loss: 0.1043 - val_loss: 0.1133 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "559/559 - 5s - loss: 0.1009 - val_loss: 0.1151 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "559/559 - 5s - loss: 0.1011 - val_loss: 0.1112 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "559/559 - 5s - loss: 0.0986 - val_loss: 0.1184 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "559/559 - 5s - loss: 0.0963 - val_loss: 0.1103 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "559/559 - 5s - loss: 0.0972 - val_loss: 0.1124 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "559/559 - 5s - loss: 0.0944 - val_loss: 0.1080 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "559/559 - 5s - loss: 0.0950 - val_loss: 0.0985 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "559/559 - 5s - loss: 0.0924 - val_loss: 0.1116 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "559/559 - 5s - loss: 0.0920 - val_loss: 0.1019 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "559/559 - 5s - loss: 0.0944 - val_loss: 0.0988 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "559/559 - 5s - loss: 0.0897 - val_loss: 0.1020 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "559/559 - 5s - loss: 0.0889 - val_loss: 0.1024 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "559/559 - 5s - loss: 0.0914 - val_loss: 0.0991 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "559/559 - 5s - loss: 0.0908 - val_loss: 0.1119 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "559/559 - 5s - loss: 0.0922 - val_loss: 0.0983 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "559/559 - 5s - loss: 0.0860 - val_loss: 0.1015 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "559/559 - 5s - loss: 0.0851 - val_loss: 0.0950 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "559/559 - 5s - loss: 0.0861 - val_loss: 0.0995 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "559/559 - 5s - loss: 0.0826 - val_loss: 0.0970 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "559/559 - 5s - loss: 0.0867 - val_loss: 0.0930 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "559/559 - 5s - loss: 0.0822 - val_loss: 0.0959 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "559/559 - 5s - loss: 0.0822 - val_loss: 0.0926 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "559/559 - 5s - loss: 0.0836 - val_loss: 0.0917 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "559/559 - 5s - loss: 0.0790 - val_loss: 0.0894 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "559/559 - 5s - loss: 0.0777 - val_loss: 0.0947 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "559/559 - 5s - loss: 0.0806 - val_loss: 0.0867 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "559/559 - 5s - loss: 0.0768 - val_loss: 0.0905 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "559/559 - 5s - loss: 0.0788 - val_loss: 0.0880 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "559/559 - 5s - loss: 0.0753 - val_loss: 0.0882 - 5s/epoch - 9ms/step\n",
      "Epoch 63/10000\n",
      "559/559 - 5s - loss: 0.0770 - val_loss: 0.0890 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "559/559 - 5s - loss: 0.0772 - val_loss: 0.0833 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "559/559 - 5s - loss: 0.0751 - val_loss: 0.0839 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "559/559 - 5s - loss: 0.0720 - val_loss: 0.1052 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "559/559 - 5s - loss: 0.0755 - val_loss: 0.0833 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "559/559 - 5s - loss: 0.0776 - val_loss: 0.0815 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "559/559 - 5s - loss: 0.0710 - val_loss: 0.0894 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "559/559 - 5s - loss: 0.0699 - val_loss: 0.0824 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "559/559 - 5s - loss: 0.0739 - val_loss: 0.1075 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "559/559 - 5s - loss: 0.0767 - val_loss: 0.0822 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "559/559 - 5s - loss: 0.0685 - val_loss: 0.0835 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "559/559 - 5s - loss: 0.0700 - val_loss: 0.0788 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "559/559 - 5s - loss: 0.0701 - val_loss: 0.0843 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "559/559 - 5s - loss: 0.0686 - val_loss: 0.0773 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "559/559 - 5s - loss: 0.0685 - val_loss: 0.0879 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "559/559 - 5s - loss: 0.0688 - val_loss: 0.0859 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "559/559 - 5s - loss: 0.0664 - val_loss: 0.0789 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "559/559 - 5s - loss: 0.0657 - val_loss: 0.0758 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "559/559 - 5s - loss: 0.0675 - val_loss: 0.0775 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "559/559 - 5s - loss: 0.0681 - val_loss: 0.0783 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "559/559 - 5s - loss: 0.0657 - val_loss: 0.0791 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "559/559 - 5s - loss: 0.0650 - val_loss: 0.0789 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "559/559 - 5s - loss: 0.0643 - val_loss: 0.0781 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "559/559 - 5s - loss: 0.0650 - val_loss: 0.0812 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "559/559 - 5s - loss: 0.0639 - val_loss: 0.0734 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "559/559 - 5s - loss: 0.0636 - val_loss: 0.0874 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "559/559 - 5s - loss: 0.0634 - val_loss: 0.0734 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "559/559 - 5s - loss: 0.0628 - val_loss: 0.0759 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "559/559 - 5s - loss: 0.0658 - val_loss: 0.0869 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "559/559 - 5s - loss: 0.0622 - val_loss: 0.0720 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "559/559 - 5s - loss: 0.0607 - val_loss: 0.0740 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "559/559 - 5s - loss: 0.0618 - val_loss: 0.0746 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "559/559 - 5s - loss: 0.0628 - val_loss: 0.0718 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "559/559 - 5s - loss: 0.0607 - val_loss: 0.0727 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "559/559 - 5s - loss: 0.0604 - val_loss: 0.0756 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "559/559 - 5s - loss: 0.0608 - val_loss: 0.0712 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "559/559 - 5s - loss: 0.0599 - val_loss: 0.0726 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "559/559 - 5s - loss: 0.0587 - val_loss: 0.0741 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "559/559 - 5s - loss: 0.0601 - val_loss: 0.0715 - 5s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "559/559 - 5s - loss: 0.0599 - val_loss: 0.0738 - 5s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "559/559 - 5s - loss: 0.0592 - val_loss: 0.0816 - 5s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "559/559 - 5s - loss: 0.0596 - val_loss: 0.0690 - 5s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "559/559 - 5s - loss: 0.0590 - val_loss: 0.0723 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "559/559 - 5s - loss: 0.0592 - val_loss: 0.0699 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "559/559 - 5s - loss: 0.0590 - val_loss: 0.0743 - 5s/epoch - 9ms/step\n",
      "Epoch 108/10000\n",
      "559/559 - 5s - loss: 0.0589 - val_loss: 0.0703 - 5s/epoch - 8ms/step\n",
      "Epoch 109/10000\n",
      "559/559 - 5s - loss: 0.0561 - val_loss: 0.0680 - 5s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "559/559 - 4s - loss: 0.0566 - val_loss: 0.0678 - 4s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "559/559 - 5s - loss: 0.0585 - val_loss: 0.0668 - 5s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "559/559 - 4s - loss: 0.0572 - val_loss: 0.0663 - 4s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "559/559 - 5s - loss: 0.0573 - val_loss: 0.0702 - 5s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "559/559 - 5s - loss: 0.0578 - val_loss: 0.0739 - 5s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "559/559 - 5s - loss: 0.0561 - val_loss: 0.0637 - 5s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "559/559 - 5s - loss: 0.0558 - val_loss: 0.0732 - 5s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "559/559 - 5s - loss: 0.0557 - val_loss: 0.0737 - 5s/epoch - 8ms/step\n",
      "Epoch 118/10000\n",
      "559/559 - 5s - loss: 0.0559 - val_loss: 0.0666 - 5s/epoch - 9ms/step\n",
      "Epoch 119/10000\n",
      "559/559 - 5s - loss: 0.0567 - val_loss: 0.0717 - 5s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "559/559 - 5s - loss: 0.0564 - val_loss: 0.0680 - 5s/epoch - 8ms/step\n",
      "Epoch 121/10000\n",
      "559/559 - 5s - loss: 0.0550 - val_loss: 0.0664 - 5s/epoch - 8ms/step\n",
      "Epoch 122/10000\n",
      "559/559 - 5s - loss: 0.0538 - val_loss: 0.0680 - 5s/epoch - 8ms/step\n",
      "Epoch 123/10000\n",
      "559/559 - 5s - loss: 0.0544 - val_loss: 0.0665 - 5s/epoch - 9ms/step\n",
      "Epoch 124/10000\n",
      "559/559 - 5s - loss: 0.0560 - val_loss: 0.0642 - 5s/epoch - 9ms/step\n",
      "Epoch 125/10000\n",
      "559/559 - 5s - loss: 0.0539 - val_loss: 0.0636 - 5s/epoch - 8ms/step\n",
      "Epoch 126/10000\n",
      "559/559 - 5s - loss: 0.0539 - val_loss: 0.0626 - 5s/epoch - 8ms/step\n",
      "Epoch 127/10000\n",
      "559/559 - 5s - loss: 0.0549 - val_loss: 0.0710 - 5s/epoch - 8ms/step\n",
      "Epoch 128/10000\n",
      "559/559 - 4s - loss: 0.0530 - val_loss: 0.0647 - 4s/epoch - 8ms/step\n",
      "Epoch 129/10000\n",
      "559/559 - 5s - loss: 0.0537 - val_loss: 0.0657 - 5s/epoch - 8ms/step\n",
      "Epoch 130/10000\n",
      "559/559 - 4s - loss: 0.0535 - val_loss: 0.0628 - 4s/epoch - 8ms/step\n",
      "Epoch 131/10000\n",
      "559/559 - 4s - loss: 0.0562 - val_loss: 0.0703 - 4s/epoch - 8ms/step\n",
      "Epoch 132/10000\n",
      "559/559 - 5s - loss: 0.0602 - val_loss: 0.0735 - 5s/epoch - 8ms/step\n",
      "Epoch 133/10000\n",
      "559/559 - 5s - loss: 0.0536 - val_loss: 0.0608 - 5s/epoch - 8ms/step\n",
      "Epoch 134/10000\n",
      "559/559 - 5s - loss: 0.0512 - val_loss: 0.0630 - 5s/epoch - 8ms/step\n",
      "Epoch 135/10000\n",
      "559/559 - 5s - loss: 0.0512 - val_loss: 0.0649 - 5s/epoch - 8ms/step\n",
      "Epoch 136/10000\n",
      "559/559 - 5s - loss: 0.0527 - val_loss: 0.0631 - 5s/epoch - 8ms/step\n",
      "Epoch 137/10000\n",
      "559/559 - 5s - loss: 0.0513 - val_loss: 0.0656 - 5s/epoch - 8ms/step\n",
      "Epoch 138/10000\n",
      "559/559 - 5s - loss: 0.0519 - val_loss: 0.0680 - 5s/epoch - 8ms/step\n",
      "Epoch 139/10000\n",
      "559/559 - 4s - loss: 0.0536 - val_loss: 0.0636 - 4s/epoch - 8ms/step\n",
      "Epoch 140/10000\n",
      "559/559 - 5s - loss: 0.0517 - val_loss: 0.0624 - 5s/epoch - 8ms/step\n",
      "Epoch 141/10000\n",
      "559/559 - 5s - loss: 0.0508 - val_loss: 0.0612 - 5s/epoch - 8ms/step\n",
      "Epoch 142/10000\n",
      "559/559 - 5s - loss: 0.0515 - val_loss: 0.0729 - 5s/epoch - 8ms/step\n",
      "Epoch 143/10000\n",
      "559/559 - 5s - loss: 0.0590 - val_loss: 0.0613 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_46_layer_call_fn, gru_cell_46_layer_call_and_return_conditional_losses, gru_cell_47_layer_call_fn, gru_cell_47_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_30_1_50_1_50_datt_seq2seq_gru_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_30_1_50_1_50_datt_seq2seq_gru_4\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000264150ACA00> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002647925BD60> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.845978</td>\n",
       "      <td>0.880563</td>\n",
       "      <td>0.921975</td>\n",
       "      <td>0.882838</td>\n",
       "      <td>7.742055</td>\n",
       "      <td>6.076717</td>\n",
       "      <td>4.278665</td>\n",
       "      <td>6.032479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.778696</td>\n",
       "      <td>0.831525</td>\n",
       "      <td>0.844711</td>\n",
       "      <td>0.81831</td>\n",
       "      <td>9.282256</td>\n",
       "      <td>7.217664</td>\n",
       "      <td>6.039153</td>\n",
       "      <td>7.513024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.677618</td>\n",
       "      <td>0.792472</td>\n",
       "      <td>0.763131</td>\n",
       "      <td>0.744407</td>\n",
       "      <td>11.205632</td>\n",
       "      <td>8.011357</td>\n",
       "      <td>7.462145</td>\n",
       "      <td>8.893045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.579906</td>\n",
       "      <td>0.760989</td>\n",
       "      <td>0.698112</td>\n",
       "      <td>0.679669</td>\n",
       "      <td>12.793722</td>\n",
       "      <td>8.598886</td>\n",
       "      <td>8.427554</td>\n",
       "      <td>9.940054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.485913</td>\n",
       "      <td>0.730736</td>\n",
       "      <td>0.66776</td>\n",
       "      <td>0.628137</td>\n",
       "      <td>14.153305</td>\n",
       "      <td>9.128688</td>\n",
       "      <td>8.844089</td>\n",
       "      <td>10.708694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.395781</td>\n",
       "      <td>0.703991</td>\n",
       "      <td>0.660862</td>\n",
       "      <td>0.586878</td>\n",
       "      <td>15.343137</td>\n",
       "      <td>9.573725</td>\n",
       "      <td>8.937661</td>\n",
       "      <td>11.284841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.311935</td>\n",
       "      <td>0.681852</td>\n",
       "      <td>0.65833</td>\n",
       "      <td>0.550706</td>\n",
       "      <td>16.371673</td>\n",
       "      <td>9.927655</td>\n",
       "      <td>8.973072</td>\n",
       "      <td>11.757467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.235698</td>\n",
       "      <td>0.663585</td>\n",
       "      <td>0.651496</td>\n",
       "      <td>0.516926</td>\n",
       "      <td>17.253484</td>\n",
       "      <td>10.210959</td>\n",
       "      <td>9.064664</td>\n",
       "      <td>12.176369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.167745</td>\n",
       "      <td>0.64973</td>\n",
       "      <td>0.632201</td>\n",
       "      <td>0.483225</td>\n",
       "      <td>18.003049</td>\n",
       "      <td>10.421883</td>\n",
       "      <td>9.314325</td>\n",
       "      <td>12.579752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.10931</td>\n",
       "      <td>0.637306</td>\n",
       "      <td>0.595738</td>\n",
       "      <td>0.447451</td>\n",
       "      <td>18.624135</td>\n",
       "      <td>10.606844</td>\n",
       "      <td>9.767926</td>\n",
       "      <td>12.999635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.060592</td>\n",
       "      <td>0.623587</td>\n",
       "      <td>0.549077</td>\n",
       "      <td>0.411085</td>\n",
       "      <td>19.127752</td>\n",
       "      <td>10.80737</td>\n",
       "      <td>10.319256</td>\n",
       "      <td>13.418126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.020242</td>\n",
       "      <td>0.608454</td>\n",
       "      <td>0.511302</td>\n",
       "      <td>0.38</td>\n",
       "      <td>19.175381</td>\n",
       "      <td>11.024164</td>\n",
       "      <td>10.746406</td>\n",
       "      <td>13.64865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.015854</td>\n",
       "      <td>0.59201</td>\n",
       "      <td>0.492241</td>\n",
       "      <td>0.356132</td>\n",
       "      <td>18.667316</td>\n",
       "      <td>11.256029</td>\n",
       "      <td>10.958121</td>\n",
       "      <td>13.627155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.053278</td>\n",
       "      <td>0.575574</td>\n",
       "      <td>0.482746</td>\n",
       "      <td>0.335014</td>\n",
       "      <td>18.366463</td>\n",
       "      <td>11.483752</td>\n",
       "      <td>11.06424</td>\n",
       "      <td>13.638151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.09358</td>\n",
       "      <td>0.559539</td>\n",
       "      <td>0.472655</td>\n",
       "      <td>0.312871</td>\n",
       "      <td>18.408211</td>\n",
       "      <td>11.702875</td>\n",
       "      <td>11.175946</td>\n",
       "      <td>13.762344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.134955</td>\n",
       "      <td>0.544534</td>\n",
       "      <td>0.458317</td>\n",
       "      <td>0.289299</td>\n",
       "      <td>18.68472</td>\n",
       "      <td>11.904143</td>\n",
       "      <td>11.330614</td>\n",
       "      <td>13.973159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.174372</td>\n",
       "      <td>0.530726</td>\n",
       "      <td>0.441007</td>\n",
       "      <td>0.265787</td>\n",
       "      <td>18.630445</td>\n",
       "      <td>12.084252</td>\n",
       "      <td>11.514059</td>\n",
       "      <td>14.076252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.210032</td>\n",
       "      <td>0.516661</td>\n",
       "      <td>0.424809</td>\n",
       "      <td>0.243812</td>\n",
       "      <td>18.54785</td>\n",
       "      <td>12.265084</td>\n",
       "      <td>11.682995</td>\n",
       "      <td>14.16531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.241642</td>\n",
       "      <td>0.502254</td>\n",
       "      <td>0.412067</td>\n",
       "      <td>0.224226</td>\n",
       "      <td>18.510665</td>\n",
       "      <td>12.448289</td>\n",
       "      <td>11.814713</td>\n",
       "      <td>14.257889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.271673</td>\n",
       "      <td>0.487226</td>\n",
       "      <td>0.402188</td>\n",
       "      <td>0.205914</td>\n",
       "      <td>18.465048</td>\n",
       "      <td>12.635721</td>\n",
       "      <td>11.915976</td>\n",
       "      <td>14.338915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.301703</td>\n",
       "      <td>0.470796</td>\n",
       "      <td>0.392681</td>\n",
       "      <td>0.187258</td>\n",
       "      <td>18.49302</td>\n",
       "      <td>12.838422</td>\n",
       "      <td>12.01284</td>\n",
       "      <td>14.448094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.331689</td>\n",
       "      <td>0.453925</td>\n",
       "      <td>0.382686</td>\n",
       "      <td>0.168307</td>\n",
       "      <td>18.591011</td>\n",
       "      <td>13.042177</td>\n",
       "      <td>12.114062</td>\n",
       "      <td>14.582417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.362152</td>\n",
       "      <td>0.436191</td>\n",
       "      <td>0.371057</td>\n",
       "      <td>0.148365</td>\n",
       "      <td>18.549186</td>\n",
       "      <td>13.252105</td>\n",
       "      <td>12.230704</td>\n",
       "      <td>14.677332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.392392</td>\n",
       "      <td>0.419029</td>\n",
       "      <td>0.356719</td>\n",
       "      <td>0.127785</td>\n",
       "      <td>18.505769</td>\n",
       "      <td>13.453929</td>\n",
       "      <td>12.372543</td>\n",
       "      <td>14.777414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.423606</td>\n",
       "      <td>0.402904</td>\n",
       "      <td>0.341445</td>\n",
       "      <td>0.106914</td>\n",
       "      <td>18.528012</td>\n",
       "      <td>13.640818</td>\n",
       "      <td>12.521259</td>\n",
       "      <td>14.896696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.452897</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>0.328575</td>\n",
       "      <td>0.088193</td>\n",
       "      <td>18.535722</td>\n",
       "      <td>13.8007</td>\n",
       "      <td>12.646002</td>\n",
       "      <td>14.994141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.479859</td>\n",
       "      <td>0.377039</td>\n",
       "      <td>0.318774</td>\n",
       "      <td>0.071985</td>\n",
       "      <td>18.527592</td>\n",
       "      <td>13.932642</td>\n",
       "      <td>12.741357</td>\n",
       "      <td>15.067197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.504494</td>\n",
       "      <td>0.365357</td>\n",
       "      <td>0.310317</td>\n",
       "      <td>0.05706</td>\n",
       "      <td>18.636836</td>\n",
       "      <td>14.061258</td>\n",
       "      <td>12.824187</td>\n",
       "      <td>15.174094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.525479</td>\n",
       "      <td>0.353675</td>\n",
       "      <td>0.301467</td>\n",
       "      <td>0.043221</td>\n",
       "      <td>18.790844</td>\n",
       "      <td>14.189538</td>\n",
       "      <td>12.909819</td>\n",
       "      <td>15.296733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.54267</td>\n",
       "      <td>0.343037</td>\n",
       "      <td>0.292963</td>\n",
       "      <td>0.03111</td>\n",
       "      <td>18.923718</td>\n",
       "      <td>14.305778</td>\n",
       "      <td>12.9915</td>\n",
       "      <td>15.406999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.028097</td>\n",
       "      <td>0.562806</td>\n",
       "      <td>0.50458</td>\n",
       "      <td>0.34643</td>\n",
       "      <td>17.1146</td>\n",
       "      <td>11.463447</td>\n",
       "      <td>10.633195</td>\n",
       "      <td>13.070414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.845978  0.880563  0.921975  0.882838   7.742055   6.076717   \n",
       "1      0.778696  0.831525  0.844711   0.81831   9.282256   7.217664   \n",
       "2      0.677618  0.792472  0.763131  0.744407  11.205632   8.011357   \n",
       "3      0.579906  0.760989  0.698112  0.679669  12.793722   8.598886   \n",
       "4      0.485913  0.730736   0.66776  0.628137  14.153305   9.128688   \n",
       "5      0.395781  0.703991  0.660862  0.586878  15.343137   9.573725   \n",
       "6      0.311935  0.681852   0.65833  0.550706  16.371673   9.927655   \n",
       "7      0.235698  0.663585  0.651496  0.516926  17.253484  10.210959   \n",
       "8      0.167745   0.64973  0.632201  0.483225  18.003049  10.421883   \n",
       "9       0.10931  0.637306  0.595738  0.447451  18.624135  10.606844   \n",
       "10     0.060592  0.623587  0.549077  0.411085  19.127752   10.80737   \n",
       "11     0.020242  0.608454  0.511302      0.38  19.175381  11.024164   \n",
       "12    -0.015854   0.59201  0.492241  0.356132  18.667316  11.256029   \n",
       "13    -0.053278  0.575574  0.482746  0.335014  18.366463  11.483752   \n",
       "14     -0.09358  0.559539  0.472655  0.312871  18.408211  11.702875   \n",
       "15    -0.134955  0.544534  0.458317  0.289299   18.68472  11.904143   \n",
       "16    -0.174372  0.530726  0.441007  0.265787  18.630445  12.084252   \n",
       "17    -0.210032  0.516661  0.424809  0.243812   18.54785  12.265084   \n",
       "18    -0.241642  0.502254  0.412067  0.224226  18.510665  12.448289   \n",
       "19    -0.271673  0.487226  0.402188  0.205914  18.465048  12.635721   \n",
       "20    -0.301703  0.470796  0.392681  0.187258   18.49302  12.838422   \n",
       "21    -0.331689  0.453925  0.382686  0.168307  18.591011  13.042177   \n",
       "22    -0.362152  0.436191  0.371057  0.148365  18.549186  13.252105   \n",
       "23    -0.392392  0.419029  0.356719  0.127785  18.505769  13.453929   \n",
       "24    -0.423606  0.402904  0.341445  0.106914  18.528012  13.640818   \n",
       "25    -0.452897    0.3889  0.328575  0.088193  18.535722    13.8007   \n",
       "26    -0.479859  0.377039  0.318774  0.071985  18.527592  13.932642   \n",
       "27    -0.504494  0.365357  0.310317   0.05706  18.636836  14.061258   \n",
       "28    -0.525479  0.353675  0.301467  0.043221  18.790844  14.189538   \n",
       "29     -0.54267  0.343037  0.292963   0.03111  18.923718  14.305778   \n",
       "mean  -0.028097  0.562806   0.50458   0.34643    17.1146  11.463447   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       4.278665   6.032479  \n",
       "1       6.039153   7.513024  \n",
       "2       7.462145   8.893045  \n",
       "3       8.427554   9.940054  \n",
       "4       8.844089  10.708694  \n",
       "5       8.937661  11.284841  \n",
       "6       8.973072  11.757467  \n",
       "7       9.064664  12.176369  \n",
       "8       9.314325  12.579752  \n",
       "9       9.767926  12.999635  \n",
       "10     10.319256  13.418126  \n",
       "11     10.746406   13.64865  \n",
       "12     10.958121  13.627155  \n",
       "13      11.06424  13.638151  \n",
       "14     11.175946  13.762344  \n",
       "15     11.330614  13.973159  \n",
       "16     11.514059  14.076252  \n",
       "17     11.682995   14.16531  \n",
       "18     11.814713  14.257889  \n",
       "19     11.915976  14.338915  \n",
       "20      12.01284  14.448094  \n",
       "21     12.114062  14.582417  \n",
       "22     12.230704  14.677332  \n",
       "23     12.372543  14.777414  \n",
       "24     12.521259  14.896696  \n",
       "25     12.646002  14.994141  \n",
       "26     12.741357  15.067197  \n",
       "27     12.824187  15.174094  \n",
       "28     12.909819  15.296733  \n",
       "29       12.9915  15.406999  \n",
       "mean   10.633195  13.070414  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/40_30_1_50_1_50_datt_seq2seq_gru_4.csv\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "end opitmization\n"
     ]
    }
   ],
   "source": [
    "target_list = cts_list\n",
    "\n",
    "# history size and future size\n",
    "history_list = [10, 20, 30, 40]\n",
    "future_list = [10, 20, 30]\n",
    "step = 1\n",
    "\n",
    "# variable selection\n",
    "history_var = process_var\n",
    "future_var = output_var\n",
    "\n",
    "history_num = len(history_var)\n",
    "future_num = len(future_var)\n",
    "\n",
    "# supervised attention factor\n",
    "delta = 1\n",
    "att_type = 'linear'\n",
    "        \n",
    "# test data split        \n",
    "test_size = 0.2\n",
    "test_num = -1\n",
    "\n",
    "# model structure\n",
    "num_layers = 1\n",
    "num_neurons = 50\n",
    "dense_layers = 1\n",
    "dense_neurons = 50\n",
    "model_type = 'datt_seq2seq_gru'\n",
    "\n",
    "iteration_list = [3,4]\n",
    "for iteration in iteration_list:\n",
    "    for history_size in history_list:\n",
    "        for future_size in future_list:\n",
    "            print(f\"{iteration}th iteration\")\n",
    "            print(f\"history size: {history_size}\")\n",
    "            print(f\"future size: {future_size}\")\n",
    "            history_series = []\n",
    "            future_series = []\n",
    "\n",
    "            # data to series\n",
    "            for i in range(len(target_list)):\n",
    "                history, future = data2series(target_list[i], history_size, history_var, future_size, future_var,\n",
    "                                            step, start_idx=0, end_idx=None)\n",
    "                if not i:\n",
    "                    history_series = history\n",
    "                    future_series = future\n",
    "                else:\n",
    "                    history_series = np.concatenate([history_series, history], axis=0)\n",
    "                    future_series = np.concatenate([future_series, future], axis=0)\n",
    "            \n",
    "            factor = rnn.super_attention(delta, future_size, future_num, att_type)\n",
    "            # Dual-attention Seq2Seq model\n",
    "            DATT_seq2seq_GRU = rnn.RNN(history_series, history_var, future_series, future_var)\n",
    "            # TT split\n",
    "            DATT_seq2seq_GRU.train_test(test_size=test_size, test_num=test_num)\n",
    "            # TV split\n",
    "            valid_size = DATT_seq2seq_GRU.history_test.shape[0]/DATT_seq2seq_GRU.history_train.shape[0]\n",
    "            DATT_seq2seq_GRU.train_valid(valid_size=valid_size)\n",
    "            # scaling\n",
    "            DATT_seq2seq_GRU.scaling()\n",
    "            # modeling\n",
    "            DATT_seq2seq_GRU.build_model(num_layers=num_layers, num_neurons=num_neurons, dense_layers=dense_layers, dense_neurons=dense_neurons, model_type=model_type, factor=factor)\n",
    "            # training\n",
    "            model_num = iteration\n",
    "            model_name = f\"{history_size}_{future_size}_{num_layers}_{num_neurons}_{dense_layers}_{dense_neurons}_{model_type}_{model_num}\"\n",
    "            if not exists(\"./model\", model_name, 'model'):\n",
    "                DATT_seq2seq_GRU.train()\n",
    "                DATT_seq2seq_GRU.save_model(f\"./model/{model_name}\")\n",
    "            else:\n",
    "                DATT_seq2seq_GRU.model = loadfile(\"./model\", model_name, 'model')\n",
    "            # test\n",
    "            test_result = DATT_seq2seq_GRU.test()\n",
    "            if not exists('./result', model_name):\n",
    "                savefile(test_result, './result', model_name)\n",
    "            print(\"\\n\")\n",
    "    print('\\n\\n')\n",
    "print('end opitmization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
