{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"..\") \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.utility import loadfile, savefile, exists\n",
    "from src.dataprocessing import *\n",
    "from src import rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "input_var   = [\"FT-3061-2\", \"FT-3061-3\", \"FT-3061-4\", \"FT-3062-1\"]\n",
    "output_var  = [\"TT-3061-3\", \"TT-3061-5\", \"LT-3061-2\"]\n",
    "process_var = input_var + output_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_1.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_2.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_3.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_4.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_5.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_6.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_7.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_8.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_9.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_10.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_11.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_12.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_13.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_14.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_15.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_16.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_17.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_18.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_19.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_20.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_21.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_22.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_23.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_24.csv\n"
     ]
    }
   ],
   "source": [
    "min_len = 100\n",
    "continuous_path = './data/3_continuous'\n",
    "\n",
    "cts_list = []\n",
    "i = 1\n",
    "while exists(f\"{continuous_path}/cts_{min_len}/dataset {min_len}_{i}.csv\"):\n",
    "    cts_df = loadfile(continuous_path, f\"cts_{min_len}/dataset {min_len}_{i}\", 'csv')\n",
    "    cts_list.append(cts_df)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# future = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "end opitmization\n"
     ]
    }
   ],
   "source": [
    "target_list = cts_list\n",
    "\n",
    "# history size and future size\n",
    "history_size = 40\n",
    "future_size = 10\n",
    "step = 1\n",
    "                \n",
    "# variable selection\n",
    "history_var = process_var\n",
    "future_var = output_var\n",
    "\n",
    "history_num = len(history_var)\n",
    "future_num = len(future_var)\n",
    "\n",
    "# data to series\n",
    "history_series = []\n",
    "future_series = []\n",
    "for i in range(len(target_list)):\n",
    "    history, future = data2series(target_list[i], history_size, history_var, future_size, future_var,\n",
    "                                step, start_idx=0, end_idx=None)\n",
    "    if not i:\n",
    "        history_series = history\n",
    "        future_series = future\n",
    "    else:\n",
    "        history_series = np.concatenate([history_series, history], axis=0)\n",
    "        future_series = np.concatenate([future_series, future], axis=0)\n",
    "\n",
    "# supervised attention factor\n",
    "delta = 1.5\n",
    "att_type = 'exp'\n",
    "factor = rnn.super_attention(delta, future_size, future_num, att_type)\n",
    "        \n",
    "# test data split        \n",
    "test_size = 0.2\n",
    "test_num = -1\n",
    "\n",
    "# model structure\n",
    "num_layers_list = [1, 2]\n",
    "num_neurons_list = [20, 40, 60]\n",
    "dense_layers = 1\n",
    "dense_neurons = 40\n",
    "model_list = ['seq2seq_gru', 'att_seq2seq_gru', 'datt_seq2seq_gru']\n",
    "\n",
    "iteration_list = [x for x in range(1,7)]\n",
    "for iteration in iteration_list:\n",
    "    for num_layers in num_layers_list:\n",
    "        for num_neurons in num_neurons_list:\n",
    "            for model_type in model_list:\n",
    "                print(f\"{iteration}th iteration\")\n",
    "                print(f\"num_layers: {num_layers}\")\n",
    "                print(f\"num_neurons: {num_neurons}\")\n",
    "                print(f\"model_type: {model_type}\")\n",
    "                \n",
    "                # Dual-attention Seq2Seq model\n",
    "                RNN_model = rnn.RNN(history_series, history_var, future_series, future_var)\n",
    "                # TT split\n",
    "                RNN_model.train_test(test_size=test_size, test_num=test_num)\n",
    "                # TV split\n",
    "                valid_size = RNN_model.history_test.shape[0]/RNN_model.history_train.shape[0]\n",
    "                RNN_model.train_valid(valid_size=valid_size)\n",
    "                # scaling\n",
    "                RNN_model.scaling()\n",
    "                # modeling\n",
    "                RNN_model.build_model(num_layers=num_layers, num_neurons=num_neurons, dense_layers=dense_layers, dense_neurons=dense_neurons, model_type=model_type, factor=factor)\n",
    "                # training\n",
    "                model_num = iteration\n",
    "                model_name = f\"{history_size}_{future_size}_{num_layers}_{num_neurons}_{dense_layers}_{dense_neurons}_{model_type}_{att_type}_{delta}_{model_num}\"\n",
    "                if not exists(f\"./model/{model_name}.h5\"):\n",
    "                    RNN_model.train()\n",
    "                    RNN_model.save_model(f\"./model/{model_name}\", 'weights')\n",
    "                    \n",
    "                else:\n",
    "                    RNN_model.model.load_weights(f\"./model/{model_name}.h5\")\n",
    "                # test\n",
    "                test_result = RNN_model.test(False)\n",
    "                if not exists(f'./result/{model_name}.csv', ):\n",
    "                    savefile(test_result, './result', model_name)\n",
    "                print(\"\\n\")\n",
    "print('end opitmization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "3th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "4th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "5th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "6th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "end opitmization\n"
     ]
    }
   ],
   "source": [
    "target_list = cts_list\n",
    "\n",
    "# history size and future size\n",
    "history_size = 40\n",
    "future_size = 10\n",
    "step = 1\n",
    "                \n",
    "# variable selection\n",
    "history_var = process_var\n",
    "future_var = output_var\n",
    "\n",
    "history_num = len(history_var)\n",
    "future_num = len(future_var)\n",
    "\n",
    "# data to series\n",
    "history_series = []\n",
    "future_series = []\n",
    "for i in range(len(target_list)):\n",
    "    history, future = data2series(target_list[i], history_size, history_var, future_size, future_var,\n",
    "                                step, start_idx=0, end_idx=None)\n",
    "    if not i:\n",
    "        history_series = history\n",
    "        future_series = future\n",
    "    else:\n",
    "        history_series = np.concatenate([history_series, history], axis=0)\n",
    "        future_series = np.concatenate([future_series, future], axis=0)\n",
    "\n",
    "# supervised attention factor\n",
    "delta = 1.5\n",
    "att_type = 'exp'\n",
    "factor = rnn.super_attention(delta, future_size, future_num, att_type)\n",
    "        \n",
    "# test data split        \n",
    "test_size = 0.2\n",
    "test_num = -1\n",
    "\n",
    "# model structure\n",
    "num_layers = 1\n",
    "num_neurons = 20\n",
    "dense_layers_list = [1, 2]\n",
    "dense_neurons_list = [20, 40, 60]\n",
    "model_list = ['seq2seq_gru', 'att_seq2seq_gru', 'datt_seq2seq_gru']\n",
    "\n",
    "iteration_list = [x for x in range(1,7)]\n",
    "for iteration in iteration_list:\n",
    "    for dense_layers in dense_layers_list:\n",
    "        for dense_neurons in dense_neurons_list:\n",
    "            for model_type in model_list:\n",
    "                print(f\"{iteration}th iteration\")\n",
    "                print(f\"num_layers: {dense_layers}\")\n",
    "                print(f\"num_neurons: {dense_neurons}\")\n",
    "                print(f\"model_type: {model_type}\")\n",
    "                \n",
    "                # Dual-attention Seq2Seq model\n",
    "                RNN_model = rnn.RNN(history_series, history_var, future_series, future_var)\n",
    "                # TT split\n",
    "                RNN_model.train_test(test_size=test_size, test_num=test_num)\n",
    "                # TV split\n",
    "                valid_size = RNN_model.history_test.shape[0]/RNN_model.history_train.shape[0]\n",
    "                RNN_model.train_valid(valid_size=valid_size)\n",
    "                # scaling\n",
    "                RNN_model.scaling()\n",
    "                # modeling\n",
    "                RNN_model.build_model(num_layers=num_layers, num_neurons=num_neurons, dense_layers=dense_layers, dense_neurons=dense_neurons, model_type=model_type, factor=factor)\n",
    "                # training\n",
    "                model_num = iteration\n",
    "                model_name = f\"{history_size}_{future_size}_{num_layers}_{num_neurons}_{dense_layers}_{dense_neurons}_{model_type}_{att_type}_{delta}_{model_num}\"\n",
    "                if not exists(f\"./model/{model_name}.h5\"):\n",
    "                    RNN_model.train()\n",
    "                    RNN_model.save_model(f\"./model/{model_name}\", 'weights')\n",
    "                    \n",
    "                else:\n",
    "                    RNN_model.model.load_weights(f\"./model/{model_name}.h5\")\n",
    "                # test\n",
    "                test_result = RNN_model.test(False)\n",
    "                if not exists(f'./result/{model_name}.csv', ):\n",
    "                    savefile(test_result, './result', model_name)\n",
    "                print(\"\\n\")\n",
    "print('end opitmization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# future = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 80\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 80\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 80\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 100\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 100\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 100\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 150\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 150\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 150\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 200\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 200\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 200\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 250\n",
      "model_type: seq2seq_gru\n",
      "Epoch 1/10000\n",
      "563/563 - 8s - loss: 0.1312 - val_loss: 0.1305 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.1098 - val_loss: 0.1122 - 5s/epoch - 9ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.0985 - val_loss: 0.1061 - 5s/epoch - 9ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 5s - loss: 0.0921 - val_loss: 0.0966 - 5s/epoch - 9ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.0790 - val_loss: 0.0871 - 5s/epoch - 9ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.0700 - val_loss: 0.0740 - 5s/epoch - 9ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.0631 - val_loss: 0.0696 - 5s/epoch - 9ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.0587 - val_loss: 0.0669 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.0553 - val_loss: 0.0711 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.0518 - val_loss: 0.0576 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.0466 - val_loss: 0.0556 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.0424 - val_loss: 0.0539 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.0399 - val_loss: 0.0501 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.0392 - val_loss: 0.0498 - 5s/epoch - 9ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.0361 - val_loss: 0.0528 - 5s/epoch - 9ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.0346 - val_loss: 0.0404 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.0331 - val_loss: 0.0421 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.0317 - val_loss: 0.0474 - 5s/epoch - 9ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.0298 - val_loss: 0.0441 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.0292 - val_loss: 0.0355 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.0268 - val_loss: 0.0506 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.0262 - val_loss: 0.0616 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.0286 - val_loss: 0.0524 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.0255 - val_loss: 0.0334 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.0218 - val_loss: 0.0286 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.0209 - val_loss: 0.0254 - 5s/epoch - 9ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.0206 - val_loss: 0.0272 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0190 - val_loss: 0.0305 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 6s - loss: 0.0184 - val_loss: 0.0240 - 6s/epoch - 10ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 6s - loss: 0.0179 - val_loss: 0.0270 - 6s/epoch - 11ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 6s - loss: 0.0196 - val_loss: 0.0354 - 6s/epoch - 11ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 6s - loss: 0.0166 - val_loss: 0.0229 - 6s/epoch - 10ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 6s - loss: 0.0152 - val_loss: 0.0228 - 6s/epoch - 10ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.0170 - val_loss: 0.0216 - 5s/epoch - 9ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.0145 - val_loss: 0.0221 - 5s/epoch - 9ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0158 - val_loss: 0.0277 - 5s/epoch - 9ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0149 - val_loss: 0.0320 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0193 - val_loss: 0.0333 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.0159 - val_loss: 0.0253 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0143 - val_loss: 0.0211 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0136 - val_loss: 0.0225 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0121 - val_loss: 0.0197 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0122 - val_loss: 0.0191 - 5s/epoch - 9ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0119 - val_loss: 0.0219 - 5s/epoch - 9ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0122 - val_loss: 0.0217 - 5s/epoch - 9ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0116 - val_loss: 0.0200 - 5s/epoch - 9ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 6s - loss: 0.0119 - val_loss: 0.0215 - 6s/epoch - 11ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 11s - loss: 0.0127 - val_loss: 0.0212 - 11s/epoch - 19ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 13s - loss: 0.0164 - val_loss: 0.0219 - 13s/epoch - 23ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 13s - loss: 0.0114 - val_loss: 0.0179 - 13s/epoch - 23ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 11s - loss: 0.0117 - val_loss: 0.0159 - 11s/epoch - 20ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 11s - loss: 0.0098 - val_loss: 0.0145 - 11s/epoch - 20ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 11s - loss: 0.0119 - val_loss: 0.0253 - 11s/epoch - 20ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 10s - loss: 0.0126 - val_loss: 0.0178 - 10s/epoch - 18ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 16s - loss: 0.0099 - val_loss: 0.0152 - 16s/epoch - 28ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 17s - loss: 0.0120 - val_loss: 0.0165 - 17s/epoch - 29ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 13s - loss: 0.0091 - val_loss: 0.0155 - 13s/epoch - 24ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 16s - loss: 0.0099 - val_loss: 0.0181 - 16s/epoch - 28ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 12s - loss: 0.0099 - val_loss: 0.0150 - 12s/epoch - 21ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 12s - loss: 0.0086 - val_loss: 0.0161 - 12s/epoch - 21ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 9s - loss: 0.0099 - val_loss: 0.0163 - 9s/epoch - 16ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 9s - loss: 0.0113 - val_loss: 0.0234 - 9s/epoch - 16ms/step\n",
      "csv file is saved to: ./result/40_20_1_250_1_40_seq2seq_gru_linear_0.5_1.csv\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 250\n",
      "model_type: att_seq2seq_gru\n",
      "Epoch 1/10000\n",
      "563/563 - 13s - loss: 0.1289 - val_loss: 0.1227 - 13s/epoch - 23ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 8s - loss: 0.1071 - val_loss: 0.1118 - 8s/epoch - 15ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 8s - loss: 0.0959 - val_loss: 0.1013 - 8s/epoch - 14ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 11s - loss: 0.0857 - val_loss: 0.0971 - 11s/epoch - 20ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 11s - loss: 0.0787 - val_loss: 0.0905 - 11s/epoch - 19ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 9s - loss: 0.0690 - val_loss: 0.0748 - 9s/epoch - 16ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 10s - loss: 0.0631 - val_loss: 0.0737 - 10s/epoch - 18ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 9s - loss: 0.0582 - val_loss: 0.0692 - 9s/epoch - 16ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 9s - loss: 0.0530 - val_loss: 0.0586 - 9s/epoch - 16ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 11s - loss: 0.0501 - val_loss: 0.0613 - 11s/epoch - 19ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 9s - loss: 0.0458 - val_loss: 0.0584 - 9s/epoch - 15ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 7s - loss: 0.0432 - val_loss: 0.0598 - 7s/epoch - 12ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 8s - loss: 0.0416 - val_loss: 0.0543 - 8s/epoch - 14ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 9s - loss: 0.0382 - val_loss: 0.0582 - 9s/epoch - 15ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 8s - loss: 0.0365 - val_loss: 0.0434 - 8s/epoch - 15ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 8s - loss: 0.0347 - val_loss: 0.0463 - 8s/epoch - 15ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 8s - loss: 0.0335 - val_loss: 0.0491 - 8s/epoch - 15ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 9s - loss: 0.0335 - val_loss: 0.0385 - 9s/epoch - 17ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 8s - loss: 0.0283 - val_loss: 0.0395 - 8s/epoch - 15ms/step\n",
      "Epoch 20/10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Project\\dual_attention\\6_LayerNeuron.ipynb Cell 12'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/dual_attention/6_LayerNeuron.ipynb#ch0000011?line=66'>67</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mhistory_size\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mfuture_size\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mnum_layers\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mnum_neurons\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mdense_layers\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mdense_neurons\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00matt_type\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mdelta\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mmodel_num\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/dual_attention/6_LayerNeuron.ipynb#ch0000011?line=67'>68</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exists(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./model/\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m.h5\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Project/dual_attention/6_LayerNeuron.ipynb#ch0000011?line=68'>69</a>\u001b[0m     RNN_model\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/dual_attention/6_LayerNeuron.ipynb#ch0000011?line=69'>70</a>\u001b[0m     RNN_model\u001b[39m.\u001b[39msave_model(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./model/\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mweights\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/dual_attention/6_LayerNeuron.ipynb#ch0000011?line=71'>72</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\src\\rnn.py:184\u001b[0m, in \u001b[0;36mRNN.train\u001b[1;34m(self, epochs, verbose, batch_size, patience, monitor)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/src/rnn.py?line=179'>180</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory_train_sc, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture_train_wt, \n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/src/rnn.py?line=180'>181</a>\u001b[0m                                   epochs\u001b[39m=\u001b[39mepochs, callbacks\u001b[39m=\u001b[39m[early_stopping_cb], verbose\u001b[39m=\u001b[39mverbose, batch_size\u001b[39m=\u001b[39mbatch_size, \n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/src/rnn.py?line=181'>182</a>\u001b[0m                                   validation_data\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory_valid_sc, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture_valid_wt))\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/src/rnn.py?line=182'>183</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Project/dual_attention/src/rnn.py?line=183'>184</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistory_train_sc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfuture_train_sc, \n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/src/rnn.py?line=184'>185</a>\u001b[0m                                   epochs\u001b[39m=\u001b[39;49mepochs, callbacks\u001b[39m=\u001b[39;49m[early_stopping_cb], verbose\u001b[39m=\u001b[39;49mverbose, batch_size\u001b[39m=\u001b[39;49mbatch_size, \n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/src/rnn.py?line=185'>186</a>\u001b[0m                                   validation_data\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistory_valid_sc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfuture_valid_sc))\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/src/rnn.py?line=186'>187</a>\u001b[0m time_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/src/rnn.py?line=187'>188</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_time \u001b[39m=\u001b[39m time_start \u001b[39m-\u001b[39m time_end\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\venv\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1376'>1377</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1377'>1378</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1378'>1379</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1379'>1380</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1380'>1381</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1381'>1382</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1382'>1383</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1383'>1384</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1384'>1385</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1385'>1386</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=2952'>2953</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=2953'>2954</a>\u001b[0m   (graph_function,\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=2954'>2955</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=2955'>2956</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=2956'>2957</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1848'>1849</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1849'>1850</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1850'>1851</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1851'>1852</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1852'>1853</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1853'>1854</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1854'>1855</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m     args,\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m     possible_gradient_type,\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     executing_eagerly)\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=506'>507</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=507'>508</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=510'>511</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=511'>512</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "target_list = cts_list\n",
    "\n",
    "# history size and future size\n",
    "history_size = 40\n",
    "future_size = 20\n",
    "step = 1\n",
    "                \n",
    "# variable selection\n",
    "history_var = process_var\n",
    "future_var = output_var\n",
    "\n",
    "history_num = len(history_var)\n",
    "future_num = len(future_var)\n",
    "\n",
    "# data to series\n",
    "history_series = []\n",
    "future_series = []\n",
    "for i in range(len(target_list)):\n",
    "    history, future = data2series(target_list[i], history_size, history_var, future_size, future_var,\n",
    "                                step, start_idx=0, end_idx=None)\n",
    "    if not i:\n",
    "        history_series = history\n",
    "        future_series = future\n",
    "    else:\n",
    "        history_series = np.concatenate([history_series, history], axis=0)\n",
    "        future_series = np.concatenate([future_series, future], axis=0)\n",
    "\n",
    "# supervised attention factor\n",
    "delta = 0.5\n",
    "att_type = 'linear'\n",
    "factor = rnn.super_attention(delta, future_size, future_num, att_type)\n",
    "        \n",
    "# test data split        \n",
    "test_size = 0.2\n",
    "test_num = -1\n",
    "\n",
    "# model structure\n",
    "num_layers_list = [1]\n",
    "num_neurons_list = [20, 40, 60, 80, 100, 150, 200, 250, 300, 350, 400]\n",
    "dense_layers = 1\n",
    "dense_neurons = 40\n",
    "model_list = ['seq2seq_gru', 'att_seq2seq_gru', 'datt_seq2seq_gru']\n",
    "\n",
    "iteration_list = [x for x in range(1,7)]\n",
    "for iteration in iteration_list:\n",
    "    for num_layers in num_layers_list:\n",
    "        for num_neurons in num_neurons_list:\n",
    "            for model_type in model_list:\n",
    "                print(f\"{iteration}th iteration\")\n",
    "                print(f\"num_layers: {num_layers}\")\n",
    "                print(f\"num_neurons: {num_neurons}\")\n",
    "                print(f\"model_type: {model_type}\")\n",
    "                \n",
    "                # Dual-attention Seq2Seq model\n",
    "                RNN_model = rnn.RNN(history_series, history_var, future_series, future_var)\n",
    "                # TT split\n",
    "                RNN_model.train_test(test_size=test_size, test_num=test_num)\n",
    "                # TV split\n",
    "                valid_size = RNN_model.history_test.shape[0]/RNN_model.history_train.shape[0]\n",
    "                RNN_model.train_valid(valid_size=valid_size)\n",
    "                # scaling\n",
    "                RNN_model.scaling()\n",
    "                # modeling\n",
    "                RNN_model.build_model(num_layers=num_layers, num_neurons=num_neurons, dense_layers=dense_layers, dense_neurons=dense_neurons, model_type=model_type, factor=factor)\n",
    "                # training\n",
    "                model_num = iteration\n",
    "                model_name = f\"{history_size}_{future_size}_{num_layers}_{num_neurons}_{dense_layers}_{dense_neurons}_{model_type}_{att_type}_{delta}_{model_num}\"\n",
    "                if not exists(f\"./model/{model_name}.h5\"):\n",
    "                    RNN_model.train()\n",
    "                    RNN_model.save_model(f\"./model/{model_name}\", 'weights')\n",
    "                    \n",
    "                else:\n",
    "                    RNN_model.model.load_weights(f\"./model/{model_name}.h5\")\n",
    "                # test\n",
    "                test_result = RNN_model.test(False)\n",
    "                if not exists(f'./result/{model_name}.csv', ):\n",
    "                    savefile(test_result, './result', model_name)\n",
    "                print(\"\\n\")\n",
    "print('end opitmization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "Epoch 1/10000\n",
      "563/563 - 10s - loss: 0.1323 - val_loss: 0.1230 - 10s/epoch - 19ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 4s - loss: 0.1092 - val_loss: 0.1133 - 4s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 4s - loss: 0.1002 - val_loss: 0.1054 - 4s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 4s - loss: 0.0926 - val_loss: 0.0958 - 4s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 4s - loss: 0.0853 - val_loss: 0.0944 - 4s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 4s - loss: 0.0784 - val_loss: 0.0824 - 4s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 4s - loss: 0.0722 - val_loss: 0.0795 - 4s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 4s - loss: 0.0683 - val_loss: 0.0759 - 4s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 4s - loss: 0.0632 - val_loss: 0.0675 - 4s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 4s - loss: 0.0601 - val_loss: 0.0660 - 4s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.0546 - val_loss: 0.0633 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 4s - loss: 0.0524 - val_loss: 0.0585 - 4s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 4s - loss: 0.0485 - val_loss: 0.0588 - 4s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 4s - loss: 0.0459 - val_loss: 0.0571 - 4s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 4s - loss: 0.0442 - val_loss: 0.0530 - 4s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.0435 - val_loss: 0.0491 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 4s - loss: 0.0401 - val_loss: 0.0480 - 4s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 4s - loss: 0.0407 - val_loss: 0.0507 - 4s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 4s - loss: 0.0368 - val_loss: 0.0467 - 4s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.0358 - val_loss: 0.0450 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 4s - loss: 0.0354 - val_loss: 0.0476 - 4s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 4s - loss: 0.0347 - val_loss: 0.0410 - 4s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 4s - loss: 0.0350 - val_loss: 0.0458 - 4s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 4s - loss: 0.0335 - val_loss: 0.0431 - 4s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 4s - loss: 0.0298 - val_loss: 0.0393 - 4s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 4s - loss: 0.0312 - val_loss: 0.0362 - 4s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 4s - loss: 0.0284 - val_loss: 0.0368 - 4s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0274 - val_loss: 0.0357 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 7s - loss: 0.0296 - val_loss: 0.0373 - 7s/epoch - 13ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 6s - loss: 0.0261 - val_loss: 0.0327 - 6s/epoch - 11ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.0273 - val_loss: 0.0315 - 5s/epoch - 9ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.0247 - val_loss: 0.0315 - 5s/epoch - 9ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.0251 - val_loss: 0.0277 - 5s/epoch - 9ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.0235 - val_loss: 0.0340 - 5s/epoch - 10ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 4s - loss: 0.0266 - val_loss: 0.0328 - 4s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0234 - val_loss: 0.0279 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0223 - val_loss: 0.0292 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0223 - val_loss: 0.0395 - 5s/epoch - 9ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.0241 - val_loss: 0.0330 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0219 - val_loss: 0.0270 - 5s/epoch - 9ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0226 - val_loss: 0.0282 - 5s/epoch - 9ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0219 - val_loss: 0.0242 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0198 - val_loss: 0.0281 - 5s/epoch - 9ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0196 - val_loss: 0.0253 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0193 - val_loss: 0.0234 - 5s/epoch - 9ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0183 - val_loss: 0.0229 - 5s/epoch - 9ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0191 - val_loss: 0.0236 - 5s/epoch - 9ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0185 - val_loss: 0.0271 - 5s/epoch - 9ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 4s - loss: 0.0198 - val_loss: 0.0239 - 4s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 4s - loss: 0.0173 - val_loss: 0.0228 - 4s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 4s - loss: 0.0177 - val_loss: 0.0217 - 4s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 4s - loss: 0.0190 - val_loss: 0.0306 - 4s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0179 - val_loss: 0.0219 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0157 - val_loss: 0.0218 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 4s - loss: 0.0164 - val_loss: 0.0228 - 4s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 4s - loss: 0.0169 - val_loss: 0.0209 - 4s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0158 - val_loss: 0.0206 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0164 - val_loss: 0.0229 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0166 - val_loss: 0.0229 - 5s/epoch - 9ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0152 - val_loss: 0.0205 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0166 - val_loss: 0.0220 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0149 - val_loss: 0.0211 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 4s - loss: 0.0148 - val_loss: 0.0208 - 4s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 4s - loss: 0.0162 - val_loss: 0.0205 - 4s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0147 - val_loss: 0.0191 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0145 - val_loss: 0.0211 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0144 - val_loss: 0.0218 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 5s - loss: 0.0142 - val_loss: 0.0192 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 4s - loss: 0.0139 - val_loss: 0.0218 - 4s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 5s - loss: 0.0158 - val_loss: 0.0201 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 5s - loss: 0.0144 - val_loss: 0.0185 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 4s - loss: 0.0143 - val_loss: 0.0185 - 4s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 4s - loss: 0.0133 - val_loss: 0.0182 - 4s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 4s - loss: 0.0132 - val_loss: 0.0199 - 4s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 4s - loss: 0.0131 - val_loss: 0.0181 - 4s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 4s - loss: 0.0138 - val_loss: 0.0209 - 4s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 4s - loss: 0.0138 - val_loss: 0.0185 - 4s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 4s - loss: 0.0144 - val_loss: 0.0192 - 4s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 5s - loss: 0.0122 - val_loss: 0.0200 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 4s - loss: 0.0127 - val_loss: 0.0172 - 4s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 4s - loss: 0.0136 - val_loss: 0.0214 - 4s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 5s - loss: 0.0199 - val_loss: 0.0299 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 5s - loss: 0.0169 - val_loss: 0.0268 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "563/563 - 5s - loss: 0.0166 - val_loss: 0.0222 - 5s/epoch - 9ms/step\n",
      "Epoch 85/10000\n",
      "563/563 - 5s - loss: 0.0133 - val_loss: 0.0203 - 5s/epoch - 9ms/step\n",
      "Epoch 86/10000\n",
      "563/563 - 5s - loss: 0.0125 - val_loss: 0.0222 - 5s/epoch - 9ms/step\n",
      "Epoch 87/10000\n",
      "563/563 - 5s - loss: 0.0120 - val_loss: 0.0188 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "563/563 - 4s - loss: 0.0199 - val_loss: 0.0243 - 4s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "563/563 - 4s - loss: 0.0136 - val_loss: 0.0213 - 4s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "563/563 - 4s - loss: 0.0130 - val_loss: 0.0222 - 4s/epoch - 8ms/step\n",
      "csv file is saved to: ./result/40_20_1_100_1_20_seq2seq_gru_linear_0.5_1.csv\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "Epoch 1/10000\n",
      "563/563 - 8s - loss: 0.1343 - val_loss: 0.1249 - 8s/epoch - 13ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.1068 - val_loss: 0.1118 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.0971 - val_loss: 0.1056 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 5s - loss: 0.0888 - val_loss: 0.0986 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.0823 - val_loss: 0.0885 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.0757 - val_loss: 0.0840 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.0712 - val_loss: 0.0791 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.0661 - val_loss: 0.0736 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.0626 - val_loss: 0.0780 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.0598 - val_loss: 0.0673 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.0541 - val_loss: 0.0647 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.0517 - val_loss: 0.0697 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.0511 - val_loss: 0.0577 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.0478 - val_loss: 0.0585 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.0450 - val_loss: 0.0549 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.0435 - val_loss: 0.0546 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.0419 - val_loss: 0.0526 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.0424 - val_loss: 0.0504 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.0381 - val_loss: 0.0493 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.0368 - val_loss: 0.0497 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.0362 - val_loss: 0.0499 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.0360 - val_loss: 0.0516 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.0337 - val_loss: 0.0425 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.0332 - val_loss: 0.0413 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.0309 - val_loss: 0.0497 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.0320 - val_loss: 0.0430 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.0305 - val_loss: 0.0371 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0314 - val_loss: 0.0381 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.0283 - val_loss: 0.0348 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.0291 - val_loss: 0.0337 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.0271 - val_loss: 0.0387 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.0303 - val_loss: 0.0433 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.0277 - val_loss: 0.0377 - 5s/epoch - 9ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.0289 - val_loss: 0.0378 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.0253 - val_loss: 0.0359 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0252 - val_loss: 0.0347 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0250 - val_loss: 0.0360 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0272 - val_loss: 0.0329 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 4s - loss: 0.0227 - val_loss: 0.0293 - 4s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0229 - val_loss: 0.0304 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 4s - loss: 0.0249 - val_loss: 0.0305 - 4s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 4s - loss: 0.0223 - val_loss: 0.0297 - 4s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 4s - loss: 0.0230 - val_loss: 0.0294 - 4s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0215 - val_loss: 0.0285 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0220 - val_loss: 0.0307 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0209 - val_loss: 0.0328 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0224 - val_loss: 0.0274 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0205 - val_loss: 0.0272 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 4s - loss: 0.0191 - val_loss: 0.0307 - 4s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0193 - val_loss: 0.0273 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 4s - loss: 0.0216 - val_loss: 0.0303 - 4s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0209 - val_loss: 0.0244 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0182 - val_loss: 0.0242 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 4s - loss: 0.0179 - val_loss: 0.0261 - 4s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0178 - val_loss: 0.0256 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0177 - val_loss: 0.0260 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0174 - val_loss: 0.0251 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0172 - val_loss: 0.0234 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0168 - val_loss: 0.0261 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0176 - val_loss: 0.0261 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0160 - val_loss: 0.0239 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0177 - val_loss: 0.0234 - 5s/epoch - 9ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 5s - loss: 0.0161 - val_loss: 0.0240 - 5s/epoch - 9ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0176 - val_loss: 0.0264 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0178 - val_loss: 0.0244 - 5s/epoch - 9ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0156 - val_loss: 0.0217 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0150 - val_loss: 0.0318 - 5s/epoch - 9ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 5s - loss: 0.0160 - val_loss: 0.0260 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 5s - loss: 0.0153 - val_loss: 0.0212 - 5s/epoch - 9ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 6s - loss: 0.0147 - val_loss: 0.0218 - 6s/epoch - 10ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 5s - loss: 0.0141 - val_loss: 0.0227 - 5s/epoch - 9ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 5s - loss: 0.0159 - val_loss: 0.0210 - 5s/epoch - 9ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 5s - loss: 0.0182 - val_loss: 0.0302 - 5s/epoch - 9ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 5s - loss: 0.0168 - val_loss: 0.0279 - 5s/epoch - 9ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 6s - loss: 0.0166 - val_loss: 0.0234 - 6s/epoch - 10ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 5s - loss: 0.0137 - val_loss: 0.0271 - 5s/epoch - 10ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 5s - loss: 0.0143 - val_loss: 0.0227 - 5s/epoch - 9ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 5s - loss: 0.0135 - val_loss: 0.0221 - 5s/epoch - 9ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 5s - loss: 0.0134 - val_loss: 0.0234 - 5s/epoch - 9ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 5s - loss: 0.0152 - val_loss: 0.0216 - 5s/epoch - 9ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 5s - loss: 0.0133 - val_loss: 0.0228 - 5s/epoch - 9ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 6s - loss: 0.0138 - val_loss: 0.0212 - 6s/epoch - 11ms/step\n",
      "csv file is saved to: ./result/40_20_1_100_1_20_att_seq2seq_gru_linear_0.5_1.csv\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "Epoch 1/10000\n",
      "563/563 - 8s - loss: 0.1902 - val_loss: 0.1761 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.1538 - val_loss: 0.1643 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.1411 - val_loss: 0.1546 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 5s - loss: 0.1304 - val_loss: 0.1369 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.1204 - val_loss: 0.1334 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.1153 - val_loss: 0.1267 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.1065 - val_loss: 0.1213 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.0971 - val_loss: 0.1080 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.0902 - val_loss: 0.1054 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.0849 - val_loss: 0.0984 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.0810 - val_loss: 0.0917 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.0770 - val_loss: 0.0920 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.0730 - val_loss: 0.0867 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.0720 - val_loss: 0.0843 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.0662 - val_loss: 0.0795 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.0638 - val_loss: 0.0785 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.0649 - val_loss: 0.0743 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.0606 - val_loss: 0.0740 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.0578 - val_loss: 0.0714 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.0559 - val_loss: 0.0698 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.0533 - val_loss: 0.0710 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.0530 - val_loss: 0.0684 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.0492 - val_loss: 0.0732 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.0507 - val_loss: 0.0613 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.0481 - val_loss: 0.0623 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.0481 - val_loss: 0.0656 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.0462 - val_loss: 0.0616 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0453 - val_loss: 0.0656 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.0438 - val_loss: 0.0602 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.0413 - val_loss: 0.0566 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.0426 - val_loss: 0.0560 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.0385 - val_loss: 0.0545 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.0414 - val_loss: 0.0561 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.0365 - val_loss: 0.0573 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.0342 - val_loss: 0.0553 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0355 - val_loss: 0.0509 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0377 - val_loss: 0.0539 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0379 - val_loss: 0.0569 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.0429 - val_loss: 0.0594 - 5s/epoch - 9ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0373 - val_loss: 0.0521 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0360 - val_loss: 0.0515 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0348 - val_loss: 0.0633 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0376 - val_loss: 0.0502 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0321 - val_loss: 0.0451 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0297 - val_loss: 0.0447 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0297 - val_loss: 0.0454 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0294 - val_loss: 0.0444 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0314 - val_loss: 0.0460 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0299 - val_loss: 0.0440 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0287 - val_loss: 0.0697 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0325 - val_loss: 0.0448 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0265 - val_loss: 0.0399 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0259 - val_loss: 0.0403 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0268 - val_loss: 0.0410 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0260 - val_loss: 0.0383 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0310 - val_loss: 0.0467 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0268 - val_loss: 0.0420 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0262 - val_loss: 0.0562 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0282 - val_loss: 0.0475 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0263 - val_loss: 0.0415 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0362 - val_loss: 0.0520 - 5s/epoch - 9ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0290 - val_loss: 0.0401 - 5s/epoch - 9ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 5s - loss: 0.0257 - val_loss: 0.0418 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0241 - val_loss: 0.0436 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0271 - val_loss: 0.0488 - 5s/epoch - 9ms/step\n",
      "csv file is saved to: ./result/40_20_1_100_1_20_datt_seq2seq_gru_linear_0.5_1.csv\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "Epoch 1/10000\n",
      "563/563 - 7s - loss: 0.1358 - val_loss: 0.1261 - 7s/epoch - 13ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 4s - loss: 0.1094 - val_loss: 0.1174 - 4s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 4s - loss: 0.1005 - val_loss: 0.1142 - 4s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 4s - loss: 0.0925 - val_loss: 0.1019 - 4s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.0846 - val_loss: 0.0903 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.0780 - val_loss: 0.0849 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 4s - loss: 0.0740 - val_loss: 0.0799 - 4s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 4s - loss: 0.0675 - val_loss: 0.0764 - 4s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 4s - loss: 0.0645 - val_loss: 0.0733 - 4s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 4s - loss: 0.0602 - val_loss: 0.0719 - 4s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 4s - loss: 0.0555 - val_loss: 0.0643 - 4s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 4s - loss: 0.0521 - val_loss: 0.0630 - 4s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 4s - loss: 0.0499 - val_loss: 0.0579 - 4s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 4s - loss: 0.0472 - val_loss: 0.0595 - 4s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 4s - loss: 0.0440 - val_loss: 0.0566 - 4s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 4s - loss: 0.0428 - val_loss: 0.0552 - 4s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 4s - loss: 0.0406 - val_loss: 0.0493 - 4s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 4s - loss: 0.0397 - val_loss: 0.0475 - 4s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 4s - loss: 0.0379 - val_loss: 0.0446 - 4s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 4s - loss: 0.0360 - val_loss: 0.0519 - 4s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 4s - loss: 0.0350 - val_loss: 0.0428 - 4s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 4s - loss: 0.0327 - val_loss: 0.0382 - 4s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 4s - loss: 0.0319 - val_loss: 0.0386 - 4s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 4s - loss: 0.0380 - val_loss: 0.0428 - 4s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 4s - loss: 0.0315 - val_loss: 0.0419 - 4s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 4s - loss: 0.0297 - val_loss: 0.0418 - 4s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 4s - loss: 0.0294 - val_loss: 0.0466 - 4s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 4s - loss: 0.0282 - val_loss: 0.0345 - 4s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 4s - loss: 0.0273 - val_loss: 0.0399 - 4s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 4s - loss: 0.0268 - val_loss: 0.0389 - 4s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 4s - loss: 0.0285 - val_loss: 0.0333 - 4s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 4s - loss: 0.0256 - val_loss: 0.0306 - 4s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 4s - loss: 0.0241 - val_loss: 0.0298 - 4s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 4s - loss: 0.0234 - val_loss: 0.0348 - 4s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 4s - loss: 0.0243 - val_loss: 0.0302 - 4s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 4s - loss: 0.0224 - val_loss: 0.0533 - 4s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 4s - loss: 0.0280 - val_loss: 0.0278 - 4s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0206 - val_loss: 0.0273 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 4s - loss: 0.0208 - val_loss: 0.0297 - 4s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 4s - loss: 0.0208 - val_loss: 0.0282 - 4s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 4s - loss: 0.0202 - val_loss: 0.0361 - 4s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 4s - loss: 0.0212 - val_loss: 0.0275 - 4s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 4s - loss: 0.0197 - val_loss: 0.0281 - 4s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 4s - loss: 0.0204 - val_loss: 0.0296 - 4s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 4s - loss: 0.0244 - val_loss: 0.0348 - 4s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0217 - val_loss: 0.0301 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 4s - loss: 0.0208 - val_loss: 0.0257 - 4s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 4s - loss: 0.0212 - val_loss: 0.0336 - 4s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0200 - val_loss: 0.0261 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0190 - val_loss: 0.0253 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 4s - loss: 0.0192 - val_loss: 0.0269 - 4s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 4s - loss: 0.0177 - val_loss: 0.0270 - 4s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 4s - loss: 0.0171 - val_loss: 0.0242 - 4s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 4s - loss: 0.0174 - val_loss: 0.0258 - 4s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 4s - loss: 0.0168 - val_loss: 0.0265 - 4s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 4s - loss: 0.0170 - val_loss: 0.0244 - 4s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 4s - loss: 0.0158 - val_loss: 0.0249 - 4s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 4s - loss: 0.0174 - val_loss: 0.0258 - 4s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 4s - loss: 0.0167 - val_loss: 0.0258 - 4s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 4s - loss: 0.0165 - val_loss: 0.0239 - 4s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 4s - loss: 0.0154 - val_loss: 0.0258 - 4s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 4s - loss: 0.0156 - val_loss: 0.0233 - 4s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 4s - loss: 0.0151 - val_loss: 0.0249 - 4s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 4s - loss: 0.0169 - val_loss: 0.0265 - 4s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 4s - loss: 0.0146 - val_loss: 0.0223 - 4s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 4s - loss: 0.0146 - val_loss: 0.0241 - 4s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 4s - loss: 0.0160 - val_loss: 0.0249 - 4s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 4s - loss: 0.0149 - val_loss: 0.0215 - 4s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 4s - loss: 0.0137 - val_loss: 0.0237 - 4s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 4s - loss: 0.0160 - val_loss: 0.0269 - 4s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 4s - loss: 0.0156 - val_loss: 0.0223 - 4s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 4s - loss: 0.0134 - val_loss: 0.0216 - 4s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 4s - loss: 0.0138 - val_loss: 0.0256 - 4s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 4s - loss: 0.0141 - val_loss: 0.0206 - 4s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 4s - loss: 0.0127 - val_loss: 0.0214 - 4s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 4s - loss: 0.0136 - val_loss: 0.0221 - 4s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 4s - loss: 0.0135 - val_loss: 0.0206 - 4s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 4s - loss: 0.0134 - val_loss: 0.0217 - 4s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 4s - loss: 0.0136 - val_loss: 0.0269 - 4s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 4s - loss: 0.0194 - val_loss: 0.0208 - 4s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 4s - loss: 0.0122 - val_loss: 0.0200 - 4s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 4s - loss: 0.0119 - val_loss: 0.0198 - 4s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 4s - loss: 0.0124 - val_loss: 0.0200 - 4s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "563/563 - 5s - loss: 0.0129 - val_loss: 0.0205 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "563/563 - 4s - loss: 0.0223 - val_loss: 0.0268 - 4s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "563/563 - 4s - loss: 0.0164 - val_loss: 0.0245 - 4s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "563/563 - 5s - loss: 0.0161 - val_loss: 0.0254 - 5s/epoch - 9ms/step\n",
      "Epoch 88/10000\n",
      "563/563 - 5s - loss: 0.0250 - val_loss: 0.0277 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "563/563 - 5s - loss: 0.0155 - val_loss: 0.0241 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "563/563 - 4s - loss: 0.0134 - val_loss: 0.0232 - 4s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "563/563 - 4s - loss: 0.0133 - val_loss: 0.0207 - 4s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "563/563 - 5s - loss: 0.0123 - val_loss: 0.0212 - 5s/epoch - 8ms/step\n",
      "csv file is saved to: ./result/40_20_1_100_1_60_seq2seq_gru_linear_0.5_1.csv\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "Epoch 1/10000\n",
      "563/563 - 8s - loss: 0.1342 - val_loss: 0.1252 - 8s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.1069 - val_loss: 0.1154 - 5s/epoch - 9ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.0977 - val_loss: 0.1034 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 5s - loss: 0.0896 - val_loss: 0.0957 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.0846 - val_loss: 0.0921 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.0774 - val_loss: 0.0843 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.0707 - val_loss: 0.0831 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.0667 - val_loss: 0.0760 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.0641 - val_loss: 0.0704 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.0594 - val_loss: 0.0667 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.0550 - val_loss: 0.0653 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.0529 - val_loss: 0.0651 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.0487 - val_loss: 0.0597 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.0487 - val_loss: 0.0568 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.0453 - val_loss: 0.0540 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.0427 - val_loss: 0.0535 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.0421 - val_loss: 0.0514 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.0395 - val_loss: 0.0498 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.0375 - val_loss: 0.0493 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.0370 - val_loss: 0.0447 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.0352 - val_loss: 0.0460 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.0382 - val_loss: 0.0430 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.0330 - val_loss: 0.0440 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.0318 - val_loss: 0.0387 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.0304 - val_loss: 0.0418 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.0315 - val_loss: 0.0395 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.0305 - val_loss: 0.0410 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0295 - val_loss: 0.0359 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.0339 - val_loss: 0.0588 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.0356 - val_loss: 0.0446 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.0289 - val_loss: 0.0351 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.0268 - val_loss: 0.0401 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.0259 - val_loss: 0.0305 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.0251 - val_loss: 0.0329 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.0253 - val_loss: 0.0319 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0243 - val_loss: 0.0330 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0251 - val_loss: 0.0323 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0235 - val_loss: 0.0304 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.0232 - val_loss: 0.0298 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0230 - val_loss: 0.0306 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0219 - val_loss: 0.0280 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0208 - val_loss: 0.0303 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0213 - val_loss: 0.0307 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0202 - val_loss: 0.0323 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0213 - val_loss: 0.0289 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0205 - val_loss: 0.0247 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0188 - val_loss: 0.0268 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0206 - val_loss: 0.0281 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0195 - val_loss: 0.0237 - 5s/epoch - 9ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0178 - val_loss: 0.0254 - 5s/epoch - 9ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0182 - val_loss: 0.0260 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0185 - val_loss: 0.0256 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0177 - val_loss: 0.0261 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0180 - val_loss: 0.0258 - 5s/epoch - 9ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0182 - val_loss: 0.0280 - 5s/epoch - 9ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0168 - val_loss: 0.0222 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0170 - val_loss: 0.0305 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0182 - val_loss: 0.0228 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0187 - val_loss: 0.0249 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0162 - val_loss: 0.0208 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0161 - val_loss: 0.0249 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0155 - val_loss: 0.0212 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 5s - loss: 0.0158 - val_loss: 0.0243 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0172 - val_loss: 0.0241 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0154 - val_loss: 0.0215 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0164 - val_loss: 0.0200 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0165 - val_loss: 0.0202 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 5s - loss: 0.0167 - val_loss: 0.0197 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 5s - loss: 0.0141 - val_loss: 0.0219 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 5s - loss: 0.0145 - val_loss: 0.0186 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 5s - loss: 0.0140 - val_loss: 0.0199 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 5s - loss: 0.0142 - val_loss: 0.0189 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 5s - loss: 0.0153 - val_loss: 0.0172 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 5s - loss: 0.0142 - val_loss: 0.0254 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 5s - loss: 0.0144 - val_loss: 0.0191 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 5s - loss: 0.0132 - val_loss: 0.0178 - 5s/epoch - 9ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 5s - loss: 0.0139 - val_loss: 0.0193 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 5s - loss: 0.0140 - val_loss: 0.0184 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 5s - loss: 0.0133 - val_loss: 0.0207 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 5s - loss: 0.0137 - val_loss: 0.0184 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 5s - loss: 0.0130 - val_loss: 0.0175 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 5s - loss: 0.0132 - val_loss: 0.0179 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 5s - loss: 0.0131 - val_loss: 0.0202 - 5s/epoch - 8ms/step\n",
      "csv file is saved to: ./result/40_20_1_100_1_60_att_seq2seq_gru_linear_0.5_1.csv\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "Epoch 1/10000\n",
      "563/563 - 8s - loss: 0.1919 - val_loss: 0.1770 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.1530 - val_loss: 0.1605 - 5s/epoch - 9ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.1410 - val_loss: 0.1528 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 5s - loss: 0.1316 - val_loss: 0.1402 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.1208 - val_loss: 0.1355 - 5s/epoch - 9ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.1152 - val_loss: 0.1332 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.1066 - val_loss: 0.1152 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.0975 - val_loss: 0.1104 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.0929 - val_loss: 0.1141 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.0865 - val_loss: 0.1031 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.0805 - val_loss: 0.0918 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.0748 - val_loss: 0.0873 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.0731 - val_loss: 0.0854 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.0696 - val_loss: 0.0834 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.0663 - val_loss: 0.0804 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.0635 - val_loss: 0.0773 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.0600 - val_loss: 0.0761 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.0598 - val_loss: 0.0716 - 5s/epoch - 9ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.0557 - val_loss: 0.0717 - 5s/epoch - 9ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.0578 - val_loss: 0.0772 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.0538 - val_loss: 0.0675 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.0509 - val_loss: 0.0659 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.0526 - val_loss: 0.0656 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.0474 - val_loss: 0.0614 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.0470 - val_loss: 0.0609 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.0468 - val_loss: 0.0618 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.0461 - val_loss: 0.0605 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0438 - val_loss: 0.0518 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.0412 - val_loss: 0.0565 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.0406 - val_loss: 0.0526 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.0432 - val_loss: 0.0605 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.0386 - val_loss: 0.0526 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.0371 - val_loss: 0.0696 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.0391 - val_loss: 0.0525 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.0374 - val_loss: 0.0496 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0352 - val_loss: 0.0492 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0361 - val_loss: 0.0497 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0333 - val_loss: 0.0465 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.0340 - val_loss: 0.0450 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0339 - val_loss: 0.0552 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0367 - val_loss: 0.0556 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0334 - val_loss: 0.0513 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0318 - val_loss: 0.0453 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0301 - val_loss: 0.0499 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0307 - val_loss: 0.0421 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0299 - val_loss: 0.0427 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0292 - val_loss: 0.0416 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0283 - val_loss: 0.0433 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0319 - val_loss: 0.0467 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0285 - val_loss: 0.0434 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0267 - val_loss: 0.0377 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0266 - val_loss: 0.0405 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0275 - val_loss: 0.0398 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0260 - val_loss: 0.0388 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0257 - val_loss: 0.0378 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0256 - val_loss: 0.0415 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0281 - val_loss: 0.0556 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0315 - val_loss: 0.0410 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0274 - val_loss: 0.0350 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0264 - val_loss: 0.0638 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0289 - val_loss: 0.0390 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0245 - val_loss: 0.0351 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 5s - loss: 0.0228 - val_loss: 0.0383 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0228 - val_loss: 0.0386 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0235 - val_loss: 0.0337 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0223 - val_loss: 0.0350 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0232 - val_loss: 0.0357 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 5s - loss: 0.0224 - val_loss: 0.0355 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 5s - loss: 0.0248 - val_loss: 0.0343 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 5s - loss: 0.0211 - val_loss: 0.0302 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 5s - loss: 0.0238 - val_loss: 0.0340 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 5s - loss: 0.0229 - val_loss: 0.0333 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 5s - loss: 0.0207 - val_loss: 0.0352 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 5s - loss: 0.0216 - val_loss: 0.0385 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 5s - loss: 0.0217 - val_loss: 0.0333 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 5s - loss: 0.0210 - val_loss: 0.0497 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 5s - loss: 0.0242 - val_loss: 0.0391 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 5s - loss: 0.0240 - val_loss: 0.0277 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 5s - loss: 0.0212 - val_loss: 0.0317 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 5s - loss: 0.0207 - val_loss: 0.0348 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 5s - loss: 0.0200 - val_loss: 0.0321 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 5s - loss: 0.0193 - val_loss: 0.0336 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 5s - loss: 0.0205 - val_loss: 0.0313 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "563/563 - 5s - loss: 0.0196 - val_loss: 0.0313 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "563/563 - 5s - loss: 0.0192 - val_loss: 0.0329 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "563/563 - 5s - loss: 0.0192 - val_loss: 0.0304 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "563/563 - 5s - loss: 0.0217 - val_loss: 0.0353 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "563/563 - 5s - loss: 0.0240 - val_loss: 0.0353 - 5s/epoch - 8ms/step\n",
      "csv file is saved to: ./result/40_20_1_100_1_60_datt_seq2seq_gru_linear_0.5_1.csv\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "Epoch 1/10000\n",
      "563/563 - 8s - loss: 0.1344 - val_loss: 0.1282 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.1106 - val_loss: 0.1160 - 5s/epoch - 9ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.1008 - val_loss: 0.1078 - 5s/epoch - 9ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 5s - loss: 0.0922 - val_loss: 0.0974 - 5s/epoch - 9ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.0860 - val_loss: 0.0894 - 5s/epoch - 9ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.0796 - val_loss: 0.0871 - 5s/epoch - 9ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.0742 - val_loss: 0.0808 - 5s/epoch - 9ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.0688 - val_loss: 0.0781 - 5s/epoch - 9ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.0628 - val_loss: 0.0744 - 5s/epoch - 9ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.0598 - val_loss: 0.0692 - 5s/epoch - 9ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.0552 - val_loss: 0.0655 - 5s/epoch - 9ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.0512 - val_loss: 0.0642 - 5s/epoch - 9ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.0497 - val_loss: 0.0610 - 5s/epoch - 9ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.0498 - val_loss: 0.0579 - 5s/epoch - 9ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.0451 - val_loss: 0.0534 - 5s/epoch - 9ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.0429 - val_loss: 0.0533 - 5s/epoch - 9ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.0412 - val_loss: 0.0517 - 5s/epoch - 9ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.0398 - val_loss: 0.0524 - 5s/epoch - 9ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.0376 - val_loss: 0.0520 - 5s/epoch - 9ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.0384 - val_loss: 0.0480 - 5s/epoch - 9ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.0370 - val_loss: 0.0496 - 5s/epoch - 9ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.0359 - val_loss: 0.0426 - 5s/epoch - 9ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.0327 - val_loss: 0.0443 - 5s/epoch - 9ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.0341 - val_loss: 0.0419 - 5s/epoch - 9ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.0305 - val_loss: 0.0447 - 5s/epoch - 9ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.0302 - val_loss: 0.0398 - 5s/epoch - 9ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.0316 - val_loss: 0.0381 - 5s/epoch - 9ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0278 - val_loss: 0.0389 - 5s/epoch - 9ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.0314 - val_loss: 0.0486 - 5s/epoch - 9ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.0284 - val_loss: 0.0362 - 5s/epoch - 9ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.0279 - val_loss: 0.0493 - 5s/epoch - 9ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.0346 - val_loss: 0.0369 - 5s/epoch - 9ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.0259 - val_loss: 0.0331 - 5s/epoch - 9ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.0252 - val_loss: 0.0362 - 5s/epoch - 9ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.0246 - val_loss: 0.0318 - 5s/epoch - 9ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0223 - val_loss: 0.0314 - 5s/epoch - 9ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0233 - val_loss: 0.0298 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0247 - val_loss: 0.0354 - 5s/epoch - 9ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.0243 - val_loss: 0.0339 - 5s/epoch - 9ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0252 - val_loss: 0.0300 - 5s/epoch - 9ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0204 - val_loss: 0.0281 - 5s/epoch - 9ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0248 - val_loss: 0.0374 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0250 - val_loss: 0.0326 - 5s/epoch - 9ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0201 - val_loss: 0.0267 - 5s/epoch - 9ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0193 - val_loss: 0.0271 - 5s/epoch - 9ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0195 - val_loss: 0.0233 - 5s/epoch - 9ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0194 - val_loss: 0.0255 - 5s/epoch - 9ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0218 - val_loss: 0.0286 - 5s/epoch - 9ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0184 - val_loss: 0.0279 - 5s/epoch - 9ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0221 - val_loss: 0.0321 - 5s/epoch - 9ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0262 - val_loss: 0.0335 - 5s/epoch - 9ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0202 - val_loss: 0.0307 - 5s/epoch - 9ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0182 - val_loss: 0.0259 - 5s/epoch - 9ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0171 - val_loss: 0.0244 - 5s/epoch - 9ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0174 - val_loss: 0.0261 - 5s/epoch - 9ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0186 - val_loss: 0.0248 - 5s/epoch - 9ms/step\n",
      "csv file is saved to: ./result/40_20_1_100_2_20_seq2seq_gru_linear_0.5_1.csv\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: att_seq2seq_gru\n",
      "Epoch 1/10000\n",
      "563/563 - 8s - loss: 0.1335 - val_loss: 0.1239 - 8s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.1095 - val_loss: 0.1115 - 5s/epoch - 9ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.1006 - val_loss: 0.1076 - 5s/epoch - 9ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 5s - loss: 0.0947 - val_loss: 0.1056 - 5s/epoch - 9ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.0869 - val_loss: 0.0957 - 5s/epoch - 9ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.0822 - val_loss: 0.0893 - 5s/epoch - 9ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.0764 - val_loss: 0.0842 - 5s/epoch - 9ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.0700 - val_loss: 0.0778 - 5s/epoch - 9ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.0664 - val_loss: 0.0751 - 5s/epoch - 9ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.0607 - val_loss: 0.0652 - 5s/epoch - 9ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.0580 - val_loss: 0.0661 - 5s/epoch - 9ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.0534 - val_loss: 0.0620 - 5s/epoch - 9ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.0519 - val_loss: 0.0638 - 5s/epoch - 9ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.0475 - val_loss: 0.0566 - 5s/epoch - 9ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.0464 - val_loss: 0.0559 - 5s/epoch - 9ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.0426 - val_loss: 0.0530 - 5s/epoch - 9ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.0427 - val_loss: 0.0555 - 5s/epoch - 9ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.0412 - val_loss: 0.0525 - 5s/epoch - 9ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.0398 - val_loss: 0.0524 - 5s/epoch - 9ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.0378 - val_loss: 0.0470 - 5s/epoch - 9ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.0374 - val_loss: 0.0507 - 5s/epoch - 9ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.0370 - val_loss: 0.0469 - 5s/epoch - 9ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.0327 - val_loss: 0.0439 - 5s/epoch - 9ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.0353 - val_loss: 0.0447 - 5s/epoch - 9ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.0311 - val_loss: 0.0479 - 5s/epoch - 9ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.0319 - val_loss: 0.0433 - 5s/epoch - 9ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.0304 - val_loss: 0.0416 - 5s/epoch - 9ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0295 - val_loss: 0.0381 - 5s/epoch - 9ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.0279 - val_loss: 0.0360 - 5s/epoch - 9ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.0279 - val_loss: 0.0311 - 5s/epoch - 9ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.0254 - val_loss: 0.0392 - 5s/epoch - 9ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.0270 - val_loss: 0.0321 - 5s/epoch - 9ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.0246 - val_loss: 0.0397 - 5s/epoch - 9ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.0249 - val_loss: 0.0367 - 5s/epoch - 9ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.0255 - val_loss: 0.0301 - 5s/epoch - 9ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0224 - val_loss: 0.0289 - 5s/epoch - 9ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0228 - val_loss: 0.0285 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0223 - val_loss: 0.0286 - 5s/epoch - 9ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.0218 - val_loss: 0.0280 - 5s/epoch - 9ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0234 - val_loss: 0.0338 - 5s/epoch - 9ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0219 - val_loss: 0.0278 - 5s/epoch - 9ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0217 - val_loss: 0.0415 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0250 - val_loss: 0.0324 - 5s/epoch - 9ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0215 - val_loss: 0.0312 - 5s/epoch - 9ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0211 - val_loss: 0.0282 - 5s/epoch - 9ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0192 - val_loss: 0.0264 - 5s/epoch - 9ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0190 - val_loss: 0.0247 - 5s/epoch - 9ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0181 - val_loss: 0.0307 - 5s/epoch - 9ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0190 - val_loss: 0.0263 - 5s/epoch - 10ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0180 - val_loss: 0.0239 - 5s/epoch - 9ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0170 - val_loss: 0.0251 - 5s/epoch - 9ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0173 - val_loss: 0.0250 - 5s/epoch - 9ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0172 - val_loss: 0.0263 - 5s/epoch - 9ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0179 - val_loss: 0.0245 - 5s/epoch - 10ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 6s - loss: 0.0167 - val_loss: 0.0236 - 6s/epoch - 10ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 6s - loss: 0.0166 - val_loss: 0.0237 - 6s/epoch - 10ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0159 - val_loss: 0.0229 - 5s/epoch - 9ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0164 - val_loss: 0.0268 - 5s/epoch - 9ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0162 - val_loss: 0.0213 - 5s/epoch - 9ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0149 - val_loss: 0.0280 - 5s/epoch - 9ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0161 - val_loss: 0.0236 - 5s/epoch - 9ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0152 - val_loss: 0.0228 - 5s/epoch - 9ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 5s - loss: 0.0152 - val_loss: 0.0218 - 5s/epoch - 9ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0152 - val_loss: 0.0205 - 5s/epoch - 9ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0147 - val_loss: 0.0212 - 5s/epoch - 9ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0150 - val_loss: 0.0210 - 5s/epoch - 9ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0140 - val_loss: 0.0219 - 5s/epoch - 9ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 5s - loss: 0.0145 - val_loss: 0.0208 - 5s/epoch - 9ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 5s - loss: 0.0146 - val_loss: 0.0227 - 5s/epoch - 9ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 5s - loss: 0.0135 - val_loss: 0.0197 - 5s/epoch - 9ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 5s - loss: 0.0191 - val_loss: 0.0303 - 5s/epoch - 9ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 5s - loss: 0.0170 - val_loss: 0.0224 - 5s/epoch - 9ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 5s - loss: 0.0132 - val_loss: 0.0261 - 5s/epoch - 9ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 5s - loss: 0.0162 - val_loss: 0.0242 - 5s/epoch - 9ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 5s - loss: 0.0169 - val_loss: 0.0296 - 5s/epoch - 9ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 5s - loss: 0.0132 - val_loss: 0.0212 - 5s/epoch - 9ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 5s - loss: 0.0127 - val_loss: 0.0196 - 5s/epoch - 9ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 5s - loss: 0.0127 - val_loss: 0.0196 - 5s/epoch - 9ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 5s - loss: 0.0128 - val_loss: 0.0181 - 5s/epoch - 9ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 5s - loss: 0.0130 - val_loss: 0.0216 - 5s/epoch - 9ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 5s - loss: 0.0135 - val_loss: 0.0196 - 5s/epoch - 9ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 5s - loss: 0.0123 - val_loss: 0.0187 - 5s/epoch - 9ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 5s - loss: 0.0126 - val_loss: 0.0184 - 5s/epoch - 9ms/step\n",
      "Epoch 84/10000\n",
      "563/563 - 5s - loss: 0.0122 - val_loss: 0.0179 - 5s/epoch - 9ms/step\n",
      "Epoch 85/10000\n",
      "563/563 - 5s - loss: 0.0122 - val_loss: 0.0201 - 5s/epoch - 9ms/step\n",
      "Epoch 86/10000\n",
      "563/563 - 5s - loss: 0.0134 - val_loss: 0.0200 - 5s/epoch - 9ms/step\n",
      "Epoch 87/10000\n",
      "563/563 - 5s - loss: 0.0123 - val_loss: 0.0187 - 5s/epoch - 9ms/step\n",
      "Epoch 88/10000\n",
      "563/563 - 5s - loss: 0.0116 - val_loss: 0.0168 - 5s/epoch - 9ms/step\n",
      "Epoch 89/10000\n",
      "563/563 - 5s - loss: 0.0122 - val_loss: 0.0175 - 5s/epoch - 9ms/step\n",
      "Epoch 90/10000\n",
      "563/563 - 5s - loss: 0.0230 - val_loss: 0.0210 - 5s/epoch - 9ms/step\n",
      "Epoch 91/10000\n",
      "563/563 - 5s - loss: 0.0141 - val_loss: 0.0180 - 5s/epoch - 9ms/step\n",
      "Epoch 92/10000\n",
      "563/563 - 5s - loss: 0.0108 - val_loss: 0.0144 - 5s/epoch - 9ms/step\n",
      "Epoch 93/10000\n",
      "563/563 - 5s - loss: 0.0108 - val_loss: 0.0156 - 5s/epoch - 9ms/step\n",
      "Epoch 94/10000\n",
      "563/563 - 5s - loss: 0.0109 - val_loss: 0.0157 - 5s/epoch - 9ms/step\n",
      "Epoch 95/10000\n",
      "563/563 - 5s - loss: 0.0114 - val_loss: 0.0161 - 5s/epoch - 9ms/step\n",
      "Epoch 96/10000\n",
      "563/563 - 5s - loss: 0.0117 - val_loss: 0.0158 - 5s/epoch - 9ms/step\n",
      "Epoch 97/10000\n",
      "563/563 - 5s - loss: 0.0121 - val_loss: 0.0224 - 5s/epoch - 9ms/step\n",
      "Epoch 98/10000\n",
      "563/563 - 5s - loss: 0.0126 - val_loss: 0.0163 - 5s/epoch - 9ms/step\n",
      "Epoch 99/10000\n",
      "563/563 - 5s - loss: 0.0107 - val_loss: 0.0172 - 5s/epoch - 9ms/step\n",
      "Epoch 100/10000\n",
      "563/563 - 5s - loss: 0.0121 - val_loss: 0.0153 - 5s/epoch - 9ms/step\n",
      "Epoch 101/10000\n",
      "563/563 - 5s - loss: 0.0106 - val_loss: 0.0149 - 5s/epoch - 9ms/step\n",
      "Epoch 102/10000\n",
      "563/563 - 5s - loss: 0.0113 - val_loss: 0.0166 - 5s/epoch - 9ms/step\n",
      "csv file is saved to: ./result/40_20_1_100_2_20_att_seq2seq_gru_linear_0.5_1.csv\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 20\n",
      "model_type: datt_seq2seq_gru\n",
      "Epoch 1/10000\n",
      "563/563 - 8s - loss: 0.1955 - val_loss: 0.1805 - 8s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.1567 - val_loss: 0.1631 - 5s/epoch - 9ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.1447 - val_loss: 0.1490 - 5s/epoch - 9ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 5s - loss: 0.1340 - val_loss: 0.1423 - 5s/epoch - 9ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.1229 - val_loss: 0.1328 - 5s/epoch - 9ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.1150 - val_loss: 0.1336 - 5s/epoch - 9ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.1069 - val_loss: 0.1169 - 5s/epoch - 10ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.1004 - val_loss: 0.1071 - 5s/epoch - 9ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.0954 - val_loss: 0.1065 - 5s/epoch - 9ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.0863 - val_loss: 0.0970 - 5s/epoch - 9ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.0869 - val_loss: 0.0972 - 5s/epoch - 9ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.0795 - val_loss: 0.0935 - 5s/epoch - 9ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.0752 - val_loss: 0.0907 - 5s/epoch - 9ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.0724 - val_loss: 0.0833 - 5s/epoch - 9ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.0692 - val_loss: 0.0883 - 5s/epoch - 9ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.0675 - val_loss: 0.0938 - 5s/epoch - 9ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.0650 - val_loss: 0.0811 - 5s/epoch - 9ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.0616 - val_loss: 0.0755 - 5s/epoch - 9ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.0586 - val_loss: 0.0732 - 5s/epoch - 9ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.0587 - val_loss: 0.0735 - 5s/epoch - 9ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.0554 - val_loss: 0.0737 - 5s/epoch - 9ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.0558 - val_loss: 0.0662 - 5s/epoch - 9ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.0514 - val_loss: 0.0656 - 5s/epoch - 9ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.0488 - val_loss: 0.0647 - 5s/epoch - 9ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.0485 - val_loss: 0.0641 - 5s/epoch - 9ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.0460 - val_loss: 0.0616 - 5s/epoch - 9ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.0445 - val_loss: 0.0604 - 5s/epoch - 9ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0453 - val_loss: 0.0615 - 5s/epoch - 9ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.0419 - val_loss: 0.0655 - 5s/epoch - 9ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.0460 - val_loss: 0.0618 - 5s/epoch - 9ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.0404 - val_loss: 0.0510 - 5s/epoch - 9ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.0415 - val_loss: 0.0774 - 5s/epoch - 10ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.0460 - val_loss: 0.0541 - 5s/epoch - 10ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.0399 - val_loss: 0.0672 - 5s/epoch - 9ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.0403 - val_loss: 0.0530 - 5s/epoch - 9ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0389 - val_loss: 0.0525 - 5s/epoch - 9ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0349 - val_loss: 0.0501 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0356 - val_loss: 0.0523 - 5s/epoch - 9ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.0351 - val_loss: 0.0411 - 5s/epoch - 9ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0319 - val_loss: 0.0467 - 5s/epoch - 9ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0354 - val_loss: 0.0504 - 5s/epoch - 9ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0325 - val_loss: 0.0421 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0298 - val_loss: 0.0407 - 5s/epoch - 9ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0291 - val_loss: 0.0531 - 5s/epoch - 9ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0307 - val_loss: 0.0393 - 5s/epoch - 9ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0277 - val_loss: 0.0408 - 5s/epoch - 9ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0305 - val_loss: 0.0388 - 5s/epoch - 9ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0277 - val_loss: 0.0507 - 5s/epoch - 9ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0296 - val_loss: 0.0350 - 5s/epoch - 9ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0261 - val_loss: 0.0383 - 5s/epoch - 9ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0267 - val_loss: 0.0412 - 5s/epoch - 9ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0268 - val_loss: 0.0395 - 5s/epoch - 9ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0247 - val_loss: 0.0384 - 5s/epoch - 9ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0238 - val_loss: 0.0376 - 5s/epoch - 9ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0250 - val_loss: 0.0359 - 5s/epoch - 10ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0246 - val_loss: 0.0391 - 5s/epoch - 9ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0243 - val_loss: 0.0328 - 5s/epoch - 9ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0233 - val_loss: 0.0369 - 5s/epoch - 9ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0233 - val_loss: 0.0353 - 5s/epoch - 9ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0217 - val_loss: 0.0356 - 5s/epoch - 10ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0225 - val_loss: 0.0348 - 5s/epoch - 9ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 6s - loss: 0.0224 - val_loss: 0.0324 - 6s/epoch - 10ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 5s - loss: 0.0262 - val_loss: 0.0312 - 5s/epoch - 10ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 6s - loss: 0.0239 - val_loss: 0.0446 - 6s/epoch - 10ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 6s - loss: 0.0255 - val_loss: 0.0348 - 6s/epoch - 10ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 6s - loss: 0.0210 - val_loss: 0.0343 - 6s/epoch - 11ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 6s - loss: 0.0219 - val_loss: 0.0328 - 6s/epoch - 11ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 6s - loss: 0.0200 - val_loss: 0.0318 - 6s/epoch - 10ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 6s - loss: 0.0201 - val_loss: 0.0359 - 6s/epoch - 10ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 5s - loss: 0.0203 - val_loss: 0.0344 - 5s/epoch - 10ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 6s - loss: 0.0201 - val_loss: 0.0329 - 6s/epoch - 10ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 6s - loss: 0.0203 - val_loss: 0.0280 - 6s/epoch - 10ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 6s - loss: 0.0187 - val_loss: 0.0332 - 6s/epoch - 10ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 5s - loss: 0.0194 - val_loss: 0.0318 - 5s/epoch - 10ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 5s - loss: 0.0197 - val_loss: 0.0302 - 5s/epoch - 10ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 5s - loss: 0.0184 - val_loss: 0.0322 - 5s/epoch - 10ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 5s - loss: 0.0198 - val_loss: 0.0332 - 5s/epoch - 10ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 5s - loss: 0.0186 - val_loss: 0.0300 - 5s/epoch - 9ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 5s - loss: 0.0184 - val_loss: 0.0283 - 5s/epoch - 10ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 6s - loss: 0.0184 - val_loss: 0.0313 - 6s/epoch - 10ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 5s - loss: 0.0177 - val_loss: 0.0358 - 5s/epoch - 10ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 5s - loss: 0.0182 - val_loss: 0.0310 - 5s/epoch - 10ms/step\n",
      "csv file is saved to: ./result/40_20_1_100_2_20_datt_seq2seq_gru_linear_0.5_1.csv\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: seq2seq_gru\n",
      "Epoch 1/10000\n",
      "563/563 - 8s - loss: 0.1330 - val_loss: 0.1272 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.1104 - val_loss: 0.1211 - 5s/epoch - 9ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.1016 - val_loss: 0.1111 - 5s/epoch - 9ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 5s - loss: 0.0926 - val_loss: 0.0977 - 5s/epoch - 9ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.0873 - val_loss: 0.0996 - 5s/epoch - 9ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.0793 - val_loss: 0.0910 - 5s/epoch - 9ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.0750 - val_loss: 0.0804 - 5s/epoch - 9ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.0685 - val_loss: 0.0778 - 5s/epoch - 9ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.0620 - val_loss: 0.0728 - 5s/epoch - 10ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 6s - loss: 0.0603 - val_loss: 0.0705 - 6s/epoch - 10ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 6s - loss: 0.0573 - val_loss: 0.0623 - 6s/epoch - 10ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.0515 - val_loss: 0.0664 - 5s/epoch - 9ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.0501 - val_loss: 0.0583 - 5s/epoch - 9ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.0475 - val_loss: 0.0548 - 5s/epoch - 9ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.0456 - val_loss: 0.0530 - 5s/epoch - 9ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.0439 - val_loss: 0.0508 - 5s/epoch - 9ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.0422 - val_loss: 0.0514 - 5s/epoch - 9ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 6s - loss: 0.0415 - val_loss: 0.0522 - 6s/epoch - 11ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 6s - loss: 0.0394 - val_loss: 0.0505 - 6s/epoch - 10ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.0382 - val_loss: 0.0458 - 5s/epoch - 10ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.0360 - val_loss: 0.0451 - 5s/epoch - 9ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.0345 - val_loss: 0.0436 - 5s/epoch - 10ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.0341 - val_loss: 0.0432 - 5s/epoch - 9ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.0331 - val_loss: 0.0414 - 5s/epoch - 9ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.0310 - val_loss: 0.0400 - 5s/epoch - 9ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.0303 - val_loss: 0.0389 - 5s/epoch - 9ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.0320 - val_loss: 0.0389 - 5s/epoch - 9ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0299 - val_loss: 0.0343 - 5s/epoch - 9ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.0278 - val_loss: 0.0399 - 5s/epoch - 9ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.0268 - val_loss: 0.0385 - 5s/epoch - 9ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.0271 - val_loss: 0.0370 - 5s/epoch - 9ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.0257 - val_loss: 0.0333 - 5s/epoch - 9ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.0252 - val_loss: 0.0321 - 5s/epoch - 9ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.0264 - val_loss: 0.0366 - 5s/epoch - 9ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.0241 - val_loss: 0.0306 - 5s/epoch - 9ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0239 - val_loss: 0.0306 - 5s/epoch - 9ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0266 - val_loss: 0.0316 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0231 - val_loss: 0.0342 - 5s/epoch - 9ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.0220 - val_loss: 0.0301 - 5s/epoch - 10ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0228 - val_loss: 0.0374 - 5s/epoch - 10ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 6s - loss: 0.0226 - val_loss: 0.0344 - 6s/epoch - 11ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 6s - loss: 0.0208 - val_loss: 0.0326 - 6s/epoch - 10ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0200 - val_loss: 0.0251 - 5s/epoch - 10ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 6s - loss: 0.0213 - val_loss: 0.0324 - 6s/epoch - 10ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 6s - loss: 0.0201 - val_loss: 0.0257 - 6s/epoch - 10ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 6s - loss: 0.0183 - val_loss: 0.0252 - 6s/epoch - 10ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 6s - loss: 0.0183 - val_loss: 0.0272 - 6s/epoch - 10ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0193 - val_loss: 0.0262 - 5s/epoch - 10ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 6s - loss: 0.0181 - val_loss: 0.0304 - 6s/epoch - 11ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 8s - loss: 0.0189 - val_loss: 0.0241 - 8s/epoch - 14ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 10s - loss: 0.0226 - val_loss: 0.0286 - 10s/epoch - 18ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 7s - loss: 0.0191 - val_loss: 0.0247 - 7s/epoch - 13ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 6s - loss: 0.0179 - val_loss: 0.0299 - 6s/epoch - 11ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 6s - loss: 0.0190 - val_loss: 0.0347 - 6s/epoch - 10ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0176 - val_loss: 0.0366 - 5s/epoch - 9ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0192 - val_loss: 0.0269 - 5s/epoch - 9ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0154 - val_loss: 0.0197 - 5s/epoch - 9ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0158 - val_loss: 0.0236 - 5s/epoch - 9ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0164 - val_loss: 0.0227 - 5s/epoch - 9ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0152 - val_loss: 0.0238 - 5s/epoch - 9ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0167 - val_loss: 0.0236 - 5s/epoch - 10ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0153 - val_loss: 0.0217 - 5s/epoch - 9ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 5s - loss: 0.0151 - val_loss: 0.0212 - 5s/epoch - 9ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0145 - val_loss: 0.0216 - 5s/epoch - 9ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0160 - val_loss: 0.0231 - 5s/epoch - 9ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0166 - val_loss: 0.0244 - 5s/epoch - 9ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0154 - val_loss: 0.0220 - 5s/epoch - 9ms/step\n",
      "csv file is saved to: ./result/40_20_1_100_2_40_seq2seq_gru_linear_0.5_1.csv\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: att_seq2seq_gru\n",
      "Epoch 1/10000\n",
      "563/563 - 9s - loss: 0.1326 - val_loss: 0.1293 - 9s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 6s - loss: 0.1091 - val_loss: 0.1125 - 6s/epoch - 10ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.1002 - val_loss: 0.1055 - 5s/epoch - 10ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 6s - loss: 0.0920 - val_loss: 0.1046 - 6s/epoch - 10ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 6s - loss: 0.0852 - val_loss: 0.0926 - 6s/epoch - 10ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.0791 - val_loss: 0.0864 - 5s/epoch - 10ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.0756 - val_loss: 0.0810 - 5s/epoch - 10ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.0709 - val_loss: 0.0766 - 5s/epoch - 10ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.0658 - val_loss: 0.0795 - 5s/epoch - 10ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.0614 - val_loss: 0.0729 - 5s/epoch - 10ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.0583 - val_loss: 0.0687 - 5s/epoch - 10ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 6s - loss: 0.0553 - val_loss: 0.0628 - 6s/epoch - 10ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 6s - loss: 0.0518 - val_loss: 0.0587 - 6s/epoch - 11ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 6s - loss: 0.0505 - val_loss: 0.0598 - 6s/epoch - 10ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.0475 - val_loss: 0.0606 - 5s/epoch - 9ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 6s - loss: 0.0449 - val_loss: 0.0544 - 6s/epoch - 10ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 6s - loss: 0.0436 - val_loss: 0.0577 - 6s/epoch - 10ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 6s - loss: 0.0418 - val_loss: 0.0518 - 6s/epoch - 10ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.0409 - val_loss: 0.0493 - 5s/epoch - 10ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 6s - loss: 0.0389 - val_loss: 0.0498 - 6s/epoch - 10ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 6s - loss: 0.0379 - val_loss: 0.0477 - 6s/epoch - 10ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.0355 - val_loss: 0.0474 - 5s/epoch - 9ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.0343 - val_loss: 0.0478 - 5s/epoch - 9ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.0339 - val_loss: 0.0437 - 5s/epoch - 9ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.0317 - val_loss: 0.0433 - 5s/epoch - 10ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.0333 - val_loss: 0.0417 - 5s/epoch - 10ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.0313 - val_loss: 0.0399 - 5s/epoch - 10ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0302 - val_loss: 0.0363 - 5s/epoch - 10ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.0302 - val_loss: 0.0367 - 5s/epoch - 10ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.0279 - val_loss: 0.0407 - 5s/epoch - 10ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.0273 - val_loss: 0.0368 - 5s/epoch - 10ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.0268 - val_loss: 0.0340 - 5s/epoch - 10ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.0261 - val_loss: 0.0417 - 5s/epoch - 10ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.0260 - val_loss: 0.0365 - 5s/epoch - 10ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 6s - loss: 0.0239 - val_loss: 0.0322 - 6s/epoch - 10ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0235 - val_loss: 0.0332 - 5s/epoch - 10ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0226 - val_loss: 0.0372 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0240 - val_loss: 0.0317 - 5s/epoch - 9ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.0217 - val_loss: 0.0282 - 5s/epoch - 9ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0224 - val_loss: 0.0313 - 5s/epoch - 9ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0212 - val_loss: 0.0297 - 5s/epoch - 10ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0197 - val_loss: 0.0294 - 5s/epoch - 10ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0241 - val_loss: 0.0352 - 5s/epoch - 10ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0217 - val_loss: 0.0306 - 5s/epoch - 10ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 6s - loss: 0.0189 - val_loss: 0.0271 - 6s/epoch - 10ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0184 - val_loss: 0.0253 - 5s/epoch - 10ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0195 - val_loss: 0.0280 - 5s/epoch - 10ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 6s - loss: 0.0192 - val_loss: 0.0258 - 6s/epoch - 11ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 6s - loss: 0.0177 - val_loss: 0.0245 - 6s/epoch - 10ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0175 - val_loss: 0.0277 - 5s/epoch - 10ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0177 - val_loss: 0.0243 - 5s/epoch - 9ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0175 - val_loss: 0.0311 - 5s/epoch - 10ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 6s - loss: 0.0177 - val_loss: 0.0283 - 6s/epoch - 10ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0167 - val_loss: 0.0226 - 5s/epoch - 10ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 6s - loss: 0.0157 - val_loss: 0.0227 - 6s/epoch - 10ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0172 - val_loss: 0.0238 - 5s/epoch - 10ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0166 - val_loss: 0.0225 - 5s/epoch - 10ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0147 - val_loss: 0.0248 - 5s/epoch - 10ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 6s - loss: 0.0152 - val_loss: 0.0238 - 6s/epoch - 10ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0150 - val_loss: 0.0233 - 5s/epoch - 10ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0162 - val_loss: 0.0223 - 5s/epoch - 10ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0145 - val_loss: 0.0222 - 5s/epoch - 10ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 6s - loss: 0.0151 - val_loss: 0.0213 - 6s/epoch - 10ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 6s - loss: 0.0150 - val_loss: 0.0206 - 6s/epoch - 10ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0140 - val_loss: 0.0226 - 5s/epoch - 9ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0153 - val_loss: 0.0225 - 5s/epoch - 10ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0147 - val_loss: 0.0229 - 5s/epoch - 10ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 6s - loss: 0.0145 - val_loss: 0.0220 - 6s/epoch - 10ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 6s - loss: 0.0130 - val_loss: 0.0208 - 6s/epoch - 10ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 6s - loss: 0.0132 - val_loss: 0.0192 - 6s/epoch - 10ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 6s - loss: 0.0134 - val_loss: 0.0205 - 6s/epoch - 10ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 6s - loss: 0.0135 - val_loss: 0.0203 - 6s/epoch - 10ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 6s - loss: 0.0135 - val_loss: 0.0228 - 6s/epoch - 11ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 5s - loss: 0.0153 - val_loss: 0.0225 - 5s/epoch - 10ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 5s - loss: 0.0129 - val_loss: 0.0205 - 5s/epoch - 10ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 5s - loss: 0.0122 - val_loss: 0.0183 - 5s/epoch - 10ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 5s - loss: 0.0122 - val_loss: 0.0225 - 5s/epoch - 10ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 5s - loss: 0.0130 - val_loss: 0.0188 - 5s/epoch - 10ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 6s - loss: 0.0127 - val_loss: 0.0176 - 6s/epoch - 10ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 5s - loss: 0.0122 - val_loss: 0.0196 - 5s/epoch - 10ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 5s - loss: 0.0121 - val_loss: 0.0194 - 5s/epoch - 9ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 5s - loss: 0.0162 - val_loss: 0.0289 - 5s/epoch - 10ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 5s - loss: 0.0139 - val_loss: 0.0186 - 5s/epoch - 10ms/step\n",
      "Epoch 84/10000\n",
      "563/563 - 6s - loss: 0.0112 - val_loss: 0.0179 - 6s/epoch - 10ms/step\n",
      "Epoch 85/10000\n",
      "563/563 - 5s - loss: 0.0110 - val_loss: 0.0158 - 5s/epoch - 10ms/step\n",
      "Epoch 86/10000\n",
      "563/563 - 6s - loss: 0.0111 - val_loss: 0.0186 - 6s/epoch - 11ms/step\n",
      "Epoch 87/10000\n",
      "563/563 - 6s - loss: 0.0117 - val_loss: 0.0183 - 6s/epoch - 10ms/step\n",
      "Epoch 88/10000\n",
      "563/563 - 6s - loss: 0.0117 - val_loss: 0.0169 - 6s/epoch - 10ms/step\n",
      "Epoch 89/10000\n",
      "563/563 - 6s - loss: 0.0125 - val_loss: 0.0171 - 6s/epoch - 10ms/step\n",
      "Epoch 90/10000\n",
      "563/563 - 6s - loss: 0.0118 - val_loss: 0.0173 - 6s/epoch - 10ms/step\n",
      "Epoch 91/10000\n",
      "563/563 - 5s - loss: 0.0122 - val_loss: 0.0160 - 5s/epoch - 10ms/step\n",
      "Epoch 92/10000\n",
      "563/563 - 5s - loss: 0.0111 - val_loss: 0.0160 - 5s/epoch - 10ms/step\n",
      "Epoch 93/10000\n",
      "563/563 - 6s - loss: 0.0112 - val_loss: 0.0178 - 6s/epoch - 10ms/step\n",
      "Epoch 94/10000\n",
      "563/563 - 5s - loss: 0.0130 - val_loss: 0.0174 - 5s/epoch - 9ms/step\n",
      "Epoch 95/10000\n",
      "563/563 - 6s - loss: 0.0102 - val_loss: 0.0178 - 6s/epoch - 10ms/step\n",
      "csv file is saved to: ./result/40_20_1_100_2_40_att_seq2seq_gru_linear_0.5_1.csv\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 40\n",
      "model_type: datt_seq2seq_gru\n",
      "Epoch 1/10000\n",
      "563/563 - 9s - loss: 0.1988 - val_loss: 0.1903 - 9s/epoch - 16ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.1581 - val_loss: 0.1646 - 5s/epoch - 10ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 6s - loss: 0.1452 - val_loss: 0.1634 - 6s/epoch - 10ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 6s - loss: 0.1345 - val_loss: 0.1458 - 6s/epoch - 11ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 6s - loss: 0.1285 - val_loss: 0.1434 - 6s/epoch - 10ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.1189 - val_loss: 0.1286 - 5s/epoch - 9ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.1099 - val_loss: 0.1298 - 5s/epoch - 10ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.1019 - val_loss: 0.1144 - 5s/epoch - 10ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.0975 - val_loss: 0.1074 - 5s/epoch - 9ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 7s - loss: 0.0894 - val_loss: 0.1046 - 7s/epoch - 12ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 6s - loss: 0.0868 - val_loss: 0.1047 - 6s/epoch - 10ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 6s - loss: 0.0807 - val_loss: 0.1002 - 6s/epoch - 11ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 6s - loss: 0.0767 - val_loss: 0.0904 - 6s/epoch - 11ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.0736 - val_loss: 0.0844 - 5s/epoch - 10ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.0691 - val_loss: 0.0802 - 5s/epoch - 10ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.0673 - val_loss: 0.0830 - 5s/epoch - 10ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 6s - loss: 0.0662 - val_loss: 0.0829 - 6s/epoch - 10ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.0616 - val_loss: 0.0734 - 5s/epoch - 9ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.0587 - val_loss: 0.0752 - 5s/epoch - 9ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.0565 - val_loss: 0.0691 - 5s/epoch - 9ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.0542 - val_loss: 0.0707 - 5s/epoch - 10ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 6s - loss: 0.0555 - val_loss: 0.0645 - 6s/epoch - 11ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 6s - loss: 0.0490 - val_loss: 0.0668 - 6s/epoch - 10ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.0498 - val_loss: 0.0649 - 5s/epoch - 9ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.0487 - val_loss: 0.0638 - 5s/epoch - 10ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 6s - loss: 0.0461 - val_loss: 0.0635 - 6s/epoch - 10ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.0431 - val_loss: 0.0578 - 5s/epoch - 9ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0429 - val_loss: 0.0572 - 5s/epoch - 9ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 6s - loss: 0.0403 - val_loss: 0.0566 - 6s/epoch - 10ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.0402 - val_loss: 0.0563 - 5s/epoch - 10ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.0406 - val_loss: 0.0551 - 5s/epoch - 9ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.0459 - val_loss: 0.0562 - 5s/epoch - 10ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.0518 - val_loss: 0.0618 - 5s/epoch - 10ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.0398 - val_loss: 0.0532 - 5s/epoch - 10ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.0363 - val_loss: 0.0843 - 5s/epoch - 10ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0402 - val_loss: 0.0524 - 5s/epoch - 9ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0354 - val_loss: 0.0480 - 5s/epoch - 10ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 6s - loss: 0.0349 - val_loss: 0.0442 - 6s/epoch - 10ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 6s - loss: 0.0351 - val_loss: 0.0448 - 6s/epoch - 10ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0328 - val_loss: 0.0456 - 5s/epoch - 9ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0339 - val_loss: 0.0447 - 5s/epoch - 10ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0317 - val_loss: 0.0422 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0312 - val_loss: 0.0459 - 5s/epoch - 9ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0308 - val_loss: 0.0461 - 5s/epoch - 9ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0334 - val_loss: 0.0515 - 5s/epoch - 10ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0341 - val_loss: 0.0427 - 5s/epoch - 9ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0303 - val_loss: 0.0432 - 5s/epoch - 9ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0339 - val_loss: 0.0500 - 5s/epoch - 10ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0286 - val_loss: 0.0394 - 5s/epoch - 10ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0276 - val_loss: 0.0403 - 5s/epoch - 10ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0264 - val_loss: 0.0378 - 5s/epoch - 10ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0268 - val_loss: 0.0384 - 5s/epoch - 9ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0261 - val_loss: 0.0388 - 5s/epoch - 10ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0254 - val_loss: 0.0383 - 5s/epoch - 9ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0298 - val_loss: 0.0377 - 5s/epoch - 10ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0236 - val_loss: 0.0341 - 5s/epoch - 10ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0245 - val_loss: 0.0381 - 5s/epoch - 10ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0265 - val_loss: 0.0419 - 5s/epoch - 9ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0238 - val_loss: 0.0350 - 5s/epoch - 9ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0229 - val_loss: 0.0370 - 5s/epoch - 9ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0237 - val_loss: 0.0353 - 5s/epoch - 10ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0232 - val_loss: 0.0324 - 5s/epoch - 9ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 6s - loss: 0.0224 - val_loss: 0.0371 - 6s/epoch - 10ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0243 - val_loss: 0.0409 - 5s/epoch - 10ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0218 - val_loss: 0.0315 - 5s/epoch - 9ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 6s - loss: 0.0211 - val_loss: 0.0322 - 6s/epoch - 10ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0206 - val_loss: 0.0318 - 5s/epoch - 10ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 6s - loss: 0.0211 - val_loss: 0.0353 - 6s/epoch - 10ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 5s - loss: 0.0219 - val_loss: 0.0348 - 5s/epoch - 10ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 5s - loss: 0.0210 - val_loss: 0.0332 - 5s/epoch - 10ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 5s - loss: 0.0210 - val_loss: 0.0378 - 5s/epoch - 10ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 5s - loss: 0.0205 - val_loss: 0.0362 - 5s/epoch - 9ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 5s - loss: 0.0200 - val_loss: 0.0295 - 5s/epoch - 10ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 5s - loss: 0.0193 - val_loss: 0.0309 - 5s/epoch - 10ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 5s - loss: 0.0197 - val_loss: 0.0332 - 5s/epoch - 10ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 5s - loss: 0.0205 - val_loss: 0.0307 - 5s/epoch - 9ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 5s - loss: 0.0185 - val_loss: 0.0317 - 5s/epoch - 10ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 5s - loss: 0.0186 - val_loss: 0.0291 - 5s/epoch - 10ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 5s - loss: 0.0177 - val_loss: 0.0359 - 5s/epoch - 10ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 5s - loss: 0.0222 - val_loss: 0.0339 - 5s/epoch - 10ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 5s - loss: 0.0237 - val_loss: 0.0370 - 5s/epoch - 10ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 5s - loss: 0.0189 - val_loss: 0.0338 - 5s/epoch - 9ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 5s - loss: 0.0190 - val_loss: 0.0348 - 5s/epoch - 10ms/step\n",
      "Epoch 84/10000\n",
      "563/563 - 5s - loss: 0.0194 - val_loss: 0.0331 - 5s/epoch - 9ms/step\n",
      "Epoch 85/10000\n",
      "563/563 - 5s - loss: 0.0183 - val_loss: 0.0276 - 5s/epoch - 10ms/step\n",
      "Epoch 86/10000\n",
      "563/563 - 5s - loss: 0.0240 - val_loss: 0.0302 - 5s/epoch - 10ms/step\n",
      "Epoch 87/10000\n",
      "563/563 - 5s - loss: 0.0169 - val_loss: 0.0269 - 5s/epoch - 10ms/step\n",
      "Epoch 88/10000\n",
      "563/563 - 5s - loss: 0.0159 - val_loss: 0.0285 - 5s/epoch - 10ms/step\n",
      "Epoch 89/10000\n",
      "563/563 - 5s - loss: 0.0175 - val_loss: 0.0282 - 5s/epoch - 10ms/step\n",
      "Epoch 90/10000\n",
      "563/563 - 6s - loss: 0.0176 - val_loss: 0.0284 - 6s/epoch - 10ms/step\n",
      "Epoch 91/10000\n",
      "563/563 - 6s - loss: 0.0163 - val_loss: 0.0321 - 6s/epoch - 10ms/step\n",
      "Epoch 92/10000\n",
      "563/563 - 6s - loss: 0.0166 - val_loss: 0.0283 - 6s/epoch - 11ms/step\n",
      "Epoch 93/10000\n",
      "563/563 - 5s - loss: 0.0168 - val_loss: 0.0286 - 5s/epoch - 10ms/step\n",
      "Epoch 94/10000\n",
      "563/563 - 5s - loss: 0.0169 - val_loss: 0.0271 - 5s/epoch - 10ms/step\n",
      "Epoch 95/10000\n",
      "563/563 - 5s - loss: 0.0164 - val_loss: 0.0294 - 5s/epoch - 10ms/step\n",
      "Epoch 96/10000\n",
      "563/563 - 5s - loss: 0.0159 - val_loss: 0.0288 - 5s/epoch - 10ms/step\n",
      "Epoch 97/10000\n",
      "563/563 - 5s - loss: 0.0164 - val_loss: 0.0286 - 5s/epoch - 10ms/step\n",
      "csv file is saved to: ./result/40_20_1_100_2_40_datt_seq2seq_gru_linear_0.5_1.csv\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: seq2seq_gru\n",
      "Epoch 1/10000\n",
      "563/563 - 8s - loss: 0.1354 - val_loss: 0.1343 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.1125 - val_loss: 0.1187 - 5s/epoch - 9ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.1028 - val_loss: 0.1113 - 5s/epoch - 9ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 5s - loss: 0.0968 - val_loss: 0.1082 - 5s/epoch - 9ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.0896 - val_loss: 0.0970 - 5s/epoch - 9ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.0841 - val_loss: 0.0924 - 5s/epoch - 9ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.0791 - val_loss: 0.0878 - 5s/epoch - 9ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.0720 - val_loss: 0.0852 - 5s/epoch - 9ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.0662 - val_loss: 0.0765 - 5s/epoch - 9ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.0626 - val_loss: 0.0746 - 5s/epoch - 9ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.0571 - val_loss: 0.0642 - 5s/epoch - 9ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.0559 - val_loss: 0.0644 - 5s/epoch - 9ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.0528 - val_loss: 0.0656 - 5s/epoch - 9ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.0506 - val_loss: 0.0575 - 5s/epoch - 9ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.0469 - val_loss: 0.0560 - 5s/epoch - 9ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.0447 - val_loss: 0.0544 - 5s/epoch - 9ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.0426 - val_loss: 0.0513 - 5s/epoch - 9ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.0415 - val_loss: 0.0510 - 5s/epoch - 9ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.0399 - val_loss: 0.0504 - 5s/epoch - 9ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.0381 - val_loss: 0.0592 - 5s/epoch - 9ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.0395 - val_loss: 0.0495 - 5s/epoch - 9ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.0361 - val_loss: 0.0463 - 5s/epoch - 9ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.0334 - val_loss: 0.0453 - 5s/epoch - 9ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.0346 - val_loss: 0.0515 - 5s/epoch - 9ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.0341 - val_loss: 0.0425 - 5s/epoch - 9ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.0300 - val_loss: 0.0411 - 5s/epoch - 9ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.0299 - val_loss: 0.0355 - 5s/epoch - 9ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0297 - val_loss: 0.0408 - 5s/epoch - 9ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.0274 - val_loss: 0.0357 - 5s/epoch - 9ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.0272 - val_loss: 0.0351 - 5s/epoch - 9ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.0264 - val_loss: 0.0327 - 5s/epoch - 9ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.0271 - val_loss: 0.0341 - 5s/epoch - 10ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.0275 - val_loss: 0.0407 - 5s/epoch - 9ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.0269 - val_loss: 0.0323 - 5s/epoch - 9ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.0260 - val_loss: 0.0382 - 5s/epoch - 9ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0251 - val_loss: 0.0285 - 5s/epoch - 9ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0213 - val_loss: 0.0306 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0222 - val_loss: 0.0305 - 5s/epoch - 9ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.0209 - val_loss: 0.0286 - 5s/epoch - 9ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0207 - val_loss: 0.0306 - 5s/epoch - 9ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0197 - val_loss: 0.0287 - 5s/epoch - 9ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0198 - val_loss: 0.0358 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0206 - val_loss: 0.0272 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0189 - val_loss: 0.0257 - 5s/epoch - 9ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0207 - val_loss: 0.0273 - 5s/epoch - 9ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0180 - val_loss: 0.0271 - 5s/epoch - 9ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0221 - val_loss: 0.0272 - 5s/epoch - 9ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0205 - val_loss: 0.0274 - 5s/epoch - 9ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0175 - val_loss: 0.0258 - 5s/epoch - 9ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0165 - val_loss: 0.0252 - 5s/epoch - 9ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0184 - val_loss: 0.0338 - 5s/epoch - 9ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0189 - val_loss: 0.0259 - 5s/epoch - 9ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0198 - val_loss: 0.0274 - 5s/epoch - 9ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0190 - val_loss: 0.0335 - 5s/epoch - 9ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0223 - val_loss: 0.0259 - 5s/epoch - 9ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0164 - val_loss: 0.0276 - 5s/epoch - 9ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0152 - val_loss: 0.0241 - 5s/epoch - 9ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0153 - val_loss: 0.0278 - 5s/epoch - 9ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0153 - val_loss: 0.0259 - 5s/epoch - 9ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0154 - val_loss: 0.0254 - 5s/epoch - 9ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0152 - val_loss: 0.0257 - 5s/epoch - 9ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 6s - loss: 0.0163 - val_loss: 0.0271 - 6s/epoch - 10ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 5s - loss: 0.0150 - val_loss: 0.0252 - 5s/epoch - 9ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0152 - val_loss: 0.0275 - 5s/epoch - 9ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0155 - val_loss: 0.0241 - 5s/epoch - 9ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0147 - val_loss: 0.0244 - 5s/epoch - 9ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0138 - val_loss: 0.0233 - 5s/epoch - 9ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 5s - loss: 0.0164 - val_loss: 0.0275 - 5s/epoch - 9ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 5s - loss: 0.0147 - val_loss: 0.0229 - 5s/epoch - 9ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 5s - loss: 0.0131 - val_loss: 0.0231 - 5s/epoch - 9ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 5s - loss: 0.0133 - val_loss: 0.0216 - 5s/epoch - 9ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 5s - loss: 0.0137 - val_loss: 0.0233 - 5s/epoch - 9ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 5s - loss: 0.0127 - val_loss: 0.0219 - 5s/epoch - 9ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 5s - loss: 0.0133 - val_loss: 0.0215 - 5s/epoch - 9ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 5s - loss: 0.0128 - val_loss: 0.0229 - 5s/epoch - 9ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 5s - loss: 0.0136 - val_loss: 0.0205 - 5s/epoch - 9ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 5s - loss: 0.0124 - val_loss: 0.0227 - 5s/epoch - 9ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 5s - loss: 0.0138 - val_loss: 0.0240 - 5s/epoch - 9ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 5s - loss: 0.0131 - val_loss: 0.0255 - 5s/epoch - 9ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 5s - loss: 0.0126 - val_loss: 0.0216 - 5s/epoch - 9ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 5s - loss: 0.0111 - val_loss: 0.0225 - 5s/epoch - 9ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 5s - loss: 0.0118 - val_loss: 0.0223 - 5s/epoch - 9ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 5s - loss: 0.0123 - val_loss: 0.0205 - 5s/epoch - 10ms/step\n",
      "Epoch 84/10000\n",
      "563/563 - 5s - loss: 0.0126 - val_loss: 0.0199 - 5s/epoch - 9ms/step\n",
      "Epoch 85/10000\n",
      "563/563 - 6s - loss: 0.0118 - val_loss: 0.0198 - 6s/epoch - 10ms/step\n",
      "Epoch 86/10000\n",
      "563/563 - 5s - loss: 0.0112 - val_loss: 0.0206 - 5s/epoch - 9ms/step\n",
      "Epoch 87/10000\n",
      "563/563 - 5s - loss: 0.0155 - val_loss: 0.0382 - 5s/epoch - 9ms/step\n",
      "Epoch 88/10000\n",
      "563/563 - 5s - loss: 0.0172 - val_loss: 0.0232 - 5s/epoch - 9ms/step\n",
      "Epoch 89/10000\n",
      "563/563 - 5s - loss: 0.0123 - val_loss: 0.0218 - 5s/epoch - 9ms/step\n",
      "Epoch 90/10000\n",
      "563/563 - 5s - loss: 0.0123 - val_loss: 0.0209 - 5s/epoch - 9ms/step\n",
      "Epoch 91/10000\n",
      "563/563 - 5s - loss: 0.0121 - val_loss: 0.0215 - 5s/epoch - 9ms/step\n",
      "Epoch 92/10000\n",
      "563/563 - 5s - loss: 0.0167 - val_loss: 0.0309 - 5s/epoch - 9ms/step\n",
      "Epoch 93/10000\n",
      "563/563 - 5s - loss: 0.0156 - val_loss: 0.0352 - 5s/epoch - 9ms/step\n",
      "Epoch 94/10000\n",
      "563/563 - 5s - loss: 0.0219 - val_loss: 0.0216 - 5s/epoch - 9ms/step\n",
      "Epoch 95/10000\n",
      "563/563 - 5s - loss: 0.0123 - val_loss: 0.0184 - 5s/epoch - 9ms/step\n",
      "Epoch 96/10000\n",
      "563/563 - 5s - loss: 0.0134 - val_loss: 0.0179 - 5s/epoch - 9ms/step\n",
      "Epoch 97/10000\n",
      "563/563 - 5s - loss: 0.0111 - val_loss: 0.0153 - 5s/epoch - 9ms/step\n",
      "Epoch 98/10000\n",
      "563/563 - 5s - loss: 0.0109 - val_loss: 0.0172 - 5s/epoch - 10ms/step\n",
      "Epoch 99/10000\n",
      "563/563 - 5s - loss: 0.0112 - val_loss: 0.0177 - 5s/epoch - 9ms/step\n",
      "Epoch 100/10000\n",
      "563/563 - 5s - loss: 0.0109 - val_loss: 0.0190 - 5s/epoch - 9ms/step\n",
      "Epoch 101/10000\n",
      "563/563 - 5s - loss: 0.0135 - val_loss: 0.0323 - 5s/epoch - 9ms/step\n",
      "Epoch 102/10000\n",
      "563/563 - 5s - loss: 0.0143 - val_loss: 0.0195 - 5s/epoch - 9ms/step\n",
      "Epoch 103/10000\n",
      "563/563 - 5s - loss: 0.0147 - val_loss: 0.0182 - 5s/epoch - 9ms/step\n",
      "Epoch 104/10000\n",
      "563/563 - 5s - loss: 0.0105 - val_loss: 0.0174 - 5s/epoch - 9ms/step\n",
      "Epoch 105/10000\n",
      "563/563 - 5s - loss: 0.0102 - val_loss: 0.0159 - 5s/epoch - 9ms/step\n",
      "Epoch 106/10000\n",
      "563/563 - 5s - loss: 0.0104 - val_loss: 0.0170 - 5s/epoch - 9ms/step\n",
      "Epoch 107/10000\n",
      "563/563 - 5s - loss: 0.0103 - val_loss: 0.0167 - 5s/epoch - 9ms/step\n",
      "csv file is saved to: ./result/40_20_1_100_2_60_seq2seq_gru_linear_0.5_1.csv\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: att_seq2seq_gru\n",
      "Epoch 1/10000\n",
      "563/563 - 8s - loss: 0.1336 - val_loss: 0.1252 - 8s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.1109 - val_loss: 0.1151 - 5s/epoch - 10ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 6s - loss: 0.1025 - val_loss: 0.1089 - 6s/epoch - 10ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 6s - loss: 0.0939 - val_loss: 0.0986 - 6s/epoch - 10ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.0871 - val_loss: 0.0961 - 5s/epoch - 10ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.0834 - val_loss: 0.0889 - 5s/epoch - 9ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.0771 - val_loss: 0.0858 - 5s/epoch - 9ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.0704 - val_loss: 0.0813 - 5s/epoch - 9ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.0663 - val_loss: 0.0719 - 5s/epoch - 10ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.0613 - val_loss: 0.0698 - 5s/epoch - 9ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.0588 - val_loss: 0.0708 - 5s/epoch - 9ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.0545 - val_loss: 0.0677 - 5s/epoch - 9ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.0520 - val_loss: 0.0592 - 5s/epoch - 9ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.0491 - val_loss: 0.0580 - 5s/epoch - 9ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.0470 - val_loss: 0.0581 - 5s/epoch - 9ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 6s - loss: 0.0454 - val_loss: 0.0578 - 6s/epoch - 10ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 6s - loss: 0.0430 - val_loss: 0.0531 - 6s/epoch - 10ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.0428 - val_loss: 0.0563 - 5s/epoch - 9ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.0428 - val_loss: 0.0504 - 5s/epoch - 10ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.0378 - val_loss: 0.0485 - 5s/epoch - 9ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.0376 - val_loss: 0.0485 - 5s/epoch - 10ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.0356 - val_loss: 0.0488 - 5s/epoch - 9ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.0346 - val_loss: 0.0474 - 5s/epoch - 10ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.0343 - val_loss: 0.0462 - 5s/epoch - 9ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 6s - loss: 0.0347 - val_loss: 0.0465 - 6s/epoch - 11ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.0329 - val_loss: 0.0422 - 5s/epoch - 10ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.0325 - val_loss: 0.0431 - 5s/epoch - 10ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0301 - val_loss: 0.0426 - 5s/epoch - 9ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.0338 - val_loss: 0.0387 - 5s/epoch - 9ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.0302 - val_loss: 0.0387 - 5s/epoch - 9ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.0262 - val_loss: 0.0345 - 5s/epoch - 10ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.0260 - val_loss: 0.0443 - 5s/epoch - 9ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.0272 - val_loss: 0.0342 - 5s/epoch - 10ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.0260 - val_loss: 0.0334 - 5s/epoch - 10ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 6s - loss: 0.0285 - val_loss: 0.0384 - 6s/epoch - 10ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0254 - val_loss: 0.0342 - 5s/epoch - 10ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0225 - val_loss: 0.0319 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0228 - val_loss: 0.0335 - 5s/epoch - 9ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.0230 - val_loss: 0.0393 - 5s/epoch - 10ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0234 - val_loss: 0.0305 - 5s/epoch - 10ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0207 - val_loss: 0.0338 - 5s/epoch - 9ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0222 - val_loss: 0.0308 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0209 - val_loss: 0.0347 - 5s/epoch - 10ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0219 - val_loss: 0.0303 - 5s/epoch - 9ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0199 - val_loss: 0.0264 - 5s/epoch - 10ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0191 - val_loss: 0.0287 - 5s/epoch - 10ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 6s - loss: 0.0193 - val_loss: 0.0268 - 6s/epoch - 10ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0179 - val_loss: 0.0258 - 5s/epoch - 10ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0178 - val_loss: 0.0247 - 5s/epoch - 9ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0191 - val_loss: 0.0268 - 5s/epoch - 10ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0174 - val_loss: 0.0245 - 5s/epoch - 10ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0175 - val_loss: 0.0254 - 5s/epoch - 9ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0166 - val_loss: 0.0246 - 5s/epoch - 9ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0165 - val_loss: 0.0242 - 5s/epoch - 9ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0167 - val_loss: 0.0259 - 5s/epoch - 9ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0165 - val_loss: 0.0235 - 5s/epoch - 9ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0151 - val_loss: 0.0240 - 5s/epoch - 9ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 6s - loss: 0.0152 - val_loss: 0.0241 - 6s/epoch - 10ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0154 - val_loss: 0.0259 - 5s/epoch - 10ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0155 - val_loss: 0.0242 - 5s/epoch - 9ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0149 - val_loss: 0.0236 - 5s/epoch - 9ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0162 - val_loss: 0.0411 - 5s/epoch - 10ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 5s - loss: 0.0188 - val_loss: 0.0318 - 5s/epoch - 9ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0184 - val_loss: 0.0268 - 5s/epoch - 9ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0161 - val_loss: 0.0220 - 5s/epoch - 9ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0151 - val_loss: 0.0318 - 5s/epoch - 9ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 6s - loss: 0.0179 - val_loss: 0.0272 - 6s/epoch - 10ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 5s - loss: 0.0153 - val_loss: 0.0231 - 5s/epoch - 9ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 6s - loss: 0.0148 - val_loss: 0.0362 - 6s/epoch - 10ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 5s - loss: 0.0151 - val_loss: 0.0244 - 5s/epoch - 10ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 6s - loss: 0.0138 - val_loss: 0.0212 - 6s/epoch - 10ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 5s - loss: 0.0129 - val_loss: 0.0223 - 5s/epoch - 10ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 5s - loss: 0.0134 - val_loss: 0.0222 - 5s/epoch - 9ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 5s - loss: 0.0127 - val_loss: 0.0209 - 5s/epoch - 9ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 5s - loss: 0.0125 - val_loss: 0.0237 - 5s/epoch - 9ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 5s - loss: 0.0129 - val_loss: 0.0217 - 5s/epoch - 9ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 5s - loss: 0.0131 - val_loss: 0.0234 - 5s/epoch - 9ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 6s - loss: 0.0144 - val_loss: 0.0214 - 6s/epoch - 10ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 5s - loss: 0.0120 - val_loss: 0.0229 - 5s/epoch - 10ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 5s - loss: 0.0129 - val_loss: 0.0227 - 5s/epoch - 10ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 6s - loss: 0.0125 - val_loss: 0.0206 - 6s/epoch - 10ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 6s - loss: 0.0121 - val_loss: 0.0215 - 6s/epoch - 10ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 6s - loss: 0.0120 - val_loss: 0.0212 - 6s/epoch - 10ms/step\n",
      "Epoch 84/10000\n",
      "563/563 - 6s - loss: 0.0115 - val_loss: 0.0205 - 6s/epoch - 10ms/step\n",
      "Epoch 85/10000\n",
      "563/563 - 6s - loss: 0.0120 - val_loss: 0.0212 - 6s/epoch - 10ms/step\n",
      "Epoch 86/10000\n",
      "563/563 - 6s - loss: 0.0136 - val_loss: 0.0240 - 6s/epoch - 12ms/step\n",
      "Epoch 87/10000\n",
      "563/563 - 6s - loss: 0.0122 - val_loss: 0.0210 - 6s/epoch - 11ms/step\n",
      "Epoch 88/10000\n",
      "563/563 - 6s - loss: 0.0111 - val_loss: 0.0199 - 6s/epoch - 10ms/step\n",
      "Epoch 89/10000\n",
      "563/563 - 5s - loss: 0.0125 - val_loss: 0.0205 - 5s/epoch - 10ms/step\n",
      "Epoch 90/10000\n",
      "563/563 - 5s - loss: 0.0116 - val_loss: 0.0190 - 5s/epoch - 10ms/step\n",
      "Epoch 91/10000\n",
      "563/563 - 6s - loss: 0.0106 - val_loss: 0.0205 - 6s/epoch - 10ms/step\n",
      "Epoch 92/10000\n",
      "563/563 - 6s - loss: 0.0111 - val_loss: 0.0202 - 6s/epoch - 11ms/step\n",
      "Epoch 93/10000\n",
      "563/563 - 6s - loss: 0.0109 - val_loss: 0.0201 - 6s/epoch - 11ms/step\n",
      "Epoch 94/10000\n",
      "563/563 - 6s - loss: 0.0113 - val_loss: 0.0195 - 6s/epoch - 10ms/step\n",
      "Epoch 95/10000\n",
      "563/563 - 6s - loss: 0.0112 - val_loss: 0.0195 - 6s/epoch - 11ms/step\n",
      "Epoch 96/10000\n",
      "563/563 - 6s - loss: 0.0105 - val_loss: 0.0195 - 6s/epoch - 11ms/step\n",
      "Epoch 97/10000\n",
      "563/563 - 6s - loss: 0.0113 - val_loss: 0.0196 - 6s/epoch - 10ms/step\n",
      "Epoch 98/10000\n",
      "563/563 - 5s - loss: 0.0151 - val_loss: 0.0351 - 5s/epoch - 10ms/step\n",
      "Epoch 99/10000\n",
      "563/563 - 6s - loss: 0.0156 - val_loss: 0.0242 - 6s/epoch - 10ms/step\n",
      "Epoch 100/10000\n",
      "563/563 - 6s - loss: 0.0120 - val_loss: 0.0217 - 6s/epoch - 10ms/step\n",
      "csv file is saved to: ./result/40_20_1_100_2_60_att_seq2seq_gru_linear_0.5_1.csv\n",
      "\n",
      "\n",
      "1th iteration\n",
      "num_layers: 2\n",
      "num_neurons: 60\n",
      "model_type: datt_seq2seq_gru\n",
      "Epoch 1/10000\n",
      "563/563 - 8s - loss: 0.1926 - val_loss: 0.1857 - 8s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.1582 - val_loss: 0.1749 - 5s/epoch - 9ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.1459 - val_loss: 0.1489 - 5s/epoch - 10ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 6s - loss: 0.1353 - val_loss: 0.1423 - 6s/epoch - 10ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 6s - loss: 0.1263 - val_loss: 0.1351 - 6s/epoch - 10ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.1135 - val_loss: 0.1238 - 5s/epoch - 10ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 6s - loss: 0.1103 - val_loss: 0.1181 - 6s/epoch - 11ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 6s - loss: 0.1013 - val_loss: 0.1107 - 6s/epoch - 11ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 6s - loss: 0.0945 - val_loss: 0.1075 - 6s/epoch - 10ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.0900 - val_loss: 0.1142 - 5s/epoch - 9ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.0869 - val_loss: 0.0945 - 5s/epoch - 9ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.0802 - val_loss: 0.0913 - 5s/epoch - 9ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.0767 - val_loss: 0.0932 - 5s/epoch - 10ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.0732 - val_loss: 0.0823 - 5s/epoch - 10ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.0702 - val_loss: 0.0930 - 5s/epoch - 9ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.0693 - val_loss: 0.0797 - 5s/epoch - 10ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.0645 - val_loss: 0.0808 - 5s/epoch - 9ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.0616 - val_loss: 0.0741 - 5s/epoch - 9ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.0599 - val_loss: 0.0747 - 5s/epoch - 10ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.0572 - val_loss: 0.0684 - 5s/epoch - 9ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.0562 - val_loss: 0.0741 - 5s/epoch - 9ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.0538 - val_loss: 0.0741 - 5s/epoch - 9ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.0541 - val_loss: 0.0695 - 5s/epoch - 9ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.0492 - val_loss: 0.0646 - 5s/epoch - 9ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 6s - loss: 0.0491 - val_loss: 0.0728 - 6s/epoch - 10ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.0491 - val_loss: 0.0629 - 5s/epoch - 9ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.0461 - val_loss: 0.0587 - 5s/epoch - 9ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0440 - val_loss: 0.0560 - 5s/epoch - 9ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.0422 - val_loss: 0.0574 - 5s/epoch - 9ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.0408 - val_loss: 0.0566 - 5s/epoch - 9ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.0392 - val_loss: 0.0517 - 5s/epoch - 9ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.0404 - val_loss: 0.0592 - 5s/epoch - 9ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.0415 - val_loss: 0.0533 - 5s/epoch - 9ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.0386 - val_loss: 0.0614 - 5s/epoch - 9ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.0386 - val_loss: 0.0545 - 5s/epoch - 9ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0349 - val_loss: 0.0537 - 5s/epoch - 9ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0360 - val_loss: 0.0514 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0409 - val_loss: 0.0515 - 5s/epoch - 9ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.0355 - val_loss: 0.0485 - 5s/epoch - 9ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0333 - val_loss: 0.0685 - 5s/epoch - 9ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0357 - val_loss: 0.0493 - 5s/epoch - 9ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0313 - val_loss: 0.0621 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0321 - val_loss: 0.0406 - 5s/epoch - 9ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0295 - val_loss: 0.0415 - 5s/epoch - 9ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0277 - val_loss: 0.0377 - 5s/epoch - 9ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0344 - val_loss: 0.0485 - 5s/epoch - 9ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0308 - val_loss: 0.0440 - 5s/epoch - 9ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0269 - val_loss: 0.0403 - 5s/epoch - 9ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0268 - val_loss: 0.0442 - 5s/epoch - 10ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0331 - val_loss: 0.0498 - 5s/epoch - 9ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0487 - val_loss: 0.0583 - 5s/epoch - 9ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0378 - val_loss: 0.0466 - 5s/epoch - 9ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0290 - val_loss: 0.0429 - 5s/epoch - 9ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0280 - val_loss: 0.0463 - 5s/epoch - 9ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0292 - val_loss: 0.0420 - 5s/epoch - 9ms/step\n",
      "csv file is saved to: ./result/40_20_1_100_2_60_datt_seq2seq_gru_linear_0.5_1.csv\n",
      "\n",
      "\n",
      "2th iteration\n",
      "num_layers: 1\n",
      "num_neurons: 20\n",
      "model_type: seq2seq_gru\n",
      "Epoch 1/10000\n",
      "563/563 - 7s - loss: 0.1355 - val_loss: 0.1239 - 7s/epoch - 12ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 4s - loss: 0.1095 - val_loss: 0.1145 - 4s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 4s - loss: 0.1001 - val_loss: 0.1044 - 4s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 4s - loss: 0.0932 - val_loss: 0.0980 - 4s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 4s - loss: 0.0842 - val_loss: 0.0909 - 4s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.0802 - val_loss: 0.0856 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.0724 - val_loss: 0.0800 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.0681 - val_loss: 0.0803 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 4s - loss: 0.0642 - val_loss: 0.0725 - 4s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 4s - loss: 0.0608 - val_loss: 0.0664 - 4s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 4s - loss: 0.0566 - val_loss: 0.0703 - 4s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 4s - loss: 0.0535 - val_loss: 0.0618 - 4s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 4s - loss: 0.0491 - val_loss: 0.0637 - 4s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.0483 - val_loss: 0.0658 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.0449 - val_loss: 0.0542 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.0429 - val_loss: 0.0569 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.0406 - val_loss: 0.0502 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.0396 - val_loss: 0.0505 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 4s - loss: 0.0388 - val_loss: 0.0478 - 4s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 4s - loss: 0.0375 - val_loss: 0.0481 - 4s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 4s - loss: 0.0374 - val_loss: 0.0501 - 4s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 4s - loss: 0.0363 - val_loss: 0.0447 - 4s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 4s - loss: 0.0319 - val_loss: 0.0416 - 4s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 4s - loss: 0.0328 - val_loss: 0.0431 - 4s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.0332 - val_loss: 0.0417 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.0310 - val_loss: 0.0515 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.0312 - val_loss: 0.0380 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0324 - val_loss: 0.0405 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 4s - loss: 0.0295 - val_loss: 0.0407 - 4s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 4s - loss: 0.0290 - val_loss: 0.0345 - 4s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 4s - loss: 0.0272 - val_loss: 0.0378 - 4s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 4s - loss: 0.0256 - val_loss: 0.0304 - 4s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 4s - loss: 0.0267 - val_loss: 0.0349 - 4s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 4s - loss: 0.0260 - val_loss: 0.0357 - 4s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 4s - loss: 0.0256 - val_loss: 0.0327 - 4s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 4s - loss: 0.0239 - val_loss: 0.0350 - 4s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 4s - loss: 0.0227 - val_loss: 0.0372 - 4s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0237 - val_loss: 0.0374 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.0267 - val_loss: 0.0337 - 5s/epoch - 9ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0220 - val_loss: 0.0303 - 5s/epoch - 9ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0205 - val_loss: 0.0305 - 5s/epoch - 9ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0215 - val_loss: 0.0306 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0197 - val_loss: 0.0314 - 5s/epoch - 9ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0212 - val_loss: 0.0338 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0204 - val_loss: 0.0276 - 5s/epoch - 9ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0194 - val_loss: 0.0285 - 5s/epoch - 9ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0199 - val_loss: 0.0314 - 5s/epoch - 10ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0190 - val_loss: 0.0267 - 5s/epoch - 9ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0187 - val_loss: 0.0279 - 5s/epoch - 9ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0188 - val_loss: 0.0266 - 5s/epoch - 9ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0180 - val_loss: 0.0253 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0173 - val_loss: 0.0264 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0186 - val_loss: 0.0254 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0176 - val_loss: 0.0272 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0167 - val_loss: 0.0267 - 5s/epoch - 9ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0220 - val_loss: 0.0348 - 5s/epoch - 9ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0178 - val_loss: 0.0245 - 5s/epoch - 9ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0166 - val_loss: 0.0250 - 5s/epoch - 9ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0163 - val_loss: 0.0259 - 5s/epoch - 9ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0158 - val_loss: 0.0237 - 5s/epoch - 9ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0154 - val_loss: 0.0238 - 5s/epoch - 9ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 4s - loss: 0.0152 - val_loss: 0.0248 - 4s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 5s - loss: 0.0152 - val_loss: 0.0231 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0158 - val_loss: 0.0239 - 5s/epoch - 9ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0158 - val_loss: 0.0262 - 5s/epoch - 9ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0153 - val_loss: 0.0234 - 5s/epoch - 9ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0149 - val_loss: 0.0230 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 5s - loss: 0.0153 - val_loss: 0.0214 - 5s/epoch - 9ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 5s - loss: 0.0139 - val_loss: 0.0247 - 5s/epoch - 9ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 5s - loss: 0.0162 - val_loss: 0.0219 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 5s - loss: 0.0149 - val_loss: 0.0238 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 5s - loss: 0.0151 - val_loss: 0.0224 - 5s/epoch - 9ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 5s - loss: 0.0136 - val_loss: 0.0219 - 5s/epoch - 9ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 4s - loss: 0.0137 - val_loss: 0.0230 - 4s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 5s - loss: 0.0141 - val_loss: 0.0217 - 5s/epoch - 9ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 5s - loss: 0.0132 - val_loss: 0.0206 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 5s - loss: 0.0135 - val_loss: 0.0252 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 5s - loss: 0.0158 - val_loss: 0.0243 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 5s - loss: 0.0192 - val_loss: 0.0340 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 5s - loss: 0.0156 - val_loss: 0.0178 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 5s - loss: 0.0122 - val_loss: 0.0181 - 5s/epoch - 9ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 4s - loss: 0.0118 - val_loss: 0.0181 - 4s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 4s - loss: 0.0129 - val_loss: 0.0214 - 4s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "563/563 - 5s - loss: 0.0123 - val_loss: 0.0211 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "563/563 - 5s - loss: 0.0132 - val_loss: 0.0219 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "563/563 - 5s - loss: 0.0127 - val_loss: 0.0228 - 5s/epoch - 9ms/step\n",
      "Epoch 87/10000\n",
      "563/563 - 4s - loss: 0.0136 - val_loss: 0.0205 - 4s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "563/563 - 4s - loss: 0.0123 - val_loss: 0.0204 - 4s/epoch - 8ms/step\n",
      "Epoch 89/10000\n"
     ]
    }
   ],
   "source": [
    "target_list = cts_list\n",
    "\n",
    "# history size and future size\n",
    "history_size = 40\n",
    "future_size = 20\n",
    "step = 1\n",
    "                \n",
    "# variable selection\n",
    "history_var = process_var\n",
    "future_var = output_var\n",
    "\n",
    "history_num = len(history_var)\n",
    "future_num = len(future_var)\n",
    "\n",
    "# data to series\n",
    "history_series = []\n",
    "future_series = []\n",
    "for i in range(len(target_list)):\n",
    "    history, future = data2series(target_list[i], history_size, history_var, future_size, future_var,\n",
    "                                step, start_idx=0, end_idx=None)\n",
    "    if not i:\n",
    "        history_series = history\n",
    "        future_series = future\n",
    "    else:\n",
    "        history_series = np.concatenate([history_series, history], axis=0)\n",
    "        future_series = np.concatenate([future_series, future], axis=0)\n",
    "\n",
    "# supervised attention factor\n",
    "delta = 0.5\n",
    "att_type = 'linear'\n",
    "factor = rnn.super_attention(delta, future_size, future_num, att_type)\n",
    "        \n",
    "# test data split        \n",
    "test_size = 0.2\n",
    "test_num = -1\n",
    "\n",
    "# model structure\n",
    "num_layers = 1\n",
    "num_neurons = 100\n",
    "dense_layers_list = [1, 2]\n",
    "dense_neurons_list = [20, 40, 60]\n",
    "model_list = ['seq2seq_gru', 'att_seq2seq_gru', 'datt_seq2seq_gru']\n",
    "\n",
    "iteration_list = [x for x in range(1,6)]\n",
    "for iteration in iteration_list:\n",
    "    for dense_layers in dense_layers_list:\n",
    "        for dense_neurons in dense_neurons_list:\n",
    "            for model_type in model_list:\n",
    "                print(f\"{iteration}th iteration\")\n",
    "                print(f\"num_layers: {dense_layers}\")\n",
    "                print(f\"num_neurons: {dense_neurons}\")\n",
    "                print(f\"model_type: {model_type}\")\n",
    "                \n",
    "                # Dual-attention Seq2Seq model\n",
    "                RNN_model = rnn.RNN(history_series, history_var, future_series, future_var)\n",
    "                # TT split\n",
    "                RNN_model.train_test(test_size=test_size, test_num=test_num)\n",
    "                # TV split\n",
    "                valid_size = RNN_model.history_test.shape[0]/RNN_model.history_train.shape[0]\n",
    "                RNN_model.train_valid(valid_size=valid_size)\n",
    "                # scaling\n",
    "                RNN_model.scaling()\n",
    "                # modeling\n",
    "                RNN_model.build_model(num_layers=num_layers, num_neurons=num_neurons, dense_layers=dense_layers, dense_neurons=dense_neurons, model_type=model_type, factor=factor)\n",
    "                # training\n",
    "                model_num = iteration\n",
    "                model_name = f\"{history_size}_{future_size}_{num_layers}_{num_neurons}_{dense_layers}_{dense_neurons}_{model_type}_{att_type}_{delta}_{model_num}\"\n",
    "                if not exists(f\"./model/{model_name}.h5\"):\n",
    "                    RNN_model.train()\n",
    "                    RNN_model.save_model(f\"./model/{model_name}\", 'weights')\n",
    "                    \n",
    "                else:\n",
    "                    RNN_model.model.load_weights(f\"./model/{model_name}.h5\")\n",
    "                # test\n",
    "                test_result = RNN_model.test(False)\n",
    "                if not exists(f'./result/{model_name}.csv', ):\n",
    "                    savefile(test_result, './result', model_name)\n",
    "                print(\"\\n\")\n",
    "print('end opitmization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3cf56d631085086b1721b0064da1454dfad7e026414ec6e1cab7db73e55aa6df"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
