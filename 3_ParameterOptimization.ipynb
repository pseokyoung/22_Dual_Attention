{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"..\") \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.utility import loadfile, savefile, exists\n",
    "from src.dataprocessing import *\n",
    "from src import rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "input_var   = [\"FT-3061-2\", \"FT-3061-3\", \"FT-3061-4\", \"FT-3062-1\"]\n",
    "output_var  = [\"TT-3061-3\", \"TT-3061-5\", \"LT-3061-2\"]\n",
    "process_var = input_var + output_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_1.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_2.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_3.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_4.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_5.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_6.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_7.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_8.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_9.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_10.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_11.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_12.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_13.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_14.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_15.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_16.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_17.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_18.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_19.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_20.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_21.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_22.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_23.csv\n",
      "csv file is loaded from ./data/3_continuous/cts_100/dataset 100_24.csv\n"
     ]
    }
   ],
   "source": [
    "min_len = 100\n",
    "continuous_path = './data/3_continuous'\n",
    "\n",
    "cts_list = []\n",
    "i = 1\n",
    "while exists(continuous_path, f\"cts_{min_len}/dataset {min_len}_{i}\", 'csv'):\n",
    "    cts_df = loadfile(continuous_path, f\"cts_{min_len}/dataset {min_len}_{i}\", 'csv')\n",
    "    cts_list.append(cts_df)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th iteration\n",
      "history size: 10\n",
      "future size: 10\n",
      "model is loaded from: ./model/10_10_1_50_1_50_datt_seq2seq_gru_1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.900671</td>\n",
       "      <td>0.947878</td>\n",
       "      <td>0.969716</td>\n",
       "      <td>0.939422</td>\n",
       "      <td>5.117202</td>\n",
       "      <td>3.992389</td>\n",
       "      <td>2.71327</td>\n",
       "      <td>3.940954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.799622</td>\n",
       "      <td>0.894724</td>\n",
       "      <td>0.944357</td>\n",
       "      <td>0.879568</td>\n",
       "      <td>7.221528</td>\n",
       "      <td>5.674612</td>\n",
       "      <td>3.676933</td>\n",
       "      <td>5.524358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.676867</td>\n",
       "      <td>0.829835</td>\n",
       "      <td>0.906914</td>\n",
       "      <td>0.804539</td>\n",
       "      <td>9.045753</td>\n",
       "      <td>7.215126</td>\n",
       "      <td>4.754602</td>\n",
       "      <td>7.00516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.551501</td>\n",
       "      <td>0.78096</td>\n",
       "      <td>0.868805</td>\n",
       "      <td>0.733755</td>\n",
       "      <td>10.515697</td>\n",
       "      <td>8.187118</td>\n",
       "      <td>5.643127</td>\n",
       "      <td>8.115314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.433028</td>\n",
       "      <td>0.74443</td>\n",
       "      <td>0.835415</td>\n",
       "      <td>0.670957</td>\n",
       "      <td>11.707734</td>\n",
       "      <td>8.844738</td>\n",
       "      <td>6.318536</td>\n",
       "      <td>8.957003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.319931</td>\n",
       "      <td>0.717127</td>\n",
       "      <td>0.805756</td>\n",
       "      <td>0.614272</td>\n",
       "      <td>12.698957</td>\n",
       "      <td>9.306761</td>\n",
       "      <td>6.862569</td>\n",
       "      <td>9.622762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.215111</td>\n",
       "      <td>0.696835</td>\n",
       "      <td>0.778403</td>\n",
       "      <td>0.56345</td>\n",
       "      <td>13.51328</td>\n",
       "      <td>9.635625</td>\n",
       "      <td>7.328513</td>\n",
       "      <td>10.159139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.122681</td>\n",
       "      <td>0.682295</td>\n",
       "      <td>0.756525</td>\n",
       "      <td>0.5205</td>\n",
       "      <td>14.252891</td>\n",
       "      <td>9.864339</td>\n",
       "      <td>7.681024</td>\n",
       "      <td>10.599418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.045372</td>\n",
       "      <td>0.671004</td>\n",
       "      <td>0.741679</td>\n",
       "      <td>0.486019</td>\n",
       "      <td>14.884819</td>\n",
       "      <td>10.038568</td>\n",
       "      <td>7.911127</td>\n",
       "      <td>10.944838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.016795</td>\n",
       "      <td>0.656142</td>\n",
       "      <td>0.730816</td>\n",
       "      <td>0.456721</td>\n",
       "      <td>15.379879</td>\n",
       "      <td>10.263135</td>\n",
       "      <td>8.075168</td>\n",
       "      <td>11.239394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.404799</td>\n",
       "      <td>0.762123</td>\n",
       "      <td>0.833839</td>\n",
       "      <td>0.66692</td>\n",
       "      <td>11.433774</td>\n",
       "      <td>8.302241</td>\n",
       "      <td>6.096487</td>\n",
       "      <td>8.610834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE     nRMSE   \n",
       "0      0.900671  0.947878  0.969716  0.939422   5.117202   3.992389   2.71327   \n",
       "1      0.799622  0.894724  0.944357  0.879568   7.221528   5.674612  3.676933   \n",
       "2      0.676867  0.829835  0.906914  0.804539   9.045753   7.215126  4.754602   \n",
       "3      0.551501   0.78096  0.868805  0.733755  10.515697   8.187118  5.643127   \n",
       "4      0.433028   0.74443  0.835415  0.670957  11.707734   8.844738  6.318536   \n",
       "5      0.319931  0.717127  0.805756  0.614272  12.698957   9.306761  6.862569   \n",
       "6      0.215111  0.696835  0.778403   0.56345   13.51328   9.635625  7.328513   \n",
       "7      0.122681  0.682295  0.756525    0.5205  14.252891   9.864339  7.681024   \n",
       "8      0.045372  0.671004  0.741679  0.486019  14.884819  10.038568  7.911127   \n",
       "9     -0.016795  0.656142  0.730816  0.456721  15.379879  10.263135  8.075168   \n",
       "mean   0.404799  0.762123  0.833839   0.66692  11.433774   8.302241  6.096487   \n",
       "\n",
       "            mean  \n",
       "index      nRMSE  \n",
       "0       3.940954  \n",
       "1       5.524358  \n",
       "2        7.00516  \n",
       "3       8.115314  \n",
       "4       8.957003  \n",
       "5       9.622762  \n",
       "6      10.159139  \n",
       "7      10.599418  \n",
       "8      10.944838  \n",
       "9      11.239394  \n",
       "mean    8.610834  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end training\n",
      "\n",
      "history size: 10\n",
      "future size: 20\n",
      "model is loaded from: ./model/10_20_1_50_1_50_datt_seq2seq_gru_1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.877108</td>\n",
       "      <td>0.919942</td>\n",
       "      <td>0.941436</td>\n",
       "      <td>0.912829</td>\n",
       "      <td>6.924128</td>\n",
       "      <td>4.943281</td>\n",
       "      <td>3.765284</td>\n",
       "      <td>5.210898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.823525</td>\n",
       "      <td>0.879394</td>\n",
       "      <td>0.917997</td>\n",
       "      <td>0.873639</td>\n",
       "      <td>8.146417</td>\n",
       "      <td>6.069187</td>\n",
       "      <td>4.455002</td>\n",
       "      <td>6.223535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.735699</td>\n",
       "      <td>0.845673</td>\n",
       "      <td>0.882344</td>\n",
       "      <td>0.821239</td>\n",
       "      <td>9.534244</td>\n",
       "      <td>6.867994</td>\n",
       "      <td>5.335568</td>\n",
       "      <td>7.245935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.630755</td>\n",
       "      <td>0.816324</td>\n",
       "      <td>0.842684</td>\n",
       "      <td>0.763254</td>\n",
       "      <td>10.892952</td>\n",
       "      <td>7.49522</td>\n",
       "      <td>6.168187</td>\n",
       "      <td>8.185453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.523513</td>\n",
       "      <td>0.789441</td>\n",
       "      <td>0.809465</td>\n",
       "      <td>0.707473</td>\n",
       "      <td>12.176909</td>\n",
       "      <td>8.028594</td>\n",
       "      <td>6.786194</td>\n",
       "      <td>8.997232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.419288</td>\n",
       "      <td>0.766111</td>\n",
       "      <td>0.787075</td>\n",
       "      <td>0.657491</td>\n",
       "      <td>13.398944</td>\n",
       "      <td>8.465843</td>\n",
       "      <td>7.17125</td>\n",
       "      <td>9.678679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.321144</td>\n",
       "      <td>0.74225</td>\n",
       "      <td>0.77384</td>\n",
       "      <td>0.612411</td>\n",
       "      <td>14.204363</td>\n",
       "      <td>8.8899</td>\n",
       "      <td>7.38873</td>\n",
       "      <td>10.160998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.231333</td>\n",
       "      <td>0.715846</td>\n",
       "      <td>0.764961</td>\n",
       "      <td>0.570713</td>\n",
       "      <td>14.825685</td>\n",
       "      <td>9.337125</td>\n",
       "      <td>7.530679</td>\n",
       "      <td>10.564497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.151558</td>\n",
       "      <td>0.688594</td>\n",
       "      <td>0.755916</td>\n",
       "      <td>0.532023</td>\n",
       "      <td>15.34495</td>\n",
       "      <td>9.777842</td>\n",
       "      <td>7.672545</td>\n",
       "      <td>10.931779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.083268</td>\n",
       "      <td>0.663302</td>\n",
       "      <td>0.744894</td>\n",
       "      <td>0.497155</td>\n",
       "      <td>15.718665</td>\n",
       "      <td>10.169101</td>\n",
       "      <td>7.841672</td>\n",
       "      <td>11.243146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.024289</td>\n",
       "      <td>0.640644</td>\n",
       "      <td>0.729393</td>\n",
       "      <td>0.464775</td>\n",
       "      <td>16.04789</td>\n",
       "      <td>10.507431</td>\n",
       "      <td>8.074582</td>\n",
       "      <td>11.543301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.030112</td>\n",
       "      <td>0.619778</td>\n",
       "      <td>0.708702</td>\n",
       "      <td>0.432789</td>\n",
       "      <td>16.384415</td>\n",
       "      <td>10.80954</td>\n",
       "      <td>8.376072</td>\n",
       "      <td>11.856676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.078209</td>\n",
       "      <td>0.597773</td>\n",
       "      <td>0.687198</td>\n",
       "      <td>0.402254</td>\n",
       "      <td>16.534638</td>\n",
       "      <td>11.119073</td>\n",
       "      <td>8.677936</td>\n",
       "      <td>12.110549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.121658</td>\n",
       "      <td>0.575754</td>\n",
       "      <td>0.667806</td>\n",
       "      <td>0.373968</td>\n",
       "      <td>16.640237</td>\n",
       "      <td>11.421121</td>\n",
       "      <td>8.940942</td>\n",
       "      <td>12.3341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.162473</td>\n",
       "      <td>0.556228</td>\n",
       "      <td>0.652198</td>\n",
       "      <td>0.348651</td>\n",
       "      <td>16.772046</td>\n",
       "      <td>11.68262</td>\n",
       "      <td>9.146378</td>\n",
       "      <td>12.533682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.198146</td>\n",
       "      <td>0.538253</td>\n",
       "      <td>0.63921</td>\n",
       "      <td>0.326439</td>\n",
       "      <td>16.85956</td>\n",
       "      <td>11.918649</td>\n",
       "      <td>9.313857</td>\n",
       "      <td>12.697355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.228128</td>\n",
       "      <td>0.520616</td>\n",
       "      <td>0.628364</td>\n",
       "      <td>0.306951</td>\n",
       "      <td>16.903784</td>\n",
       "      <td>12.144762</td>\n",
       "      <td>9.451744</td>\n",
       "      <td>12.83343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.252536</td>\n",
       "      <td>0.502314</td>\n",
       "      <td>0.61772</td>\n",
       "      <td>0.289166</td>\n",
       "      <td>17.027873</td>\n",
       "      <td>12.37468</td>\n",
       "      <td>9.585709</td>\n",
       "      <td>12.996088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.272418</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>0.604767</td>\n",
       "      <td>0.272241</td>\n",
       "      <td>17.179492</td>\n",
       "      <td>12.5962</td>\n",
       "      <td>9.746381</td>\n",
       "      <td>13.174024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.29059</td>\n",
       "      <td>0.464965</td>\n",
       "      <td>0.58903</td>\n",
       "      <td>0.254468</td>\n",
       "      <td>17.318634</td>\n",
       "      <td>12.831303</td>\n",
       "      <td>9.938271</td>\n",
       "      <td>13.362736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.15936</td>\n",
       "      <td>0.666379</td>\n",
       "      <td>0.73725</td>\n",
       "      <td>0.520996</td>\n",
       "      <td>14.441791</td>\n",
       "      <td>9.872473</td>\n",
       "      <td>7.768349</td>\n",
       "      <td>10.694205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE     nRMSE   \n",
       "0      0.877108  0.919942  0.941436  0.912829   6.924128   4.943281  3.765284   \n",
       "1      0.823525  0.879394  0.917997  0.873639   8.146417   6.069187  4.455002   \n",
       "2      0.735699  0.845673  0.882344  0.821239   9.534244   6.867994  5.335568   \n",
       "3      0.630755  0.816324  0.842684  0.763254  10.892952    7.49522  6.168187   \n",
       "4      0.523513  0.789441  0.809465  0.707473  12.176909   8.028594  6.786194   \n",
       "5      0.419288  0.766111  0.787075  0.657491  13.398944   8.465843   7.17125   \n",
       "6      0.321144   0.74225   0.77384  0.612411  14.204363     8.8899   7.38873   \n",
       "7      0.231333  0.715846  0.764961  0.570713  14.825685   9.337125  7.530679   \n",
       "8      0.151558  0.688594  0.755916  0.532023   15.34495   9.777842  7.672545   \n",
       "9      0.083268  0.663302  0.744894  0.497155  15.718665  10.169101  7.841672   \n",
       "10     0.024289  0.640644  0.729393  0.464775   16.04789  10.507431  8.074582   \n",
       "11    -0.030112  0.619778  0.708702  0.432789  16.384415   10.80954  8.376072   \n",
       "12    -0.078209  0.597773  0.687198  0.402254  16.534638  11.119073  8.677936   \n",
       "13    -0.121658  0.575754  0.667806  0.373968  16.640237  11.421121  8.940942   \n",
       "14    -0.162473  0.556228  0.652198  0.348651  16.772046   11.68262  9.146378   \n",
       "15    -0.198146  0.538253   0.63921  0.326439   16.85956  11.918649  9.313857   \n",
       "16    -0.228128  0.520616  0.628364  0.306951  16.903784  12.144762  9.451744   \n",
       "17    -0.252536  0.502314   0.61772  0.289166  17.027873   12.37468  9.585709   \n",
       "18    -0.272418  0.484375  0.604767  0.272241  17.179492    12.5962  9.746381   \n",
       "19     -0.29059  0.464965   0.58903  0.254468  17.318634  12.831303  9.938271   \n",
       "mean    0.15936  0.666379   0.73725  0.520996  14.441791   9.872473  7.768349   \n",
       "\n",
       "            mean  \n",
       "index      nRMSE  \n",
       "0       5.210898  \n",
       "1       6.223535  \n",
       "2       7.245935  \n",
       "3       8.185453  \n",
       "4       8.997232  \n",
       "5       9.678679  \n",
       "6      10.160998  \n",
       "7      10.564497  \n",
       "8      10.931779  \n",
       "9      11.243146  \n",
       "10     11.543301  \n",
       "11     11.856676  \n",
       "12     12.110549  \n",
       "13       12.3341  \n",
       "14     12.533682  \n",
       "15     12.697355  \n",
       "16      12.83343  \n",
       "17     12.996088  \n",
       "18     13.174024  \n",
       "19     13.362736  \n",
       "mean   10.694205  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end training\n",
      "\n",
      "history size: 10\n",
      "future size: 30\n",
      "Epoch 1/10000\n",
      "572/572 - 8s - loss: 0.3846 - val_loss: 0.3025 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "572/572 - 5s - loss: 0.2971 - val_loss: 0.2776 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "572/572 - 4s - loss: 0.2764 - val_loss: 0.2656 - 4s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "572/572 - 5s - loss: 0.2599 - val_loss: 0.2521 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "572/572 - 6s - loss: 0.2470 - val_loss: 0.2455 - 6s/epoch - 10ms/step\n",
      "Epoch 6/10000\n",
      "572/572 - 5s - loss: 0.2359 - val_loss: 0.2326 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "572/572 - 4s - loss: 0.2279 - val_loss: 0.2245 - 4s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "572/572 - 4s - loss: 0.2204 - val_loss: 0.2144 - 4s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "572/572 - 5s - loss: 0.2129 - val_loss: 0.2212 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "572/572 - 5s - loss: 0.2064 - val_loss: 0.2101 - 5s/epoch - 9ms/step\n",
      "Epoch 11/10000\n",
      "572/572 - 6s - loss: 0.1978 - val_loss: 0.2055 - 6s/epoch - 11ms/step\n",
      "Epoch 12/10000\n",
      "572/572 - 5s - loss: 0.1934 - val_loss: 0.1910 - 5s/epoch - 9ms/step\n",
      "Epoch 13/10000\n",
      "572/572 - 5s - loss: 0.1894 - val_loss: 0.1888 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "572/572 - 5s - loss: 0.1801 - val_loss: 0.1941 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "572/572 - 5s - loss: 0.1774 - val_loss: 0.1842 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "572/572 - 5s - loss: 0.1732 - val_loss: 0.1747 - 5s/epoch - 9ms/step\n",
      "Epoch 17/10000\n",
      "572/572 - 5s - loss: 0.1664 - val_loss: 0.1741 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "572/572 - 5s - loss: 0.1671 - val_loss: 0.1658 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "572/572 - 5s - loss: 0.1601 - val_loss: 0.1666 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "572/572 - 5s - loss: 0.1600 - val_loss: 0.1669 - 5s/epoch - 9ms/step\n",
      "Epoch 21/10000\n",
      "572/572 - 5s - loss: 0.1552 - val_loss: 0.1654 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "572/572 - 5s - loss: 0.1530 - val_loss: 0.1632 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "572/572 - 5s - loss: 0.1495 - val_loss: 0.1731 - 5s/epoch - 9ms/step\n",
      "Epoch 24/10000\n",
      "572/572 - 5s - loss: 0.1473 - val_loss: 0.1556 - 5s/epoch - 9ms/step\n",
      "Epoch 25/10000\n",
      "572/572 - 5s - loss: 0.1433 - val_loss: 0.1520 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "572/572 - 5s - loss: 0.1437 - val_loss: 0.1614 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "572/572 - 5s - loss: 0.1419 - val_loss: 0.1490 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "572/572 - 5s - loss: 0.1357 - val_loss: 0.1456 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "572/572 - 5s - loss: 0.1343 - val_loss: 0.1450 - 5s/epoch - 9ms/step\n",
      "Epoch 30/10000\n",
      "572/572 - 4s - loss: 0.1399 - val_loss: 0.1523 - 4s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "572/572 - 5s - loss: 0.1327 - val_loss: 0.1468 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "572/572 - 5s - loss: 0.1321 - val_loss: 0.1429 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "572/572 - 5s - loss: 0.1345 - val_loss: 0.1448 - 5s/epoch - 9ms/step\n",
      "Epoch 34/10000\n",
      "572/572 - 5s - loss: 0.1334 - val_loss: 0.1408 - 5s/epoch - 9ms/step\n",
      "Epoch 35/10000\n",
      "572/572 - 5s - loss: 0.1257 - val_loss: 0.1434 - 5s/epoch - 9ms/step\n",
      "Epoch 36/10000\n",
      "572/572 - 5s - loss: 0.1265 - val_loss: 0.1402 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "572/572 - 5s - loss: 0.1240 - val_loss: 0.1438 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "572/572 - 5s - loss: 0.1267 - val_loss: 0.1371 - 5s/epoch - 9ms/step\n",
      "Epoch 39/10000\n",
      "572/572 - 5s - loss: 0.1249 - val_loss: 0.1320 - 5s/epoch - 9ms/step\n",
      "Epoch 40/10000\n",
      "572/572 - 5s - loss: 0.1209 - val_loss: 0.1352 - 5s/epoch - 9ms/step\n",
      "Epoch 41/10000\n",
      "572/572 - 5s - loss: 0.1191 - val_loss: 0.1386 - 5s/epoch - 9ms/step\n",
      "Epoch 42/10000\n",
      "572/572 - 6s - loss: 0.1285 - val_loss: 0.1329 - 6s/epoch - 10ms/step\n",
      "Epoch 43/10000\n",
      "572/572 - 6s - loss: 0.1211 - val_loss: 0.1264 - 6s/epoch - 10ms/step\n",
      "Epoch 44/10000\n",
      "572/572 - 5s - loss: 0.1156 - val_loss: 0.1259 - 5s/epoch - 9ms/step\n",
      "Epoch 45/10000\n",
      "572/572 - 5s - loss: 0.1169 - val_loss: 0.1338 - 5s/epoch - 9ms/step\n",
      "Epoch 46/10000\n",
      "572/572 - 5s - loss: 0.1143 - val_loss: 0.1242 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "572/572 - 5s - loss: 0.1143 - val_loss: 0.1234 - 5s/epoch - 9ms/step\n",
      "Epoch 48/10000\n",
      "572/572 - 5s - loss: 0.1128 - val_loss: 0.1284 - 5s/epoch - 9ms/step\n",
      "Epoch 49/10000\n",
      "572/572 - 5s - loss: 0.1133 - val_loss: 0.1328 - 5s/epoch - 9ms/step\n",
      "Epoch 50/10000\n",
      "572/572 - 6s - loss: 0.1181 - val_loss: 0.1283 - 6s/epoch - 11ms/step\n",
      "Epoch 51/10000\n",
      "572/572 - 6s - loss: 0.1161 - val_loss: 0.1204 - 6s/epoch - 10ms/step\n",
      "Epoch 52/10000\n",
      "572/572 - 5s - loss: 0.1094 - val_loss: 0.1238 - 5s/epoch - 9ms/step\n",
      "Epoch 53/10000\n",
      "572/572 - 5s - loss: 0.1085 - val_loss: 0.1194 - 5s/epoch - 9ms/step\n",
      "Epoch 54/10000\n",
      "572/572 - 5s - loss: 0.1075 - val_loss: 0.1289 - 5s/epoch - 9ms/step\n",
      "Epoch 55/10000\n",
      "572/572 - 5s - loss: 0.1124 - val_loss: 0.1233 - 5s/epoch - 9ms/step\n",
      "Epoch 56/10000\n",
      "572/572 - 5s - loss: 0.1094 - val_loss: 0.1468 - 5s/epoch - 9ms/step\n",
      "Epoch 57/10000\n",
      "572/572 - 5s - loss: 0.1064 - val_loss: 0.1152 - 5s/epoch - 9ms/step\n",
      "Epoch 58/10000\n",
      "572/572 - 6s - loss: 0.1062 - val_loss: 0.1133 - 6s/epoch - 10ms/step\n",
      "Epoch 59/10000\n",
      "572/572 - 5s - loss: 0.1031 - val_loss: 0.1214 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "572/572 - 5s - loss: 0.1042 - val_loss: 0.1156 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "572/572 - 5s - loss: 0.1077 - val_loss: 0.1156 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "572/572 - 5s - loss: 0.1021 - val_loss: 0.1195 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "572/572 - 5s - loss: 0.1037 - val_loss: 0.1192 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "572/572 - 5s - loss: 0.1018 - val_loss: 0.1154 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "572/572 - 5s - loss: 0.0998 - val_loss: 0.1197 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "572/572 - 5s - loss: 0.1022 - val_loss: 0.1154 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "572/572 - 5s - loss: 0.0982 - val_loss: 0.1103 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "572/572 - 5s - loss: 0.1024 - val_loss: 0.1236 - 5s/epoch - 9ms/step\n",
      "Epoch 69/10000\n",
      "572/572 - 5s - loss: 0.1027 - val_loss: 0.1157 - 5s/epoch - 9ms/step\n",
      "Epoch 70/10000\n",
      "572/572 - 5s - loss: 0.0988 - val_loss: 0.1158 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "572/572 - 5s - loss: 0.0976 - val_loss: 0.1066 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "572/572 - 5s - loss: 0.0975 - val_loss: 0.1097 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "572/572 - 5s - loss: 0.0983 - val_loss: 0.1050 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "572/572 - 5s - loss: 0.0918 - val_loss: 0.1084 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "572/572 - 5s - loss: 0.0961 - val_loss: 0.1109 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "572/572 - 5s - loss: 0.1078 - val_loss: 0.1267 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "572/572 - 5s - loss: 0.0981 - val_loss: 0.1082 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "572/572 - 5s - loss: 0.0940 - val_loss: 0.1110 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "572/572 - 5s - loss: 0.0928 - val_loss: 0.1083 - 5s/epoch - 9ms/step\n",
      "Epoch 80/10000\n",
      "572/572 - 5s - loss: 0.0938 - val_loss: 0.1050 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "572/572 - 5s - loss: 0.0905 - val_loss: 0.1125 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "572/572 - 5s - loss: 0.0910 - val_loss: 0.1066 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "572/572 - 5s - loss: 0.0913 - val_loss: 0.1051 - 5s/epoch - 9ms/step\n",
      "Epoch 84/10000\n",
      "572/572 - 5s - loss: 0.0933 - val_loss: 0.1039 - 5s/epoch - 9ms/step\n",
      "Epoch 85/10000\n",
      "572/572 - 5s - loss: 0.0872 - val_loss: 0.1173 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "572/572 - 5s - loss: 0.0892 - val_loss: 0.1059 - 5s/epoch - 9ms/step\n",
      "Epoch 87/10000\n",
      "572/572 - 5s - loss: 0.0926 - val_loss: 0.1131 - 5s/epoch - 9ms/step\n",
      "Epoch 88/10000\n",
      "572/572 - 5s - loss: 0.0889 - val_loss: 0.1059 - 5s/epoch - 9ms/step\n",
      "Epoch 89/10000\n",
      "572/572 - 5s - loss: 0.0869 - val_loss: 0.1016 - 5s/epoch - 9ms/step\n",
      "Epoch 90/10000\n",
      "572/572 - 5s - loss: 0.0842 - val_loss: 0.1018 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "572/572 - 5s - loss: 0.0867 - val_loss: 0.1016 - 5s/epoch - 9ms/step\n",
      "Epoch 92/10000\n",
      "572/572 - 5s - loss: 0.0879 - val_loss: 0.1033 - 5s/epoch - 9ms/step\n",
      "Epoch 93/10000\n",
      "572/572 - 5s - loss: 0.0857 - val_loss: 0.1060 - 5s/epoch - 9ms/step\n",
      "Epoch 94/10000\n",
      "572/572 - 5s - loss: 0.0844 - val_loss: 0.1013 - 5s/epoch - 9ms/step\n",
      "Epoch 95/10000\n",
      "572/572 - 5s - loss: 0.0869 - val_loss: 0.1012 - 5s/epoch - 9ms/step\n",
      "Epoch 96/10000\n",
      "572/572 - 5s - loss: 0.0863 - val_loss: 0.1092 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "572/572 - 5s - loss: 0.0857 - val_loss: 0.1043 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "572/572 - 5s - loss: 0.0859 - val_loss: 0.1035 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "572/572 - 5s - loss: 0.0843 - val_loss: 0.0988 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "572/572 - 5s - loss: 0.0863 - val_loss: 0.0975 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "572/572 - 5s - loss: 0.0849 - val_loss: 0.0938 - 5s/epoch - 9ms/step\n",
      "Epoch 102/10000\n",
      "572/572 - 5s - loss: 0.0805 - val_loss: 0.0960 - 5s/epoch - 9ms/step\n",
      "Epoch 103/10000\n",
      "572/572 - 5s - loss: 0.0785 - val_loss: 0.0952 - 5s/epoch - 9ms/step\n",
      "Epoch 104/10000\n",
      "572/572 - 5s - loss: 0.0814 - val_loss: 0.1081 - 5s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "572/572 - 5s - loss: 0.0857 - val_loss: 0.0952 - 5s/epoch - 9ms/step\n",
      "Epoch 106/10000\n",
      "572/572 - 5s - loss: 0.0849 - val_loss: 0.1050 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "572/572 - 5s - loss: 0.0892 - val_loss: 0.0995 - 5s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "572/572 - 5s - loss: 0.0784 - val_loss: 0.0955 - 5s/epoch - 9ms/step\n",
      "Epoch 109/10000\n",
      "572/572 - 5s - loss: 0.0801 - val_loss: 0.0992 - 5s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "572/572 - 5s - loss: 0.0782 - val_loss: 0.0970 - 5s/epoch - 9ms/step\n",
      "Epoch 111/10000\n",
      "572/572 - 5s - loss: 0.0810 - val_loss: 0.0938 - 5s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "572/572 - 5s - loss: 0.0761 - val_loss: 0.1037 - 5s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "572/572 - 5s - loss: 0.0814 - val_loss: 0.0987 - 5s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "572/572 - 5s - loss: 0.0809 - val_loss: 0.0935 - 5s/epoch - 9ms/step\n",
      "Epoch 115/10000\n",
      "572/572 - 5s - loss: 0.0772 - val_loss: 0.0924 - 5s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "572/572 - 5s - loss: 0.0796 - val_loss: 0.0953 - 5s/epoch - 9ms/step\n",
      "Epoch 117/10000\n",
      "572/572 - 5s - loss: 0.0760 - val_loss: 0.0944 - 5s/epoch - 8ms/step\n",
      "Epoch 118/10000\n",
      "572/572 - 5s - loss: 0.0760 - val_loss: 0.1004 - 5s/epoch - 8ms/step\n",
      "Epoch 119/10000\n",
      "572/572 - 5s - loss: 0.0786 - val_loss: 0.0980 - 5s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "572/572 - 5s - loss: 0.0813 - val_loss: 0.1001 - 5s/epoch - 8ms/step\n",
      "Epoch 121/10000\n",
      "572/572 - 5s - loss: 0.0741 - val_loss: 0.0944 - 5s/epoch - 8ms/step\n",
      "Epoch 122/10000\n",
      "572/572 - 5s - loss: 0.0751 - val_loss: 0.0946 - 5s/epoch - 8ms/step\n",
      "Epoch 123/10000\n",
      "572/572 - 5s - loss: 0.0750 - val_loss: 0.1095 - 5s/epoch - 8ms/step\n",
      "Epoch 124/10000\n",
      "572/572 - 5s - loss: 0.0754 - val_loss: 0.0919 - 5s/epoch - 8ms/step\n",
      "Epoch 125/10000\n",
      "572/572 - 5s - loss: 0.0768 - val_loss: 0.0995 - 5s/epoch - 8ms/step\n",
      "Epoch 126/10000\n",
      "572/572 - 5s - loss: 0.0765 - val_loss: 0.0905 - 5s/epoch - 8ms/step\n",
      "Epoch 127/10000\n",
      "572/572 - 5s - loss: 0.0778 - val_loss: 0.1056 - 5s/epoch - 8ms/step\n",
      "Epoch 128/10000\n",
      "572/572 - 5s - loss: 0.0736 - val_loss: 0.0957 - 5s/epoch - 8ms/step\n",
      "Epoch 129/10000\n",
      "572/572 - 5s - loss: 0.0747 - val_loss: 0.0921 - 5s/epoch - 8ms/step\n",
      "Epoch 130/10000\n",
      "572/572 - 5s - loss: 0.0732 - val_loss: 0.0934 - 5s/epoch - 8ms/step\n",
      "Epoch 131/10000\n",
      "572/572 - 5s - loss: 0.0759 - val_loss: 0.0969 - 5s/epoch - 8ms/step\n",
      "Epoch 132/10000\n",
      "572/572 - 5s - loss: 0.0730 - val_loss: 0.0893 - 5s/epoch - 8ms/step\n",
      "Epoch 133/10000\n",
      "572/572 - 5s - loss: 0.0737 - val_loss: 0.0941 - 5s/epoch - 9ms/step\n",
      "Epoch 134/10000\n",
      "572/572 - 5s - loss: 0.0742 - val_loss: 0.1008 - 5s/epoch - 8ms/step\n",
      "Epoch 135/10000\n",
      "572/572 - 5s - loss: 0.0773 - val_loss: 0.1250 - 5s/epoch - 8ms/step\n",
      "Epoch 136/10000\n",
      "572/572 - 5s - loss: 0.0803 - val_loss: 0.0898 - 5s/epoch - 8ms/step\n",
      "Epoch 137/10000\n",
      "572/572 - 5s - loss: 0.0768 - val_loss: 0.0948 - 5s/epoch - 8ms/step\n",
      "Epoch 138/10000\n",
      "572/572 - 5s - loss: 0.0712 - val_loss: 0.0899 - 5s/epoch - 9ms/step\n",
      "Epoch 139/10000\n",
      "572/572 - 5s - loss: 0.0713 - val_loss: 0.0959 - 5s/epoch - 8ms/step\n",
      "Epoch 140/10000\n",
      "572/572 - 5s - loss: 0.0748 - val_loss: 0.0922 - 5s/epoch - 8ms/step\n",
      "Epoch 141/10000\n",
      "572/572 - 5s - loss: 0.0763 - val_loss: 0.0872 - 5s/epoch - 8ms/step\n",
      "Epoch 142/10000\n",
      "572/572 - 5s - loss: 0.0808 - val_loss: 0.0932 - 5s/epoch - 8ms/step\n",
      "Epoch 143/10000\n",
      "572/572 - 5s - loss: 0.0716 - val_loss: 0.0917 - 5s/epoch - 8ms/step\n",
      "Epoch 144/10000\n",
      "572/572 - 5s - loss: 0.0714 - val_loss: 0.0884 - 5s/epoch - 8ms/step\n",
      "Epoch 145/10000\n",
      "572/572 - 5s - loss: 0.0688 - val_loss: 0.0894 - 5s/epoch - 8ms/step\n",
      "Epoch 146/10000\n",
      "572/572 - 5s - loss: 0.0698 - val_loss: 0.0927 - 5s/epoch - 8ms/step\n",
      "Epoch 147/10000\n",
      "572/572 - 5s - loss: 0.0706 - val_loss: 0.0934 - 5s/epoch - 8ms/step\n",
      "Epoch 148/10000\n",
      "572/572 - 5s - loss: 0.0719 - val_loss: 0.0890 - 5s/epoch - 8ms/step\n",
      "Epoch 149/10000\n",
      "572/572 - 5s - loss: 0.0699 - val_loss: 0.0914 - 5s/epoch - 8ms/step\n",
      "Epoch 150/10000\n",
      "572/572 - 5s - loss: 0.0698 - val_loss: 0.0918 - 5s/epoch - 8ms/step\n",
      "Epoch 151/10000\n",
      "572/572 - 5s - loss: 0.0764 - val_loss: 0.0952 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_8_layer_call_fn, gru_cell_8_layer_call_and_return_conditional_losses, gru_cell_9_layer_call_fn, gru_cell_9_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_30_1_50_1_50_datt_seq2seq_gru_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_30_1_50_1_50_datt_seq2seq_gru_1\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000294D88631F0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000294D8832100> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.870886</td>\n",
       "      <td>0.915074</td>\n",
       "      <td>0.93305</td>\n",
       "      <td>0.906337</td>\n",
       "      <td>7.093633</td>\n",
       "      <td>5.090365</td>\n",
       "      <td>4.010632</td>\n",
       "      <td>5.39821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.800189</td>\n",
       "      <td>0.886922</td>\n",
       "      <td>0.906882</td>\n",
       "      <td>0.864664</td>\n",
       "      <td>8.824636</td>\n",
       "      <td>5.874057</td>\n",
       "      <td>4.729979</td>\n",
       "      <td>6.476224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.698838</td>\n",
       "      <td>0.843951</td>\n",
       "      <td>0.870347</td>\n",
       "      <td>0.804379</td>\n",
       "      <td>10.8356</td>\n",
       "      <td>6.900989</td>\n",
       "      <td>5.580742</td>\n",
       "      <td>7.772444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.592336</td>\n",
       "      <td>0.815744</td>\n",
       "      <td>0.821852</td>\n",
       "      <td>0.743311</td>\n",
       "      <td>12.609185</td>\n",
       "      <td>7.499755</td>\n",
       "      <td>6.540103</td>\n",
       "      <td>8.883015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.484388</td>\n",
       "      <td>0.787725</td>\n",
       "      <td>0.785112</td>\n",
       "      <td>0.685742</td>\n",
       "      <td>14.182501</td>\n",
       "      <td>8.051284</td>\n",
       "      <td>7.180672</td>\n",
       "      <td>9.804819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.380928</td>\n",
       "      <td>0.765492</td>\n",
       "      <td>0.765882</td>\n",
       "      <td>0.637434</td>\n",
       "      <td>15.541737</td>\n",
       "      <td>8.464806</td>\n",
       "      <td>7.492776</td>\n",
       "      <td>10.499773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.284702</td>\n",
       "      <td>0.742422</td>\n",
       "      <td>0.749785</td>\n",
       "      <td>0.592303</td>\n",
       "      <td>16.70739</td>\n",
       "      <td>8.874032</td>\n",
       "      <td>7.744292</td>\n",
       "      <td>11.108571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.199804</td>\n",
       "      <td>0.719688</td>\n",
       "      <td>0.729267</td>\n",
       "      <td>0.549587</td>\n",
       "      <td>17.67092</td>\n",
       "      <td>9.260147</td>\n",
       "      <td>8.054232</td>\n",
       "      <td>11.661766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.129104</td>\n",
       "      <td>0.693941</td>\n",
       "      <td>0.701805</td>\n",
       "      <td>0.508283</td>\n",
       "      <td>18.434107</td>\n",
       "      <td>9.679998</td>\n",
       "      <td>8.451276</td>\n",
       "      <td>12.18846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.071927</td>\n",
       "      <td>0.664606</td>\n",
       "      <td>0.668206</td>\n",
       "      <td>0.468246</td>\n",
       "      <td>19.030637</td>\n",
       "      <td>10.136727</td>\n",
       "      <td>8.912953</td>\n",
       "      <td>12.693439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.022941</td>\n",
       "      <td>0.631727</td>\n",
       "      <td>0.627397</td>\n",
       "      <td>0.427355</td>\n",
       "      <td>19.530727</td>\n",
       "      <td>10.624831</td>\n",
       "      <td>9.443706</td>\n",
       "      <td>13.199755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.02043</td>\n",
       "      <td>0.598321</td>\n",
       "      <td>0.583933</td>\n",
       "      <td>0.387274</td>\n",
       "      <td>19.597424</td>\n",
       "      <td>11.099815</td>\n",
       "      <td>9.977955</td>\n",
       "      <td>13.558398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.061273</td>\n",
       "      <td>0.565225</td>\n",
       "      <td>0.541715</td>\n",
       "      <td>0.348556</td>\n",
       "      <td>19.113728</td>\n",
       "      <td>11.55236</td>\n",
       "      <td>10.470492</td>\n",
       "      <td>13.712193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.101067</td>\n",
       "      <td>0.531604</td>\n",
       "      <td>0.501012</td>\n",
       "      <td>0.310516</td>\n",
       "      <td>18.818396</td>\n",
       "      <td>11.99494</td>\n",
       "      <td>10.924094</td>\n",
       "      <td>13.912477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.141629</td>\n",
       "      <td>0.498884</td>\n",
       "      <td>0.461757</td>\n",
       "      <td>0.273004</td>\n",
       "      <td>18.854333</td>\n",
       "      <td>12.41239</td>\n",
       "      <td>11.344545</td>\n",
       "      <td>14.203756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.178986</td>\n",
       "      <td>0.466798</td>\n",
       "      <td>0.42434</td>\n",
       "      <td>0.237384</td>\n",
       "      <td>19.093681</td>\n",
       "      <td>12.809974</td>\n",
       "      <td>11.730447</td>\n",
       "      <td>14.544701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.210731</td>\n",
       "      <td>0.433223</td>\n",
       "      <td>0.390064</td>\n",
       "      <td>0.204185</td>\n",
       "      <td>18.967583</td>\n",
       "      <td>13.210928</td>\n",
       "      <td>12.073014</td>\n",
       "      <td>14.750508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.236104</td>\n",
       "      <td>0.399847</td>\n",
       "      <td>0.36094</td>\n",
       "      <td>0.174894</td>\n",
       "      <td>18.795605</td>\n",
       "      <td>13.598657</td>\n",
       "      <td>12.355384</td>\n",
       "      <td>14.916549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.258191</td>\n",
       "      <td>0.366987</td>\n",
       "      <td>0.335142</td>\n",
       "      <td>0.147979</td>\n",
       "      <td>18.67788</td>\n",
       "      <td>13.970611</td>\n",
       "      <td>12.599141</td>\n",
       "      <td>15.082544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.276637</td>\n",
       "      <td>0.33724</td>\n",
       "      <td>0.308964</td>\n",
       "      <td>0.123189</td>\n",
       "      <td>18.536543</td>\n",
       "      <td>14.297844</td>\n",
       "      <td>12.840738</td>\n",
       "      <td>15.225042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.296538</td>\n",
       "      <td>0.308601</td>\n",
       "      <td>0.282311</td>\n",
       "      <td>0.098125</td>\n",
       "      <td>18.48058</td>\n",
       "      <td>14.605959</td>\n",
       "      <td>13.082472</td>\n",
       "      <td>15.38967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.318591</td>\n",
       "      <td>0.279273</td>\n",
       "      <td>0.256453</td>\n",
       "      <td>0.072378</td>\n",
       "      <td>18.51337</td>\n",
       "      <td>14.914332</td>\n",
       "      <td>13.313783</td>\n",
       "      <td>15.580495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.34054</td>\n",
       "      <td>0.249758</td>\n",
       "      <td>0.231967</td>\n",
       "      <td>0.047062</td>\n",
       "      <td>18.407946</td>\n",
       "      <td>15.218074</td>\n",
       "      <td>13.530184</td>\n",
       "      <td>15.718735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.361125</td>\n",
       "      <td>0.220061</td>\n",
       "      <td>0.207513</td>\n",
       "      <td>0.02215</td>\n",
       "      <td>18.298828</td>\n",
       "      <td>15.51837</td>\n",
       "      <td>13.744024</td>\n",
       "      <td>15.853741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.380145</td>\n",
       "      <td>0.193656</td>\n",
       "      <td>0.184812</td>\n",
       "      <td>-0.000559</td>\n",
       "      <td>18.243339</td>\n",
       "      <td>15.78095</td>\n",
       "      <td>13.939099</td>\n",
       "      <td>15.987796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.395634</td>\n",
       "      <td>0.169307</td>\n",
       "      <td>0.165006</td>\n",
       "      <td>-0.02044</td>\n",
       "      <td>18.167908</td>\n",
       "      <td>16.019884</td>\n",
       "      <td>14.107522</td>\n",
       "      <td>16.098438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.406855</td>\n",
       "      <td>0.14654</td>\n",
       "      <td>0.14919</td>\n",
       "      <td>-0.037041</td>\n",
       "      <td>18.068358</td>\n",
       "      <td>16.239088</td>\n",
       "      <td>14.241179</td>\n",
       "      <td>16.182875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.414424</td>\n",
       "      <td>0.12563</td>\n",
       "      <td>0.136221</td>\n",
       "      <td>-0.050858</td>\n",
       "      <td>18.077062</td>\n",
       "      <td>16.437244</td>\n",
       "      <td>14.350585</td>\n",
       "      <td>16.288297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.419919</td>\n",
       "      <td>0.10584</td>\n",
       "      <td>0.122676</td>\n",
       "      <td>-0.063801</td>\n",
       "      <td>18.137578</td>\n",
       "      <td>16.622693</td>\n",
       "      <td>14.463637</td>\n",
       "      <td>16.407969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.427413</td>\n",
       "      <td>0.088403</td>\n",
       "      <td>0.108406</td>\n",
       "      <td>-0.076868</td>\n",
       "      <td>18.211522</td>\n",
       "      <td>16.784091</td>\n",
       "      <td>14.582442</td>\n",
       "      <td>16.526018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.023673</td>\n",
       "      <td>0.485083</td>\n",
       "      <td>0.477067</td>\n",
       "      <td>0.312826</td>\n",
       "      <td>17.117425</td>\n",
       "      <td>12.118173</td>\n",
       "      <td>10.72707</td>\n",
       "      <td>13.320889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.870886  0.915074   0.93305  0.906337   7.093633   5.090365   \n",
       "1      0.800189  0.886922  0.906882  0.864664   8.824636   5.874057   \n",
       "2      0.698838  0.843951  0.870347  0.804379    10.8356   6.900989   \n",
       "3      0.592336  0.815744  0.821852  0.743311  12.609185   7.499755   \n",
       "4      0.484388  0.787725  0.785112  0.685742  14.182501   8.051284   \n",
       "5      0.380928  0.765492  0.765882  0.637434  15.541737   8.464806   \n",
       "6      0.284702  0.742422  0.749785  0.592303   16.70739   8.874032   \n",
       "7      0.199804  0.719688  0.729267  0.549587   17.67092   9.260147   \n",
       "8      0.129104  0.693941  0.701805  0.508283  18.434107   9.679998   \n",
       "9      0.071927  0.664606  0.668206  0.468246  19.030637  10.136727   \n",
       "10     0.022941  0.631727  0.627397  0.427355  19.530727  10.624831   \n",
       "11     -0.02043  0.598321  0.583933  0.387274  19.597424  11.099815   \n",
       "12    -0.061273  0.565225  0.541715  0.348556  19.113728   11.55236   \n",
       "13    -0.101067  0.531604  0.501012  0.310516  18.818396   11.99494   \n",
       "14    -0.141629  0.498884  0.461757  0.273004  18.854333   12.41239   \n",
       "15    -0.178986  0.466798   0.42434  0.237384  19.093681  12.809974   \n",
       "16    -0.210731  0.433223  0.390064  0.204185  18.967583  13.210928   \n",
       "17    -0.236104  0.399847   0.36094  0.174894  18.795605  13.598657   \n",
       "18    -0.258191  0.366987  0.335142  0.147979   18.67788  13.970611   \n",
       "19    -0.276637   0.33724  0.308964  0.123189  18.536543  14.297844   \n",
       "20    -0.296538  0.308601  0.282311  0.098125   18.48058  14.605959   \n",
       "21    -0.318591  0.279273  0.256453  0.072378   18.51337  14.914332   \n",
       "22     -0.34054  0.249758  0.231967  0.047062  18.407946  15.218074   \n",
       "23    -0.361125  0.220061  0.207513   0.02215  18.298828   15.51837   \n",
       "24    -0.380145  0.193656  0.184812 -0.000559  18.243339   15.78095   \n",
       "25    -0.395634  0.169307  0.165006  -0.02044  18.167908  16.019884   \n",
       "26    -0.406855   0.14654   0.14919 -0.037041  18.068358  16.239088   \n",
       "27    -0.414424   0.12563  0.136221 -0.050858  18.077062  16.437244   \n",
       "28    -0.419919   0.10584  0.122676 -0.063801  18.137578  16.622693   \n",
       "29    -0.427413  0.088403  0.108406 -0.076868  18.211522  16.784091   \n",
       "mean  -0.023673  0.485083  0.477067  0.312826  17.117425  12.118173   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       4.010632    5.39821  \n",
       "1       4.729979   6.476224  \n",
       "2       5.580742   7.772444  \n",
       "3       6.540103   8.883015  \n",
       "4       7.180672   9.804819  \n",
       "5       7.492776  10.499773  \n",
       "6       7.744292  11.108571  \n",
       "7       8.054232  11.661766  \n",
       "8       8.451276   12.18846  \n",
       "9       8.912953  12.693439  \n",
       "10      9.443706  13.199755  \n",
       "11      9.977955  13.558398  \n",
       "12     10.470492  13.712193  \n",
       "13     10.924094  13.912477  \n",
       "14     11.344545  14.203756  \n",
       "15     11.730447  14.544701  \n",
       "16     12.073014  14.750508  \n",
       "17     12.355384  14.916549  \n",
       "18     12.599141  15.082544  \n",
       "19     12.840738  15.225042  \n",
       "20     13.082472   15.38967  \n",
       "21     13.313783  15.580495  \n",
       "22     13.530184  15.718735  \n",
       "23     13.744024  15.853741  \n",
       "24     13.939099  15.987796  \n",
       "25     14.107522  16.098438  \n",
       "26     14.241179  16.182875  \n",
       "27     14.350585  16.288297  \n",
       "28     14.463637  16.407969  \n",
       "29     14.582442  16.526018  \n",
       "mean    10.72707  13.320889  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/10_30_1_50_1_50_datt_seq2seq_gru_1.csv\n",
      "end training\n",
      "\n",
      "history size: 20\n",
      "future size: 10\n",
      "Epoch 1/10000\n",
      "577/577 - 8s - loss: 0.2242 - val_loss: 0.1675 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "577/577 - 5s - loss: 0.1503 - val_loss: 0.1472 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "577/577 - 4s - loss: 0.1392 - val_loss: 0.1371 - 4s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "577/577 - 5s - loss: 0.1314 - val_loss: 0.1287 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "577/577 - 4s - loss: 0.1246 - val_loss: 0.1274 - 4s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "577/577 - 5s - loss: 0.1153 - val_loss: 0.1160 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "577/577 - 5s - loss: 0.1087 - val_loss: 0.1107 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "577/577 - 5s - loss: 0.1028 - val_loss: 0.1046 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "577/577 - 5s - loss: 0.0984 - val_loss: 0.1039 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "577/577 - 4s - loss: 0.0934 - val_loss: 0.0970 - 4s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "577/577 - 4s - loss: 0.0903 - val_loss: 0.0970 - 4s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "577/577 - 5s - loss: 0.0870 - val_loss: 0.0972 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "577/577 - 4s - loss: 0.0856 - val_loss: 0.0916 - 4s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "577/577 - 4s - loss: 0.0822 - val_loss: 0.0946 - 4s/epoch - 7ms/step\n",
      "Epoch 15/10000\n",
      "577/577 - 4s - loss: 0.0807 - val_loss: 0.0896 - 4s/epoch - 7ms/step\n",
      "Epoch 16/10000\n",
      "577/577 - 4s - loss: 0.0783 - val_loss: 0.0935 - 4s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "577/577 - 5s - loss: 0.0769 - val_loss: 0.0871 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "577/577 - 5s - loss: 0.0753 - val_loss: 0.0857 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "577/577 - 5s - loss: 0.0726 - val_loss: 0.0831 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "577/577 - 4s - loss: 0.0711 - val_loss: 0.0845 - 4s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "577/577 - 5s - loss: 0.0719 - val_loss: 0.0816 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "577/577 - 4s - loss: 0.0711 - val_loss: 0.0805 - 4s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "577/577 - 5s - loss: 0.0675 - val_loss: 0.0797 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "577/577 - 5s - loss: 0.0649 - val_loss: 0.0790 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "577/577 - 5s - loss: 0.0642 - val_loss: 0.0755 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "577/577 - 5s - loss: 0.0651 - val_loss: 0.0860 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "577/577 - 5s - loss: 0.0632 - val_loss: 0.0747 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "577/577 - 5s - loss: 0.0605 - val_loss: 0.0763 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "577/577 - 5s - loss: 0.0605 - val_loss: 0.0745 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "577/577 - 5s - loss: 0.0593 - val_loss: 0.0762 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "577/577 - 5s - loss: 0.0602 - val_loss: 0.0716 - 5s/epoch - 9ms/step\n",
      "Epoch 32/10000\n",
      "577/577 - 5s - loss: 0.0587 - val_loss: 0.0714 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "577/577 - 5s - loss: 0.0569 - val_loss: 0.0721 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "577/577 - 5s - loss: 0.0574 - val_loss: 0.0698 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "577/577 - 4s - loss: 0.0566 - val_loss: 0.0693 - 4s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "577/577 - 5s - loss: 0.0548 - val_loss: 0.0712 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "577/577 - 4s - loss: 0.0561 - val_loss: 0.0691 - 4s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "577/577 - 4s - loss: 0.0529 - val_loss: 0.0664 - 4s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "577/577 - 4s - loss: 0.0536 - val_loss: 0.0719 - 4s/epoch - 7ms/step\n",
      "Epoch 40/10000\n",
      "577/577 - 4s - loss: 0.0528 - val_loss: 0.0660 - 4s/epoch - 7ms/step\n",
      "Epoch 41/10000\n",
      "577/577 - 4s - loss: 0.0516 - val_loss: 0.0641 - 4s/epoch - 7ms/step\n",
      "Epoch 42/10000\n",
      "577/577 - 4s - loss: 0.0538 - val_loss: 0.0669 - 4s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "577/577 - 4s - loss: 0.0506 - val_loss: 0.0635 - 4s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "577/577 - 5s - loss: 0.0501 - val_loss: 0.0645 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "577/577 - 4s - loss: 0.0518 - val_loss: 0.0642 - 4s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "577/577 - 4s - loss: 0.0509 - val_loss: 0.0677 - 4s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "577/577 - 5s - loss: 0.0488 - val_loss: 0.0616 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "577/577 - 5s - loss: 0.0473 - val_loss: 0.0620 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "577/577 - 4s - loss: 0.0493 - val_loss: 0.0654 - 4s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "577/577 - 4s - loss: 0.0478 - val_loss: 0.0603 - 4s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "577/577 - 4s - loss: 0.0461 - val_loss: 0.0629 - 4s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "577/577 - 5s - loss: 0.0463 - val_loss: 0.0628 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "577/577 - 5s - loss: 0.0461 - val_loss: 0.0727 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "577/577 - 4s - loss: 0.0465 - val_loss: 0.0618 - 4s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "577/577 - 4s - loss: 0.0484 - val_loss: 0.0616 - 4s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "577/577 - 5s - loss: 0.0446 - val_loss: 0.0611 - 5s/epoch - 9ms/step\n",
      "Epoch 57/10000\n",
      "577/577 - 5s - loss: 0.0451 - val_loss: 0.0586 - 5s/epoch - 9ms/step\n",
      "Epoch 58/10000\n",
      "577/577 - 5s - loss: 0.0443 - val_loss: 0.0581 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "577/577 - 4s - loss: 0.0445 - val_loss: 0.0630 - 4s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "577/577 - 4s - loss: 0.0449 - val_loss: 0.0569 - 4s/epoch - 7ms/step\n",
      "Epoch 61/10000\n",
      "577/577 - 5s - loss: 0.0432 - val_loss: 0.0573 - 5s/epoch - 9ms/step\n",
      "Epoch 62/10000\n",
      "577/577 - 5s - loss: 0.0430 - val_loss: 0.0574 - 5s/epoch - 9ms/step\n",
      "Epoch 63/10000\n",
      "577/577 - 4s - loss: 0.0439 - val_loss: 0.0612 - 4s/epoch - 7ms/step\n",
      "Epoch 64/10000\n",
      "577/577 - 4s - loss: 0.0428 - val_loss: 0.0569 - 4s/epoch - 7ms/step\n",
      "Epoch 65/10000\n",
      "577/577 - 4s - loss: 0.0417 - val_loss: 0.0559 - 4s/epoch - 7ms/step\n",
      "Epoch 66/10000\n",
      "577/577 - 4s - loss: 0.0423 - val_loss: 0.0547 - 4s/epoch - 7ms/step\n",
      "Epoch 67/10000\n",
      "577/577 - 4s - loss: 0.0423 - val_loss: 0.0573 - 4s/epoch - 7ms/step\n",
      "Epoch 68/10000\n",
      "577/577 - 4s - loss: 0.0417 - val_loss: 0.0570 - 4s/epoch - 7ms/step\n",
      "Epoch 69/10000\n",
      "577/577 - 4s - loss: 0.0408 - val_loss: 0.0569 - 4s/epoch - 7ms/step\n",
      "Epoch 70/10000\n",
      "577/577 - 4s - loss: 0.0413 - val_loss: 0.0630 - 4s/epoch - 7ms/step\n",
      "Epoch 71/10000\n",
      "577/577 - 4s - loss: 0.0423 - val_loss: 0.0584 - 4s/epoch - 7ms/step\n",
      "Epoch 72/10000\n",
      "577/577 - 4s - loss: 0.0400 - val_loss: 0.0545 - 4s/epoch - 7ms/step\n",
      "Epoch 73/10000\n",
      "577/577 - 5s - loss: 0.0397 - val_loss: 0.0556 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "577/577 - 4s - loss: 0.0410 - val_loss: 0.0556 - 4s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "577/577 - 5s - loss: 0.0390 - val_loss: 0.0536 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "577/577 - 5s - loss: 0.0387 - val_loss: 0.0519 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "577/577 - 5s - loss: 0.0384 - val_loss: 0.0524 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "577/577 - 5s - loss: 0.0392 - val_loss: 0.0534 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "577/577 - 5s - loss: 0.0385 - val_loss: 0.0533 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "577/577 - 4s - loss: 0.0378 - val_loss: 0.0542 - 4s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "577/577 - 4s - loss: 0.0381 - val_loss: 0.0520 - 4s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "577/577 - 4s - loss: 0.0386 - val_loss: 0.0531 - 4s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "577/577 - 4s - loss: 0.0383 - val_loss: 0.0529 - 4s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "577/577 - 4s - loss: 0.0399 - val_loss: 0.0514 - 4s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "577/577 - 5s - loss: 0.0362 - val_loss: 0.0521 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "577/577 - 4s - loss: 0.0368 - val_loss: 0.0514 - 4s/epoch - 7ms/step\n",
      "Epoch 87/10000\n",
      "577/577 - 5s - loss: 0.0369 - val_loss: 0.0542 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "577/577 - 4s - loss: 0.0349 - val_loss: 0.0510 - 4s/epoch - 7ms/step\n",
      "Epoch 89/10000\n",
      "577/577 - 5s - loss: 0.0364 - val_loss: 0.0500 - 5s/epoch - 9ms/step\n",
      "Epoch 90/10000\n",
      "577/577 - 5s - loss: 0.0360 - val_loss: 0.0522 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "577/577 - 4s - loss: 0.0365 - val_loss: 0.0518 - 4s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "577/577 - 4s - loss: 0.0353 - val_loss: 0.0513 - 4s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "577/577 - 5s - loss: 0.0357 - val_loss: 0.0520 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "577/577 - 5s - loss: 0.0351 - val_loss: 0.0561 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "577/577 - 5s - loss: 0.0351 - val_loss: 0.0501 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "577/577 - 4s - loss: 0.0373 - val_loss: 0.0522 - 4s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "577/577 - 5s - loss: 0.0347 - val_loss: 0.0509 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "577/577 - 5s - loss: 0.0354 - val_loss: 0.0490 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "577/577 - 4s - loss: 0.0337 - val_loss: 0.0499 - 4s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "577/577 - 4s - loss: 0.0337 - val_loss: 0.0525 - 4s/epoch - 7ms/step\n",
      "Epoch 101/10000\n",
      "577/577 - 5s - loss: 0.0353 - val_loss: 0.0494 - 5s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "577/577 - 4s - loss: 0.0338 - val_loss: 0.0485 - 4s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "577/577 - 4s - loss: 0.0331 - val_loss: 0.0493 - 4s/epoch - 7ms/step\n",
      "Epoch 104/10000\n",
      "577/577 - 4s - loss: 0.0335 - val_loss: 0.0499 - 4s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "577/577 - 4s - loss: 0.0335 - val_loss: 0.0509 - 4s/epoch - 7ms/step\n",
      "Epoch 106/10000\n",
      "577/577 - 4s - loss: 0.0328 - val_loss: 0.0523 - 4s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "577/577 - 4s - loss: 0.0347 - val_loss: 0.0508 - 4s/epoch - 7ms/step\n",
      "Epoch 108/10000\n",
      "577/577 - 4s - loss: 0.0337 - val_loss: 0.0507 - 4s/epoch - 7ms/step\n",
      "Epoch 109/10000\n",
      "577/577 - 4s - loss: 0.0325 - val_loss: 0.0490 - 4s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "577/577 - 4s - loss: 0.0337 - val_loss: 0.0532 - 4s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "577/577 - 4s - loss: 0.0318 - val_loss: 0.0498 - 4s/epoch - 7ms/step\n",
      "Epoch 112/10000\n",
      "577/577 - 4s - loss: 0.0317 - val_loss: 0.0485 - 4s/epoch - 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_10_layer_call_fn, gru_cell_10_layer_call_and_return_conditional_losses, gru_cell_11_layer_call_fn, gru_cell_11_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_10_1_50_1_50_datt_seq2seq_gru_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_10_1_50_1_50_datt_seq2seq_gru_1\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000294BE2BCA90> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000294BA7DD220> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.930833</td>\n",
       "      <td>0.941856</td>\n",
       "      <td>0.960919</td>\n",
       "      <td>0.944536</td>\n",
       "      <td>4.272753</td>\n",
       "      <td>4.226545</td>\n",
       "      <td>3.068553</td>\n",
       "      <td>3.85595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.865856</td>\n",
       "      <td>0.882785</td>\n",
       "      <td>0.922947</td>\n",
       "      <td>0.890529</td>\n",
       "      <td>5.91254</td>\n",
       "      <td>6.001782</td>\n",
       "      <td>4.307919</td>\n",
       "      <td>5.407414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.769033</td>\n",
       "      <td>0.83022</td>\n",
       "      <td>0.866382</td>\n",
       "      <td>0.821878</td>\n",
       "      <td>7.65277</td>\n",
       "      <td>7.223975</td>\n",
       "      <td>5.671709</td>\n",
       "      <td>6.849485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.659251</td>\n",
       "      <td>0.788311</td>\n",
       "      <td>0.826112</td>\n",
       "      <td>0.757891</td>\n",
       "      <td>9.171626</td>\n",
       "      <td>8.067692</td>\n",
       "      <td>6.468783</td>\n",
       "      <td>7.902701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.551052</td>\n",
       "      <td>0.756131</td>\n",
       "      <td>0.812412</td>\n",
       "      <td>0.706532</td>\n",
       "      <td>10.422997</td>\n",
       "      <td>8.660408</td>\n",
       "      <td>6.717166</td>\n",
       "      <td>8.60019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.451201</td>\n",
       "      <td>0.731382</td>\n",
       "      <td>0.817231</td>\n",
       "      <td>0.666605</td>\n",
       "      <td>11.410327</td>\n",
       "      <td>9.090607</td>\n",
       "      <td>6.629079</td>\n",
       "      <td>9.043337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.362825</td>\n",
       "      <td>0.710452</td>\n",
       "      <td>0.826314</td>\n",
       "      <td>0.633197</td>\n",
       "      <td>12.175627</td>\n",
       "      <td>9.438594</td>\n",
       "      <td>6.461535</td>\n",
       "      <td>9.358585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.286013</td>\n",
       "      <td>0.692627</td>\n",
       "      <td>0.827928</td>\n",
       "      <td>0.602189</td>\n",
       "      <td>12.85613</td>\n",
       "      <td>9.724989</td>\n",
       "      <td>6.431153</td>\n",
       "      <td>9.670757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.220179</td>\n",
       "      <td>0.677121</td>\n",
       "      <td>0.814939</td>\n",
       "      <td>0.570746</td>\n",
       "      <td>13.449089</td>\n",
       "      <td>9.967636</td>\n",
       "      <td>6.669215</td>\n",
       "      <td>10.028647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.163951</td>\n",
       "      <td>0.661983</td>\n",
       "      <td>0.787599</td>\n",
       "      <td>0.537844</td>\n",
       "      <td>13.939116</td>\n",
       "      <td>10.198801</td>\n",
       "      <td>7.144701</td>\n",
       "      <td>10.427539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.526019</td>\n",
       "      <td>0.767287</td>\n",
       "      <td>0.846278</td>\n",
       "      <td>0.713195</td>\n",
       "      <td>10.126298</td>\n",
       "      <td>8.260103</td>\n",
       "      <td>5.956981</td>\n",
       "      <td>8.114461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE     nRMSE   \n",
       "0      0.930833  0.941856  0.960919  0.944536   4.272753   4.226545  3.068553   \n",
       "1      0.865856  0.882785  0.922947  0.890529    5.91254   6.001782  4.307919   \n",
       "2      0.769033   0.83022  0.866382  0.821878    7.65277   7.223975  5.671709   \n",
       "3      0.659251  0.788311  0.826112  0.757891   9.171626   8.067692  6.468783   \n",
       "4      0.551052  0.756131  0.812412  0.706532  10.422997   8.660408  6.717166   \n",
       "5      0.451201  0.731382  0.817231  0.666605  11.410327   9.090607  6.629079   \n",
       "6      0.362825  0.710452  0.826314  0.633197  12.175627   9.438594  6.461535   \n",
       "7      0.286013  0.692627  0.827928  0.602189   12.85613   9.724989  6.431153   \n",
       "8      0.220179  0.677121  0.814939  0.570746  13.449089   9.967636  6.669215   \n",
       "9      0.163951  0.661983  0.787599  0.537844  13.939116  10.198801  7.144701   \n",
       "mean   0.526019  0.767287  0.846278  0.713195  10.126298   8.260103  5.956981   \n",
       "\n",
       "            mean  \n",
       "index      nRMSE  \n",
       "0        3.85595  \n",
       "1       5.407414  \n",
       "2       6.849485  \n",
       "3       7.902701  \n",
       "4        8.60019  \n",
       "5       9.043337  \n",
       "6       9.358585  \n",
       "7       9.670757  \n",
       "8      10.028647  \n",
       "9      10.427539  \n",
       "mean    8.114461  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/20_10_1_50_1_50_datt_seq2seq_gru_1.csv\n",
      "end training\n",
      "\n",
      "history size: 20\n",
      "future size: 20\n",
      "Epoch 1/10000\n",
      "572/572 - 7s - loss: 0.3015 - val_loss: 0.2318 - 7s/epoch - 13ms/step\n",
      "Epoch 2/10000\n",
      "572/572 - 5s - loss: 0.2258 - val_loss: 0.2188 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "572/572 - 4s - loss: 0.2105 - val_loss: 0.2054 - 4s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "572/572 - 5s - loss: 0.1992 - val_loss: 0.1965 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "572/572 - 5s - loss: 0.1870 - val_loss: 0.1880 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "572/572 - 4s - loss: 0.1789 - val_loss: 0.1844 - 4s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "572/572 - 4s - loss: 0.1690 - val_loss: 0.1754 - 4s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "572/572 - 4s - loss: 0.1644 - val_loss: 0.1707 - 4s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "572/572 - 4s - loss: 0.1564 - val_loss: 0.1672 - 4s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "572/572 - 5s - loss: 0.1520 - val_loss: 0.1587 - 5s/epoch - 9ms/step\n",
      "Epoch 11/10000\n",
      "572/572 - 5s - loss: 0.1439 - val_loss: 0.1544 - 5s/epoch - 9ms/step\n",
      "Epoch 12/10000\n",
      "572/572 - 5s - loss: 0.1375 - val_loss: 0.1546 - 5s/epoch - 9ms/step\n",
      "Epoch 13/10000\n",
      "572/572 - 5s - loss: 0.1351 - val_loss: 0.1515 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "572/572 - 5s - loss: 0.1290 - val_loss: 0.1379 - 5s/epoch - 9ms/step\n",
      "Epoch 15/10000\n",
      "572/572 - 5s - loss: 0.1241 - val_loss: 0.1393 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "572/572 - 5s - loss: 0.1215 - val_loss: 0.1309 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "572/572 - 5s - loss: 0.1159 - val_loss: 0.1325 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "572/572 - 5s - loss: 0.1121 - val_loss: 0.1249 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "572/572 - 5s - loss: 0.1102 - val_loss: 0.1233 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "572/572 - 5s - loss: 0.1068 - val_loss: 0.1230 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "572/572 - 5s - loss: 0.1066 - val_loss: 0.1192 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "572/572 - 5s - loss: 0.1041 - val_loss: 0.1296 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "572/572 - 5s - loss: 0.1046 - val_loss: 0.1154 - 5s/epoch - 9ms/step\n",
      "Epoch 24/10000\n",
      "572/572 - 5s - loss: 0.0995 - val_loss: 0.1209 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "572/572 - 5s - loss: 0.0991 - val_loss: 0.1168 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "572/572 - 5s - loss: 0.0988 - val_loss: 0.1152 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "572/572 - 5s - loss: 0.0947 - val_loss: 0.1082 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "572/572 - 5s - loss: 0.0945 - val_loss: 0.1151 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "572/572 - 5s - loss: 0.0933 - val_loss: 0.1114 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "572/572 - 5s - loss: 0.0929 - val_loss: 0.1047 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "572/572 - 5s - loss: 0.0887 - val_loss: 0.1051 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "572/572 - 5s - loss: 0.0898 - val_loss: 0.1069 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "572/572 - 5s - loss: 0.0901 - val_loss: 0.1008 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "572/572 - 5s - loss: 0.0868 - val_loss: 0.1015 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "572/572 - 5s - loss: 0.0862 - val_loss: 0.1018 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "572/572 - 5s - loss: 0.0853 - val_loss: 0.1003 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "572/572 - 5s - loss: 0.0849 - val_loss: 0.1046 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "572/572 - 5s - loss: 0.0848 - val_loss: 0.0986 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "572/572 - 5s - loss: 0.0818 - val_loss: 0.0983 - 5s/epoch - 10ms/step\n",
      "Epoch 40/10000\n",
      "572/572 - 5s - loss: 0.0816 - val_loss: 0.0962 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "572/572 - 5s - loss: 0.0805 - val_loss: 0.1069 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "572/572 - 4s - loss: 0.0824 - val_loss: 0.0952 - 4s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "572/572 - 4s - loss: 0.0778 - val_loss: 0.0928 - 4s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "572/572 - 4s - loss: 0.0784 - val_loss: 0.0979 - 4s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "572/572 - 5s - loss: 0.0770 - val_loss: 0.0939 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "572/572 - 5s - loss: 0.0769 - val_loss: 0.0950 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "572/572 - 5s - loss: 0.0768 - val_loss: 0.0916 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "572/572 - 5s - loss: 0.0747 - val_loss: 0.0930 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "572/572 - 5s - loss: 0.0752 - val_loss: 0.0952 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "572/572 - 4s - loss: 0.0721 - val_loss: 0.0937 - 4s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "572/572 - 5s - loss: 0.0743 - val_loss: 0.0885 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "572/572 - 5s - loss: 0.0722 - val_loss: 0.0924 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "572/572 - 5s - loss: 0.0715 - val_loss: 0.0917 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "572/572 - 5s - loss: 0.0765 - val_loss: 0.0966 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "572/572 - 5s - loss: 0.0732 - val_loss: 0.0872 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "572/572 - 5s - loss: 0.0680 - val_loss: 0.0867 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "572/572 - 5s - loss: 0.0711 - val_loss: 0.0850 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "572/572 - 5s - loss: 0.0713 - val_loss: 0.0832 - 5s/epoch - 9ms/step\n",
      "Epoch 59/10000\n",
      "572/572 - 5s - loss: 0.0713 - val_loss: 0.0878 - 5s/epoch - 9ms/step\n",
      "Epoch 60/10000\n",
      "572/572 - 5s - loss: 0.0690 - val_loss: 0.0820 - 5s/epoch - 9ms/step\n",
      "Epoch 61/10000\n",
      "572/572 - 5s - loss: 0.0660 - val_loss: 0.0818 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "572/572 - 5s - loss: 0.0654 - val_loss: 0.0790 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "572/572 - 5s - loss: 0.0645 - val_loss: 0.0791 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "572/572 - 5s - loss: 0.0674 - val_loss: 0.0772 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "572/572 - 5s - loss: 0.0643 - val_loss: 0.0814 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "572/572 - 5s - loss: 0.0637 - val_loss: 0.0782 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "572/572 - 5s - loss: 0.0627 - val_loss: 0.0777 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "572/572 - 5s - loss: 0.0642 - val_loss: 0.0776 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "572/572 - 5s - loss: 0.0637 - val_loss: 0.0742 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "572/572 - 4s - loss: 0.0613 - val_loss: 0.0803 - 4s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "572/572 - 5s - loss: 0.0703 - val_loss: 0.0855 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "572/572 - 5s - loss: 0.0668 - val_loss: 0.0800 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "572/572 - 5s - loss: 0.0647 - val_loss: 0.0794 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "572/572 - 5s - loss: 0.0687 - val_loss: 0.0810 - 5s/epoch - 9ms/step\n",
      "Epoch 75/10000\n",
      "572/572 - 5s - loss: 0.0627 - val_loss: 0.0786 - 5s/epoch - 9ms/step\n",
      "Epoch 76/10000\n",
      "572/572 - 5s - loss: 0.0590 - val_loss: 0.0755 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "572/572 - 5s - loss: 0.0581 - val_loss: 0.0762 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "572/572 - 5s - loss: 0.0613 - val_loss: 0.0856 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "572/572 - 5s - loss: 0.0613 - val_loss: 0.0766 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_12_layer_call_fn, gru_cell_12_layer_call_and_return_conditional_losses, gru_cell_13_layer_call_fn, gru_cell_13_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_20_1_50_1_50_datt_seq2seq_gru_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_20_1_50_1_50_datt_seq2seq_gru_1\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000294CE4CF4F0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000294D8861580> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.891104</td>\n",
       "      <td>0.903938</td>\n",
       "      <td>0.930305</td>\n",
       "      <td>0.908449</td>\n",
       "      <td>6.520242</td>\n",
       "      <td>5.426423</td>\n",
       "      <td>4.084329</td>\n",
       "      <td>5.343664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.811955</td>\n",
       "      <td>0.846649</td>\n",
       "      <td>0.904724</td>\n",
       "      <td>0.854443</td>\n",
       "      <td>8.41275</td>\n",
       "      <td>6.858346</td>\n",
       "      <td>4.774768</td>\n",
       "      <td>6.681955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.714969</td>\n",
       "      <td>0.798636</td>\n",
       "      <td>0.867596</td>\n",
       "      <td>0.793734</td>\n",
       "      <td>9.905543</td>\n",
       "      <td>7.861926</td>\n",
       "      <td>5.627945</td>\n",
       "      <td>7.798471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.615725</td>\n",
       "      <td>0.761602</td>\n",
       "      <td>0.826606</td>\n",
       "      <td>0.734644</td>\n",
       "      <td>11.117234</td>\n",
       "      <td>8.557417</td>\n",
       "      <td>6.439588</td>\n",
       "      <td>8.704747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.515354</td>\n",
       "      <td>0.734181</td>\n",
       "      <td>0.79238</td>\n",
       "      <td>0.680638</td>\n",
       "      <td>12.284601</td>\n",
       "      <td>9.040221</td>\n",
       "      <td>7.045835</td>\n",
       "      <td>9.456886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.415209</td>\n",
       "      <td>0.716008</td>\n",
       "      <td>0.768545</td>\n",
       "      <td>0.633254</td>\n",
       "      <td>13.44732</td>\n",
       "      <td>9.348786</td>\n",
       "      <td>7.438145</td>\n",
       "      <td>10.078084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.327808</td>\n",
       "      <td>0.70336</td>\n",
       "      <td>0.752337</td>\n",
       "      <td>0.594502</td>\n",
       "      <td>14.133014</td>\n",
       "      <td>9.557442</td>\n",
       "      <td>7.693158</td>\n",
       "      <td>10.461205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.261475</td>\n",
       "      <td>0.691936</td>\n",
       "      <td>0.738125</td>\n",
       "      <td>0.563845</td>\n",
       "      <td>14.528181</td>\n",
       "      <td>9.742841</td>\n",
       "      <td>7.909205</td>\n",
       "      <td>10.726742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.210488</td>\n",
       "      <td>0.678205</td>\n",
       "      <td>0.720273</td>\n",
       "      <td>0.536322</td>\n",
       "      <td>14.795634</td>\n",
       "      <td>9.960898</td>\n",
       "      <td>8.172294</td>\n",
       "      <td>10.976275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.168256</td>\n",
       "      <td>0.659516</td>\n",
       "      <td>0.692282</td>\n",
       "      <td>0.506685</td>\n",
       "      <td>14.962012</td>\n",
       "      <td>10.248032</td>\n",
       "      <td>8.568721</td>\n",
       "      <td>11.259588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.131443</td>\n",
       "      <td>0.63523</td>\n",
       "      <td>0.650409</td>\n",
       "      <td>0.472361</td>\n",
       "      <td>15.125931</td>\n",
       "      <td>10.609022</td>\n",
       "      <td>9.13066</td>\n",
       "      <td>11.621871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.097431</td>\n",
       "      <td>0.608137</td>\n",
       "      <td>0.598881</td>\n",
       "      <td>0.434816</td>\n",
       "      <td>15.316889</td>\n",
       "      <td>10.997288</td>\n",
       "      <td>9.778765</td>\n",
       "      <td>12.030981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.063785</td>\n",
       "      <td>0.578267</td>\n",
       "      <td>0.546165</td>\n",
       "      <td>0.396072</td>\n",
       "      <td>15.383432</td>\n",
       "      <td>11.409793</td>\n",
       "      <td>10.400708</td>\n",
       "      <td>12.397978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.029071</td>\n",
       "      <td>0.547196</td>\n",
       "      <td>0.497796</td>\n",
       "      <td>0.358021</td>\n",
       "      <td>15.454968</td>\n",
       "      <td>11.824178</td>\n",
       "      <td>10.941025</td>\n",
       "      <td>12.740057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.004063</td>\n",
       "      <td>0.516059</td>\n",
       "      <td>0.45554</td>\n",
       "      <td>0.322512</td>\n",
       "      <td>15.560461</td>\n",
       "      <td>12.225568</td>\n",
       "      <td>11.391705</td>\n",
       "      <td>13.059245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.031917</td>\n",
       "      <td>0.485118</td>\n",
       "      <td>0.420584</td>\n",
       "      <td>0.291262</td>\n",
       "      <td>15.622177</td>\n",
       "      <td>12.612253</td>\n",
       "      <td>11.751797</td>\n",
       "      <td>13.328742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.054065</td>\n",
       "      <td>0.454018</td>\n",
       "      <td>0.39022</td>\n",
       "      <td>0.263391</td>\n",
       "      <td>15.639674</td>\n",
       "      <td>12.988511</td>\n",
       "      <td>12.056361</td>\n",
       "      <td>13.561516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.072159</td>\n",
       "      <td>0.4246</td>\n",
       "      <td>0.360514</td>\n",
       "      <td>0.237652</td>\n",
       "      <td>15.738657</td>\n",
       "      <td>13.334182</td>\n",
       "      <td>12.347628</td>\n",
       "      <td>13.806822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.086735</td>\n",
       "      <td>0.397913</td>\n",
       "      <td>0.331274</td>\n",
       "      <td>0.214151</td>\n",
       "      <td>15.867532</td>\n",
       "      <td>13.64028</td>\n",
       "      <td>12.627617</td>\n",
       "      <td>14.045143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.099874</td>\n",
       "      <td>0.371746</td>\n",
       "      <td>0.303438</td>\n",
       "      <td>0.19177</td>\n",
       "      <td>15.986116</td>\n",
       "      <td>13.933623</td>\n",
       "      <td>12.889223</td>\n",
       "      <td>14.269654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.245263</td>\n",
       "      <td>0.625616</td>\n",
       "      <td>0.6274</td>\n",
       "      <td>0.499426</td>\n",
       "      <td>13.790118</td>\n",
       "      <td>10.508852</td>\n",
       "      <td>9.053474</td>\n",
       "      <td>11.117481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.891104  0.903938  0.930305  0.908449   6.520242   5.426423   \n",
       "1      0.811955  0.846649  0.904724  0.854443    8.41275   6.858346   \n",
       "2      0.714969  0.798636  0.867596  0.793734   9.905543   7.861926   \n",
       "3      0.615725  0.761602  0.826606  0.734644  11.117234   8.557417   \n",
       "4      0.515354  0.734181   0.79238  0.680638  12.284601   9.040221   \n",
       "5      0.415209  0.716008  0.768545  0.633254   13.44732   9.348786   \n",
       "6      0.327808   0.70336  0.752337  0.594502  14.133014   9.557442   \n",
       "7      0.261475  0.691936  0.738125  0.563845  14.528181   9.742841   \n",
       "8      0.210488  0.678205  0.720273  0.536322  14.795634   9.960898   \n",
       "9      0.168256  0.659516  0.692282  0.506685  14.962012  10.248032   \n",
       "10     0.131443   0.63523  0.650409  0.472361  15.125931  10.609022   \n",
       "11     0.097431  0.608137  0.598881  0.434816  15.316889  10.997288   \n",
       "12     0.063785  0.578267  0.546165  0.396072  15.383432  11.409793   \n",
       "13     0.029071  0.547196  0.497796  0.358021  15.454968  11.824178   \n",
       "14    -0.004063  0.516059   0.45554  0.322512  15.560461  12.225568   \n",
       "15    -0.031917  0.485118  0.420584  0.291262  15.622177  12.612253   \n",
       "16    -0.054065  0.454018   0.39022  0.263391  15.639674  12.988511   \n",
       "17    -0.072159    0.4246  0.360514  0.237652  15.738657  13.334182   \n",
       "18    -0.086735  0.397913  0.331274  0.214151  15.867532   13.64028   \n",
       "19    -0.099874  0.371746  0.303438   0.19177  15.986116  13.933623   \n",
       "mean   0.245263  0.625616    0.6274  0.499426  13.790118  10.508852   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       4.084329   5.343664  \n",
       "1       4.774768   6.681955  \n",
       "2       5.627945   7.798471  \n",
       "3       6.439588   8.704747  \n",
       "4       7.045835   9.456886  \n",
       "5       7.438145  10.078084  \n",
       "6       7.693158  10.461205  \n",
       "7       7.909205  10.726742  \n",
       "8       8.172294  10.976275  \n",
       "9       8.568721  11.259588  \n",
       "10       9.13066  11.621871  \n",
       "11      9.778765  12.030981  \n",
       "12     10.400708  12.397978  \n",
       "13     10.941025  12.740057  \n",
       "14     11.391705  13.059245  \n",
       "15     11.751797  13.328742  \n",
       "16     12.056361  13.561516  \n",
       "17     12.347628  13.806822  \n",
       "18     12.627617  14.045143  \n",
       "19     12.889223  14.269654  \n",
       "mean    9.053474  11.117481  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/20_20_1_50_1_50_datt_seq2seq_gru_1.csv\n",
      "end training\n",
      "\n",
      "history size: 20\n",
      "future size: 30\n",
      "Epoch 1/10000\n",
      "568/568 - 8s - loss: 0.3634 - val_loss: 0.3258 - 8s/epoch - 13ms/step\n",
      "Epoch 2/10000\n",
      "568/568 - 4s - loss: 0.2869 - val_loss: 0.3143 - 4s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "568/568 - 4s - loss: 0.2689 - val_loss: 0.2773 - 4s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "568/568 - 4s - loss: 0.2511 - val_loss: 0.2681 - 4s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "568/568 - 4s - loss: 0.2359 - val_loss: 0.2625 - 4s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "568/568 - 5s - loss: 0.2220 - val_loss: 0.2418 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "568/568 - 6s - loss: 0.2111 - val_loss: 0.2534 - 6s/epoch - 11ms/step\n",
      "Epoch 8/10000\n",
      "568/568 - 4s - loss: 0.2029 - val_loss: 0.2243 - 4s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "568/568 - 5s - loss: 0.1944 - val_loss: 0.2106 - 5s/epoch - 9ms/step\n",
      "Epoch 10/10000\n",
      "568/568 - 4s - loss: 0.1861 - val_loss: 0.2076 - 4s/epoch - 7ms/step\n",
      "Epoch 11/10000\n",
      "568/568 - 4s - loss: 0.1793 - val_loss: 0.2019 - 4s/epoch - 7ms/step\n",
      "Epoch 12/10000\n",
      "568/568 - 4s - loss: 0.1727 - val_loss: 0.2041 - 4s/epoch - 7ms/step\n",
      "Epoch 13/10000\n",
      "568/568 - 4s - loss: 0.1696 - val_loss: 0.1891 - 4s/epoch - 7ms/step\n",
      "Epoch 14/10000\n",
      "568/568 - 4s - loss: 0.1626 - val_loss: 0.1855 - 4s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "568/568 - 4s - loss: 0.1592 - val_loss: 0.1844 - 4s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "568/568 - 4s - loss: 0.1549 - val_loss: 0.1777 - 4s/epoch - 7ms/step\n",
      "Epoch 17/10000\n",
      "568/568 - 4s - loss: 0.1526 - val_loss: 0.1793 - 4s/epoch - 7ms/step\n",
      "Epoch 18/10000\n",
      "568/568 - 4s - loss: 0.1473 - val_loss: 0.1742 - 4s/epoch - 7ms/step\n",
      "Epoch 19/10000\n",
      "568/568 - 4s - loss: 0.1428 - val_loss: 0.1653 - 4s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "568/568 - 4s - loss: 0.1411 - val_loss: 0.1665 - 4s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "568/568 - 4s - loss: 0.1379 - val_loss: 0.1607 - 4s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "568/568 - 4s - loss: 0.1349 - val_loss: 0.1611 - 4s/epoch - 7ms/step\n",
      "Epoch 23/10000\n",
      "568/568 - 4s - loss: 0.1344 - val_loss: 0.1591 - 4s/epoch - 7ms/step\n",
      "Epoch 24/10000\n",
      "568/568 - 4s - loss: 0.1327 - val_loss: 0.1507 - 4s/epoch - 7ms/step\n",
      "Epoch 25/10000\n",
      "568/568 - 4s - loss: 0.1290 - val_loss: 0.1515 - 4s/epoch - 7ms/step\n",
      "Epoch 26/10000\n",
      "568/568 - 4s - loss: 0.1281 - val_loss: 0.1493 - 4s/epoch - 7ms/step\n",
      "Epoch 27/10000\n",
      "568/568 - 4s - loss: 0.1258 - val_loss: 0.1483 - 4s/epoch - 7ms/step\n",
      "Epoch 28/10000\n",
      "568/568 - 4s - loss: 0.1216 - val_loss: 0.1421 - 4s/epoch - 7ms/step\n",
      "Epoch 29/10000\n",
      "568/568 - 4s - loss: 0.1205 - val_loss: 0.1401 - 4s/epoch - 7ms/step\n",
      "Epoch 30/10000\n",
      "568/568 - 4s - loss: 0.1203 - val_loss: 0.1459 - 4s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "568/568 - 4s - loss: 0.1168 - val_loss: 0.1400 - 4s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "568/568 - 4s - loss: 0.1148 - val_loss: 0.1265 - 4s/epoch - 7ms/step\n",
      "Epoch 33/10000\n",
      "568/568 - 4s - loss: 0.1156 - val_loss: 0.1238 - 4s/epoch - 7ms/step\n",
      "Epoch 34/10000\n",
      "568/568 - 4s - loss: 0.1080 - val_loss: 0.1244 - 4s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "568/568 - 4s - loss: 0.1117 - val_loss: 0.1289 - 4s/epoch - 7ms/step\n",
      "Epoch 36/10000\n",
      "568/568 - 4s - loss: 0.1072 - val_loss: 0.1219 - 4s/epoch - 7ms/step\n",
      "Epoch 37/10000\n",
      "568/568 - 5s - loss: 0.1062 - val_loss: 0.1214 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "568/568 - 4s - loss: 0.1034 - val_loss: 0.1263 - 4s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "568/568 - 4s - loss: 0.1044 - val_loss: 0.1164 - 4s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "568/568 - 4s - loss: 0.1039 - val_loss: 0.1154 - 4s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "568/568 - 5s - loss: 0.0993 - val_loss: 0.1139 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "568/568 - 4s - loss: 0.0986 - val_loss: 0.1168 - 4s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "568/568 - 4s - loss: 0.1017 - val_loss: 0.1183 - 4s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "568/568 - 4s - loss: 0.1008 - val_loss: 0.1130 - 4s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "568/568 - 4s - loss: 0.0959 - val_loss: 0.1105 - 4s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "568/568 - 4s - loss: 0.0942 - val_loss: 0.1127 - 4s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "568/568 - 4s - loss: 0.0952 - val_loss: 0.1097 - 4s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "568/568 - 4s - loss: 0.0939 - val_loss: 0.1079 - 4s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "568/568 - 4s - loss: 0.1027 - val_loss: 0.1152 - 4s/epoch - 7ms/step\n",
      "Epoch 50/10000\n",
      "568/568 - 4s - loss: 0.0906 - val_loss: 0.1095 - 4s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "568/568 - 4s - loss: 0.0924 - val_loss: 0.1119 - 4s/epoch - 7ms/step\n",
      "Epoch 52/10000\n",
      "568/568 - 4s - loss: 0.0930 - val_loss: 0.1026 - 4s/epoch - 7ms/step\n",
      "Epoch 53/10000\n",
      "568/568 - 4s - loss: 0.0858 - val_loss: 0.1015 - 4s/epoch - 7ms/step\n",
      "Epoch 54/10000\n",
      "568/568 - 4s - loss: 0.0882 - val_loss: 0.1031 - 4s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "568/568 - 4s - loss: 0.0864 - val_loss: 0.1034 - 4s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "568/568 - 4s - loss: 0.0878 - val_loss: 0.0997 - 4s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "568/568 - 4s - loss: 0.0855 - val_loss: 0.1020 - 4s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "568/568 - 4s - loss: 0.0921 - val_loss: 0.1013 - 4s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "568/568 - 4s - loss: 0.0855 - val_loss: 0.1021 - 4s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "568/568 - 4s - loss: 0.0827 - val_loss: 0.0966 - 4s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "568/568 - 4s - loss: 0.0824 - val_loss: 0.0981 - 4s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "568/568 - 5s - loss: 0.0834 - val_loss: 0.0920 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "568/568 - 4s - loss: 0.0806 - val_loss: 0.0982 - 4s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "568/568 - 5s - loss: 0.0789 - val_loss: 0.0941 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "568/568 - 5s - loss: 0.0795 - val_loss: 0.0956 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "568/568 - 4s - loss: 0.0805 - val_loss: 0.1018 - 4s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "568/568 - 4s - loss: 0.0784 - val_loss: 0.1000 - 4s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "568/568 - 4s - loss: 0.0765 - val_loss: 0.0986 - 4s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "568/568 - 5s - loss: 0.0783 - val_loss: 0.1093 - 5s/epoch - 9ms/step\n",
      "Epoch 70/10000\n",
      "568/568 - 5s - loss: 0.0772 - val_loss: 0.0964 - 5s/epoch - 9ms/step\n",
      "Epoch 71/10000\n",
      "568/568 - 5s - loss: 0.0759 - val_loss: 0.0940 - 5s/epoch - 9ms/step\n",
      "Epoch 72/10000\n",
      "568/568 - 5s - loss: 0.0755 - val_loss: 0.0970 - 5s/epoch - 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_14_layer_call_fn, gru_cell_14_layer_call_and_return_conditional_losses, gru_cell_15_layer_call_fn, gru_cell_15_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_30_1_50_1_50_datt_seq2seq_gru_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_30_1_50_1_50_datt_seq2seq_gru_1\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000294BD263C70> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000294DD760730> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.882487</td>\n",
       "      <td>0.899029</td>\n",
       "      <td>0.945168</td>\n",
       "      <td>0.908894</td>\n",
       "      <td>6.768261</td>\n",
       "      <td>5.562033</td>\n",
       "      <td>3.606043</td>\n",
       "      <td>5.312112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.798981</td>\n",
       "      <td>0.854179</td>\n",
       "      <td>0.914051</td>\n",
       "      <td>0.855737</td>\n",
       "      <td>8.852385</td>\n",
       "      <td>6.684545</td>\n",
       "      <td>4.5149</td>\n",
       "      <td>6.683943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.703577</td>\n",
       "      <td>0.804527</td>\n",
       "      <td>0.86436</td>\n",
       "      <td>0.790821</td>\n",
       "      <td>10.751006</td>\n",
       "      <td>7.740068</td>\n",
       "      <td>5.672047</td>\n",
       "      <td>8.054374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.608306</td>\n",
       "      <td>0.771862</td>\n",
       "      <td>0.814559</td>\n",
       "      <td>0.731576</td>\n",
       "      <td>12.360281</td>\n",
       "      <td>8.362859</td>\n",
       "      <td>6.6322</td>\n",
       "      <td>9.118446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.514328</td>\n",
       "      <td>0.748434</td>\n",
       "      <td>0.774455</td>\n",
       "      <td>0.679072</td>\n",
       "      <td>13.762991</td>\n",
       "      <td>8.783432</td>\n",
       "      <td>7.314228</td>\n",
       "      <td>9.95355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.424574</td>\n",
       "      <td>0.72833</td>\n",
       "      <td>0.740119</td>\n",
       "      <td>0.631008</td>\n",
       "      <td>14.978369</td>\n",
       "      <td>9.130328</td>\n",
       "      <td>7.850478</td>\n",
       "      <td>10.653058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.342283</td>\n",
       "      <td>0.708621</td>\n",
       "      <td>0.708657</td>\n",
       "      <td>0.58652</td>\n",
       "      <td>16.010785</td>\n",
       "      <td>9.458473</td>\n",
       "      <td>8.310962</td>\n",
       "      <td>11.260073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.270406</td>\n",
       "      <td>0.687727</td>\n",
       "      <td>0.679461</td>\n",
       "      <td>0.545864</td>\n",
       "      <td>16.859332</td>\n",
       "      <td>9.79467</td>\n",
       "      <td>8.715795</td>\n",
       "      <td>11.789932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.20886</td>\n",
       "      <td>0.665246</td>\n",
       "      <td>0.650619</td>\n",
       "      <td>0.508242</td>\n",
       "      <td>17.550986</td>\n",
       "      <td>10.145148</td>\n",
       "      <td>9.097334</td>\n",
       "      <td>12.26449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.153565</td>\n",
       "      <td>0.64044</td>\n",
       "      <td>0.6204</td>\n",
       "      <td>0.471468</td>\n",
       "      <td>18.149907</td>\n",
       "      <td>10.517879</td>\n",
       "      <td>9.481345</td>\n",
       "      <td>12.716377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.101215</td>\n",
       "      <td>0.614924</td>\n",
       "      <td>0.587926</td>\n",
       "      <td>0.434688</td>\n",
       "      <td>18.700342</td>\n",
       "      <td>10.887656</td>\n",
       "      <td>9.87812</td>\n",
       "      <td>13.155373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.051202</td>\n",
       "      <td>0.590316</td>\n",
       "      <td>0.553462</td>\n",
       "      <td>0.398327</td>\n",
       "      <td>18.859259</td>\n",
       "      <td>11.233572</td>\n",
       "      <td>10.283561</td>\n",
       "      <td>13.458797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.005121</td>\n",
       "      <td>0.565181</td>\n",
       "      <td>0.518737</td>\n",
       "      <td>0.363013</td>\n",
       "      <td>18.463539</td>\n",
       "      <td>11.577288</td>\n",
       "      <td>10.677086</td>\n",
       "      <td>13.572638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.036621</td>\n",
       "      <td>0.540293</td>\n",
       "      <td>0.485154</td>\n",
       "      <td>0.329608</td>\n",
       "      <td>18.213738</td>\n",
       "      <td>11.907983</td>\n",
       "      <td>11.0444</td>\n",
       "      <td>13.72204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.072924</td>\n",
       "      <td>0.515531</td>\n",
       "      <td>0.453661</td>\n",
       "      <td>0.298756</td>\n",
       "      <td>18.232371</td>\n",
       "      <td>12.229883</td>\n",
       "      <td>11.377896</td>\n",
       "      <td>13.946717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.10331</td>\n",
       "      <td>0.491866</td>\n",
       "      <td>0.424074</td>\n",
       "      <td>0.270877</td>\n",
       "      <td>18.427905</td>\n",
       "      <td>12.531065</td>\n",
       "      <td>11.681732</td>\n",
       "      <td>14.213567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.128034</td>\n",
       "      <td>0.470726</td>\n",
       "      <td>0.396381</td>\n",
       "      <td>0.246358</td>\n",
       "      <td>18.270766</td>\n",
       "      <td>12.792805</td>\n",
       "      <td>11.959381</td>\n",
       "      <td>14.340984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.146973</td>\n",
       "      <td>0.452224</td>\n",
       "      <td>0.370212</td>\n",
       "      <td>0.225155</td>\n",
       "      <td>18.075031</td>\n",
       "      <td>13.018444</td>\n",
       "      <td>12.216157</td>\n",
       "      <td>14.436544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.161484</td>\n",
       "      <td>0.435221</td>\n",
       "      <td>0.345521</td>\n",
       "      <td>0.206419</td>\n",
       "      <td>17.924165</td>\n",
       "      <td>13.222971</td>\n",
       "      <td>12.454042</td>\n",
       "      <td>14.533726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.173072</td>\n",
       "      <td>0.41828</td>\n",
       "      <td>0.321932</td>\n",
       "      <td>0.189047</td>\n",
       "      <td>17.756549</td>\n",
       "      <td>13.422159</td>\n",
       "      <td>12.677247</td>\n",
       "      <td>14.618652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.180717</td>\n",
       "      <td>0.399792</td>\n",
       "      <td>0.300238</td>\n",
       "      <td>0.173104</td>\n",
       "      <td>17.633564</td>\n",
       "      <td>13.635922</td>\n",
       "      <td>12.879508</td>\n",
       "      <td>14.716331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.185981</td>\n",
       "      <td>0.381048</td>\n",
       "      <td>0.281444</td>\n",
       "      <td>0.158837</td>\n",
       "      <td>17.563918</td>\n",
       "      <td>13.848646</td>\n",
       "      <td>13.052975</td>\n",
       "      <td>14.821846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.189098</td>\n",
       "      <td>0.361484</td>\n",
       "      <td>0.26604</td>\n",
       "      <td>0.146142</td>\n",
       "      <td>17.349493</td>\n",
       "      <td>14.067012</td>\n",
       "      <td>13.194018</td>\n",
       "      <td>14.870174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.18943</td>\n",
       "      <td>0.341673</td>\n",
       "      <td>0.252966</td>\n",
       "      <td>0.13507</td>\n",
       "      <td>17.121794</td>\n",
       "      <td>14.285607</td>\n",
       "      <td>13.312662</td>\n",
       "      <td>14.906688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.188275</td>\n",
       "      <td>0.322664</td>\n",
       "      <td>0.24198</td>\n",
       "      <td>0.125456</td>\n",
       "      <td>16.944484</td>\n",
       "      <td>14.492293</td>\n",
       "      <td>13.411101</td>\n",
       "      <td>14.949293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.187747</td>\n",
       "      <td>0.305559</td>\n",
       "      <td>0.231695</td>\n",
       "      <td>0.116502</td>\n",
       "      <td>16.775327</td>\n",
       "      <td>14.67564</td>\n",
       "      <td>13.503238</td>\n",
       "      <td>14.984735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.187094</td>\n",
       "      <td>0.289312</td>\n",
       "      <td>0.222221</td>\n",
       "      <td>0.108146</td>\n",
       "      <td>16.609219</td>\n",
       "      <td>14.846169</td>\n",
       "      <td>13.588633</td>\n",
       "      <td>15.014674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.187728</td>\n",
       "      <td>0.274327</td>\n",
       "      <td>0.213197</td>\n",
       "      <td>0.099932</td>\n",
       "      <td>16.573424</td>\n",
       "      <td>15.000741</td>\n",
       "      <td>13.671183</td>\n",
       "      <td>15.081783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.189922</td>\n",
       "      <td>0.258877</td>\n",
       "      <td>0.204414</td>\n",
       "      <td>0.091123</td>\n",
       "      <td>16.608122</td>\n",
       "      <td>15.157862</td>\n",
       "      <td>13.751643</td>\n",
       "      <td>15.172543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.192212</td>\n",
       "      <td>0.24292</td>\n",
       "      <td>0.197181</td>\n",
       "      <td>0.082629</td>\n",
       "      <td>16.643216</td>\n",
       "      <td>15.317608</td>\n",
       "      <td>13.818827</td>\n",
       "      <td>15.259884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.078809</td>\n",
       "      <td>0.52602</td>\n",
       "      <td>0.486009</td>\n",
       "      <td>0.363613</td>\n",
       "      <td>16.293018</td>\n",
       "      <td>11.811025</td>\n",
       "      <td>10.654291</td>\n",
       "      <td>12.919445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.882487  0.899029  0.945168  0.908894   6.768261   5.562033   \n",
       "1      0.798981  0.854179  0.914051  0.855737   8.852385   6.684545   \n",
       "2      0.703577  0.804527   0.86436  0.790821  10.751006   7.740068   \n",
       "3      0.608306  0.771862  0.814559  0.731576  12.360281   8.362859   \n",
       "4      0.514328  0.748434  0.774455  0.679072  13.762991   8.783432   \n",
       "5      0.424574   0.72833  0.740119  0.631008  14.978369   9.130328   \n",
       "6      0.342283  0.708621  0.708657   0.58652  16.010785   9.458473   \n",
       "7      0.270406  0.687727  0.679461  0.545864  16.859332    9.79467   \n",
       "8       0.20886  0.665246  0.650619  0.508242  17.550986  10.145148   \n",
       "9      0.153565   0.64044    0.6204  0.471468  18.149907  10.517879   \n",
       "10     0.101215  0.614924  0.587926  0.434688  18.700342  10.887656   \n",
       "11     0.051202  0.590316  0.553462  0.398327  18.859259  11.233572   \n",
       "12     0.005121  0.565181  0.518737  0.363013  18.463539  11.577288   \n",
       "13    -0.036621  0.540293  0.485154  0.329608  18.213738  11.907983   \n",
       "14    -0.072924  0.515531  0.453661  0.298756  18.232371  12.229883   \n",
       "15     -0.10331  0.491866  0.424074  0.270877  18.427905  12.531065   \n",
       "16    -0.128034  0.470726  0.396381  0.246358  18.270766  12.792805   \n",
       "17    -0.146973  0.452224  0.370212  0.225155  18.075031  13.018444   \n",
       "18    -0.161484  0.435221  0.345521  0.206419  17.924165  13.222971   \n",
       "19    -0.173072   0.41828  0.321932  0.189047  17.756549  13.422159   \n",
       "20    -0.180717  0.399792  0.300238  0.173104  17.633564  13.635922   \n",
       "21    -0.185981  0.381048  0.281444  0.158837  17.563918  13.848646   \n",
       "22    -0.189098  0.361484   0.26604  0.146142  17.349493  14.067012   \n",
       "23     -0.18943  0.341673  0.252966   0.13507  17.121794  14.285607   \n",
       "24    -0.188275  0.322664   0.24198  0.125456  16.944484  14.492293   \n",
       "25    -0.187747  0.305559  0.231695  0.116502  16.775327   14.67564   \n",
       "26    -0.187094  0.289312  0.222221  0.108146  16.609219  14.846169   \n",
       "27    -0.187728  0.274327  0.213197  0.099932  16.573424  15.000741   \n",
       "28    -0.189922  0.258877  0.204414  0.091123  16.608122  15.157862   \n",
       "29    -0.192212   0.24292  0.197181  0.082629  16.643216  15.317608   \n",
       "mean   0.078809   0.52602  0.486009  0.363613  16.293018  11.811025   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       3.606043   5.312112  \n",
       "1         4.5149   6.683943  \n",
       "2       5.672047   8.054374  \n",
       "3         6.6322   9.118446  \n",
       "4       7.314228    9.95355  \n",
       "5       7.850478  10.653058  \n",
       "6       8.310962  11.260073  \n",
       "7       8.715795  11.789932  \n",
       "8       9.097334   12.26449  \n",
       "9       9.481345  12.716377  \n",
       "10       9.87812  13.155373  \n",
       "11     10.283561  13.458797  \n",
       "12     10.677086  13.572638  \n",
       "13       11.0444   13.72204  \n",
       "14     11.377896  13.946717  \n",
       "15     11.681732  14.213567  \n",
       "16     11.959381  14.340984  \n",
       "17     12.216157  14.436544  \n",
       "18     12.454042  14.533726  \n",
       "19     12.677247  14.618652  \n",
       "20     12.879508  14.716331  \n",
       "21     13.052975  14.821846  \n",
       "22     13.194018  14.870174  \n",
       "23     13.312662  14.906688  \n",
       "24     13.411101  14.949293  \n",
       "25     13.503238  14.984735  \n",
       "26     13.588633  15.014674  \n",
       "27     13.671183  15.081783  \n",
       "28     13.751643  15.172543  \n",
       "29     13.818827  15.259884  \n",
       "mean   10.654291  12.919445  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/20_30_1_50_1_50_datt_seq2seq_gru_1.csv\n",
      "end training\n",
      "\n",
      "history size: 30\n",
      "future size: 10\n",
      "Epoch 1/10000\n",
      "572/572 - 8s - loss: 0.2337 - val_loss: 0.1645 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "572/572 - 5s - loss: 0.1518 - val_loss: 0.1396 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "572/572 - 5s - loss: 0.1399 - val_loss: 0.1303 - 5s/epoch - 9ms/step\n",
      "Epoch 4/10000\n",
      "572/572 - 4s - loss: 0.1294 - val_loss: 0.1221 - 4s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "572/572 - 5s - loss: 0.1205 - val_loss: 0.1094 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "572/572 - 5s - loss: 0.1114 - val_loss: 0.1070 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "572/572 - 5s - loss: 0.1042 - val_loss: 0.0991 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "572/572 - 4s - loss: 0.1004 - val_loss: 0.0983 - 4s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "572/572 - 5s - loss: 0.0966 - val_loss: 0.0933 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "572/572 - 5s - loss: 0.0940 - val_loss: 0.0911 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "572/572 - 5s - loss: 0.0898 - val_loss: 0.0899 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "572/572 - 5s - loss: 0.0871 - val_loss: 0.0898 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "572/572 - 5s - loss: 0.0845 - val_loss: 0.0864 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "572/572 - 5s - loss: 0.0829 - val_loss: 0.0825 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "572/572 - 5s - loss: 0.0796 - val_loss: 0.0852 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "572/572 - 5s - loss: 0.0798 - val_loss: 0.0831 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "572/572 - 5s - loss: 0.0752 - val_loss: 0.0794 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "572/572 - 4s - loss: 0.0747 - val_loss: 0.0802 - 4s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "572/572 - 4s - loss: 0.0705 - val_loss: 0.0773 - 4s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "572/572 - 5s - loss: 0.0673 - val_loss: 0.0806 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "572/572 - 5s - loss: 0.0685 - val_loss: 0.0759 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "572/572 - 4s - loss: 0.0682 - val_loss: 0.0763 - 4s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "572/572 - 5s - loss: 0.0652 - val_loss: 0.0746 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "572/572 - 5s - loss: 0.0674 - val_loss: 0.0771 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "572/572 - 5s - loss: 0.0639 - val_loss: 0.0729 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "572/572 - 4s - loss: 0.0620 - val_loss: 0.0709 - 4s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "572/572 - 4s - loss: 0.0597 - val_loss: 0.0722 - 4s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "572/572 - 4s - loss: 0.0603 - val_loss: 0.0676 - 4s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "572/572 - 4s - loss: 0.0596 - val_loss: 0.0711 - 4s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "572/572 - 5s - loss: 0.0580 - val_loss: 0.0681 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "572/572 - 5s - loss: 0.0575 - val_loss: 0.0773 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "572/572 - 4s - loss: 0.0605 - val_loss: 0.0643 - 4s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "572/572 - 5s - loss: 0.0548 - val_loss: 0.0650 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "572/572 - 5s - loss: 0.0551 - val_loss: 0.0685 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "572/572 - 5s - loss: 0.0537 - val_loss: 0.0627 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "572/572 - 5s - loss: 0.0522 - val_loss: 0.0659 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "572/572 - 4s - loss: 0.0534 - val_loss: 0.0650 - 4s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "572/572 - 4s - loss: 0.0533 - val_loss: 0.0634 - 4s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "572/572 - 5s - loss: 0.0521 - val_loss: 0.0645 - 5s/epoch - 9ms/step\n",
      "Epoch 40/10000\n",
      "572/572 - 4s - loss: 0.0507 - val_loss: 0.0610 - 4s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "572/572 - 5s - loss: 0.0509 - val_loss: 0.0625 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "572/572 - 5s - loss: 0.0524 - val_loss: 0.0639 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "572/572 - 5s - loss: 0.0495 - val_loss: 0.0601 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "572/572 - 5s - loss: 0.0488 - val_loss: 0.0589 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "572/572 - 5s - loss: 0.0480 - val_loss: 0.0571 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "572/572 - 5s - loss: 0.0473 - val_loss: 0.0572 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "572/572 - 5s - loss: 0.0481 - val_loss: 0.0660 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "572/572 - 5s - loss: 0.0476 - val_loss: 0.0585 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "572/572 - 5s - loss: 0.0477 - val_loss: 0.0574 - 5s/epoch - 9ms/step\n",
      "Epoch 50/10000\n",
      "572/572 - 5s - loss: 0.0469 - val_loss: 0.0609 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "572/572 - 5s - loss: 0.0461 - val_loss: 0.0551 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "572/572 - 5s - loss: 0.0441 - val_loss: 0.0562 - 5s/epoch - 9ms/step\n",
      "Epoch 53/10000\n",
      "572/572 - 5s - loss: 0.0494 - val_loss: 0.0617 - 5s/epoch - 9ms/step\n",
      "Epoch 54/10000\n",
      "572/572 - 5s - loss: 0.0461 - val_loss: 0.0555 - 5s/epoch - 10ms/step\n",
      "Epoch 55/10000\n",
      "572/572 - 6s - loss: 0.0437 - val_loss: 0.0542 - 6s/epoch - 10ms/step\n",
      "Epoch 56/10000\n",
      "572/572 - 6s - loss: 0.0433 - val_loss: 0.0542 - 6s/epoch - 10ms/step\n",
      "Epoch 57/10000\n",
      "572/572 - 5s - loss: 0.0433 - val_loss: 0.0542 - 5s/epoch - 9ms/step\n",
      "Epoch 58/10000\n",
      "572/572 - 5s - loss: 0.0429 - val_loss: 0.0543 - 5s/epoch - 9ms/step\n",
      "Epoch 59/10000\n",
      "572/572 - 6s - loss: 0.0432 - val_loss: 0.0598 - 6s/epoch - 10ms/step\n",
      "Epoch 60/10000\n",
      "572/572 - 5s - loss: 0.0430 - val_loss: 0.0530 - 5s/epoch - 9ms/step\n",
      "Epoch 61/10000\n",
      "572/572 - 5s - loss: 0.0411 - val_loss: 0.0520 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "572/572 - 5s - loss: 0.0411 - val_loss: 0.0525 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "572/572 - 4s - loss: 0.0419 - val_loss: 0.0540 - 4s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "572/572 - 4s - loss: 0.0419 - val_loss: 0.0516 - 4s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "572/572 - 5s - loss: 0.0403 - val_loss: 0.0549 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "572/572 - 5s - loss: 0.0418 - val_loss: 0.0588 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "572/572 - 5s - loss: 0.0406 - val_loss: 0.0526 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "572/572 - 5s - loss: 0.0394 - val_loss: 0.0502 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "572/572 - 5s - loss: 0.0387 - val_loss: 0.0532 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "572/572 - 5s - loss: 0.0398 - val_loss: 0.0491 - 5s/epoch - 9ms/step\n",
      "Epoch 71/10000\n",
      "572/572 - 5s - loss: 0.0396 - val_loss: 0.0501 - 5s/epoch - 9ms/step\n",
      "Epoch 72/10000\n",
      "572/572 - 6s - loss: 0.0389 - val_loss: 0.0489 - 6s/epoch - 10ms/step\n",
      "Epoch 73/10000\n",
      "572/572 - 5s - loss: 0.0381 - val_loss: 0.0480 - 5s/epoch - 9ms/step\n",
      "Epoch 74/10000\n",
      "572/572 - 5s - loss: 0.0378 - val_loss: 0.0491 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "572/572 - 5s - loss: 0.0378 - val_loss: 0.0494 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "572/572 - 5s - loss: 0.0412 - val_loss: 0.0547 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "572/572 - 5s - loss: 0.0382 - val_loss: 0.0490 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "572/572 - 4s - loss: 0.0362 - val_loss: 0.0509 - 4s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "572/572 - 4s - loss: 0.0377 - val_loss: 0.0492 - 4s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "572/572 - 5s - loss: 0.0369 - val_loss: 0.0501 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "572/572 - 5s - loss: 0.0360 - val_loss: 0.0482 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "572/572 - 4s - loss: 0.0365 - val_loss: 0.0492 - 4s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "572/572 - 4s - loss: 0.0359 - val_loss: 0.0475 - 4s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "572/572 - 4s - loss: 0.0360 - val_loss: 0.0516 - 4s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "572/572 - 5s - loss: 0.0360 - val_loss: 0.0468 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "572/572 - 5s - loss: 0.0342 - val_loss: 0.0464 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "572/572 - 5s - loss: 0.0354 - val_loss: 0.0483 - 5s/epoch - 9ms/step\n",
      "Epoch 88/10000\n",
      "572/572 - 5s - loss: 0.0345 - val_loss: 0.0459 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "572/572 - 5s - loss: 0.0352 - val_loss: 0.0487 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "572/572 - 5s - loss: 0.0339 - val_loss: 0.0460 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "572/572 - 5s - loss: 0.0333 - val_loss: 0.0476 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "572/572 - 4s - loss: 0.0347 - val_loss: 0.0498 - 4s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "572/572 - 5s - loss: 0.0354 - val_loss: 0.0442 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "572/572 - 5s - loss: 0.0328 - val_loss: 0.0454 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "572/572 - 5s - loss: 0.0330 - val_loss: 0.0463 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "572/572 - 5s - loss: 0.0328 - val_loss: 0.0442 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "572/572 - 5s - loss: 0.0339 - val_loss: 0.0458 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "572/572 - 4s - loss: 0.0328 - val_loss: 0.0471 - 4s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "572/572 - 5s - loss: 0.0323 - val_loss: 0.0435 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "572/572 - 4s - loss: 0.0319 - val_loss: 0.0469 - 4s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "572/572 - 4s - loss: 0.0324 - val_loss: 0.0475 - 4s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "572/572 - 5s - loss: 0.0338 - val_loss: 0.0438 - 5s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "572/572 - 5s - loss: 0.0318 - val_loss: 0.0430 - 5s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "572/572 - 5s - loss: 0.0321 - val_loss: 0.0430 - 5s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "572/572 - 4s - loss: 0.0317 - val_loss: 0.0444 - 4s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "572/572 - 4s - loss: 0.0311 - val_loss: 0.0416 - 4s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "572/572 - 5s - loss: 0.0306 - val_loss: 0.0423 - 5s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "572/572 - 5s - loss: 0.0347 - val_loss: 0.0474 - 5s/epoch - 8ms/step\n",
      "Epoch 109/10000\n",
      "572/572 - 5s - loss: 0.0320 - val_loss: 0.0434 - 5s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "572/572 - 4s - loss: 0.0301 - val_loss: 0.0411 - 4s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "572/572 - 4s - loss: 0.0306 - val_loss: 0.0407 - 4s/epoch - 7ms/step\n",
      "Epoch 112/10000\n",
      "572/572 - 4s - loss: 0.0301 - val_loss: 0.0440 - 4s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "572/572 - 4s - loss: 0.0305 - val_loss: 0.0450 - 4s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "572/572 - 4s - loss: 0.0304 - val_loss: 0.0455 - 4s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "572/572 - 4s - loss: 0.0301 - val_loss: 0.0423 - 4s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "572/572 - 5s - loss: 0.0304 - val_loss: 0.0431 - 5s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "572/572 - 5s - loss: 0.0306 - val_loss: 0.0410 - 5s/epoch - 8ms/step\n",
      "Epoch 118/10000\n",
      "572/572 - 5s - loss: 0.0318 - val_loss: 0.0425 - 5s/epoch - 8ms/step\n",
      "Epoch 119/10000\n",
      "572/572 - 5s - loss: 0.0290 - val_loss: 0.0401 - 5s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "572/572 - 5s - loss: 0.0290 - val_loss: 0.0394 - 5s/epoch - 8ms/step\n",
      "Epoch 121/10000\n",
      "572/572 - 5s - loss: 0.0290 - val_loss: 0.0391 - 5s/epoch - 8ms/step\n",
      "Epoch 122/10000\n",
      "572/572 - 5s - loss: 0.0299 - val_loss: 0.0443 - 5s/epoch - 8ms/step\n",
      "Epoch 123/10000\n",
      "572/572 - 5s - loss: 0.0297 - val_loss: 0.0412 - 5s/epoch - 8ms/step\n",
      "Epoch 124/10000\n",
      "572/572 - 5s - loss: 0.0287 - val_loss: 0.0442 - 5s/epoch - 9ms/step\n",
      "Epoch 125/10000\n",
      "572/572 - 5s - loss: 0.0289 - val_loss: 0.0408 - 5s/epoch - 8ms/step\n",
      "Epoch 126/10000\n",
      "572/572 - 5s - loss: 0.0286 - val_loss: 0.0415 - 5s/epoch - 8ms/step\n",
      "Epoch 127/10000\n",
      "572/572 - 5s - loss: 0.0287 - val_loss: 0.0397 - 5s/epoch - 8ms/step\n",
      "Epoch 128/10000\n",
      "572/572 - 5s - loss: 0.0289 - val_loss: 0.0422 - 5s/epoch - 9ms/step\n",
      "Epoch 129/10000\n",
      "572/572 - 5s - loss: 0.0293 - val_loss: 0.0410 - 5s/epoch - 8ms/step\n",
      "Epoch 130/10000\n",
      "572/572 - 5s - loss: 0.0282 - val_loss: 0.0418 - 5s/epoch - 8ms/step\n",
      "Epoch 131/10000\n",
      "572/572 - 5s - loss: 0.0284 - val_loss: 0.0425 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_16_layer_call_fn, gru_cell_16_layer_call_and_return_conditional_losses, gru_cell_17_layer_call_fn, gru_cell_17_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_10_1_50_1_50_datt_seq2seq_gru_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_10_1_50_1_50_datt_seq2seq_gru_1\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000294BAD8BCD0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000294BCC6DA00> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.920271</td>\n",
       "      <td>0.912958</td>\n",
       "      <td>0.955727</td>\n",
       "      <td>0.929652</td>\n",
       "      <td>4.5828</td>\n",
       "      <td>5.182393</td>\n",
       "      <td>3.249324</td>\n",
       "      <td>4.338173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.826701</td>\n",
       "      <td>0.836671</td>\n",
       "      <td>0.909692</td>\n",
       "      <td>0.857688</td>\n",
       "      <td>6.711645</td>\n",
       "      <td>7.099865</td>\n",
       "      <td>4.63993</td>\n",
       "      <td>6.15048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.700891</td>\n",
       "      <td>0.778674</td>\n",
       "      <td>0.856617</td>\n",
       "      <td>0.778728</td>\n",
       "      <td>8.695206</td>\n",
       "      <td>8.265608</td>\n",
       "      <td>5.846045</td>\n",
       "      <td>7.602286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.568866</td>\n",
       "      <td>0.742091</td>\n",
       "      <td>0.811388</td>\n",
       "      <td>0.707448</td>\n",
       "      <td>10.298651</td>\n",
       "      <td>8.923791</td>\n",
       "      <td>6.705048</td>\n",
       "      <td>8.642497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.438451</td>\n",
       "      <td>0.718414</td>\n",
       "      <td>0.780049</td>\n",
       "      <td>0.645638</td>\n",
       "      <td>11.63686</td>\n",
       "      <td>9.325645</td>\n",
       "      <td>7.240498</td>\n",
       "      <td>9.401001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.308282</td>\n",
       "      <td>0.705154</td>\n",
       "      <td>0.761644</td>\n",
       "      <td>0.591693</td>\n",
       "      <td>12.790391</td>\n",
       "      <td>9.544136</td>\n",
       "      <td>7.537405</td>\n",
       "      <td>9.957311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.18415</td>\n",
       "      <td>0.695165</td>\n",
       "      <td>0.747778</td>\n",
       "      <td>0.542364</td>\n",
       "      <td>13.759393</td>\n",
       "      <td>9.70516</td>\n",
       "      <td>7.75392</td>\n",
       "      <td>10.406158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.075019</td>\n",
       "      <td>0.683099</td>\n",
       "      <td>0.732546</td>\n",
       "      <td>0.496888</td>\n",
       "      <td>14.618559</td>\n",
       "      <td>9.895632</td>\n",
       "      <td>7.985331</td>\n",
       "      <td>10.833174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.013426</td>\n",
       "      <td>0.664678</td>\n",
       "      <td>0.714087</td>\n",
       "      <td>0.455113</td>\n",
       "      <td>15.322986</td>\n",
       "      <td>10.179452</td>\n",
       "      <td>8.256851</td>\n",
       "      <td>11.253097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.079839</td>\n",
       "      <td>0.64031</td>\n",
       "      <td>0.692378</td>\n",
       "      <td>0.417617</td>\n",
       "      <td>15.839844</td>\n",
       "      <td>10.542902</td>\n",
       "      <td>8.565553</td>\n",
       "      <td>11.649433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.392937</td>\n",
       "      <td>0.737721</td>\n",
       "      <td>0.796191</td>\n",
       "      <td>0.642283</td>\n",
       "      <td>11.425634</td>\n",
       "      <td>8.866459</td>\n",
       "      <td>6.77799</td>\n",
       "      <td>9.023361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE     nRMSE   \n",
       "0      0.920271  0.912958  0.955727  0.929652     4.5828   5.182393  3.249324   \n",
       "1      0.826701  0.836671  0.909692  0.857688   6.711645   7.099865   4.63993   \n",
       "2      0.700891  0.778674  0.856617  0.778728   8.695206   8.265608  5.846045   \n",
       "3      0.568866  0.742091  0.811388  0.707448  10.298651   8.923791  6.705048   \n",
       "4      0.438451  0.718414  0.780049  0.645638   11.63686   9.325645  7.240498   \n",
       "5      0.308282  0.705154  0.761644  0.591693  12.790391   9.544136  7.537405   \n",
       "6       0.18415  0.695165  0.747778  0.542364  13.759393    9.70516   7.75392   \n",
       "7      0.075019  0.683099  0.732546  0.496888  14.618559   9.895632  7.985331   \n",
       "8     -0.013426  0.664678  0.714087  0.455113  15.322986  10.179452  8.256851   \n",
       "9     -0.079839   0.64031  0.692378  0.417617  15.839844  10.542902  8.565553   \n",
       "mean   0.392937  0.737721  0.796191  0.642283  11.425634   8.866459   6.77799   \n",
       "\n",
       "            mean  \n",
       "index      nRMSE  \n",
       "0       4.338173  \n",
       "1        6.15048  \n",
       "2       7.602286  \n",
       "3       8.642497  \n",
       "4       9.401001  \n",
       "5       9.957311  \n",
       "6      10.406158  \n",
       "7      10.833174  \n",
       "8      11.253097  \n",
       "9      11.649433  \n",
       "mean    9.023361  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/30_10_1_50_1_50_datt_seq2seq_gru_1.csv\n",
      "end training\n",
      "\n",
      "history size: 30\n",
      "future size: 20\n",
      "Epoch 1/10000\n",
      "568/568 - 8s - loss: 0.3208 - val_loss: 0.2593 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "568/568 - 4s - loss: 0.2195 - val_loss: 0.2431 - 4s/epoch - 7ms/step\n",
      "Epoch 3/10000\n",
      "568/568 - 4s - loss: 0.2047 - val_loss: 0.2272 - 4s/epoch - 7ms/step\n",
      "Epoch 4/10000\n",
      "568/568 - 4s - loss: 0.1950 - val_loss: 0.2147 - 4s/epoch - 7ms/step\n",
      "Epoch 5/10000\n",
      "568/568 - 4s - loss: 0.1869 - val_loss: 0.2140 - 4s/epoch - 7ms/step\n",
      "Epoch 6/10000\n",
      "568/568 - 4s - loss: 0.1781 - val_loss: 0.2017 - 4s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "568/568 - 4s - loss: 0.1710 - val_loss: 0.1965 - 4s/epoch - 7ms/step\n",
      "Epoch 8/10000\n",
      "568/568 - 4s - loss: 0.1644 - val_loss: 0.1913 - 4s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "568/568 - 4s - loss: 0.1577 - val_loss: 0.1791 - 4s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "568/568 - 5s - loss: 0.1535 - val_loss: 0.1720 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "568/568 - 4s - loss: 0.1430 - val_loss: 0.1724 - 4s/epoch - 7ms/step\n",
      "Epoch 12/10000\n",
      "568/568 - 5s - loss: 0.1420 - val_loss: 0.1585 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "568/568 - 4s - loss: 0.1333 - val_loss: 0.1565 - 4s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "568/568 - 4s - loss: 0.1291 - val_loss: 0.1512 - 4s/epoch - 7ms/step\n",
      "Epoch 15/10000\n",
      "568/568 - 4s - loss: 0.1245 - val_loss: 0.1450 - 4s/epoch - 7ms/step\n",
      "Epoch 16/10000\n",
      "568/568 - 4s - loss: 0.1191 - val_loss: 0.1454 - 4s/epoch - 7ms/step\n",
      "Epoch 17/10000\n",
      "568/568 - 4s - loss: 0.1134 - val_loss: 0.1355 - 4s/epoch - 7ms/step\n",
      "Epoch 18/10000\n",
      "568/568 - 4s - loss: 0.1116 - val_loss: 0.1296 - 4s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "568/568 - 4s - loss: 0.1069 - val_loss: 0.1278 - 4s/epoch - 7ms/step\n",
      "Epoch 20/10000\n",
      "568/568 - 4s - loss: 0.1052 - val_loss: 0.1227 - 4s/epoch - 7ms/step\n",
      "Epoch 21/10000\n",
      "568/568 - 4s - loss: 0.1035 - val_loss: 0.1210 - 4s/epoch - 7ms/step\n",
      "Epoch 22/10000\n",
      "568/568 - 4s - loss: 0.0997 - val_loss: 0.1269 - 4s/epoch - 7ms/step\n",
      "Epoch 23/10000\n",
      "568/568 - 4s - loss: 0.0969 - val_loss: 0.1181 - 4s/epoch - 7ms/step\n",
      "Epoch 24/10000\n",
      "568/568 - 4s - loss: 0.0959 - val_loss: 0.1166 - 4s/epoch - 7ms/step\n",
      "Epoch 25/10000\n",
      "568/568 - 4s - loss: 0.0940 - val_loss: 0.1163 - 4s/epoch - 7ms/step\n",
      "Epoch 26/10000\n",
      "568/568 - 4s - loss: 0.0923 - val_loss: 0.1169 - 4s/epoch - 7ms/step\n",
      "Epoch 27/10000\n",
      "568/568 - 4s - loss: 0.0908 - val_loss: 0.1246 - 4s/epoch - 7ms/step\n",
      "Epoch 28/10000\n",
      "568/568 - 4s - loss: 0.0886 - val_loss: 0.1101 - 4s/epoch - 7ms/step\n",
      "Epoch 29/10000\n",
      "568/568 - 4s - loss: 0.0856 - val_loss: 0.1095 - 4s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "568/568 - 4s - loss: 0.0878 - val_loss: 0.1080 - 4s/epoch - 7ms/step\n",
      "Epoch 31/10000\n",
      "568/568 - 4s - loss: 0.0855 - val_loss: 0.1062 - 4s/epoch - 7ms/step\n",
      "Epoch 32/10000\n",
      "568/568 - 4s - loss: 0.0826 - val_loss: 0.1028 - 4s/epoch - 7ms/step\n",
      "Epoch 33/10000\n",
      "568/568 - 4s - loss: 0.0833 - val_loss: 0.1070 - 4s/epoch - 7ms/step\n",
      "Epoch 34/10000\n",
      "568/568 - 4s - loss: 0.0803 - val_loss: 0.0969 - 4s/epoch - 7ms/step\n",
      "Epoch 35/10000\n",
      "568/568 - 4s - loss: 0.0794 - val_loss: 0.1038 - 4s/epoch - 7ms/step\n",
      "Epoch 36/10000\n",
      "568/568 - 4s - loss: 0.0795 - val_loss: 0.1098 - 4s/epoch - 7ms/step\n",
      "Epoch 37/10000\n",
      "568/568 - 4s - loss: 0.0768 - val_loss: 0.0916 - 4s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "568/568 - 5s - loss: 0.0792 - val_loss: 0.1004 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "568/568 - 5s - loss: 0.0748 - val_loss: 0.0990 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "568/568 - 5s - loss: 0.0750 - val_loss: 0.0963 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "568/568 - 5s - loss: 0.0812 - val_loss: 0.1036 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "568/568 - 5s - loss: 0.0755 - val_loss: 0.0942 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "568/568 - 5s - loss: 0.0731 - val_loss: 0.0885 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "568/568 - 5s - loss: 0.0699 - val_loss: 0.0912 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "568/568 - 5s - loss: 0.0726 - val_loss: 0.0884 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "568/568 - 5s - loss: 0.0712 - val_loss: 0.0908 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "568/568 - 5s - loss: 0.0712 - val_loss: 0.1124 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "568/568 - 5s - loss: 0.0709 - val_loss: 0.0885 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "568/568 - 5s - loss: 0.0687 - val_loss: 0.0958 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "568/568 - 5s - loss: 0.0693 - val_loss: 0.0868 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "568/568 - 4s - loss: 0.0665 - val_loss: 0.0835 - 4s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "568/568 - 4s - loss: 0.0652 - val_loss: 0.0865 - 4s/epoch - 7ms/step\n",
      "Epoch 53/10000\n",
      "568/568 - 4s - loss: 0.0686 - val_loss: 0.0859 - 4s/epoch - 7ms/step\n",
      "Epoch 54/10000\n",
      "568/568 - 4s - loss: 0.0648 - val_loss: 0.0782 - 4s/epoch - 7ms/step\n",
      "Epoch 55/10000\n",
      "568/568 - 4s - loss: 0.0651 - val_loss: 0.0897 - 4s/epoch - 7ms/step\n",
      "Epoch 56/10000\n",
      "568/568 - 4s - loss: 0.0650 - val_loss: 0.0872 - 4s/epoch - 7ms/step\n",
      "Epoch 57/10000\n",
      "568/568 - 4s - loss: 0.0636 - val_loss: 0.0814 - 4s/epoch - 7ms/step\n",
      "Epoch 58/10000\n",
      "568/568 - 4s - loss: 0.0645 - val_loss: 0.0797 - 4s/epoch - 7ms/step\n",
      "Epoch 59/10000\n",
      "568/568 - 4s - loss: 0.0636 - val_loss: 0.0769 - 4s/epoch - 7ms/step\n",
      "Epoch 60/10000\n",
      "568/568 - 4s - loss: 0.0611 - val_loss: 0.0872 - 4s/epoch - 7ms/step\n",
      "Epoch 61/10000\n",
      "568/568 - 4s - loss: 0.0603 - val_loss: 0.0795 - 4s/epoch - 7ms/step\n",
      "Epoch 62/10000\n",
      "568/568 - 4s - loss: 0.0595 - val_loss: 0.0738 - 4s/epoch - 7ms/step\n",
      "Epoch 63/10000\n",
      "568/568 - 4s - loss: 0.0594 - val_loss: 0.0802 - 4s/epoch - 7ms/step\n",
      "Epoch 64/10000\n",
      "568/568 - 4s - loss: 0.0629 - val_loss: 0.0839 - 4s/epoch - 7ms/step\n",
      "Epoch 65/10000\n",
      "568/568 - 4s - loss: 0.0594 - val_loss: 0.0804 - 4s/epoch - 7ms/step\n",
      "Epoch 66/10000\n",
      "568/568 - 4s - loss: 0.0620 - val_loss: 0.0746 - 4s/epoch - 7ms/step\n",
      "Epoch 67/10000\n",
      "568/568 - 4s - loss: 0.0581 - val_loss: 0.0786 - 4s/epoch - 7ms/step\n",
      "Epoch 68/10000\n",
      "568/568 - 5s - loss: 0.0576 - val_loss: 0.0761 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "568/568 - 4s - loss: 0.0553 - val_loss: 0.0735 - 4s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "568/568 - 4s - loss: 0.0581 - val_loss: 0.0821 - 4s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "568/568 - 4s - loss: 0.0596 - val_loss: 0.0787 - 4s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "568/568 - 4s - loss: 0.0587 - val_loss: 0.0776 - 4s/epoch - 7ms/step\n",
      "Epoch 73/10000\n",
      "568/568 - 4s - loss: 0.0569 - val_loss: 0.0761 - 4s/epoch - 7ms/step\n",
      "Epoch 74/10000\n",
      "568/568 - 4s - loss: 0.0552 - val_loss: 0.0747 - 4s/epoch - 7ms/step\n",
      "Epoch 75/10000\n",
      "568/568 - 4s - loss: 0.0541 - val_loss: 0.0704 - 4s/epoch - 7ms/step\n",
      "Epoch 76/10000\n",
      "568/568 - 4s - loss: 0.0562 - val_loss: 0.0696 - 4s/epoch - 7ms/step\n",
      "Epoch 77/10000\n",
      "568/568 - 4s - loss: 0.0549 - val_loss: 0.0816 - 4s/epoch - 7ms/step\n",
      "Epoch 78/10000\n",
      "568/568 - 4s - loss: 0.0570 - val_loss: 0.0762 - 4s/epoch - 7ms/step\n",
      "Epoch 79/10000\n",
      "568/568 - 4s - loss: 0.0544 - val_loss: 0.0828 - 4s/epoch - 7ms/step\n",
      "Epoch 80/10000\n",
      "568/568 - 4s - loss: 0.0543 - val_loss: 0.0723 - 4s/epoch - 7ms/step\n",
      "Epoch 81/10000\n",
      "568/568 - 4s - loss: 0.0525 - val_loss: 0.0705 - 4s/epoch - 7ms/step\n",
      "Epoch 82/10000\n",
      "568/568 - 4s - loss: 0.0541 - val_loss: 0.0762 - 4s/epoch - 7ms/step\n",
      "Epoch 83/10000\n",
      "568/568 - 5s - loss: 0.0518 - val_loss: 0.0661 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "568/568 - 4s - loss: 0.0511 - val_loss: 0.0742 - 4s/epoch - 7ms/step\n",
      "Epoch 85/10000\n",
      "568/568 - 4s - loss: 0.0523 - val_loss: 0.0697 - 4s/epoch - 7ms/step\n",
      "Epoch 86/10000\n",
      "568/568 - 4s - loss: 0.0505 - val_loss: 0.0659 - 4s/epoch - 7ms/step\n",
      "Epoch 87/10000\n",
      "568/568 - 4s - loss: 0.0500 - val_loss: 0.0672 - 4s/epoch - 7ms/step\n",
      "Epoch 88/10000\n",
      "568/568 - 4s - loss: 0.0520 - val_loss: 0.0676 - 4s/epoch - 7ms/step\n",
      "Epoch 89/10000\n",
      "568/568 - 4s - loss: 0.0493 - val_loss: 0.0659 - 4s/epoch - 7ms/step\n",
      "Epoch 90/10000\n",
      "568/568 - 4s - loss: 0.0512 - val_loss: 0.0726 - 4s/epoch - 7ms/step\n",
      "Epoch 91/10000\n",
      "568/568 - 4s - loss: 0.0497 - val_loss: 0.0685 - 4s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "568/568 - 4s - loss: 0.0487 - val_loss: 0.0678 - 4s/epoch - 7ms/step\n",
      "Epoch 93/10000\n",
      "568/568 - 5s - loss: 0.0509 - val_loss: 0.0702 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "568/568 - 4s - loss: 0.0487 - val_loss: 0.0646 - 4s/epoch - 7ms/step\n",
      "Epoch 95/10000\n",
      "568/568 - 4s - loss: 0.0499 - val_loss: 0.0627 - 4s/epoch - 7ms/step\n",
      "Epoch 96/10000\n",
      "568/568 - 4s - loss: 0.0471 - val_loss: 0.0626 - 4s/epoch - 7ms/step\n",
      "Epoch 97/10000\n",
      "568/568 - 4s - loss: 0.0482 - val_loss: 0.0634 - 4s/epoch - 7ms/step\n",
      "Epoch 98/10000\n",
      "568/568 - 4s - loss: 0.0477 - val_loss: 0.0632 - 4s/epoch - 7ms/step\n",
      "Epoch 99/10000\n",
      "568/568 - 4s - loss: 0.0473 - val_loss: 0.0649 - 4s/epoch - 7ms/step\n",
      "Epoch 100/10000\n",
      "568/568 - 4s - loss: 0.0515 - val_loss: 0.0798 - 4s/epoch - 7ms/step\n",
      "Epoch 101/10000\n",
      "568/568 - 4s - loss: 0.0548 - val_loss: 0.0677 - 4s/epoch - 7ms/step\n",
      "Epoch 102/10000\n",
      "568/568 - 4s - loss: 0.0524 - val_loss: 0.0720 - 4s/epoch - 7ms/step\n",
      "Epoch 103/10000\n",
      "568/568 - 4s - loss: 0.0587 - val_loss: 0.0801 - 4s/epoch - 7ms/step\n",
      "Epoch 104/10000\n",
      "568/568 - 4s - loss: 0.0537 - val_loss: 0.0795 - 4s/epoch - 7ms/step\n",
      "Epoch 105/10000\n",
      "568/568 - 4s - loss: 0.0522 - val_loss: 0.0630 - 4s/epoch - 7ms/step\n",
      "Epoch 106/10000\n",
      "568/568 - 4s - loss: 0.0453 - val_loss: 0.0605 - 4s/epoch - 7ms/step\n",
      "Epoch 107/10000\n",
      "568/568 - 4s - loss: 0.0456 - val_loss: 0.0630 - 4s/epoch - 7ms/step\n",
      "Epoch 108/10000\n",
      "568/568 - 4s - loss: 0.0451 - val_loss: 0.0610 - 4s/epoch - 7ms/step\n",
      "Epoch 109/10000\n",
      "568/568 - 4s - loss: 0.0446 - val_loss: 0.0608 - 4s/epoch - 7ms/step\n",
      "Epoch 110/10000\n",
      "568/568 - 4s - loss: 0.0454 - val_loss: 0.0606 - 4s/epoch - 7ms/step\n",
      "Epoch 111/10000\n",
      "568/568 - 4s - loss: 0.0456 - val_loss: 0.0697 - 4s/epoch - 7ms/step\n",
      "Epoch 112/10000\n",
      "568/568 - 4s - loss: 0.0459 - val_loss: 0.0632 - 4s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "568/568 - 5s - loss: 0.0456 - val_loss: 0.0612 - 5s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "568/568 - 5s - loss: 0.0446 - val_loss: 0.0608 - 5s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "568/568 - 4s - loss: 0.0469 - val_loss: 0.0609 - 4s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "568/568 - 4s - loss: 0.0443 - val_loss: 0.0605 - 4s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "568/568 - 4s - loss: 0.0452 - val_loss: 0.0567 - 4s/epoch - 8ms/step\n",
      "Epoch 118/10000\n",
      "568/568 - 4s - loss: 0.0447 - val_loss: 0.0590 - 4s/epoch - 7ms/step\n",
      "Epoch 119/10000\n",
      "568/568 - 4s - loss: 0.0434 - val_loss: 0.0577 - 4s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "568/568 - 4s - loss: 0.0447 - val_loss: 0.0685 - 4s/epoch - 8ms/step\n",
      "Epoch 121/10000\n",
      "568/568 - 4s - loss: 0.0484 - val_loss: 0.0630 - 4s/epoch - 8ms/step\n",
      "Epoch 122/10000\n",
      "568/568 - 4s - loss: 0.0445 - val_loss: 0.0588 - 4s/epoch - 8ms/step\n",
      "Epoch 123/10000\n",
      "568/568 - 4s - loss: 0.0525 - val_loss: 0.0712 - 4s/epoch - 8ms/step\n",
      "Epoch 124/10000\n",
      "568/568 - 4s - loss: 0.0482 - val_loss: 0.0675 - 4s/epoch - 7ms/step\n",
      "Epoch 125/10000\n",
      "568/568 - 4s - loss: 0.0452 - val_loss: 0.0634 - 4s/epoch - 7ms/step\n",
      "Epoch 126/10000\n",
      "568/568 - 4s - loss: 0.0460 - val_loss: 0.0610 - 4s/epoch - 8ms/step\n",
      "Epoch 127/10000\n",
      "568/568 - 4s - loss: 0.0418 - val_loss: 0.0558 - 4s/epoch - 7ms/step\n",
      "Epoch 128/10000\n",
      "568/568 - 4s - loss: 0.0433 - val_loss: 0.0623 - 4s/epoch - 7ms/step\n",
      "Epoch 129/10000\n",
      "568/568 - 4s - loss: 0.0426 - val_loss: 0.0574 - 4s/epoch - 8ms/step\n",
      "Epoch 130/10000\n",
      "568/568 - 5s - loss: 0.0416 - val_loss: 0.0570 - 5s/epoch - 9ms/step\n",
      "Epoch 131/10000\n",
      "568/568 - 5s - loss: 0.0413 - val_loss: 0.0623 - 5s/epoch - 9ms/step\n",
      "Epoch 132/10000\n",
      "568/568 - 4s - loss: 0.0427 - val_loss: 0.0686 - 4s/epoch - 8ms/step\n",
      "Epoch 133/10000\n",
      "568/568 - 5s - loss: 0.0434 - val_loss: 0.0557 - 5s/epoch - 8ms/step\n",
      "Epoch 134/10000\n",
      "568/568 - 6s - loss: 0.0422 - val_loss: 0.0585 - 6s/epoch - 10ms/step\n",
      "Epoch 135/10000\n",
      "568/568 - 6s - loss: 0.0404 - val_loss: 0.0535 - 6s/epoch - 10ms/step\n",
      "Epoch 136/10000\n",
      "568/568 - 5s - loss: 0.0410 - val_loss: 0.0557 - 5s/epoch - 8ms/step\n",
      "Epoch 137/10000\n",
      "568/568 - 4s - loss: 0.0424 - val_loss: 0.0557 - 4s/epoch - 8ms/step\n",
      "Epoch 138/10000\n",
      "568/568 - 4s - loss: 0.0408 - val_loss: 0.0567 - 4s/epoch - 7ms/step\n",
      "Epoch 139/10000\n",
      "568/568 - 4s - loss: 0.0419 - val_loss: 0.0556 - 4s/epoch - 8ms/step\n",
      "Epoch 140/10000\n",
      "568/568 - 5s - loss: 0.0406 - val_loss: 0.0537 - 5s/epoch - 8ms/step\n",
      "Epoch 141/10000\n",
      "568/568 - 4s - loss: 0.0404 - val_loss: 0.0550 - 4s/epoch - 8ms/step\n",
      "Epoch 142/10000\n",
      "568/568 - 5s - loss: 0.0433 - val_loss: 0.0527 - 5s/epoch - 9ms/step\n",
      "Epoch 143/10000\n",
      "568/568 - 5s - loss: 0.0393 - val_loss: 0.0543 - 5s/epoch - 9ms/step\n",
      "Epoch 144/10000\n",
      "568/568 - 5s - loss: 0.0408 - val_loss: 0.0593 - 5s/epoch - 8ms/step\n",
      "Epoch 145/10000\n",
      "568/568 - 4s - loss: 0.0409 - val_loss: 0.0542 - 4s/epoch - 8ms/step\n",
      "Epoch 146/10000\n",
      "568/568 - 4s - loss: 0.0398 - val_loss: 0.0568 - 4s/epoch - 8ms/step\n",
      "Epoch 147/10000\n",
      "568/568 - 5s - loss: 0.0411 - val_loss: 0.0531 - 5s/epoch - 8ms/step\n",
      "Epoch 148/10000\n",
      "568/568 - 5s - loss: 0.0401 - val_loss: 0.0516 - 5s/epoch - 9ms/step\n",
      "Epoch 149/10000\n",
      "568/568 - 5s - loss: 0.0387 - val_loss: 0.0543 - 5s/epoch - 8ms/step\n",
      "Epoch 150/10000\n",
      "568/568 - 5s - loss: 0.0396 - val_loss: 0.0562 - 5s/epoch - 9ms/step\n",
      "Epoch 151/10000\n",
      "568/568 - 5s - loss: 0.0398 - val_loss: 0.0554 - 5s/epoch - 8ms/step\n",
      "Epoch 152/10000\n",
      "568/568 - 4s - loss: 0.0406 - val_loss: 0.0593 - 4s/epoch - 8ms/step\n",
      "Epoch 153/10000\n",
      "568/568 - 6s - loss: 0.0384 - val_loss: 0.0523 - 6s/epoch - 11ms/step\n",
      "Epoch 154/10000\n",
      "568/568 - 6s - loss: 0.0421 - val_loss: 0.0609 - 6s/epoch - 10ms/step\n",
      "Epoch 155/10000\n",
      "568/568 - 6s - loss: 0.0404 - val_loss: 0.0523 - 6s/epoch - 10ms/step\n",
      "Epoch 156/10000\n",
      "568/568 - 5s - loss: 0.0394 - val_loss: 0.0543 - 5s/epoch - 9ms/step\n",
      "Epoch 157/10000\n",
      "568/568 - 5s - loss: 0.0385 - val_loss: 0.0528 - 5s/epoch - 8ms/step\n",
      "Epoch 158/10000\n",
      "568/568 - 5s - loss: 0.0385 - val_loss: 0.0552 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_18_layer_call_fn, gru_cell_18_layer_call_and_return_conditional_losses, gru_cell_19_layer_call_fn, gru_cell_19_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_20_1_50_1_50_datt_seq2seq_gru_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_20_1_50_1_50_datt_seq2seq_gru_1\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000029664A13A90> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000294CFE14BE0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.908417</td>\n",
       "      <td>0.903332</td>\n",
       "      <td>0.937044</td>\n",
       "      <td>0.916264</td>\n",
       "      <td>5.969385</td>\n",
       "      <td>5.455099</td>\n",
       "      <td>3.861047</td>\n",
       "      <td>5.095177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.828377</td>\n",
       "      <td>0.829522</td>\n",
       "      <td>0.904288</td>\n",
       "      <td>0.854062</td>\n",
       "      <td>8.020947</td>\n",
       "      <td>7.246495</td>\n",
       "      <td>4.760979</td>\n",
       "      <td>6.67614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.733449</td>\n",
       "      <td>0.769686</td>\n",
       "      <td>0.848557</td>\n",
       "      <td>0.783897</td>\n",
       "      <td>9.556971</td>\n",
       "      <td>8.425828</td>\n",
       "      <td>5.989446</td>\n",
       "      <td>7.990748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.632342</td>\n",
       "      <td>0.728387</td>\n",
       "      <td>0.798645</td>\n",
       "      <td>0.719791</td>\n",
       "      <td>10.847041</td>\n",
       "      <td>9.153203</td>\n",
       "      <td>6.90692</td>\n",
       "      <td>8.969054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.529697</td>\n",
       "      <td>0.69798</td>\n",
       "      <td>0.756002</td>\n",
       "      <td>0.661226</td>\n",
       "      <td>12.07112</td>\n",
       "      <td>9.656218</td>\n",
       "      <td>7.60368</td>\n",
       "      <td>9.777006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.427731</td>\n",
       "      <td>0.677536</td>\n",
       "      <td>0.722045</td>\n",
       "      <td>0.609104</td>\n",
       "      <td>13.271729</td>\n",
       "      <td>9.982509</td>\n",
       "      <td>8.115422</td>\n",
       "      <td>10.456553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.329753</td>\n",
       "      <td>0.662562</td>\n",
       "      <td>0.699107</td>\n",
       "      <td>0.563807</td>\n",
       "      <td>14.08358</td>\n",
       "      <td>10.21462</td>\n",
       "      <td>8.443709</td>\n",
       "      <td>10.91397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.238669</td>\n",
       "      <td>0.649522</td>\n",
       "      <td>0.686034</td>\n",
       "      <td>0.524742</td>\n",
       "      <td>14.726149</td>\n",
       "      <td>10.413286</td>\n",
       "      <td>8.625395</td>\n",
       "      <td>11.254943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.152333</td>\n",
       "      <td>0.634069</td>\n",
       "      <td>0.671691</td>\n",
       "      <td>0.486031</td>\n",
       "      <td>15.312465</td>\n",
       "      <td>10.643612</td>\n",
       "      <td>8.82071</td>\n",
       "      <td>11.592262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.070958</td>\n",
       "      <td>0.615825</td>\n",
       "      <td>0.64348</td>\n",
       "      <td>0.443421</td>\n",
       "      <td>15.802064</td>\n",
       "      <td>10.907614</td>\n",
       "      <td>9.192426</td>\n",
       "      <td>11.967368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.595514</td>\n",
       "      <td>0.605417</td>\n",
       "      <td>0.400749</td>\n",
       "      <td>16.217395</td>\n",
       "      <td>11.19401</td>\n",
       "      <td>9.671487</td>\n",
       "      <td>12.360964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.061687</td>\n",
       "      <td>0.576231</td>\n",
       "      <td>0.566651</td>\n",
       "      <td>0.360398</td>\n",
       "      <td>16.618076</td>\n",
       "      <td>11.458918</td>\n",
       "      <td>10.136739</td>\n",
       "      <td>12.737911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.117932</td>\n",
       "      <td>0.557238</td>\n",
       "      <td>0.534002</td>\n",
       "      <td>0.324436</td>\n",
       "      <td>16.822309</td>\n",
       "      <td>11.713891</td>\n",
       "      <td>10.513149</td>\n",
       "      <td>13.01645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.167045</td>\n",
       "      <td>0.540007</td>\n",
       "      <td>0.511276</td>\n",
       "      <td>0.294746</td>\n",
       "      <td>16.959912</td>\n",
       "      <td>11.941351</td>\n",
       "      <td>10.767785</td>\n",
       "      <td>13.223016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.210432</td>\n",
       "      <td>0.523247</td>\n",
       "      <td>0.499884</td>\n",
       "      <td>0.2709</td>\n",
       "      <td>17.101727</td>\n",
       "      <td>12.158547</td>\n",
       "      <td>10.893295</td>\n",
       "      <td>13.384523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.250164</td>\n",
       "      <td>0.506346</td>\n",
       "      <td>0.495501</td>\n",
       "      <td>0.250561</td>\n",
       "      <td>17.210461</td>\n",
       "      <td>12.373452</td>\n",
       "      <td>10.942114</td>\n",
       "      <td>13.508676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.283004</td>\n",
       "      <td>0.486963</td>\n",
       "      <td>0.491432</td>\n",
       "      <td>0.231797</td>\n",
       "      <td>17.26715</td>\n",
       "      <td>12.613895</td>\n",
       "      <td>10.988085</td>\n",
       "      <td>13.623043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.31292</td>\n",
       "      <td>0.467153</td>\n",
       "      <td>0.482319</td>\n",
       "      <td>0.212184</td>\n",
       "      <td>17.424997</td>\n",
       "      <td>12.854157</td>\n",
       "      <td>11.089303</td>\n",
       "      <td>13.789486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.338267</td>\n",
       "      <td>0.44459</td>\n",
       "      <td>0.467762</td>\n",
       "      <td>0.191362</td>\n",
       "      <td>17.612971</td>\n",
       "      <td>13.121988</td>\n",
       "      <td>11.247708</td>\n",
       "      <td>13.994223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.360865</td>\n",
       "      <td>0.418266</td>\n",
       "      <td>0.449105</td>\n",
       "      <td>0.168836</td>\n",
       "      <td>17.781481</td>\n",
       "      <td>13.427094</td>\n",
       "      <td>11.447128</td>\n",
       "      <td>14.218568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.137536</td>\n",
       "      <td>0.614199</td>\n",
       "      <td>0.638512</td>\n",
       "      <td>0.463416</td>\n",
       "      <td>14.533897</td>\n",
       "      <td>10.747789</td>\n",
       "      <td>9.000826</td>\n",
       "      <td>11.427504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.908417  0.903332  0.937044  0.916264   5.969385   5.455099   \n",
       "1      0.828377  0.829522  0.904288  0.854062   8.020947   7.246495   \n",
       "2      0.733449  0.769686  0.848557  0.783897   9.556971   8.425828   \n",
       "3      0.632342  0.728387  0.798645  0.719791  10.847041   9.153203   \n",
       "4      0.529697   0.69798  0.756002  0.661226   12.07112   9.656218   \n",
       "5      0.427731  0.677536  0.722045  0.609104  13.271729   9.982509   \n",
       "6      0.329753  0.662562  0.699107  0.563807   14.08358   10.21462   \n",
       "7      0.238669  0.649522  0.686034  0.524742  14.726149  10.413286   \n",
       "8      0.152333  0.634069  0.671691  0.486031  15.312465  10.643612   \n",
       "9      0.070958  0.615825   0.64348  0.443421  15.802064  10.907614   \n",
       "10     0.001317  0.595514  0.605417  0.400749  16.217395   11.19401   \n",
       "11    -0.061687  0.576231  0.566651  0.360398  16.618076  11.458918   \n",
       "12    -0.117932  0.557238  0.534002  0.324436  16.822309  11.713891   \n",
       "13    -0.167045  0.540007  0.511276  0.294746  16.959912  11.941351   \n",
       "14    -0.210432  0.523247  0.499884    0.2709  17.101727  12.158547   \n",
       "15    -0.250164  0.506346  0.495501  0.250561  17.210461  12.373452   \n",
       "16    -0.283004  0.486963  0.491432  0.231797   17.26715  12.613895   \n",
       "17     -0.31292  0.467153  0.482319  0.212184  17.424997  12.854157   \n",
       "18    -0.338267   0.44459  0.467762  0.191362  17.612971  13.121988   \n",
       "19    -0.360865  0.418266  0.449105  0.168836  17.781481  13.427094   \n",
       "mean   0.137536  0.614199  0.638512  0.463416  14.533897  10.747789   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       3.861047   5.095177  \n",
       "1       4.760979    6.67614  \n",
       "2       5.989446   7.990748  \n",
       "3        6.90692   8.969054  \n",
       "4        7.60368   9.777006  \n",
       "5       8.115422  10.456553  \n",
       "6       8.443709   10.91397  \n",
       "7       8.625395  11.254943  \n",
       "8        8.82071  11.592262  \n",
       "9       9.192426  11.967368  \n",
       "10      9.671487  12.360964  \n",
       "11     10.136739  12.737911  \n",
       "12     10.513149   13.01645  \n",
       "13     10.767785  13.223016  \n",
       "14     10.893295  13.384523  \n",
       "15     10.942114  13.508676  \n",
       "16     10.988085  13.623043  \n",
       "17     11.089303  13.789486  \n",
       "18     11.247708  13.994223  \n",
       "19     11.447128  14.218568  \n",
       "mean    9.000826  11.427504  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/30_20_1_50_1_50_datt_seq2seq_gru_1.csv\n",
      "end training\n",
      "\n",
      "history size: 30\n",
      "future size: 30\n",
      "Epoch 1/10000\n",
      "563/563 - 9s - loss: 0.3866 - val_loss: 0.3192 - 9s/epoch - 16ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 4s - loss: 0.2875 - val_loss: 0.2857 - 4s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 4s - loss: 0.2652 - val_loss: 0.2714 - 4s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 4s - loss: 0.2485 - val_loss: 0.2692 - 4s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 4s - loss: 0.2332 - val_loss: 0.2402 - 4s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 4s - loss: 0.2201 - val_loss: 0.2312 - 4s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 4s - loss: 0.2090 - val_loss: 0.2253 - 4s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.1981 - val_loss: 0.2121 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.1895 - val_loss: 0.2089 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 4s - loss: 0.1818 - val_loss: 0.1955 - 4s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.1758 - val_loss: 0.1898 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.1696 - val_loss: 0.1970 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.1629 - val_loss: 0.1812 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 4s - loss: 0.1590 - val_loss: 0.1742 - 4s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 4s - loss: 0.1532 - val_loss: 0.1763 - 4s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 4s - loss: 0.1480 - val_loss: 0.1663 - 4s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.1462 - val_loss: 0.1702 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 4s - loss: 0.1418 - val_loss: 0.1674 - 4s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.1393 - val_loss: 0.1603 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.1362 - val_loss: 0.1522 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 6s - loss: 0.1335 - val_loss: 0.1493 - 6s/epoch - 10ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.1309 - val_loss: 0.1485 - 5s/epoch - 9ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.1272 - val_loss: 0.1508 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 4s - loss: 0.1239 - val_loss: 0.1425 - 4s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.1262 - val_loss: 0.1595 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.1204 - val_loss: 0.1341 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.1202 - val_loss: 0.1247 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.1146 - val_loss: 0.1272 - 5s/epoch - 9ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.1167 - val_loss: 0.1272 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.1103 - val_loss: 0.1223 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.1074 - val_loss: 0.1219 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.1126 - val_loss: 0.1237 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 4s - loss: 0.1073 - val_loss: 0.1271 - 4s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.1086 - val_loss: 0.1260 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 4s - loss: 0.1041 - val_loss: 0.1157 - 4s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.1015 - val_loss: 0.1147 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.1000 - val_loss: 0.1047 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 4s - loss: 0.0997 - val_loss: 0.1321 - 4s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 4s - loss: 0.0998 - val_loss: 0.1203 - 4s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 4s - loss: 0.1038 - val_loss: 0.1157 - 4s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 4s - loss: 0.0960 - val_loss: 0.1200 - 4s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 4s - loss: 0.0970 - val_loss: 0.1253 - 4s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 4s - loss: 0.0950 - val_loss: 0.1097 - 4s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 4s - loss: 0.0915 - val_loss: 0.1115 - 4s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.1039 - val_loss: 0.1124 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0907 - val_loss: 0.1064 - 5s/epoch - 9ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0914 - val_loss: 0.0981 - 5s/epoch - 10ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0905 - val_loss: 0.1055 - 5s/epoch - 9ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0900 - val_loss: 0.0960 - 5s/epoch - 9ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0872 - val_loss: 0.0961 - 5s/epoch - 9ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 6s - loss: 0.0882 - val_loss: 0.0980 - 6s/epoch - 10ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 6s - loss: 0.0868 - val_loss: 0.1045 - 6s/epoch - 10ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 6s - loss: 0.0862 - val_loss: 0.1037 - 6s/epoch - 10ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0875 - val_loss: 0.0966 - 5s/epoch - 9ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0835 - val_loss: 0.0939 - 5s/epoch - 9ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0845 - val_loss: 0.0969 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0828 - val_loss: 0.1033 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 4s - loss: 0.0822 - val_loss: 0.0922 - 4s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0819 - val_loss: 0.0899 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0820 - val_loss: 0.0927 - 5s/epoch - 9ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0815 - val_loss: 0.0939 - 5s/epoch - 10ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0794 - val_loss: 0.0881 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 4s - loss: 0.0793 - val_loss: 0.0888 - 4s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0780 - val_loss: 0.0888 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0773 - val_loss: 0.0838 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0765 - val_loss: 0.0857 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0772 - val_loss: 0.0858 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 4s - loss: 0.0778 - val_loss: 0.0889 - 4s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 4s - loss: 0.0737 - val_loss: 0.0908 - 4s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 4s - loss: 0.0761 - val_loss: 0.0885 - 4s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 4s - loss: 0.0769 - val_loss: 0.0807 - 4s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 4s - loss: 0.0744 - val_loss: 0.0985 - 4s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 4s - loss: 0.0729 - val_loss: 0.0842 - 4s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 4s - loss: 0.0728 - val_loss: 0.0904 - 4s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 4s - loss: 0.0707 - val_loss: 0.0831 - 4s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 4s - loss: 0.0732 - val_loss: 0.0874 - 4s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 4s - loss: 0.0716 - val_loss: 0.0920 - 4s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 4s - loss: 0.0778 - val_loss: 0.1436 - 4s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 4s - loss: 0.0804 - val_loss: 0.0803 - 4s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 4s - loss: 0.0683 - val_loss: 0.0807 - 4s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 4s - loss: 0.0702 - val_loss: 0.0785 - 4s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 4s - loss: 0.0680 - val_loss: 0.0787 - 4s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 4s - loss: 0.0671 - val_loss: 0.0819 - 4s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "563/563 - 4s - loss: 0.0672 - val_loss: 0.0768 - 4s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "563/563 - 4s - loss: 0.0686 - val_loss: 0.0783 - 4s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "563/563 - 4s - loss: 0.0678 - val_loss: 0.0829 - 4s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "563/563 - 4s - loss: 0.0686 - val_loss: 0.0763 - 4s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "563/563 - 4s - loss: 0.0682 - val_loss: 0.0794 - 4s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "563/563 - 4s - loss: 0.0653 - val_loss: 0.0760 - 4s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "563/563 - 4s - loss: 0.0661 - val_loss: 0.0788 - 4s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "563/563 - 4s - loss: 0.0653 - val_loss: 0.0750 - 4s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "563/563 - 4s - loss: 0.0662 - val_loss: 0.0779 - 4s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "563/563 - 4s - loss: 0.0665 - val_loss: 0.0758 - 4s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "563/563 - 4s - loss: 0.0658 - val_loss: 0.0763 - 4s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "563/563 - 4s - loss: 0.0652 - val_loss: 0.0735 - 4s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "563/563 - 4s - loss: 0.0699 - val_loss: 0.0827 - 4s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "563/563 - 4s - loss: 0.0638 - val_loss: 0.0778 - 4s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "563/563 - 4s - loss: 0.0628 - val_loss: 0.0793 - 4s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "563/563 - 4s - loss: 0.0634 - val_loss: 0.0738 - 4s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "563/563 - 5s - loss: 0.0624 - val_loss: 0.0696 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "563/563 - 4s - loss: 0.0622 - val_loss: 0.0762 - 4s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "563/563 - 4s - loss: 0.0617 - val_loss: 0.0727 - 4s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "563/563 - 4s - loss: 0.0618 - val_loss: 0.0723 - 4s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "563/563 - 4s - loss: 0.0629 - val_loss: 0.0769 - 4s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "563/563 - 4s - loss: 0.0618 - val_loss: 0.0853 - 4s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "563/563 - 4s - loss: 0.0628 - val_loss: 0.0722 - 4s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "563/563 - 4s - loss: 0.0597 - val_loss: 0.0670 - 4s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "563/563 - 4s - loss: 0.0606 - val_loss: 0.0715 - 4s/epoch - 8ms/step\n",
      "Epoch 109/10000\n",
      "563/563 - 4s - loss: 0.0629 - val_loss: 0.0830 - 4s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "563/563 - 4s - loss: 0.0702 - val_loss: 0.0714 - 4s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "563/563 - 4s - loss: 0.0584 - val_loss: 0.0687 - 4s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "563/563 - 4s - loss: 0.0590 - val_loss: 0.0671 - 4s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "563/563 - 4s - loss: 0.0577 - val_loss: 0.0693 - 4s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "563/563 - 4s - loss: 0.0595 - val_loss: 0.0735 - 4s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "563/563 - 4s - loss: 0.0610 - val_loss: 0.0819 - 4s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "563/563 - 4s - loss: 0.0596 - val_loss: 0.0705 - 4s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "563/563 - 4s - loss: 0.0579 - val_loss: 0.0713 - 4s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_20_layer_call_fn, gru_cell_20_layer_call_and_return_conditional_losses, gru_cell_21_layer_call_fn, gru_cell_21_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_30_1_50_1_50_datt_seq2seq_gru_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_30_1_50_1_50_datt_seq2seq_gru_1\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002966FA69760> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000294CE490D00> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.838934</td>\n",
       "      <td>0.871585</td>\n",
       "      <td>0.912206</td>\n",
       "      <td>0.874241</td>\n",
       "      <td>7.910791</td>\n",
       "      <td>6.285554</td>\n",
       "      <td>4.542879</td>\n",
       "      <td>6.246408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.782766</td>\n",
       "      <td>0.821615</td>\n",
       "      <td>0.863165</td>\n",
       "      <td>0.822515</td>\n",
       "      <td>9.185136</td>\n",
       "      <td>7.408772</td>\n",
       "      <td>5.673211</td>\n",
       "      <td>7.422373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.689522</td>\n",
       "      <td>0.777158</td>\n",
       "      <td>0.811673</td>\n",
       "      <td>0.759451</td>\n",
       "      <td>10.979442</td>\n",
       "      <td>8.281425</td>\n",
       "      <td>6.657468</td>\n",
       "      <td>8.639445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.591903</td>\n",
       "      <td>0.744296</td>\n",
       "      <td>0.772054</td>\n",
       "      <td>0.702751</td>\n",
       "      <td>12.587385</td>\n",
       "      <td>8.872297</td>\n",
       "      <td>7.325943</td>\n",
       "      <td>9.595208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.495711</td>\n",
       "      <td>0.713655</td>\n",
       "      <td>0.745897</td>\n",
       "      <td>0.651755</td>\n",
       "      <td>13.992349</td>\n",
       "      <td>9.390723</td>\n",
       "      <td>7.73589</td>\n",
       "      <td>10.372987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.402988</td>\n",
       "      <td>0.686723</td>\n",
       "      <td>0.7277</td>\n",
       "      <td>0.605804</td>\n",
       "      <td>15.225283</td>\n",
       "      <td>9.825321</td>\n",
       "      <td>8.008519</td>\n",
       "      <td>11.019707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.316626</td>\n",
       "      <td>0.662071</td>\n",
       "      <td>0.710286</td>\n",
       "      <td>0.562994</td>\n",
       "      <td>16.291112</td>\n",
       "      <td>10.207809</td>\n",
       "      <td>8.261363</td>\n",
       "      <td>11.586761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.238187</td>\n",
       "      <td>0.638006</td>\n",
       "      <td>0.690759</td>\n",
       "      <td>0.522317</td>\n",
       "      <td>17.203878</td>\n",
       "      <td>10.568435</td>\n",
       "      <td>8.536542</td>\n",
       "      <td>12.102952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.169393</td>\n",
       "      <td>0.614349</td>\n",
       "      <td>0.668336</td>\n",
       "      <td>0.484026</td>\n",
       "      <td>17.968057</td>\n",
       "      <td>10.912788</td>\n",
       "      <td>8.84204</td>\n",
       "      <td>12.574295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.109246</td>\n",
       "      <td>0.591001</td>\n",
       "      <td>0.643282</td>\n",
       "      <td>0.447843</td>\n",
       "      <td>18.613814</td>\n",
       "      <td>11.242</td>\n",
       "      <td>9.171949</td>\n",
       "      <td>13.009254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.055888</td>\n",
       "      <td>0.566932</td>\n",
       "      <td>0.614775</td>\n",
       "      <td>0.412532</td>\n",
       "      <td>19.171962</td>\n",
       "      <td>11.571235</td>\n",
       "      <td>9.533726</td>\n",
       "      <td>13.425641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.008028</td>\n",
       "      <td>0.543569</td>\n",
       "      <td>0.584983</td>\n",
       "      <td>0.37886</td>\n",
       "      <td>19.298888</td>\n",
       "      <td>11.883068</td>\n",
       "      <td>9.898427</td>\n",
       "      <td>13.693461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.035145</td>\n",
       "      <td>0.520528</td>\n",
       "      <td>0.557359</td>\n",
       "      <td>0.347581</td>\n",
       "      <td>18.855647</td>\n",
       "      <td>12.183997</td>\n",
       "      <td>10.225537</td>\n",
       "      <td>13.75506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.073357</td>\n",
       "      <td>0.498634</td>\n",
       "      <td>0.533359</td>\n",
       "      <td>0.319545</td>\n",
       "      <td>18.559247</td>\n",
       "      <td>12.463465</td>\n",
       "      <td>10.501405</td>\n",
       "      <td>13.841372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.107337</td>\n",
       "      <td>0.47771</td>\n",
       "      <td>0.512784</td>\n",
       "      <td>0.294385</td>\n",
       "      <td>18.548948</td>\n",
       "      <td>12.726557</td>\n",
       "      <td>10.732852</td>\n",
       "      <td>14.002785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.137367</td>\n",
       "      <td>0.456958</td>\n",
       "      <td>0.4949</td>\n",
       "      <td>0.271497</td>\n",
       "      <td>18.73535</td>\n",
       "      <td>12.982784</td>\n",
       "      <td>10.929961</td>\n",
       "      <td>14.216032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.163528</td>\n",
       "      <td>0.43619</td>\n",
       "      <td>0.477859</td>\n",
       "      <td>0.250174</td>\n",
       "      <td>18.577981</td>\n",
       "      <td>13.231713</td>\n",
       "      <td>11.115061</td>\n",
       "      <td>14.308251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.186755</td>\n",
       "      <td>0.415173</td>\n",
       "      <td>0.461587</td>\n",
       "      <td>0.230002</td>\n",
       "      <td>18.403804</td>\n",
       "      <td>13.479092</td>\n",
       "      <td>11.289132</td>\n",
       "      <td>14.390676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.207318</td>\n",
       "      <td>0.393976</td>\n",
       "      <td>0.444879</td>\n",
       "      <td>0.210512</td>\n",
       "      <td>18.288292</td>\n",
       "      <td>13.723582</td>\n",
       "      <td>11.464991</td>\n",
       "      <td>14.492288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.228776</td>\n",
       "      <td>0.372591</td>\n",
       "      <td>0.428128</td>\n",
       "      <td>0.190648</td>\n",
       "      <td>18.181819</td>\n",
       "      <td>13.963805</td>\n",
       "      <td>11.638465</td>\n",
       "      <td>14.594696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.253137</td>\n",
       "      <td>0.35156</td>\n",
       "      <td>0.412022</td>\n",
       "      <td>0.170148</td>\n",
       "      <td>18.168465</td>\n",
       "      <td>14.196709</td>\n",
       "      <td>11.802963</td>\n",
       "      <td>14.722712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.282193</td>\n",
       "      <td>0.33199</td>\n",
       "      <td>0.397198</td>\n",
       "      <td>0.148998</td>\n",
       "      <td>18.257751</td>\n",
       "      <td>14.408493</td>\n",
       "      <td>11.952809</td>\n",
       "      <td>14.873018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.315168</td>\n",
       "      <td>0.315519</td>\n",
       "      <td>0.383506</td>\n",
       "      <td>0.127952</td>\n",
       "      <td>18.234527</td>\n",
       "      <td>14.583984</td>\n",
       "      <td>12.090644</td>\n",
       "      <td>14.969718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.348584</td>\n",
       "      <td>0.301217</td>\n",
       "      <td>0.370629</td>\n",
       "      <td>0.107754</td>\n",
       "      <td>18.213399</td>\n",
       "      <td>14.736725</td>\n",
       "      <td>12.220002</td>\n",
       "      <td>15.056709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.383445</td>\n",
       "      <td>0.287778</td>\n",
       "      <td>0.357902</td>\n",
       "      <td>0.087412</td>\n",
       "      <td>18.259023</td>\n",
       "      <td>14.878335</td>\n",
       "      <td>12.346628</td>\n",
       "      <td>15.161329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.417703</td>\n",
       "      <td>0.274535</td>\n",
       "      <td>0.34519</td>\n",
       "      <td>0.06734</td>\n",
       "      <td>18.298898</td>\n",
       "      <td>15.015259</td>\n",
       "      <td>12.472586</td>\n",
       "      <td>15.262247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.450003</td>\n",
       "      <td>0.262675</td>\n",
       "      <td>0.332158</td>\n",
       "      <td>0.048277</td>\n",
       "      <td>18.326028</td>\n",
       "      <td>15.135244</td>\n",
       "      <td>12.600859</td>\n",
       "      <td>15.354044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.478368</td>\n",
       "      <td>0.252017</td>\n",
       "      <td>0.319163</td>\n",
       "      <td>0.030937</td>\n",
       "      <td>18.459161</td>\n",
       "      <td>15.241165</td>\n",
       "      <td>12.728181</td>\n",
       "      <td>15.476169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.501314</td>\n",
       "      <td>0.242897</td>\n",
       "      <td>0.306556</td>\n",
       "      <td>0.016046</td>\n",
       "      <td>18.624386</td>\n",
       "      <td>15.331742</td>\n",
       "      <td>12.850858</td>\n",
       "      <td>15.602329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.520535</td>\n",
       "      <td>0.235185</td>\n",
       "      <td>0.295191</td>\n",
       "      <td>0.003281</td>\n",
       "      <td>18.769509</td>\n",
       "      <td>15.40806</td>\n",
       "      <td>12.960854</td>\n",
       "      <td>15.712808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.013028</td>\n",
       "      <td>0.488603</td>\n",
       "      <td>0.539183</td>\n",
       "      <td>0.338253</td>\n",
       "      <td>17.006344</td>\n",
       "      <td>12.338005</td>\n",
       "      <td>10.203725</td>\n",
       "      <td>13.182691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.838934  0.871585  0.912206  0.874241   7.910791   6.285554   \n",
       "1      0.782766  0.821615  0.863165  0.822515   9.185136   7.408772   \n",
       "2      0.689522  0.777158  0.811673  0.759451  10.979442   8.281425   \n",
       "3      0.591903  0.744296  0.772054  0.702751  12.587385   8.872297   \n",
       "4      0.495711  0.713655  0.745897  0.651755  13.992349   9.390723   \n",
       "5      0.402988  0.686723    0.7277  0.605804  15.225283   9.825321   \n",
       "6      0.316626  0.662071  0.710286  0.562994  16.291112  10.207809   \n",
       "7      0.238187  0.638006  0.690759  0.522317  17.203878  10.568435   \n",
       "8      0.169393  0.614349  0.668336  0.484026  17.968057  10.912788   \n",
       "9      0.109246  0.591001  0.643282  0.447843  18.613814     11.242   \n",
       "10     0.055888  0.566932  0.614775  0.412532  19.171962  11.571235   \n",
       "11     0.008028  0.543569  0.584983   0.37886  19.298888  11.883068   \n",
       "12    -0.035145  0.520528  0.557359  0.347581  18.855647  12.183997   \n",
       "13    -0.073357  0.498634  0.533359  0.319545  18.559247  12.463465   \n",
       "14    -0.107337   0.47771  0.512784  0.294385  18.548948  12.726557   \n",
       "15    -0.137367  0.456958    0.4949  0.271497   18.73535  12.982784   \n",
       "16    -0.163528   0.43619  0.477859  0.250174  18.577981  13.231713   \n",
       "17    -0.186755  0.415173  0.461587  0.230002  18.403804  13.479092   \n",
       "18    -0.207318  0.393976  0.444879  0.210512  18.288292  13.723582   \n",
       "19    -0.228776  0.372591  0.428128  0.190648  18.181819  13.963805   \n",
       "20    -0.253137   0.35156  0.412022  0.170148  18.168465  14.196709   \n",
       "21    -0.282193   0.33199  0.397198  0.148998  18.257751  14.408493   \n",
       "22    -0.315168  0.315519  0.383506  0.127952  18.234527  14.583984   \n",
       "23    -0.348584  0.301217  0.370629  0.107754  18.213399  14.736725   \n",
       "24    -0.383445  0.287778  0.357902  0.087412  18.259023  14.878335   \n",
       "25    -0.417703  0.274535   0.34519   0.06734  18.298898  15.015259   \n",
       "26    -0.450003  0.262675  0.332158  0.048277  18.326028  15.135244   \n",
       "27    -0.478368  0.252017  0.319163  0.030937  18.459161  15.241165   \n",
       "28    -0.501314  0.242897  0.306556  0.016046  18.624386  15.331742   \n",
       "29    -0.520535  0.235185  0.295191  0.003281  18.769509   15.40806   \n",
       "mean  -0.013028  0.488603  0.539183  0.338253  17.006344  12.338005   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       4.542879   6.246408  \n",
       "1       5.673211   7.422373  \n",
       "2       6.657468   8.639445  \n",
       "3       7.325943   9.595208  \n",
       "4        7.73589  10.372987  \n",
       "5       8.008519  11.019707  \n",
       "6       8.261363  11.586761  \n",
       "7       8.536542  12.102952  \n",
       "8        8.84204  12.574295  \n",
       "9       9.171949  13.009254  \n",
       "10      9.533726  13.425641  \n",
       "11      9.898427  13.693461  \n",
       "12     10.225537   13.75506  \n",
       "13     10.501405  13.841372  \n",
       "14     10.732852  14.002785  \n",
       "15     10.929961  14.216032  \n",
       "16     11.115061  14.308251  \n",
       "17     11.289132  14.390676  \n",
       "18     11.464991  14.492288  \n",
       "19     11.638465  14.594696  \n",
       "20     11.802963  14.722712  \n",
       "21     11.952809  14.873018  \n",
       "22     12.090644  14.969718  \n",
       "23     12.220002  15.056709  \n",
       "24     12.346628  15.161329  \n",
       "25     12.472586  15.262247  \n",
       "26     12.600859  15.354044  \n",
       "27     12.728181  15.476169  \n",
       "28     12.850858  15.602329  \n",
       "29     12.960854  15.712808  \n",
       "mean   10.203725  13.182691  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/30_30_1_50_1_50_datt_seq2seq_gru_1.csv\n",
      "end training\n",
      "\n",
      "history size: 40\n",
      "future size: 10\n",
      "Epoch 1/10000\n",
      "568/568 - 8s - loss: 0.2251 - val_loss: 0.1756 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "568/568 - 4s - loss: 0.1504 - val_loss: 0.1587 - 4s/epoch - 7ms/step\n",
      "Epoch 3/10000\n",
      "568/568 - 5s - loss: 0.1387 - val_loss: 0.1514 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "568/568 - 4s - loss: 0.1294 - val_loss: 0.1423 - 4s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "568/568 - 4s - loss: 0.1191 - val_loss: 0.1188 - 4s/epoch - 7ms/step\n",
      "Epoch 6/10000\n",
      "568/568 - 4s - loss: 0.1106 - val_loss: 0.1152 - 4s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "568/568 - 4s - loss: 0.1040 - val_loss: 0.1091 - 4s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "568/568 - 4s - loss: 0.1001 - val_loss: 0.1068 - 4s/epoch - 7ms/step\n",
      "Epoch 9/10000\n",
      "568/568 - 4s - loss: 0.0963 - val_loss: 0.1065 - 4s/epoch - 7ms/step\n",
      "Epoch 10/10000\n",
      "568/568 - 4s - loss: 0.0927 - val_loss: 0.1037 - 4s/epoch - 7ms/step\n",
      "Epoch 11/10000\n",
      "568/568 - 4s - loss: 0.0904 - val_loss: 0.0964 - 4s/epoch - 7ms/step\n",
      "Epoch 12/10000\n",
      "568/568 - 4s - loss: 0.0869 - val_loss: 0.0998 - 4s/epoch - 7ms/step\n",
      "Epoch 13/10000\n",
      "568/568 - 4s - loss: 0.0853 - val_loss: 0.0924 - 4s/epoch - 7ms/step\n",
      "Epoch 14/10000\n",
      "568/568 - 4s - loss: 0.0807 - val_loss: 0.0900 - 4s/epoch - 7ms/step\n",
      "Epoch 15/10000\n",
      "568/568 - 4s - loss: 0.0799 - val_loss: 0.0905 - 4s/epoch - 7ms/step\n",
      "Epoch 16/10000\n",
      "568/568 - 4s - loss: 0.0782 - val_loss: 0.0888 - 4s/epoch - 7ms/step\n",
      "Epoch 17/10000\n",
      "568/568 - 4s - loss: 0.0761 - val_loss: 0.0913 - 4s/epoch - 7ms/step\n",
      "Epoch 18/10000\n",
      "568/568 - 4s - loss: 0.0757 - val_loss: 0.0887 - 4s/epoch - 7ms/step\n",
      "Epoch 19/10000\n",
      "568/568 - 4s - loss: 0.0721 - val_loss: 0.0863 - 4s/epoch - 7ms/step\n",
      "Epoch 20/10000\n",
      "568/568 - 4s - loss: 0.0719 - val_loss: 0.0814 - 4s/epoch - 7ms/step\n",
      "Epoch 21/10000\n",
      "568/568 - 4s - loss: 0.0681 - val_loss: 0.0836 - 4s/epoch - 7ms/step\n",
      "Epoch 22/10000\n",
      "568/568 - 4s - loss: 0.0680 - val_loss: 0.0836 - 4s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "568/568 - 4s - loss: 0.0685 - val_loss: 0.0802 - 4s/epoch - 7ms/step\n",
      "Epoch 24/10000\n",
      "568/568 - 4s - loss: 0.0676 - val_loss: 0.0779 - 4s/epoch - 7ms/step\n",
      "Epoch 25/10000\n",
      "568/568 - 4s - loss: 0.0626 - val_loss: 0.0750 - 4s/epoch - 7ms/step\n",
      "Epoch 26/10000\n",
      "568/568 - 4s - loss: 0.0616 - val_loss: 0.0806 - 4s/epoch - 7ms/step\n",
      "Epoch 27/10000\n",
      "568/568 - 4s - loss: 0.0597 - val_loss: 0.0787 - 4s/epoch - 7ms/step\n",
      "Epoch 28/10000\n",
      "568/568 - 4s - loss: 0.0612 - val_loss: 0.0813 - 4s/epoch - 7ms/step\n",
      "Epoch 29/10000\n",
      "568/568 - 4s - loss: 0.0604 - val_loss: 0.0738 - 4s/epoch - 7ms/step\n",
      "Epoch 30/10000\n",
      "568/568 - 4s - loss: 0.0569 - val_loss: 0.0775 - 4s/epoch - 7ms/step\n",
      "Epoch 31/10000\n",
      "568/568 - 4s - loss: 0.0567 - val_loss: 0.0723 - 4s/epoch - 7ms/step\n",
      "Epoch 32/10000\n",
      "568/568 - 4s - loss: 0.0558 - val_loss: 0.0716 - 4s/epoch - 7ms/step\n",
      "Epoch 33/10000\n",
      "568/568 - 4s - loss: 0.0565 - val_loss: 0.0769 - 4s/epoch - 7ms/step\n",
      "Epoch 34/10000\n",
      "568/568 - 4s - loss: 0.0584 - val_loss: 0.0750 - 4s/epoch - 7ms/step\n",
      "Epoch 35/10000\n",
      "568/568 - 4s - loss: 0.0538 - val_loss: 0.0701 - 4s/epoch - 7ms/step\n",
      "Epoch 36/10000\n",
      "568/568 - 4s - loss: 0.0524 - val_loss: 0.0671 - 4s/epoch - 7ms/step\n",
      "Epoch 37/10000\n",
      "568/568 - 4s - loss: 0.0566 - val_loss: 0.0657 - 4s/epoch - 7ms/step\n",
      "Epoch 38/10000\n",
      "568/568 - 4s - loss: 0.0538 - val_loss: 0.0682 - 4s/epoch - 7ms/step\n",
      "Epoch 39/10000\n",
      "568/568 - 4s - loss: 0.0527 - val_loss: 0.0643 - 4s/epoch - 7ms/step\n",
      "Epoch 40/10000\n",
      "568/568 - 4s - loss: 0.0496 - val_loss: 0.0635 - 4s/epoch - 7ms/step\n",
      "Epoch 41/10000\n",
      "568/568 - 4s - loss: 0.0504 - val_loss: 0.0649 - 4s/epoch - 7ms/step\n",
      "Epoch 42/10000\n",
      "568/568 - 4s - loss: 0.0499 - val_loss: 0.0628 - 4s/epoch - 7ms/step\n",
      "Epoch 43/10000\n",
      "568/568 - 4s - loss: 0.0489 - val_loss: 0.0772 - 4s/epoch - 7ms/step\n",
      "Epoch 44/10000\n",
      "568/568 - 4s - loss: 0.0491 - val_loss: 0.0628 - 4s/epoch - 7ms/step\n",
      "Epoch 45/10000\n",
      "568/568 - 4s - loss: 0.0477 - val_loss: 0.0590 - 4s/epoch - 7ms/step\n",
      "Epoch 46/10000\n",
      "568/568 - 4s - loss: 0.0513 - val_loss: 0.0662 - 4s/epoch - 7ms/step\n",
      "Epoch 47/10000\n",
      "568/568 - 4s - loss: 0.0494 - val_loss: 0.0609 - 4s/epoch - 7ms/step\n",
      "Epoch 48/10000\n",
      "568/568 - 4s - loss: 0.0463 - val_loss: 0.0617 - 4s/epoch - 7ms/step\n",
      "Epoch 49/10000\n",
      "568/568 - 4s - loss: 0.0449 - val_loss: 0.0582 - 4s/epoch - 7ms/step\n",
      "Epoch 50/10000\n",
      "568/568 - 4s - loss: 0.0447 - val_loss: 0.0590 - 4s/epoch - 7ms/step\n",
      "Epoch 51/10000\n",
      "568/568 - 4s - loss: 0.0446 - val_loss: 0.0578 - 4s/epoch - 7ms/step\n",
      "Epoch 52/10000\n",
      "568/568 - 4s - loss: 0.0447 - val_loss: 0.0580 - 4s/epoch - 7ms/step\n",
      "Epoch 53/10000\n",
      "568/568 - 4s - loss: 0.0456 - val_loss: 0.0636 - 4s/epoch - 7ms/step\n",
      "Epoch 54/10000\n",
      "568/568 - 4s - loss: 0.0436 - val_loss: 0.0572 - 4s/epoch - 7ms/step\n",
      "Epoch 55/10000\n",
      "568/568 - 4s - loss: 0.0431 - val_loss: 0.0617 - 4s/epoch - 7ms/step\n",
      "Epoch 56/10000\n",
      "568/568 - 4s - loss: 0.0484 - val_loss: 0.0572 - 4s/epoch - 7ms/step\n",
      "Epoch 57/10000\n",
      "568/568 - 4s - loss: 0.0443 - val_loss: 0.0555 - 4s/epoch - 7ms/step\n",
      "Epoch 58/10000\n",
      "568/568 - 4s - loss: 0.0412 - val_loss: 0.0560 - 4s/epoch - 7ms/step\n",
      "Epoch 59/10000\n",
      "568/568 - 4s - loss: 0.0409 - val_loss: 0.0538 - 4s/epoch - 7ms/step\n",
      "Epoch 60/10000\n",
      "568/568 - 4s - loss: 0.0403 - val_loss: 0.0529 - 4s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "568/568 - 4s - loss: 0.0418 - val_loss: 0.0563 - 4s/epoch - 7ms/step\n",
      "Epoch 62/10000\n",
      "568/568 - 4s - loss: 0.0408 - val_loss: 0.0536 - 4s/epoch - 7ms/step\n",
      "Epoch 63/10000\n",
      "568/568 - 4s - loss: 0.0399 - val_loss: 0.0516 - 4s/epoch - 7ms/step\n",
      "Epoch 64/10000\n",
      "568/568 - 4s - loss: 0.0401 - val_loss: 0.0582 - 4s/epoch - 7ms/step\n",
      "Epoch 65/10000\n",
      "568/568 - 4s - loss: 0.0409 - val_loss: 0.0545 - 4s/epoch - 7ms/step\n",
      "Epoch 66/10000\n",
      "568/568 - 4s - loss: 0.0381 - val_loss: 0.0496 - 4s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "568/568 - 4s - loss: 0.0388 - val_loss: 0.0540 - 4s/epoch - 7ms/step\n",
      "Epoch 68/10000\n",
      "568/568 - 4s - loss: 0.0395 - val_loss: 0.0620 - 4s/epoch - 7ms/step\n",
      "Epoch 69/10000\n",
      "568/568 - 4s - loss: 0.0381 - val_loss: 0.0517 - 4s/epoch - 7ms/step\n",
      "Epoch 70/10000\n",
      "568/568 - 4s - loss: 0.0378 - val_loss: 0.0547 - 4s/epoch - 7ms/step\n",
      "Epoch 71/10000\n",
      "568/568 - 4s - loss: 0.0376 - val_loss: 0.0504 - 4s/epoch - 7ms/step\n",
      "Epoch 72/10000\n",
      "568/568 - 4s - loss: 0.0366 - val_loss: 0.0497 - 4s/epoch - 7ms/step\n",
      "Epoch 73/10000\n",
      "568/568 - 4s - loss: 0.0373 - val_loss: 0.0535 - 4s/epoch - 7ms/step\n",
      "Epoch 74/10000\n",
      "568/568 - 4s - loss: 0.0368 - val_loss: 0.0506 - 4s/epoch - 7ms/step\n",
      "Epoch 75/10000\n",
      "568/568 - 4s - loss: 0.0358 - val_loss: 0.0508 - 4s/epoch - 7ms/step\n",
      "Epoch 76/10000\n",
      "568/568 - 4s - loss: 0.0356 - val_loss: 0.0520 - 4s/epoch - 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_22_layer_call_fn, gru_cell_22_layer_call_and_return_conditional_losses, gru_cell_23_layer_call_fn, gru_cell_23_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_10_1_50_1_50_datt_seq2seq_gru_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_10_1_50_1_50_datt_seq2seq_gru_1\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000294BACEF6A0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000294BCBC7D30> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.953266</td>\n",
       "      <td>0.933363</td>\n",
       "      <td>0.969603</td>\n",
       "      <td>0.952078</td>\n",
       "      <td>3.508187</td>\n",
       "      <td>4.543498</td>\n",
       "      <td>2.684337</td>\n",
       "      <td>3.578674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.891339</td>\n",
       "      <td>0.869157</td>\n",
       "      <td>0.935123</td>\n",
       "      <td>0.89854</td>\n",
       "      <td>5.316429</td>\n",
       "      <td>6.367278</td>\n",
       "      <td>3.922161</td>\n",
       "      <td>5.201956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.804002</td>\n",
       "      <td>0.81831</td>\n",
       "      <td>0.892977</td>\n",
       "      <td>0.838429</td>\n",
       "      <td>7.043749</td>\n",
       "      <td>7.503815</td>\n",
       "      <td>5.038252</td>\n",
       "      <td>6.528605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.707981</td>\n",
       "      <td>0.78435</td>\n",
       "      <td>0.860491</td>\n",
       "      <td>0.784274</td>\n",
       "      <td>8.483707</td>\n",
       "      <td>8.176224</td>\n",
       "      <td>5.753022</td>\n",
       "      <td>7.470984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.610688</td>\n",
       "      <td>0.762327</td>\n",
       "      <td>0.841718</td>\n",
       "      <td>0.738245</td>\n",
       "      <td>9.698813</td>\n",
       "      <td>8.584691</td>\n",
       "      <td>6.128295</td>\n",
       "      <td>8.137266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.517046</td>\n",
       "      <td>0.749823</td>\n",
       "      <td>0.832363</td>\n",
       "      <td>0.699744</td>\n",
       "      <td>10.696998</td>\n",
       "      <td>8.808525</td>\n",
       "      <td>6.307479</td>\n",
       "      <td>8.604334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.43153</td>\n",
       "      <td>0.740454</td>\n",
       "      <td>0.827534</td>\n",
       "      <td>0.666506</td>\n",
       "      <td>11.493713</td>\n",
       "      <td>8.971856</td>\n",
       "      <td>6.398815</td>\n",
       "      <td>8.954795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.359076</td>\n",
       "      <td>0.729706</td>\n",
       "      <td>0.824554</td>\n",
       "      <td>0.637778</td>\n",
       "      <td>12.174667</td>\n",
       "      <td>9.155045</td>\n",
       "      <td>6.455729</td>\n",
       "      <td>9.261814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.300837</td>\n",
       "      <td>0.716659</td>\n",
       "      <td>0.818677</td>\n",
       "      <td>0.612058</td>\n",
       "      <td>12.730646</td>\n",
       "      <td>9.372325</td>\n",
       "      <td>6.565038</td>\n",
       "      <td>9.556003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.252945</td>\n",
       "      <td>0.699541</td>\n",
       "      <td>0.805451</td>\n",
       "      <td>0.585979</td>\n",
       "      <td>13.174579</td>\n",
       "      <td>9.649667</td>\n",
       "      <td>6.802628</td>\n",
       "      <td>9.875625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.582871</td>\n",
       "      <td>0.780369</td>\n",
       "      <td>0.860849</td>\n",
       "      <td>0.741363</td>\n",
       "      <td>9.432149</td>\n",
       "      <td>8.113292</td>\n",
       "      <td>5.605576</td>\n",
       "      <td>7.717006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3 TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE     nRMSE     nRMSE   \n",
       "0      0.953266  0.933363  0.969603  0.952078   3.508187  4.543498  2.684337   \n",
       "1      0.891339  0.869157  0.935123   0.89854   5.316429  6.367278  3.922161   \n",
       "2      0.804002   0.81831  0.892977  0.838429   7.043749  7.503815  5.038252   \n",
       "3      0.707981   0.78435  0.860491  0.784274   8.483707  8.176224  5.753022   \n",
       "4      0.610688  0.762327  0.841718  0.738245   9.698813  8.584691  6.128295   \n",
       "5      0.517046  0.749823  0.832363  0.699744  10.696998  8.808525  6.307479   \n",
       "6       0.43153  0.740454  0.827534  0.666506  11.493713  8.971856  6.398815   \n",
       "7      0.359076  0.729706  0.824554  0.637778  12.174667  9.155045  6.455729   \n",
       "8      0.300837  0.716659  0.818677  0.612058  12.730646  9.372325  6.565038   \n",
       "9      0.252945  0.699541  0.805451  0.585979  13.174579  9.649667  6.802628   \n",
       "mean   0.582871  0.780369  0.860849  0.741363   9.432149  8.113292  5.605576   \n",
       "\n",
       "           mean  \n",
       "index     nRMSE  \n",
       "0      3.578674  \n",
       "1      5.201956  \n",
       "2      6.528605  \n",
       "3      7.470984  \n",
       "4      8.137266  \n",
       "5      8.604334  \n",
       "6      8.954795  \n",
       "7      9.261814  \n",
       "8      9.556003  \n",
       "9      9.875625  \n",
       "mean   7.717006  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/40_10_1_50_1_50_datt_seq2seq_gru_1.csv\n",
      "end training\n",
      "\n",
      "history size: 40\n",
      "future size: 20\n",
      "Epoch 1/10000\n",
      "563/563 - 7s - loss: 0.2987 - val_loss: 0.2555 - 7s/epoch - 13ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 4s - loss: 0.2219 - val_loss: 0.2296 - 4s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.2053 - val_loss: 0.2172 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 5s - loss: 0.1921 - val_loss: 0.2024 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.1818 - val_loss: 0.2027 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.1730 - val_loss: 0.1898 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 4s - loss: 0.1654 - val_loss: 0.1855 - 4s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 4s - loss: 0.1578 - val_loss: 0.1773 - 4s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.1526 - val_loss: 0.1684 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 4s - loss: 0.1478 - val_loss: 0.1661 - 4s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 4s - loss: 0.1421 - val_loss: 0.1633 - 4s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 4s - loss: 0.1376 - val_loss: 0.1546 - 4s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 4s - loss: 0.1318 - val_loss: 0.1541 - 4s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 4s - loss: 0.1264 - val_loss: 0.1510 - 4s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 4s - loss: 0.1217 - val_loss: 0.1431 - 4s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 4s - loss: 0.1187 - val_loss: 0.1328 - 4s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 4s - loss: 0.1173 - val_loss: 0.1299 - 4s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 4s - loss: 0.1125 - val_loss: 0.1307 - 4s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 4s - loss: 0.1080 - val_loss: 0.1233 - 4s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 4s - loss: 0.1103 - val_loss: 0.1218 - 4s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 4s - loss: 0.1019 - val_loss: 0.1217 - 4s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 4s - loss: 0.1005 - val_loss: 0.1211 - 4s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 4s - loss: 0.0982 - val_loss: 0.1175 - 4s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 4s - loss: 0.0967 - val_loss: 0.1170 - 4s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 4s - loss: 0.0931 - val_loss: 0.1121 - 4s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.0946 - val_loss: 0.1120 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 4s - loss: 0.0923 - val_loss: 0.1132 - 4s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 4s - loss: 0.0875 - val_loss: 0.1078 - 4s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 4s - loss: 0.0891 - val_loss: 0.1078 - 4s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 4s - loss: 0.0857 - val_loss: 0.1181 - 4s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 4s - loss: 0.0851 - val_loss: 0.1012 - 4s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 4s - loss: 0.0907 - val_loss: 0.1220 - 4s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 4s - loss: 0.0841 - val_loss: 0.1049 - 4s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 4s - loss: 0.0814 - val_loss: 0.1032 - 4s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.0784 - val_loss: 0.1036 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0790 - val_loss: 0.1005 - 5s/epoch - 9ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0787 - val_loss: 0.0965 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0776 - val_loss: 0.0947 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 4s - loss: 0.0752 - val_loss: 0.0958 - 4s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 4s - loss: 0.0763 - val_loss: 0.0982 - 4s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 4s - loss: 0.0756 - val_loss: 0.0950 - 4s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 4s - loss: 0.0735 - val_loss: 0.0970 - 4s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 4s - loss: 0.0743 - val_loss: 0.1004 - 4s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 4s - loss: 0.0727 - val_loss: 0.0906 - 4s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 4s - loss: 0.0688 - val_loss: 0.0902 - 4s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0697 - val_loss: 0.0889 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0734 - val_loss: 0.0863 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 4s - loss: 0.0666 - val_loss: 0.0869 - 4s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0688 - val_loss: 0.0880 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0673 - val_loss: 0.0838 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0664 - val_loss: 0.0875 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0669 - val_loss: 0.0830 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 6s - loss: 0.0665 - val_loss: 0.0958 - 6s/epoch - 10ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0639 - val_loss: 0.0856 - 5s/epoch - 10ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0691 - val_loss: 0.1061 - 5s/epoch - 9ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0656 - val_loss: 0.0790 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0612 - val_loss: 0.0962 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0610 - val_loss: 0.0834 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0600 - val_loss: 0.0755 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 4s - loss: 0.0635 - val_loss: 0.0781 - 4s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0599 - val_loss: 0.0788 - 5s/epoch - 9ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0590 - val_loss: 0.0832 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 5s - loss: 0.0617 - val_loss: 0.0732 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0569 - val_loss: 0.0907 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0597 - val_loss: 0.0765 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0586 - val_loss: 0.0742 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0581 - val_loss: 0.0737 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 5s - loss: 0.0573 - val_loss: 0.0704 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 6s - loss: 0.0548 - val_loss: 0.0852 - 6s/epoch - 10ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 6s - loss: 0.0597 - val_loss: 0.0680 - 6s/epoch - 10ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 5s - loss: 0.0550 - val_loss: 0.0711 - 5s/epoch - 9ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 5s - loss: 0.0562 - val_loss: 0.0743 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 5s - loss: 0.0568 - val_loss: 0.0744 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 5s - loss: 0.0556 - val_loss: 0.0691 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 5s - loss: 0.0558 - val_loss: 0.0694 - 5s/epoch - 9ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 5s - loss: 0.0534 - val_loss: 0.0699 - 5s/epoch - 9ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 5s - loss: 0.0537 - val_loss: 0.0663 - 5s/epoch - 9ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 5s - loss: 0.0561 - val_loss: 0.0670 - 5s/epoch - 9ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 5s - loss: 0.0520 - val_loss: 0.0671 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 5s - loss: 0.0527 - val_loss: 0.0738 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 4s - loss: 0.0528 - val_loss: 0.0788 - 4s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 5s - loss: 0.0523 - val_loss: 0.0676 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 5s - loss: 0.0512 - val_loss: 0.0744 - 5s/epoch - 9ms/step\n",
      "Epoch 84/10000\n",
      "563/563 - 5s - loss: 0.0585 - val_loss: 0.0698 - 5s/epoch - 9ms/step\n",
      "Epoch 85/10000\n",
      "563/563 - 5s - loss: 0.0500 - val_loss: 0.0651 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "563/563 - 5s - loss: 0.0499 - val_loss: 0.0607 - 5s/epoch - 9ms/step\n",
      "Epoch 87/10000\n",
      "563/563 - 5s - loss: 0.0511 - val_loss: 0.0628 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "563/563 - 5s - loss: 0.0493 - val_loss: 0.0797 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "563/563 - 5s - loss: 0.0495 - val_loss: 0.0680 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "563/563 - 5s - loss: 0.0504 - val_loss: 0.0655 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "563/563 - 5s - loss: 0.0513 - val_loss: 0.0634 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "563/563 - 7s - loss: 0.0491 - val_loss: 0.0614 - 7s/epoch - 12ms/step\n",
      "Epoch 93/10000\n",
      "563/563 - 5s - loss: 0.0489 - val_loss: 0.0587 - 5s/epoch - 9ms/step\n",
      "Epoch 94/10000\n",
      "563/563 - 5s - loss: 0.0513 - val_loss: 0.0622 - 5s/epoch - 9ms/step\n",
      "Epoch 95/10000\n",
      "563/563 - 5s - loss: 0.0483 - val_loss: 0.0687 - 5s/epoch - 9ms/step\n",
      "Epoch 96/10000\n",
      "563/563 - 5s - loss: 0.0483 - val_loss: 0.0579 - 5s/epoch - 9ms/step\n",
      "Epoch 97/10000\n",
      "563/563 - 5s - loss: 0.0525 - val_loss: 0.0681 - 5s/epoch - 9ms/step\n",
      "Epoch 98/10000\n",
      "563/563 - 5s - loss: 0.0478 - val_loss: 0.0680 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "563/563 - 5s - loss: 0.0467 - val_loss: 0.0603 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "563/563 - 5s - loss: 0.0465 - val_loss: 0.0606 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "563/563 - 5s - loss: 0.0456 - val_loss: 0.0580 - 5s/epoch - 9ms/step\n",
      "Epoch 102/10000\n",
      "563/563 - 5s - loss: 0.0472 - val_loss: 0.0627 - 5s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "563/563 - 5s - loss: 0.0548 - val_loss: 0.0744 - 5s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "563/563 - 5s - loss: 0.0496 - val_loss: 0.0569 - 5s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "563/563 - 5s - loss: 0.0446 - val_loss: 0.0615 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "563/563 - 5s - loss: 0.0455 - val_loss: 0.0639 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "563/563 - 6s - loss: 0.0459 - val_loss: 0.0587 - 6s/epoch - 10ms/step\n",
      "Epoch 108/10000\n",
      "563/563 - 6s - loss: 0.0455 - val_loss: 0.0576 - 6s/epoch - 10ms/step\n",
      "Epoch 109/10000\n",
      "563/563 - 5s - loss: 0.0439 - val_loss: 0.0598 - 5s/epoch - 9ms/step\n",
      "Epoch 110/10000\n",
      "563/563 - 4s - loss: 0.0495 - val_loss: 0.0738 - 4s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "563/563 - 4s - loss: 0.0457 - val_loss: 0.0649 - 4s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "563/563 - 5s - loss: 0.0452 - val_loss: 0.0619 - 5s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "563/563 - 4s - loss: 0.0441 - val_loss: 0.0559 - 4s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "563/563 - 4s - loss: 0.0442 - val_loss: 0.0600 - 4s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "563/563 - 4s - loss: 0.0441 - val_loss: 0.0565 - 4s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "563/563 - 4s - loss: 0.0449 - val_loss: 0.0574 - 4s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "563/563 - 4s - loss: 0.0431 - val_loss: 0.0594 - 4s/epoch - 8ms/step\n",
      "Epoch 118/10000\n",
      "563/563 - 4s - loss: 0.0438 - val_loss: 0.0568 - 4s/epoch - 8ms/step\n",
      "Epoch 119/10000\n",
      "563/563 - 4s - loss: 0.0439 - val_loss: 0.0599 - 4s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "563/563 - 4s - loss: 0.0432 - val_loss: 0.0624 - 4s/epoch - 8ms/step\n",
      "Epoch 121/10000\n",
      "563/563 - 5s - loss: 0.0444 - val_loss: 0.0592 - 5s/epoch - 9ms/step\n",
      "Epoch 122/10000\n",
      "563/563 - 5s - loss: 0.0418 - val_loss: 0.0548 - 5s/epoch - 8ms/step\n",
      "Epoch 123/10000\n",
      "563/563 - 5s - loss: 0.0450 - val_loss: 0.0561 - 5s/epoch - 8ms/step\n",
      "Epoch 124/10000\n",
      "563/563 - 5s - loss: 0.0417 - val_loss: 0.0533 - 5s/epoch - 8ms/step\n",
      "Epoch 125/10000\n",
      "563/563 - 5s - loss: 0.0438 - val_loss: 0.0591 - 5s/epoch - 8ms/step\n",
      "Epoch 126/10000\n",
      "563/563 - 5s - loss: 0.0444 - val_loss: 0.0515 - 5s/epoch - 8ms/step\n",
      "Epoch 127/10000\n",
      "563/563 - 5s - loss: 0.0414 - val_loss: 0.0558 - 5s/epoch - 8ms/step\n",
      "Epoch 128/10000\n",
      "563/563 - 5s - loss: 0.0414 - val_loss: 0.0609 - 5s/epoch - 8ms/step\n",
      "Epoch 129/10000\n",
      "563/563 - 5s - loss: 0.0424 - val_loss: 0.0560 - 5s/epoch - 8ms/step\n",
      "Epoch 130/10000\n",
      "563/563 - 5s - loss: 0.0419 - val_loss: 0.0513 - 5s/epoch - 8ms/step\n",
      "Epoch 131/10000\n",
      "563/563 - 6s - loss: 0.0418 - val_loss: 0.0589 - 6s/epoch - 10ms/step\n",
      "Epoch 132/10000\n",
      "563/563 - 6s - loss: 0.0415 - val_loss: 0.0567 - 6s/epoch - 10ms/step\n",
      "Epoch 133/10000\n",
      "563/563 - 5s - loss: 0.0409 - val_loss: 0.0547 - 5s/epoch - 9ms/step\n",
      "Epoch 134/10000\n",
      "563/563 - 5s - loss: 0.0411 - val_loss: 0.0517 - 5s/epoch - 9ms/step\n",
      "Epoch 135/10000\n",
      "563/563 - 5s - loss: 0.0413 - val_loss: 0.0577 - 5s/epoch - 9ms/step\n",
      "Epoch 136/10000\n",
      "563/563 - 6s - loss: 0.0419 - val_loss: 0.0597 - 6s/epoch - 10ms/step\n",
      "Epoch 137/10000\n",
      "563/563 - 5s - loss: 0.0400 - val_loss: 0.0525 - 5s/epoch - 9ms/step\n",
      "Epoch 138/10000\n",
      "563/563 - 5s - loss: 0.0392 - val_loss: 0.0531 - 5s/epoch - 10ms/step\n",
      "Epoch 139/10000\n",
      "563/563 - 5s - loss: 0.0406 - val_loss: 0.0518 - 5s/epoch - 9ms/step\n",
      "Epoch 140/10000\n",
      "563/563 - 5s - loss: 0.0407 - val_loss: 0.0637 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_24_layer_call_fn, gru_cell_24_layer_call_and_return_conditional_losses, gru_cell_25_layer_call_fn, gru_cell_25_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_20_1_50_1_50_datt_seq2seq_gru_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_20_1_50_1_50_datt_seq2seq_gru_1\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002966FC87460> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x00000294B9E49DF0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.913741</td>\n",
       "      <td>0.89525</td>\n",
       "      <td>0.93191</td>\n",
       "      <td>0.913633</td>\n",
       "      <td>5.795058</td>\n",
       "      <td>5.69088</td>\n",
       "      <td>4.008191</td>\n",
       "      <td>5.16471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.841715</td>\n",
       "      <td>0.765577</td>\n",
       "      <td>0.876962</td>\n",
       "      <td>0.828085</td>\n",
       "      <td>7.709077</td>\n",
       "      <td>8.516104</td>\n",
       "      <td>5.389555</td>\n",
       "      <td>7.204912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.744795</td>\n",
       "      <td>0.701873</td>\n",
       "      <td>0.796194</td>\n",
       "      <td>0.747621</td>\n",
       "      <td>9.362351</td>\n",
       "      <td>9.607465</td>\n",
       "      <td>6.93855</td>\n",
       "      <td>8.636122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.640476</td>\n",
       "      <td>0.669654</td>\n",
       "      <td>0.720539</td>\n",
       "      <td>0.67689</td>\n",
       "      <td>10.741177</td>\n",
       "      <td>10.116872</td>\n",
       "      <td>8.126747</td>\n",
       "      <td>9.661598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.535155</td>\n",
       "      <td>0.650142</td>\n",
       "      <td>0.666357</td>\n",
       "      <td>0.617218</td>\n",
       "      <td>12.018037</td>\n",
       "      <td>10.415991</td>\n",
       "      <td>8.881679</td>\n",
       "      <td>10.438569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.435026</td>\n",
       "      <td>0.639643</td>\n",
       "      <td>0.637443</td>\n",
       "      <td>0.570704</td>\n",
       "      <td>13.204613</td>\n",
       "      <td>10.575908</td>\n",
       "      <td>9.26015</td>\n",
       "      <td>11.013557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.342984</td>\n",
       "      <td>0.635663</td>\n",
       "      <td>0.625937</td>\n",
       "      <td>0.534861</td>\n",
       "      <td>13.960408</td>\n",
       "      <td>10.63657</td>\n",
       "      <td>9.407852</td>\n",
       "      <td>11.334943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.263234</td>\n",
       "      <td>0.633234</td>\n",
       "      <td>0.621338</td>\n",
       "      <td>0.505935</td>\n",
       "      <td>14.500798</td>\n",
       "      <td>10.674354</td>\n",
       "      <td>9.467354</td>\n",
       "      <td>11.547502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.194559</td>\n",
       "      <td>0.626066</td>\n",
       "      <td>0.611696</td>\n",
       "      <td>0.47744</td>\n",
       "      <td>14.937544</td>\n",
       "      <td>10.780037</td>\n",
       "      <td>9.588844</td>\n",
       "      <td>11.768808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.131793</td>\n",
       "      <td>0.612444</td>\n",
       "      <td>0.585581</td>\n",
       "      <td>0.443272</td>\n",
       "      <td>15.283146</td>\n",
       "      <td>10.974784</td>\n",
       "      <td>9.907554</td>\n",
       "      <td>12.055161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.069526</td>\n",
       "      <td>0.593268</td>\n",
       "      <td>0.540382</td>\n",
       "      <td>0.401058</td>\n",
       "      <td>15.655647</td>\n",
       "      <td>11.243639</td>\n",
       "      <td>10.4354</td>\n",
       "      <td>12.444896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.004367</td>\n",
       "      <td>0.571768</td>\n",
       "      <td>0.484466</td>\n",
       "      <td>0.353533</td>\n",
       "      <td>16.088675</td>\n",
       "      <td>11.536297</td>\n",
       "      <td>11.0538</td>\n",
       "      <td>12.892924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.061008</td>\n",
       "      <td>0.548273</td>\n",
       "      <td>0.435622</td>\n",
       "      <td>0.307629</td>\n",
       "      <td>16.378084</td>\n",
       "      <td>11.84768</td>\n",
       "      <td>11.568307</td>\n",
       "      <td>13.26469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.11969</td>\n",
       "      <td>0.523981</td>\n",
       "      <td>0.404631</td>\n",
       "      <td>0.269641</td>\n",
       "      <td>16.5959</td>\n",
       "      <td>12.163045</td>\n",
       "      <td>11.885321</td>\n",
       "      <td>13.548089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.169811</td>\n",
       "      <td>0.498222</td>\n",
       "      <td>0.389114</td>\n",
       "      <td>0.239175</td>\n",
       "      <td>16.790146</td>\n",
       "      <td>12.488272</td>\n",
       "      <td>12.042818</td>\n",
       "      <td>13.773746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.210962</td>\n",
       "      <td>0.471285</td>\n",
       "      <td>0.378633</td>\n",
       "      <td>0.212985</td>\n",
       "      <td>16.912104</td>\n",
       "      <td>12.818449</td>\n",
       "      <td>12.149904</td>\n",
       "      <td>13.960152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.242466</td>\n",
       "      <td>0.441877</td>\n",
       "      <td>0.364382</td>\n",
       "      <td>0.187931</td>\n",
       "      <td>16.963919</td>\n",
       "      <td>13.168146</td>\n",
       "      <td>12.2931</td>\n",
       "      <td>14.141722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.266688</td>\n",
       "      <td>0.409501</td>\n",
       "      <td>0.342142</td>\n",
       "      <td>0.161652</td>\n",
       "      <td>17.086598</td>\n",
       "      <td>13.541968</td>\n",
       "      <td>12.511541</td>\n",
       "      <td>14.380036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.282648</td>\n",
       "      <td>0.371778</td>\n",
       "      <td>0.313139</td>\n",
       "      <td>0.13409</td>\n",
       "      <td>17.214717</td>\n",
       "      <td>13.965953</td>\n",
       "      <td>12.789713</td>\n",
       "      <td>14.656794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.292246</td>\n",
       "      <td>0.32768</td>\n",
       "      <td>0.284834</td>\n",
       "      <td>0.106756</td>\n",
       "      <td>17.303234</td>\n",
       "      <td>14.446345</td>\n",
       "      <td>13.055737</td>\n",
       "      <td>14.935105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.173593</td>\n",
       "      <td>0.579359</td>\n",
       "      <td>0.550565</td>\n",
       "      <td>0.434505</td>\n",
       "      <td>14.225062</td>\n",
       "      <td>11.260438</td>\n",
       "      <td>10.038106</td>\n",
       "      <td>11.841202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.913741   0.89525   0.93191  0.913633   5.795058    5.69088   \n",
       "1      0.841715  0.765577  0.876962  0.828085   7.709077   8.516104   \n",
       "2      0.744795  0.701873  0.796194  0.747621   9.362351   9.607465   \n",
       "3      0.640476  0.669654  0.720539   0.67689  10.741177  10.116872   \n",
       "4      0.535155  0.650142  0.666357  0.617218  12.018037  10.415991   \n",
       "5      0.435026  0.639643  0.637443  0.570704  13.204613  10.575908   \n",
       "6      0.342984  0.635663  0.625937  0.534861  13.960408   10.63657   \n",
       "7      0.263234  0.633234  0.621338  0.505935  14.500798  10.674354   \n",
       "8      0.194559  0.626066  0.611696   0.47744  14.937544  10.780037   \n",
       "9      0.131793  0.612444  0.585581  0.443272  15.283146  10.974784   \n",
       "10     0.069526  0.593268  0.540382  0.401058  15.655647  11.243639   \n",
       "11     0.004367  0.571768  0.484466  0.353533  16.088675  11.536297   \n",
       "12    -0.061008  0.548273  0.435622  0.307629  16.378084   11.84768   \n",
       "13     -0.11969  0.523981  0.404631  0.269641    16.5959  12.163045   \n",
       "14    -0.169811  0.498222  0.389114  0.239175  16.790146  12.488272   \n",
       "15    -0.210962  0.471285  0.378633  0.212985  16.912104  12.818449   \n",
       "16    -0.242466  0.441877  0.364382  0.187931  16.963919  13.168146   \n",
       "17    -0.266688  0.409501  0.342142  0.161652  17.086598  13.541968   \n",
       "18    -0.282648  0.371778  0.313139   0.13409  17.214717  13.965953   \n",
       "19    -0.292246   0.32768  0.284834  0.106756  17.303234  14.446345   \n",
       "mean   0.173593  0.579359  0.550565  0.434505  14.225062  11.260438   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       4.008191    5.16471  \n",
       "1       5.389555   7.204912  \n",
       "2        6.93855   8.636122  \n",
       "3       8.126747   9.661598  \n",
       "4       8.881679  10.438569  \n",
       "5        9.26015  11.013557  \n",
       "6       9.407852  11.334943  \n",
       "7       9.467354  11.547502  \n",
       "8       9.588844  11.768808  \n",
       "9       9.907554  12.055161  \n",
       "10       10.4354  12.444896  \n",
       "11       11.0538  12.892924  \n",
       "12     11.568307   13.26469  \n",
       "13     11.885321  13.548089  \n",
       "14     12.042818  13.773746  \n",
       "15     12.149904  13.960152  \n",
       "16       12.2931  14.141722  \n",
       "17     12.511541  14.380036  \n",
       "18     12.789713  14.656794  \n",
       "19     13.055737  14.935105  \n",
       "mean   10.038106  11.841202  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/40_20_1_50_1_50_datt_seq2seq_gru_1.csv\n",
      "end training\n",
      "\n",
      "history size: 40\n",
      "future size: 30\n",
      "Epoch 1/10000\n",
      "559/559 - 8s - loss: 0.3750 - val_loss: 0.3110 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "559/559 - 5s - loss: 0.2868 - val_loss: 0.2918 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "559/559 - 5s - loss: 0.2640 - val_loss: 0.2640 - 5s/epoch - 9ms/step\n",
      "Epoch 4/10000\n",
      "559/559 - 4s - loss: 0.2461 - val_loss: 0.2445 - 4s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "559/559 - 5s - loss: 0.2295 - val_loss: 0.2346 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "559/559 - 5s - loss: 0.2153 - val_loss: 0.2236 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "559/559 - 4s - loss: 0.2029 - val_loss: 0.2069 - 4s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "559/559 - 5s - loss: 0.1937 - val_loss: 0.2026 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "559/559 - 5s - loss: 0.1839 - val_loss: 0.1923 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "559/559 - 5s - loss: 0.1753 - val_loss: 0.1786 - 5s/epoch - 9ms/step\n",
      "Epoch 11/10000\n",
      "559/559 - 6s - loss: 0.1722 - val_loss: 0.1776 - 6s/epoch - 10ms/step\n",
      "Epoch 12/10000\n",
      "559/559 - 5s - loss: 0.1652 - val_loss: 0.1725 - 5s/epoch - 9ms/step\n",
      "Epoch 13/10000\n",
      "559/559 - 5s - loss: 0.1588 - val_loss: 0.1688 - 5s/epoch - 9ms/step\n",
      "Epoch 14/10000\n",
      "559/559 - 4s - loss: 0.1535 - val_loss: 0.1602 - 4s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "559/559 - 4s - loss: 0.1508 - val_loss: 0.1664 - 4s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "559/559 - 4s - loss: 0.1465 - val_loss: 0.1513 - 4s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "559/559 - 4s - loss: 0.1438 - val_loss: 0.1595 - 4s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "559/559 - 4s - loss: 0.1390 - val_loss: 0.1622 - 4s/epoch - 7ms/step\n",
      "Epoch 19/10000\n",
      "559/559 - 4s - loss: 0.1376 - val_loss: 0.1447 - 4s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "559/559 - 4s - loss: 0.1355 - val_loss: 0.1484 - 4s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "559/559 - 4s - loss: 0.1332 - val_loss: 0.1361 - 4s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "559/559 - 4s - loss: 0.1253 - val_loss: 0.1374 - 4s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "559/559 - 4s - loss: 0.1252 - val_loss: 0.1248 - 4s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "559/559 - 4s - loss: 0.1192 - val_loss: 0.1332 - 4s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "559/559 - 5s - loss: 0.1176 - val_loss: 0.1306 - 5s/epoch - 9ms/step\n",
      "Epoch 26/10000\n",
      "559/559 - 5s - loss: 0.1186 - val_loss: 0.1247 - 5s/epoch - 9ms/step\n",
      "Epoch 27/10000\n",
      "559/559 - 5s - loss: 0.1107 - val_loss: 0.1261 - 5s/epoch - 9ms/step\n",
      "Epoch 28/10000\n",
      "559/559 - 5s - loss: 0.1110 - val_loss: 0.1272 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "559/559 - 4s - loss: 0.1091 - val_loss: 0.1149 - 4s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "559/559 - 5s - loss: 0.1138 - val_loss: 0.1161 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "559/559 - 4s - loss: 0.1044 - val_loss: 0.1209 - 4s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "559/559 - 5s - loss: 0.1035 - val_loss: 0.1161 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "559/559 - 4s - loss: 0.1033 - val_loss: 0.1164 - 4s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "559/559 - 4s - loss: 0.1029 - val_loss: 0.1155 - 4s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "559/559 - 4s - loss: 0.0996 - val_loss: 0.1202 - 4s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "559/559 - 4s - loss: 0.1000 - val_loss: 0.1171 - 4s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "559/559 - 4s - loss: 0.0968 - val_loss: 0.1081 - 4s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "559/559 - 4s - loss: 0.0988 - val_loss: 0.1091 - 4s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "559/559 - 5s - loss: 0.0933 - val_loss: 0.1141 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "559/559 - 4s - loss: 0.0946 - val_loss: 0.1093 - 4s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "559/559 - 4s - loss: 0.0914 - val_loss: 0.1038 - 4s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "559/559 - 4s - loss: 0.0896 - val_loss: 0.1081 - 4s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "559/559 - 5s - loss: 0.0922 - val_loss: 0.1085 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "559/559 - 5s - loss: 0.0886 - val_loss: 0.0996 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "559/559 - 5s - loss: 0.0859 - val_loss: 0.1036 - 5s/epoch - 9ms/step\n",
      "Epoch 46/10000\n",
      "559/559 - 6s - loss: 0.0883 - val_loss: 0.1016 - 6s/epoch - 11ms/step\n",
      "Epoch 47/10000\n",
      "559/559 - 29s - loss: 0.0903 - val_loss: 0.1069 - 29s/epoch - 53ms/step\n",
      "Epoch 48/10000\n",
      "559/559 - 25s - loss: 0.0862 - val_loss: 0.0989 - 25s/epoch - 44ms/step\n",
      "Epoch 49/10000\n",
      "559/559 - 22s - loss: 0.0837 - val_loss: 0.0958 - 22s/epoch - 40ms/step\n",
      "Epoch 50/10000\n",
      "559/559 - 21s - loss: 0.0834 - val_loss: 0.0955 - 21s/epoch - 38ms/step\n",
      "Epoch 51/10000\n",
      "559/559 - 27s - loss: 0.0836 - val_loss: 0.0915 - 27s/epoch - 48ms/step\n",
      "Epoch 52/10000\n",
      "559/559 - 10s - loss: 0.0807 - val_loss: 0.0981 - 10s/epoch - 18ms/step\n",
      "Epoch 53/10000\n",
      "559/559 - 32s - loss: 0.0802 - val_loss: 0.0879 - 32s/epoch - 56ms/step\n",
      "Epoch 54/10000\n",
      "559/559 - 15s - loss: 0.0868 - val_loss: 0.0921 - 15s/epoch - 27ms/step\n",
      "Epoch 55/10000\n",
      "559/559 - 5s - loss: 0.0794 - val_loss: 0.0857 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "559/559 - 4s - loss: 0.0776 - val_loss: 0.0906 - 4s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "559/559 - 4s - loss: 0.0754 - val_loss: 0.0879 - 4s/epoch - 7ms/step\n",
      "Epoch 58/10000\n",
      "559/559 - 4s - loss: 0.0791 - val_loss: 0.0933 - 4s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "559/559 - 4s - loss: 0.0753 - val_loss: 0.0916 - 4s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "559/559 - 4s - loss: 0.0751 - val_loss: 0.0865 - 4s/epoch - 7ms/step\n",
      "Epoch 61/10000\n",
      "559/559 - 4s - loss: 0.0748 - val_loss: 0.0905 - 4s/epoch - 7ms/step\n",
      "Epoch 62/10000\n",
      "559/559 - 4s - loss: 0.0755 - val_loss: 0.1004 - 4s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "559/559 - 4s - loss: 0.0749 - val_loss: 0.0842 - 4s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "559/559 - 4s - loss: 0.0720 - val_loss: 0.0817 - 4s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "559/559 - 4s - loss: 0.0707 - val_loss: 0.0845 - 4s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "559/559 - 4s - loss: 0.0726 - val_loss: 0.1221 - 4s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "559/559 - 4s - loss: 0.0811 - val_loss: 0.0865 - 4s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "559/559 - 4s - loss: 0.0686 - val_loss: 0.0787 - 4s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "559/559 - 4s - loss: 0.0692 - val_loss: 0.0913 - 4s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "559/559 - 4s - loss: 0.0695 - val_loss: 0.0800 - 4s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "559/559 - 4s - loss: 0.0677 - val_loss: 0.0842 - 4s/epoch - 7ms/step\n",
      "Epoch 72/10000\n",
      "559/559 - 4s - loss: 0.0689 - val_loss: 0.0787 - 4s/epoch - 7ms/step\n",
      "Epoch 73/10000\n",
      "559/559 - 4s - loss: 0.0693 - val_loss: 0.0820 - 4s/epoch - 7ms/step\n",
      "Epoch 74/10000\n",
      "559/559 - 4s - loss: 0.0670 - val_loss: 0.0773 - 4s/epoch - 7ms/step\n",
      "Epoch 75/10000\n",
      "559/559 - 4s - loss: 0.0645 - val_loss: 0.0768 - 4s/epoch - 7ms/step\n",
      "Epoch 76/10000\n",
      "559/559 - 4s - loss: 0.0687 - val_loss: 0.0852 - 4s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "559/559 - 4s - loss: 0.0678 - val_loss: 0.0771 - 4s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "559/559 - 4s - loss: 0.0649 - val_loss: 0.0761 - 4s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "559/559 - 4s - loss: 0.0653 - val_loss: 0.0756 - 4s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "559/559 - 4s - loss: 0.0650 - val_loss: 0.0735 - 4s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "559/559 - 4s - loss: 0.0651 - val_loss: 0.0809 - 4s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "559/559 - 5s - loss: 0.0661 - val_loss: 0.0912 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "559/559 - 4s - loss: 0.0657 - val_loss: 0.0746 - 4s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "559/559 - 5s - loss: 0.0613 - val_loss: 0.0742 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "559/559 - 4s - loss: 0.0632 - val_loss: 0.0735 - 4s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "559/559 - 4s - loss: 0.0638 - val_loss: 0.0744 - 4s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "559/559 - 5s - loss: 0.0604 - val_loss: 0.0814 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "559/559 - 4s - loss: 0.0630 - val_loss: 0.0791 - 4s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "559/559 - 4s - loss: 0.0611 - val_loss: 0.0771 - 4s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "559/559 - 4s - loss: 0.0639 - val_loss: 0.0780 - 4s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "559/559 - 4s - loss: 0.0592 - val_loss: 0.0718 - 4s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "559/559 - 4s - loss: 0.0613 - val_loss: 0.0705 - 4s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "559/559 - 4s - loss: 0.0605 - val_loss: 0.0754 - 4s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "559/559 - 5s - loss: 0.0590 - val_loss: 0.0721 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "559/559 - 5s - loss: 0.0611 - val_loss: 0.0726 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "559/559 - 4s - loss: 0.0583 - val_loss: 0.0700 - 4s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "559/559 - 5s - loss: 0.0580 - val_loss: 0.0742 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "559/559 - 4s - loss: 0.0619 - val_loss: 0.0707 - 4s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "559/559 - 4s - loss: 0.0577 - val_loss: 0.0690 - 4s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "559/559 - 4s - loss: 0.0590 - val_loss: 0.0716 - 4s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "559/559 - 4s - loss: 0.0615 - val_loss: 0.0733 - 4s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "559/559 - 4s - loss: 0.0615 - val_loss: 0.0729 - 4s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "559/559 - 5s - loss: 0.0601 - val_loss: 0.0797 - 5s/epoch - 9ms/step\n",
      "Epoch 104/10000\n",
      "559/559 - 5s - loss: 0.0567 - val_loss: 0.0715 - 5s/epoch - 9ms/step\n",
      "Epoch 105/10000\n",
      "559/559 - 5s - loss: 0.0584 - val_loss: 0.0713 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "559/559 - 5s - loss: 0.0557 - val_loss: 0.0680 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "559/559 - 4s - loss: 0.0568 - val_loss: 0.0702 - 4s/epoch - 8ms/step\n",
      "Epoch 108/10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Project\\dual_attention\\3_ParameterOptimization.ipynb Cell 5'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/dual_attention/3_ParameterOptimization.ipynb#ch0000004?line=64'>65</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mhistory_size\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mfuture_size\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mnum_layers\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mnum_neurons\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mdense_layers\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mdense_neurons\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mmodel_num\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/dual_attention/3_ParameterOptimization.ipynb#ch0000004?line=65'>66</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exists(\u001b[39m\"\u001b[39m\u001b[39m./model\u001b[39m\u001b[39m\"\u001b[39m, model_name, \u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Project/dual_attention/3_ParameterOptimization.ipynb#ch0000004?line=66'>67</a>\u001b[0m     DATT_seq2seq_GRU\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/dual_attention/3_ParameterOptimization.ipynb#ch0000004?line=67'>68</a>\u001b[0m     DATT_seq2seq_GRU\u001b[39m.\u001b[39msave_model(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./model/\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Project/dual_attention/3_ParameterOptimization.ipynb#ch0000004?line=68'>69</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\src\\rnn.py:180\u001b[0m, in \u001b[0;36mRNN.train\u001b[1;34m(self, epochs, verbose, batch_size, patience, monitor)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/src/rnn.py?line=177'>178</a>\u001b[0m time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/src/rnn.py?line=178'>179</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdatt_seq2seq_lstm\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdatt_seq2seq_gru\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/Project/dual_attention/src/rnn.py?line=179'>180</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistory_train_sc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfuture_train_wt, \n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/src/rnn.py?line=180'>181</a>\u001b[0m                                   epochs\u001b[39m=\u001b[39;49mepochs, callbacks\u001b[39m=\u001b[39;49m[early_stopping_cb], verbose\u001b[39m=\u001b[39;49mverbose, batch_size\u001b[39m=\u001b[39;49mbatch_size, \n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/src/rnn.py?line=181'>182</a>\u001b[0m                                   validation_data\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistory_valid_sc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfuture_valid_wt))\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/src/rnn.py?line=182'>183</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/src/rnn.py?line=183'>184</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory_train_sc, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture_train_sc, \n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/src/rnn.py?line=184'>185</a>\u001b[0m                                   epochs\u001b[39m=\u001b[39mepochs, callbacks\u001b[39m=\u001b[39m[early_stopping_cb], verbose\u001b[39m=\u001b[39mverbose, batch_size\u001b[39m=\u001b[39mbatch_size, \n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/src/rnn.py?line=185'>186</a>\u001b[0m                                   validation_data\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory_valid_sc, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture_valid_sc))\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\venv\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1376'>1377</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1377'>1378</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1378'>1379</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1379'>1380</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1380'>1381</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1381'>1382</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1382'>1383</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1383'>1384</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1384'>1385</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/keras/engine/training.py?line=1385'>1386</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=2952'>2953</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=2953'>2954</a>\u001b[0m   (graph_function,\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=2954'>2955</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=2955'>2956</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=2956'>2957</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1848'>1849</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1849'>1850</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1850'>1851</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1851'>1852</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1852'>1853</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1853'>1854</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1854'>1855</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m     args,\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m     possible_gradient_type,\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     executing_eagerly)\n\u001b[0;32m   <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=506'>507</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=507'>508</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=510'>511</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/function.py?line=511'>512</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Project\\dual_attention\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='file:///c%3A/Project/dual_attention/venv/lib/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iteration = 10\n",
    "target_list = cts_list\n",
    "\n",
    "# history size and future size\n",
    "history_list = [10, 20, 30, 40]\n",
    "future_list = [10, 20, 30]\n",
    "step = 1\n",
    "\n",
    "# variable selection\n",
    "history_var = process_var\n",
    "future_var = output_var\n",
    "\n",
    "history_num = len(history_var)\n",
    "future_num = len(future_var)\n",
    "\n",
    "# supervised attention factor\n",
    "delta = 1\n",
    "att_type = 'linear'\n",
    "        \n",
    "# test data split        \n",
    "test_size = 0.2\n",
    "test_num = -1\n",
    "\n",
    "# model structure\n",
    "num_layers = 1\n",
    "num_neurons = 50\n",
    "dense_layers = 1\n",
    "dense_neurons = 50\n",
    "model_type = 'datt_seq2seq_gru'\n",
    "\n",
    "for iteration in range(num_iteration):\n",
    "    print(f\"{iteration+1}th iteration\")\n",
    "    for history_size in history_list:\n",
    "        for future_size in future_list:\n",
    "            print(f\"history size: {history_size}\")\n",
    "            print(f\"future size: {future_size}\")\n",
    "            history_series = []\n",
    "            future_series = []\n",
    "\n",
    "            # data to series\n",
    "            for i in range(len(target_list)):\n",
    "                history, future = data2series(target_list[i], history_size, history_var, future_size, future_var,\n",
    "                                            step, start_idx=0, end_idx=None)\n",
    "                if not i:\n",
    "                    history_series = history\n",
    "                    future_series = future\n",
    "                else:\n",
    "                    history_series = np.concatenate([history_series, history], axis=0)\n",
    "                    future_series = np.concatenate([future_series, future], axis=0)\n",
    "            \n",
    "            factor = rnn.super_attention(delta, future_size, future_num, att_type)\n",
    "            # Dual-attention Seq2Seq model\n",
    "            DATT_seq2seq_GRU = rnn.RNN(history_series, history_var, future_series, future_var)\n",
    "            # TT split\n",
    "            DATT_seq2seq_GRU.train_test(test_size=test_size, test_num=test_num)\n",
    "            # TV split\n",
    "            valid_size = DATT_seq2seq_GRU.history_test.shape[0]/DATT_seq2seq_GRU.history_train.shape[0]\n",
    "            DATT_seq2seq_GRU.train_valid(valid_size=valid_size)\n",
    "            # scaling\n",
    "            DATT_seq2seq_GRU.scaling()\n",
    "            # modeling\n",
    "            DATT_seq2seq_GRU.build_model(num_layers=num_layers, num_neurons=num_neurons, dense_layers=dense_layers, dense_neurons=dense_neurons, model_type=model_type, factor=factor)\n",
    "            # training\n",
    "            model_num = iteration+1\n",
    "            model_name = f\"{history_size}_{future_size}_{num_layers}_{num_neurons}_{dense_layers}_{dense_neurons}_{model_type}_{model_num}\"\n",
    "            if not exists(\"./model\", model_name, 'model'):\n",
    "                DATT_seq2seq_GRU.train()\n",
    "                DATT_seq2seq_GRU.save_model(f\"./model/{model_name}\")\n",
    "            else:\n",
    "                DATT_seq2seq_GRU.model = loadfile(\"./model\", model_name, 'model')\n",
    "            # test\n",
    "            test_result = DATT_seq2seq_GRU.test()\n",
    "            if not exists('./result', model_name):\n",
    "                savefile(test_result, './result', model_name)\n",
    "            print(\"end training\")\n",
    "            print(\"\")\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3cf56d631085086b1721b0064da1454dfad7e026414ec6e1cab7db73e55aa6df"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
