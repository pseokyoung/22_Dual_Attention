{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"..\") \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.utility import loadfile, savefile, exists\n",
    "from src.dataprocessing import *\n",
    "from src import rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "input_var   = [\"FT-3061-2\", \"FT-3061-3\", \"FT-3061-4\", \"FT-3062-1\"]\n",
    "output_var  = [\"TT-3061-3\", \"TT-3061-5\", \"LT-3061-2\"]\n",
    "process_var = input_var + output_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = 100\n",
    "continuous_path = './data/3_continuous'\n",
    "\n",
    "cts_list = []\n",
    "i = 1\n",
    "while exists(f\"{continuous_path}/cts_{min_len}/dataset {min_len}_{i}.csv\"):\n",
    "    cts_df = loadfile(continuous_path, f\"cts_{min_len}/dataset {min_len}_{i}\", 'csv')\n",
    "    cts_list.append(cts_df)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5th iteration\n",
      "history size: 10\n",
      "future size: 10\n",
      "model is loaded from: ./model/10_10_1_50_1_50_datt_seq2seq_gru_5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.95191</td>\n",
       "      <td>0.940408</td>\n",
       "      <td>0.968614</td>\n",
       "      <td>0.953644</td>\n",
       "      <td>3.560584</td>\n",
       "      <td>4.268898</td>\n",
       "      <td>2.762227</td>\n",
       "      <td>3.53057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.890604</td>\n",
       "      <td>0.893705</td>\n",
       "      <td>0.927472</td>\n",
       "      <td>0.903927</td>\n",
       "      <td>5.335878</td>\n",
       "      <td>5.702014</td>\n",
       "      <td>4.197904</td>\n",
       "      <td>5.078599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.803316</td>\n",
       "      <td>0.846414</td>\n",
       "      <td>0.888765</td>\n",
       "      <td>0.846165</td>\n",
       "      <td>7.057305</td>\n",
       "      <td>6.854641</td>\n",
       "      <td>5.197486</td>\n",
       "      <td>6.36981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.705844</td>\n",
       "      <td>0.807029</td>\n",
       "      <td>0.856355</td>\n",
       "      <td>0.789743</td>\n",
       "      <td>8.516202</td>\n",
       "      <td>7.684496</td>\n",
       "      <td>5.904804</td>\n",
       "      <td>7.3685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.606356</td>\n",
       "      <td>0.781837</td>\n",
       "      <td>0.834288</td>\n",
       "      <td>0.740827</td>\n",
       "      <td>9.755366</td>\n",
       "      <td>8.171844</td>\n",
       "      <td>6.340142</td>\n",
       "      <td>8.089117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.508723</td>\n",
       "      <td>0.762108</td>\n",
       "      <td>0.816329</td>\n",
       "      <td>0.69572</td>\n",
       "      <td>10.793311</td>\n",
       "      <td>8.534796</td>\n",
       "      <td>6.673193</td>\n",
       "      <td>8.6671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.416878</td>\n",
       "      <td>0.743919</td>\n",
       "      <td>0.802251</td>\n",
       "      <td>0.654349</td>\n",
       "      <td>11.6476</td>\n",
       "      <td>8.855826</td>\n",
       "      <td>6.922954</td>\n",
       "      <td>9.142127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.33084</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.793323</td>\n",
       "      <td>0.61673</td>\n",
       "      <td>12.447698</td>\n",
       "      <td>9.160305</td>\n",
       "      <td>7.076812</td>\n",
       "      <td>9.561605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.255451</td>\n",
       "      <td>0.709155</td>\n",
       "      <td>0.786049</td>\n",
       "      <td>0.583552</td>\n",
       "      <td>13.145387</td>\n",
       "      <td>9.438589</td>\n",
       "      <td>7.199726</td>\n",
       "      <td>9.927901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.188778</td>\n",
       "      <td>0.692563</td>\n",
       "      <td>0.774097</td>\n",
       "      <td>0.551813</td>\n",
       "      <td>13.737448</td>\n",
       "      <td>9.704386</td>\n",
       "      <td>7.397546</td>\n",
       "      <td>10.279793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.56587</td>\n",
       "      <td>0.790316</td>\n",
       "      <td>0.844754</td>\n",
       "      <td>0.733647</td>\n",
       "      <td>9.599678</td>\n",
       "      <td>7.837579</td>\n",
       "      <td>5.967279</td>\n",
       "      <td>7.801512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3 TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE     nRMSE     nRMSE   \n",
       "0       0.95191  0.940408  0.968614  0.953644   3.560584  4.268898  2.762227   \n",
       "1      0.890604  0.893705  0.927472  0.903927   5.335878  5.702014  4.197904   \n",
       "2      0.803316  0.846414  0.888765  0.846165   7.057305  6.854641  5.197486   \n",
       "3      0.705844  0.807029  0.856355  0.789743   8.516202  7.684496  5.904804   \n",
       "4      0.606356  0.781837  0.834288  0.740827   9.755366  8.171844  6.340142   \n",
       "5      0.508723  0.762108  0.816329   0.69572  10.793311  8.534796  6.673193   \n",
       "6      0.416878  0.743919  0.802251  0.654349    11.6476  8.855826  6.922954   \n",
       "7       0.33084  0.726027  0.793323   0.61673  12.447698  9.160305  7.076812   \n",
       "8      0.255451  0.709155  0.786049  0.583552  13.145387  9.438589  7.199726   \n",
       "9      0.188778  0.692563  0.774097  0.551813  13.737448  9.704386  7.397546   \n",
       "mean    0.56587  0.790316  0.844754  0.733647   9.599678  7.837579  5.967279   \n",
       "\n",
       "            mean  \n",
       "index      nRMSE  \n",
       "0        3.53057  \n",
       "1       5.078599  \n",
       "2        6.36981  \n",
       "3         7.3685  \n",
       "4       8.089117  \n",
       "5         8.6671  \n",
       "6       9.142127  \n",
       "7       9.561605  \n",
       "8       9.927901  \n",
       "9      10.279793  \n",
       "mean    7.801512  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "5th iteration\n",
      "history size: 10\n",
      "future size: 20\n",
      "model is loaded from: ./model/10_20_1_50_1_50_datt_seq2seq_gru_5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.907338</td>\n",
       "      <td>0.9236</td>\n",
       "      <td>0.942273</td>\n",
       "      <td>0.924404</td>\n",
       "      <td>6.0125</td>\n",
       "      <td>4.829025</td>\n",
       "      <td>3.738263</td>\n",
       "      <td>4.859929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.831534</td>\n",
       "      <td>0.876888</td>\n",
       "      <td>0.906706</td>\n",
       "      <td>0.871709</td>\n",
       "      <td>7.959419</td>\n",
       "      <td>6.131919</td>\n",
       "      <td>4.75183</td>\n",
       "      <td>6.281056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.749568</td>\n",
       "      <td>0.83968</td>\n",
       "      <td>0.867606</td>\n",
       "      <td>0.818951</td>\n",
       "      <td>9.280731</td>\n",
       "      <td>7.000062</td>\n",
       "      <td>5.659884</td>\n",
       "      <td>7.313559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.664425</td>\n",
       "      <td>0.810446</td>\n",
       "      <td>0.822773</td>\n",
       "      <td>0.765882</td>\n",
       "      <td>10.384433</td>\n",
       "      <td>7.614201</td>\n",
       "      <td>6.546914</td>\n",
       "      <td>8.18185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.576804</td>\n",
       "      <td>0.792378</td>\n",
       "      <td>0.778699</td>\n",
       "      <td>0.71596</td>\n",
       "      <td>11.475779</td>\n",
       "      <td>7.972397</td>\n",
       "      <td>7.313586</td>\n",
       "      <td>8.920587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.489491</td>\n",
       "      <td>0.776837</td>\n",
       "      <td>0.750726</td>\n",
       "      <td>0.672352</td>\n",
       "      <td>12.562948</td>\n",
       "      <td>8.269437</td>\n",
       "      <td>7.759246</td>\n",
       "      <td>9.530544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.411084</td>\n",
       "      <td>0.760598</td>\n",
       "      <td>0.734248</td>\n",
       "      <td>0.63531</td>\n",
       "      <td>13.229992</td>\n",
       "      <td>8.567639</td>\n",
       "      <td>8.009402</td>\n",
       "      <td>9.935678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.344047</td>\n",
       "      <td>0.741391</td>\n",
       "      <td>0.717437</td>\n",
       "      <td>0.600958</td>\n",
       "      <td>13.69563</td>\n",
       "      <td>8.907545</td>\n",
       "      <td>8.256984</td>\n",
       "      <td>10.286719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.286228</td>\n",
       "      <td>0.719188</td>\n",
       "      <td>0.696239</td>\n",
       "      <td>0.567218</td>\n",
       "      <td>14.074536</td>\n",
       "      <td>9.285111</td>\n",
       "      <td>8.559255</td>\n",
       "      <td>10.639634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.234349</td>\n",
       "      <td>0.696442</td>\n",
       "      <td>0.671426</td>\n",
       "      <td>0.534072</td>\n",
       "      <td>14.365142</td>\n",
       "      <td>9.655697</td>\n",
       "      <td>8.899492</td>\n",
       "      <td>10.973444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.186807</td>\n",
       "      <td>0.67481</td>\n",
       "      <td>0.643182</td>\n",
       "      <td>0.5016</td>\n",
       "      <td>14.650557</td>\n",
       "      <td>9.995468</td>\n",
       "      <td>9.272013</td>\n",
       "      <td>11.306013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.142893</td>\n",
       "      <td>0.654998</td>\n",
       "      <td>0.614376</td>\n",
       "      <td>0.470755</td>\n",
       "      <td>14.945356</td>\n",
       "      <td>10.296732</td>\n",
       "      <td>9.637258</td>\n",
       "      <td>11.626449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.101197</td>\n",
       "      <td>0.634273</td>\n",
       "      <td>0.591497</td>\n",
       "      <td>0.442322</td>\n",
       "      <td>15.096475</td>\n",
       "      <td>10.602583</td>\n",
       "      <td>9.916969</td>\n",
       "      <td>11.872009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.062245</td>\n",
       "      <td>0.615127</td>\n",
       "      <td>0.576259</td>\n",
       "      <td>0.417877</td>\n",
       "      <td>15.215075</td>\n",
       "      <td>10.878239</td>\n",
       "      <td>10.098058</td>\n",
       "      <td>12.063791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.023776</td>\n",
       "      <td>0.597048</td>\n",
       "      <td>0.56453</td>\n",
       "      <td>0.395118</td>\n",
       "      <td>15.369844</td>\n",
       "      <td>11.132355</td>\n",
       "      <td>10.2344</td>\n",
       "      <td>12.245533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.012924</td>\n",
       "      <td>0.579358</td>\n",
       "      <td>0.553092</td>\n",
       "      <td>0.373175</td>\n",
       "      <td>15.501717</td>\n",
       "      <td>11.37578</td>\n",
       "      <td>10.366</td>\n",
       "      <td>12.414499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.047204</td>\n",
       "      <td>0.560381</td>\n",
       "      <td>0.54014</td>\n",
       "      <td>0.351105</td>\n",
       "      <td>15.6091</td>\n",
       "      <td>11.630158</td>\n",
       "      <td>10.513954</td>\n",
       "      <td>12.584404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.077432</td>\n",
       "      <td>0.540719</td>\n",
       "      <td>0.525891</td>\n",
       "      <td>0.329726</td>\n",
       "      <td>15.792841</td>\n",
       "      <td>11.887635</td>\n",
       "      <td>10.675115</td>\n",
       "      <td>12.785197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.102925</td>\n",
       "      <td>0.520325</td>\n",
       "      <td>0.51133</td>\n",
       "      <td>0.309577</td>\n",
       "      <td>15.994415</td>\n",
       "      <td>12.149154</td>\n",
       "      <td>10.837388</td>\n",
       "      <td>12.993653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.125608</td>\n",
       "      <td>0.500263</td>\n",
       "      <td>0.497914</td>\n",
       "      <td>0.290856</td>\n",
       "      <td>16.173838</td>\n",
       "      <td>12.400815</td>\n",
       "      <td>10.98487</td>\n",
       "      <td>13.186507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.282285</td>\n",
       "      <td>0.690738</td>\n",
       "      <td>0.675317</td>\n",
       "      <td>0.549446</td>\n",
       "      <td>13.369516</td>\n",
       "      <td>9.529098</td>\n",
       "      <td>8.601544</td>\n",
       "      <td>10.500053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.907338    0.9236  0.942273  0.924404     6.0125   4.829025   \n",
       "1      0.831534  0.876888  0.906706  0.871709   7.959419   6.131919   \n",
       "2      0.749568   0.83968  0.867606  0.818951   9.280731   7.000062   \n",
       "3      0.664425  0.810446  0.822773  0.765882  10.384433   7.614201   \n",
       "4      0.576804  0.792378  0.778699   0.71596  11.475779   7.972397   \n",
       "5      0.489491  0.776837  0.750726  0.672352  12.562948   8.269437   \n",
       "6      0.411084  0.760598  0.734248   0.63531  13.229992   8.567639   \n",
       "7      0.344047  0.741391  0.717437  0.600958   13.69563   8.907545   \n",
       "8      0.286228  0.719188  0.696239  0.567218  14.074536   9.285111   \n",
       "9      0.234349  0.696442  0.671426  0.534072  14.365142   9.655697   \n",
       "10     0.186807   0.67481  0.643182    0.5016  14.650557   9.995468   \n",
       "11     0.142893  0.654998  0.614376  0.470755  14.945356  10.296732   \n",
       "12     0.101197  0.634273  0.591497  0.442322  15.096475  10.602583   \n",
       "13     0.062245  0.615127  0.576259  0.417877  15.215075  10.878239   \n",
       "14     0.023776  0.597048   0.56453  0.395118  15.369844  11.132355   \n",
       "15    -0.012924  0.579358  0.553092  0.373175  15.501717   11.37578   \n",
       "16    -0.047204  0.560381   0.54014  0.351105    15.6091  11.630158   \n",
       "17    -0.077432  0.540719  0.525891  0.329726  15.792841  11.887635   \n",
       "18    -0.102925  0.520325   0.51133  0.309577  15.994415  12.149154   \n",
       "19    -0.125608  0.500263  0.497914  0.290856  16.173838  12.400815   \n",
       "mean   0.282285  0.690738  0.675317  0.549446  13.369516   9.529098   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       3.738263   4.859929  \n",
       "1        4.75183   6.281056  \n",
       "2       5.659884   7.313559  \n",
       "3       6.546914    8.18185  \n",
       "4       7.313586   8.920587  \n",
       "5       7.759246   9.530544  \n",
       "6       8.009402   9.935678  \n",
       "7       8.256984  10.286719  \n",
       "8       8.559255  10.639634  \n",
       "9       8.899492  10.973444  \n",
       "10      9.272013  11.306013  \n",
       "11      9.637258  11.626449  \n",
       "12      9.916969  11.872009  \n",
       "13     10.098058  12.063791  \n",
       "14       10.2344  12.245533  \n",
       "15        10.366  12.414499  \n",
       "16     10.513954  12.584404  \n",
       "17     10.675115  12.785197  \n",
       "18     10.837388  12.993653  \n",
       "19      10.98487  13.186507  \n",
       "mean    8.601544  10.500053  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "5th iteration\n",
      "history size: 10\n",
      "future size: 30\n",
      "model is loaded from: ./model/10_30_1_50_1_50_datt_seq2seq_gru_5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.84975</td>\n",
       "      <td>0.917727</td>\n",
       "      <td>0.937251</td>\n",
       "      <td>0.901576</td>\n",
       "      <td>7.652249</td>\n",
       "      <td>5.010225</td>\n",
       "      <td>3.882742</td>\n",
       "      <td>5.515072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.752126</td>\n",
       "      <td>0.864517</td>\n",
       "      <td>0.912745</td>\n",
       "      <td>0.843129</td>\n",
       "      <td>9.828845</td>\n",
       "      <td>6.429728</td>\n",
       "      <td>4.578643</td>\n",
       "      <td>6.945739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.651748</td>\n",
       "      <td>0.82163</td>\n",
       "      <td>0.876898</td>\n",
       "      <td>0.783425</td>\n",
       "      <td>11.651974</td>\n",
       "      <td>7.378067</td>\n",
       "      <td>5.437931</td>\n",
       "      <td>8.155991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.552547</td>\n",
       "      <td>0.790024</td>\n",
       "      <td>0.845176</td>\n",
       "      <td>0.729249</td>\n",
       "      <td>13.2102</td>\n",
       "      <td>8.006101</td>\n",
       "      <td>6.096973</td>\n",
       "      <td>9.104425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.457431</td>\n",
       "      <td>0.766567</td>\n",
       "      <td>0.822817</td>\n",
       "      <td>0.682272</td>\n",
       "      <td>14.54851</td>\n",
       "      <td>8.443009</td>\n",
       "      <td>6.520342</td>\n",
       "      <td>9.837287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.368325</td>\n",
       "      <td>0.747108</td>\n",
       "      <td>0.804054</td>\n",
       "      <td>0.639829</td>\n",
       "      <td>15.699134</td>\n",
       "      <td>8.790341</td>\n",
       "      <td>6.854779</td>\n",
       "      <td>10.448085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.287541</td>\n",
       "      <td>0.727505</td>\n",
       "      <td>0.783607</td>\n",
       "      <td>0.599551</td>\n",
       "      <td>16.674204</td>\n",
       "      <td>9.127373</td>\n",
       "      <td>7.201907</td>\n",
       "      <td>11.001161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.214052</td>\n",
       "      <td>0.705857</td>\n",
       "      <td>0.759598</td>\n",
       "      <td>0.559836</td>\n",
       "      <td>17.51289</td>\n",
       "      <td>9.485865</td>\n",
       "      <td>7.58967</td>\n",
       "      <td>11.529475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.146054</td>\n",
       "      <td>0.682472</td>\n",
       "      <td>0.731642</td>\n",
       "      <td>0.520056</td>\n",
       "      <td>18.253837</td>\n",
       "      <td>9.859687</td>\n",
       "      <td>8.017323</td>\n",
       "      <td>12.043616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.084261</td>\n",
       "      <td>0.657644</td>\n",
       "      <td>0.699429</td>\n",
       "      <td>0.480444</td>\n",
       "      <td>18.903757</td>\n",
       "      <td>10.241406</td>\n",
       "      <td>8.483222</td>\n",
       "      <td>12.542795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.027734</td>\n",
       "      <td>0.633091</td>\n",
       "      <td>0.664522</td>\n",
       "      <td>0.441782</td>\n",
       "      <td>19.482765</td>\n",
       "      <td>10.605129</td>\n",
       "      <td>8.960903</td>\n",
       "      <td>13.016265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.024768</td>\n",
       "      <td>0.610327</td>\n",
       "      <td>0.629277</td>\n",
       "      <td>0.404945</td>\n",
       "      <td>19.639033</td>\n",
       "      <td>10.932667</td>\n",
       "      <td>9.418567</td>\n",
       "      <td>13.330089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.072281</td>\n",
       "      <td>0.586881</td>\n",
       "      <td>0.596732</td>\n",
       "      <td>0.370444</td>\n",
       "      <td>19.212605</td>\n",
       "      <td>11.260968</td>\n",
       "      <td>9.821921</td>\n",
       "      <td>13.431831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.114538</td>\n",
       "      <td>0.563954</td>\n",
       "      <td>0.568259</td>\n",
       "      <td>0.339225</td>\n",
       "      <td>18.93316</td>\n",
       "      <td>11.573309</td>\n",
       "      <td>10.161374</td>\n",
       "      <td>13.555948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.153779</td>\n",
       "      <td>0.540829</td>\n",
       "      <td>0.544839</td>\n",
       "      <td>0.31063</td>\n",
       "      <td>18.954404</td>\n",
       "      <td>11.881555</td>\n",
       "      <td>10.432313</td>\n",
       "      <td>13.756091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.190012</td>\n",
       "      <td>0.515685</td>\n",
       "      <td>0.527524</td>\n",
       "      <td>0.284399</td>\n",
       "      <td>19.182751</td>\n",
       "      <td>12.20862</td>\n",
       "      <td>10.627266</td>\n",
       "      <td>14.006213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.222798</td>\n",
       "      <td>0.489193</td>\n",
       "      <td>0.514066</td>\n",
       "      <td>0.260154</td>\n",
       "      <td>19.061866</td>\n",
       "      <td>12.541677</td>\n",
       "      <td>10.776124</td>\n",
       "      <td>14.126556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.253908</td>\n",
       "      <td>0.461617</td>\n",
       "      <td>0.502736</td>\n",
       "      <td>0.236815</td>\n",
       "      <td>18.930475</td>\n",
       "      <td>12.879841</td>\n",
       "      <td>10.898804</td>\n",
       "      <td>14.236373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.284004</td>\n",
       "      <td>0.434133</td>\n",
       "      <td>0.493831</td>\n",
       "      <td>0.214653</td>\n",
       "      <td>18.8685</td>\n",
       "      <td>13.20889</td>\n",
       "      <td>10.993202</td>\n",
       "      <td>14.356864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.312857</td>\n",
       "      <td>0.406135</td>\n",
       "      <td>0.486779</td>\n",
       "      <td>0.193352</td>\n",
       "      <td>18.797665</td>\n",
       "      <td>13.534306</td>\n",
       "      <td>11.06603</td>\n",
       "      <td>14.466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.342364</td>\n",
       "      <td>0.377924</td>\n",
       "      <td>0.479704</td>\n",
       "      <td>0.171755</td>\n",
       "      <td>18.804342</td>\n",
       "      <td>13.854396</td>\n",
       "      <td>11.139017</td>\n",
       "      <td>14.599251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.373558</td>\n",
       "      <td>0.351337</td>\n",
       "      <td>0.47215</td>\n",
       "      <td>0.149976</td>\n",
       "      <td>18.895306</td>\n",
       "      <td>14.149074</td>\n",
       "      <td>11.217666</td>\n",
       "      <td>14.754015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.406</td>\n",
       "      <td>0.328484</td>\n",
       "      <td>0.463248</td>\n",
       "      <td>0.128577</td>\n",
       "      <td>18.852031</td>\n",
       "      <td>14.397502</td>\n",
       "      <td>11.310991</td>\n",
       "      <td>14.853508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.439667</td>\n",
       "      <td>0.307779</td>\n",
       "      <td>0.453449</td>\n",
       "      <td>0.107187</td>\n",
       "      <td>18.819378</td>\n",
       "      <td>14.619683</td>\n",
       "      <td>11.413875</td>\n",
       "      <td>14.950979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.472933</td>\n",
       "      <td>0.289178</td>\n",
       "      <td>0.442545</td>\n",
       "      <td>0.086264</td>\n",
       "      <td>18.846625</td>\n",
       "      <td>14.816761</td>\n",
       "      <td>11.526845</td>\n",
       "      <td>15.06341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.506007</td>\n",
       "      <td>0.270718</td>\n",
       "      <td>0.430797</td>\n",
       "      <td>0.065169</td>\n",
       "      <td>18.872642</td>\n",
       "      <td>15.010211</td>\n",
       "      <td>11.647769</td>\n",
       "      <td>15.176874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.53501</td>\n",
       "      <td>0.253544</td>\n",
       "      <td>0.418493</td>\n",
       "      <td>0.045675</td>\n",
       "      <td>18.873382</td>\n",
       "      <td>15.187006</td>\n",
       "      <td>11.773547</td>\n",
       "      <td>15.277978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.562401</td>\n",
       "      <td>0.239412</td>\n",
       "      <td>0.405058</td>\n",
       "      <td>0.027356</td>\n",
       "      <td>18.999159</td>\n",
       "      <td>15.330498</td>\n",
       "      <td>11.909824</td>\n",
       "      <td>15.41316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.586287</td>\n",
       "      <td>0.227045</td>\n",
       "      <td>0.390916</td>\n",
       "      <td>0.010558</td>\n",
       "      <td>19.170715</td>\n",
       "      <td>15.455059</td>\n",
       "      <td>12.051359</td>\n",
       "      <td>15.559044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.606599</td>\n",
       "      <td>0.214775</td>\n",
       "      <td>0.375424</td>\n",
       "      <td>-0.005467</td>\n",
       "      <td>19.320801</td>\n",
       "      <td>15.577344</td>\n",
       "      <td>12.205048</td>\n",
       "      <td>15.701064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.06894</td>\n",
       "      <td>0.526103</td>\n",
       "      <td>0.601119</td>\n",
       "      <td>0.352761</td>\n",
       "      <td>17.481773</td>\n",
       "      <td>11.726543</td>\n",
       "      <td>9.467199</td>\n",
       "      <td>12.891839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0       0.84975  0.917727  0.937251  0.901576   7.652249   5.010225   \n",
       "1      0.752126  0.864517  0.912745  0.843129   9.828845   6.429728   \n",
       "2      0.651748   0.82163  0.876898  0.783425  11.651974   7.378067   \n",
       "3      0.552547  0.790024  0.845176  0.729249    13.2102   8.006101   \n",
       "4      0.457431  0.766567  0.822817  0.682272   14.54851   8.443009   \n",
       "5      0.368325  0.747108  0.804054  0.639829  15.699134   8.790341   \n",
       "6      0.287541  0.727505  0.783607  0.599551  16.674204   9.127373   \n",
       "7      0.214052  0.705857  0.759598  0.559836   17.51289   9.485865   \n",
       "8      0.146054  0.682472  0.731642  0.520056  18.253837   9.859687   \n",
       "9      0.084261  0.657644  0.699429  0.480444  18.903757  10.241406   \n",
       "10     0.027734  0.633091  0.664522  0.441782  19.482765  10.605129   \n",
       "11    -0.024768  0.610327  0.629277  0.404945  19.639033  10.932667   \n",
       "12    -0.072281  0.586881  0.596732  0.370444  19.212605  11.260968   \n",
       "13    -0.114538  0.563954  0.568259  0.339225   18.93316  11.573309   \n",
       "14    -0.153779  0.540829  0.544839   0.31063  18.954404  11.881555   \n",
       "15    -0.190012  0.515685  0.527524  0.284399  19.182751   12.20862   \n",
       "16    -0.222798  0.489193  0.514066  0.260154  19.061866  12.541677   \n",
       "17    -0.253908  0.461617  0.502736  0.236815  18.930475  12.879841   \n",
       "18    -0.284004  0.434133  0.493831  0.214653    18.8685   13.20889   \n",
       "19    -0.312857  0.406135  0.486779  0.193352  18.797665  13.534306   \n",
       "20    -0.342364  0.377924  0.479704  0.171755  18.804342  13.854396   \n",
       "21    -0.373558  0.351337   0.47215  0.149976  18.895306  14.149074   \n",
       "22       -0.406  0.328484  0.463248  0.128577  18.852031  14.397502   \n",
       "23    -0.439667  0.307779  0.453449  0.107187  18.819378  14.619683   \n",
       "24    -0.472933  0.289178  0.442545  0.086264  18.846625  14.816761   \n",
       "25    -0.506007  0.270718  0.430797  0.065169  18.872642  15.010211   \n",
       "26     -0.53501  0.253544  0.418493  0.045675  18.873382  15.187006   \n",
       "27    -0.562401  0.239412  0.405058  0.027356  18.999159  15.330498   \n",
       "28    -0.586287  0.227045  0.390916  0.010558  19.170715  15.455059   \n",
       "29    -0.606599  0.214775  0.375424 -0.005467  19.320801  15.577344   \n",
       "mean   -0.06894  0.526103  0.601119  0.352761  17.481773  11.726543   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       3.882742   5.515072  \n",
       "1       4.578643   6.945739  \n",
       "2       5.437931   8.155991  \n",
       "3       6.096973   9.104425  \n",
       "4       6.520342   9.837287  \n",
       "5       6.854779  10.448085  \n",
       "6       7.201907  11.001161  \n",
       "7        7.58967  11.529475  \n",
       "8       8.017323  12.043616  \n",
       "9       8.483222  12.542795  \n",
       "10      8.960903  13.016265  \n",
       "11      9.418567  13.330089  \n",
       "12      9.821921  13.431831  \n",
       "13     10.161374  13.555948  \n",
       "14     10.432313  13.756091  \n",
       "15     10.627266  14.006213  \n",
       "16     10.776124  14.126556  \n",
       "17     10.898804  14.236373  \n",
       "18     10.993202  14.356864  \n",
       "19      11.06603     14.466  \n",
       "20     11.139017  14.599251  \n",
       "21     11.217666  14.754015  \n",
       "22     11.310991  14.853508  \n",
       "23     11.413875  14.950979  \n",
       "24     11.526845   15.06341  \n",
       "25     11.647769  15.176874  \n",
       "26     11.773547  15.277978  \n",
       "27     11.909824   15.41316  \n",
       "28     12.051359  15.559044  \n",
       "29     12.205048  15.701064  \n",
       "mean    9.467199  12.891839  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "5th iteration\n",
      "history size: 20\n",
      "future size: 10\n",
      "model is loaded from: ./model/20_10_1_50_1_50_datt_seq2seq_gru_5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.934875</td>\n",
       "      <td>0.922298</td>\n",
       "      <td>0.959545</td>\n",
       "      <td>0.938906</td>\n",
       "      <td>4.146027</td>\n",
       "      <td>4.88596</td>\n",
       "      <td>3.122014</td>\n",
       "      <td>4.051334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.858115</td>\n",
       "      <td>0.854094</td>\n",
       "      <td>0.923919</td>\n",
       "      <td>0.87871</td>\n",
       "      <td>6.08074</td>\n",
       "      <td>6.696139</td>\n",
       "      <td>4.280635</td>\n",
       "      <td>5.685838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.760971</td>\n",
       "      <td>0.796628</td>\n",
       "      <td>0.881449</td>\n",
       "      <td>0.813016</td>\n",
       "      <td>7.785183</td>\n",
       "      <td>7.906405</td>\n",
       "      <td>5.34238</td>\n",
       "      <td>7.011323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.657556</td>\n",
       "      <td>0.751325</td>\n",
       "      <td>0.84558</td>\n",
       "      <td>0.751487</td>\n",
       "      <td>9.194411</td>\n",
       "      <td>8.744115</td>\n",
       "      <td>6.095926</td>\n",
       "      <td>8.011484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.555349</td>\n",
       "      <td>0.716578</td>\n",
       "      <td>0.8226</td>\n",
       "      <td>0.698175</td>\n",
       "      <td>10.373001</td>\n",
       "      <td>9.336355</td>\n",
       "      <td>6.532213</td>\n",
       "      <td>8.74719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.457716</td>\n",
       "      <td>0.687096</td>\n",
       "      <td>0.809709</td>\n",
       "      <td>0.651507</td>\n",
       "      <td>11.342399</td>\n",
       "      <td>9.811387</td>\n",
       "      <td>6.764113</td>\n",
       "      <td>9.305966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.369031</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.800648</td>\n",
       "      <td>0.608798</td>\n",
       "      <td>12.11619</td>\n",
       "      <td>10.277176</td>\n",
       "      <td>6.922516</td>\n",
       "      <td>9.771961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.292051</td>\n",
       "      <td>0.623629</td>\n",
       "      <td>0.790533</td>\n",
       "      <td>0.568738</td>\n",
       "      <td>12.801654</td>\n",
       "      <td>10.761297</td>\n",
       "      <td>7.095629</td>\n",
       "      <td>10.219527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.227292</td>\n",
       "      <td>0.583032</td>\n",
       "      <td>0.778671</td>\n",
       "      <td>0.529665</td>\n",
       "      <td>13.387609</td>\n",
       "      <td>11.327233</td>\n",
       "      <td>7.293505</td>\n",
       "      <td>10.669449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.172803</td>\n",
       "      <td>0.536003</td>\n",
       "      <td>0.761968</td>\n",
       "      <td>0.490258</td>\n",
       "      <td>13.865131</td>\n",
       "      <td>11.949151</td>\n",
       "      <td>7.563519</td>\n",
       "      <td>11.125934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.528576</td>\n",
       "      <td>0.71274</td>\n",
       "      <td>0.837462</td>\n",
       "      <td>0.692926</td>\n",
       "      <td>10.109235</td>\n",
       "      <td>9.169522</td>\n",
       "      <td>6.101245</td>\n",
       "      <td>8.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE     nRMSE   \n",
       "0      0.934875  0.922298  0.959545  0.938906   4.146027    4.88596  3.122014   \n",
       "1      0.858115  0.854094  0.923919   0.87871    6.08074   6.696139  4.280635   \n",
       "2      0.760971  0.796628  0.881449  0.813016   7.785183   7.906405   5.34238   \n",
       "3      0.657556  0.751325   0.84558  0.751487   9.194411   8.744115  6.095926   \n",
       "4      0.555349  0.716578    0.8226  0.698175  10.373001   9.336355  6.532213   \n",
       "5      0.457716  0.687096  0.809709  0.651507  11.342399   9.811387  6.764113   \n",
       "6      0.369031  0.656716  0.800648  0.608798   12.11619  10.277176  6.922516   \n",
       "7      0.292051  0.623629  0.790533  0.568738  12.801654  10.761297  7.095629   \n",
       "8      0.227292  0.583032  0.778671  0.529665  13.387609  11.327233  7.293505   \n",
       "9      0.172803  0.536003  0.761968  0.490258  13.865131  11.949151  7.563519   \n",
       "mean   0.528576   0.71274  0.837462  0.692926  10.109235   9.169522  6.101245   \n",
       "\n",
       "            mean  \n",
       "index      nRMSE  \n",
       "0       4.051334  \n",
       "1       5.685838  \n",
       "2       7.011323  \n",
       "3       8.011484  \n",
       "4        8.74719  \n",
       "5       9.305966  \n",
       "6       9.771961  \n",
       "7      10.219527  \n",
       "8      10.669449  \n",
       "9      11.125934  \n",
       "mean        8.46  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "5th iteration\n",
      "history size: 20\n",
      "future size: 20\n",
      "model is loaded from: ./model/20_20_1_50_1_50_datt_seq2seq_gru_5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.869674</td>\n",
       "      <td>0.914837</td>\n",
       "      <td>0.942174</td>\n",
       "      <td>0.908895</td>\n",
       "      <td>7.13302</td>\n",
       "      <td>5.1093</td>\n",
       "      <td>3.720337</td>\n",
       "      <td>5.320886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.786037</td>\n",
       "      <td>0.858779</td>\n",
       "      <td>0.914698</td>\n",
       "      <td>0.853171</td>\n",
       "      <td>8.973807</td>\n",
       "      <td>6.581517</td>\n",
       "      <td>4.517935</td>\n",
       "      <td>6.691086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.667313</td>\n",
       "      <td>0.813727</td>\n",
       "      <td>0.880548</td>\n",
       "      <td>0.787196</td>\n",
       "      <td>10.701628</td>\n",
       "      <td>7.561594</td>\n",
       "      <td>5.345601</td>\n",
       "      <td>7.869608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.540349</td>\n",
       "      <td>0.781251</td>\n",
       "      <td>0.853915</td>\n",
       "      <td>0.725172</td>\n",
       "      <td>12.158773</td>\n",
       "      <td>8.197182</td>\n",
       "      <td>5.910765</td>\n",
       "      <td>8.755573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.411033</td>\n",
       "      <td>0.754827</td>\n",
       "      <td>0.832762</td>\n",
       "      <td>0.666207</td>\n",
       "      <td>13.542357</td>\n",
       "      <td>8.682057</td>\n",
       "      <td>6.323614</td>\n",
       "      <td>9.516009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.283235</td>\n",
       "      <td>0.731871</td>\n",
       "      <td>0.814758</td>\n",
       "      <td>0.609955</td>\n",
       "      <td>14.887562</td>\n",
       "      <td>9.083947</td>\n",
       "      <td>6.654279</td>\n",
       "      <td>10.208596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.165818</td>\n",
       "      <td>0.712147</td>\n",
       "      <td>0.800544</td>\n",
       "      <td>0.559503</td>\n",
       "      <td>15.744124</td>\n",
       "      <td>9.414834</td>\n",
       "      <td>6.903943</td>\n",
       "      <td>10.687633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.06226</td>\n",
       "      <td>0.694819</td>\n",
       "      <td>0.788961</td>\n",
       "      <td>0.515347</td>\n",
       "      <td>16.3708</td>\n",
       "      <td>9.697142</td>\n",
       "      <td>7.100141</td>\n",
       "      <td>11.056028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.024628</td>\n",
       "      <td>0.678745</td>\n",
       "      <td>0.772287</td>\n",
       "      <td>0.475468</td>\n",
       "      <td>16.855336</td>\n",
       "      <td>9.952549</td>\n",
       "      <td>7.373442</td>\n",
       "      <td>11.393776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.099373</td>\n",
       "      <td>0.66193</td>\n",
       "      <td>0.743907</td>\n",
       "      <td>0.435488</td>\n",
       "      <td>17.201555</td>\n",
       "      <td>10.211643</td>\n",
       "      <td>7.816974</td>\n",
       "      <td>11.743391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.162866</td>\n",
       "      <td>0.643558</td>\n",
       "      <td>0.703471</td>\n",
       "      <td>0.394721</td>\n",
       "      <td>17.502007</td>\n",
       "      <td>10.487211</td>\n",
       "      <td>8.409214</td>\n",
       "      <td>12.132811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.217921</td>\n",
       "      <td>0.625003</td>\n",
       "      <td>0.657124</td>\n",
       "      <td>0.354735</td>\n",
       "      <td>17.792621</td>\n",
       "      <td>10.758013</td>\n",
       "      <td>9.040985</td>\n",
       "      <td>12.53054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.266928</td>\n",
       "      <td>0.608296</td>\n",
       "      <td>0.613871</td>\n",
       "      <td>0.318413</td>\n",
       "      <td>17.895401</td>\n",
       "      <td>10.99608</td>\n",
       "      <td>9.593565</td>\n",
       "      <td>12.828349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.308203</td>\n",
       "      <td>0.593975</td>\n",
       "      <td>0.578948</td>\n",
       "      <td>0.28824</td>\n",
       "      <td>17.939564</td>\n",
       "      <td>11.19676</td>\n",
       "      <td>10.018103</td>\n",
       "      <td>13.051476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.344103</td>\n",
       "      <td>0.580219</td>\n",
       "      <td>0.554428</td>\n",
       "      <td>0.263515</td>\n",
       "      <td>18.003559</td>\n",
       "      <td>11.386351</td>\n",
       "      <td>10.30539</td>\n",
       "      <td>13.231767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.37136</td>\n",
       "      <td>0.565163</td>\n",
       "      <td>0.538752</td>\n",
       "      <td>0.244185</td>\n",
       "      <td>18.00922</td>\n",
       "      <td>11.590503</td>\n",
       "      <td>10.485189</td>\n",
       "      <td>13.361637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.390911</td>\n",
       "      <td>0.548061</td>\n",
       "      <td>0.526607</td>\n",
       "      <td>0.227919</td>\n",
       "      <td>17.965684</td>\n",
       "      <td>11.817074</td>\n",
       "      <td>10.622841</td>\n",
       "      <td>13.468533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.405752</td>\n",
       "      <td>0.528287</td>\n",
       "      <td>0.512449</td>\n",
       "      <td>0.211661</td>\n",
       "      <td>18.021562</td>\n",
       "      <td>12.073144</td>\n",
       "      <td>10.781476</td>\n",
       "      <td>13.625394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.419526</td>\n",
       "      <td>0.50648</td>\n",
       "      <td>0.495892</td>\n",
       "      <td>0.194282</td>\n",
       "      <td>18.135069</td>\n",
       "      <td>12.349404</td>\n",
       "      <td>10.963751</td>\n",
       "      <td>13.816075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.436012</td>\n",
       "      <td>0.483266</td>\n",
       "      <td>0.47905</td>\n",
       "      <td>0.175435</td>\n",
       "      <td>18.266294</td>\n",
       "      <td>12.636587</td>\n",
       "      <td>11.146661</td>\n",
       "      <td>14.016514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.016907</td>\n",
       "      <td>0.664262</td>\n",
       "      <td>0.700257</td>\n",
       "      <td>0.460475</td>\n",
       "      <td>15.654997</td>\n",
       "      <td>9.989145</td>\n",
       "      <td>8.15171</td>\n",
       "      <td>11.265284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.869674  0.914837  0.942174  0.908895    7.13302     5.1093   \n",
       "1      0.786037  0.858779  0.914698  0.853171   8.973807   6.581517   \n",
       "2      0.667313  0.813727  0.880548  0.787196  10.701628   7.561594   \n",
       "3      0.540349  0.781251  0.853915  0.725172  12.158773   8.197182   \n",
       "4      0.411033  0.754827  0.832762  0.666207  13.542357   8.682057   \n",
       "5      0.283235  0.731871  0.814758  0.609955  14.887562   9.083947   \n",
       "6      0.165818  0.712147  0.800544  0.559503  15.744124   9.414834   \n",
       "7       0.06226  0.694819  0.788961  0.515347    16.3708   9.697142   \n",
       "8     -0.024628  0.678745  0.772287  0.475468  16.855336   9.952549   \n",
       "9     -0.099373   0.66193  0.743907  0.435488  17.201555  10.211643   \n",
       "10    -0.162866  0.643558  0.703471  0.394721  17.502007  10.487211   \n",
       "11    -0.217921  0.625003  0.657124  0.354735  17.792621  10.758013   \n",
       "12    -0.266928  0.608296  0.613871  0.318413  17.895401   10.99608   \n",
       "13    -0.308203  0.593975  0.578948   0.28824  17.939564   11.19676   \n",
       "14    -0.344103  0.580219  0.554428  0.263515  18.003559  11.386351   \n",
       "15     -0.37136  0.565163  0.538752  0.244185   18.00922  11.590503   \n",
       "16    -0.390911  0.548061  0.526607  0.227919  17.965684  11.817074   \n",
       "17    -0.405752  0.528287  0.512449  0.211661  18.021562  12.073144   \n",
       "18    -0.419526   0.50648  0.495892  0.194282  18.135069  12.349404   \n",
       "19    -0.436012  0.483266   0.47905  0.175435  18.266294  12.636587   \n",
       "mean   0.016907  0.664262  0.700257  0.460475  15.654997   9.989145   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       3.720337   5.320886  \n",
       "1       4.517935   6.691086  \n",
       "2       5.345601   7.869608  \n",
       "3       5.910765   8.755573  \n",
       "4       6.323614   9.516009  \n",
       "5       6.654279  10.208596  \n",
       "6       6.903943  10.687633  \n",
       "7       7.100141  11.056028  \n",
       "8       7.373442  11.393776  \n",
       "9       7.816974  11.743391  \n",
       "10      8.409214  12.132811  \n",
       "11      9.040985   12.53054  \n",
       "12      9.593565  12.828349  \n",
       "13     10.018103  13.051476  \n",
       "14      10.30539  13.231767  \n",
       "15     10.485189  13.361637  \n",
       "16     10.622841  13.468533  \n",
       "17     10.781476  13.625394  \n",
       "18     10.963751  13.816075  \n",
       "19     11.146661  14.016514  \n",
       "mean     8.15171  11.265284  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "5th iteration\n",
      "history size: 20\n",
      "future size: 30\n",
      "model is loaded from: ./model/20_30_1_50_1_50_datt_seq2seq_gru_5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.884703</td>\n",
       "      <td>0.892643</td>\n",
       "      <td>0.934689</td>\n",
       "      <td>0.904012</td>\n",
       "      <td>6.704126</td>\n",
       "      <td>5.73522</td>\n",
       "      <td>3.935563</td>\n",
       "      <td>5.458303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.777607</td>\n",
       "      <td>0.839667</td>\n",
       "      <td>0.903084</td>\n",
       "      <td>0.84012</td>\n",
       "      <td>9.311109</td>\n",
       "      <td>7.009273</td>\n",
       "      <td>4.794283</td>\n",
       "      <td>7.038222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.657652</td>\n",
       "      <td>0.806375</td>\n",
       "      <td>0.865895</td>\n",
       "      <td>0.776641</td>\n",
       "      <td>11.553851</td>\n",
       "      <td>7.703399</td>\n",
       "      <td>5.639858</td>\n",
       "      <td>8.299036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.528777</td>\n",
       "      <td>0.774943</td>\n",
       "      <td>0.829694</td>\n",
       "      <td>0.711138</td>\n",
       "      <td>13.55714</td>\n",
       "      <td>8.306188</td>\n",
       "      <td>6.355801</td>\n",
       "      <td>9.406376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.404445</td>\n",
       "      <td>0.74743</td>\n",
       "      <td>0.796293</td>\n",
       "      <td>0.64939</td>\n",
       "      <td>15.240606</td>\n",
       "      <td>8.800932</td>\n",
       "      <td>6.95112</td>\n",
       "      <td>10.330886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.290972</td>\n",
       "      <td>0.724987</td>\n",
       "      <td>0.763723</td>\n",
       "      <td>0.593228</td>\n",
       "      <td>16.626523</td>\n",
       "      <td>9.186337</td>\n",
       "      <td>7.485471</td>\n",
       "      <td>11.099444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.193932</td>\n",
       "      <td>0.704764</td>\n",
       "      <td>0.730494</td>\n",
       "      <td>0.543063</td>\n",
       "      <td>17.724697</td>\n",
       "      <td>9.520858</td>\n",
       "      <td>7.993429</td>\n",
       "      <td>11.746328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.113923</td>\n",
       "      <td>0.684738</td>\n",
       "      <td>0.696696</td>\n",
       "      <td>0.498452</td>\n",
       "      <td>18.579554</td>\n",
       "      <td>9.841437</td>\n",
       "      <td>8.478228</td>\n",
       "      <td>12.29974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.046204</td>\n",
       "      <td>0.664302</td>\n",
       "      <td>0.660696</td>\n",
       "      <td>0.457067</td>\n",
       "      <td>19.270933</td>\n",
       "      <td>10.159451</td>\n",
       "      <td>8.96518</td>\n",
       "      <td>12.798521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.014245</td>\n",
       "      <td>0.643249</td>\n",
       "      <td>0.621479</td>\n",
       "      <td>0.416828</td>\n",
       "      <td>19.867771</td>\n",
       "      <td>10.476707</td>\n",
       "      <td>9.467853</td>\n",
       "      <td>13.270777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.072174</td>\n",
       "      <td>0.62095</td>\n",
       "      <td>0.577706</td>\n",
       "      <td>0.375494</td>\n",
       "      <td>20.424637</td>\n",
       "      <td>10.802127</td>\n",
       "      <td>9.99986</td>\n",
       "      <td>13.742208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.12889</td>\n",
       "      <td>0.596716</td>\n",
       "      <td>0.531564</td>\n",
       "      <td>0.33313</td>\n",
       "      <td>20.571391</td>\n",
       "      <td>11.145479</td>\n",
       "      <td>10.532687</td>\n",
       "      <td>14.083186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.183929</td>\n",
       "      <td>0.570672</td>\n",
       "      <td>0.485802</td>\n",
       "      <td>0.290848</td>\n",
       "      <td>20.14154</td>\n",
       "      <td>11.503948</td>\n",
       "      <td>11.036383</td>\n",
       "      <td>14.22729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.233632</td>\n",
       "      <td>0.544283</td>\n",
       "      <td>0.442132</td>\n",
       "      <td>0.250927</td>\n",
       "      <td>19.869264</td>\n",
       "      <td>11.856199</td>\n",
       "      <td>11.496594</td>\n",
       "      <td>14.407353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.279066</td>\n",
       "      <td>0.517547</td>\n",
       "      <td>0.401201</td>\n",
       "      <td>0.213227</td>\n",
       "      <td>19.90697</td>\n",
       "      <td>12.204407</td>\n",
       "      <td>11.911639</td>\n",
       "      <td>14.674339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.31975</td>\n",
       "      <td>0.492266</td>\n",
       "      <td>0.363046</td>\n",
       "      <td>0.178521</td>\n",
       "      <td>20.15455</td>\n",
       "      <td>12.526128</td>\n",
       "      <td>12.285072</td>\n",
       "      <td>14.988583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.355656</td>\n",
       "      <td>0.468535</td>\n",
       "      <td>0.328386</td>\n",
       "      <td>0.147088</td>\n",
       "      <td>20.029518</td>\n",
       "      <td>12.819252</td>\n",
       "      <td>12.614988</td>\n",
       "      <td>15.154586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.385938</td>\n",
       "      <td>0.445732</td>\n",
       "      <td>0.298408</td>\n",
       "      <td>0.119401</td>\n",
       "      <td>19.868924</td>\n",
       "      <td>13.095365</td>\n",
       "      <td>12.893767</td>\n",
       "      <td>15.286019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.412587</td>\n",
       "      <td>0.424849</td>\n",
       "      <td>0.273857</td>\n",
       "      <td>0.095373</td>\n",
       "      <td>19.766966</td>\n",
       "      <td>13.343829</td>\n",
       "      <td>13.118172</td>\n",
       "      <td>15.409656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.43714</td>\n",
       "      <td>0.405593</td>\n",
       "      <td>0.254949</td>\n",
       "      <td>0.074467</td>\n",
       "      <td>19.653763</td>\n",
       "      <td>13.567733</td>\n",
       "      <td>13.288666</td>\n",
       "      <td>15.503387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.459809</td>\n",
       "      <td>0.387731</td>\n",
       "      <td>0.239743</td>\n",
       "      <td>0.055888</td>\n",
       "      <td>19.607186</td>\n",
       "      <td>13.772249</td>\n",
       "      <td>13.424689</td>\n",
       "      <td>15.601375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.48145</td>\n",
       "      <td>0.371219</td>\n",
       "      <td>0.22686</td>\n",
       "      <td>0.038876</td>\n",
       "      <td>19.630259</td>\n",
       "      <td>13.958178</td>\n",
       "      <td>13.539675</td>\n",
       "      <td>15.709371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.501683</td>\n",
       "      <td>0.355614</td>\n",
       "      <td>0.21368</td>\n",
       "      <td>0.022537</td>\n",
       "      <td>19.496969</td>\n",
       "      <td>14.131523</td>\n",
       "      <td>13.656531</td>\n",
       "      <td>15.761675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.520756</td>\n",
       "      <td>0.34172</td>\n",
       "      <td>0.200715</td>\n",
       "      <td>0.007226</td>\n",
       "      <td>19.360188</td>\n",
       "      <td>14.285099</td>\n",
       "      <td>13.770366</td>\n",
       "      <td>15.805218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.539265</td>\n",
       "      <td>0.328278</td>\n",
       "      <td>0.189183</td>\n",
       "      <td>-0.007268</td>\n",
       "      <td>19.285305</td>\n",
       "      <td>14.432103</td>\n",
       "      <td>13.870282</td>\n",
       "      <td>15.862563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.555604</td>\n",
       "      <td>0.314712</td>\n",
       "      <td>0.180535</td>\n",
       "      <td>-0.020119</td>\n",
       "      <td>19.198111</td>\n",
       "      <td>14.578607</td>\n",
       "      <td>13.945568</td>\n",
       "      <td>15.907429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.570704</td>\n",
       "      <td>0.301421</td>\n",
       "      <td>0.172799</td>\n",
       "      <td>-0.032161</td>\n",
       "      <td>19.105298</td>\n",
       "      <td>14.719139</td>\n",
       "      <td>14.013711</td>\n",
       "      <td>15.946049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.584131</td>\n",
       "      <td>0.288126</td>\n",
       "      <td>0.161847</td>\n",
       "      <td>-0.044719</td>\n",
       "      <td>19.140321</td>\n",
       "      <td>14.85743</td>\n",
       "      <td>14.110254</td>\n",
       "      <td>16.036001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.59536</td>\n",
       "      <td>0.275183</td>\n",
       "      <td>0.14696</td>\n",
       "      <td>-0.057739</td>\n",
       "      <td>19.230497</td>\n",
       "      <td>14.990183</td>\n",
       "      <td>14.239527</td>\n",
       "      <td>16.153402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.605146</td>\n",
       "      <td>0.263162</td>\n",
       "      <td>0.127876</td>\n",
       "      <td>-0.071369</td>\n",
       "      <td>19.311576</td>\n",
       "      <td>15.111443</td>\n",
       "      <td>14.402944</td>\n",
       "      <td>16.275321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.144623</td>\n",
       "      <td>0.52658</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.278652</td>\n",
       "      <td>18.072985</td>\n",
       "      <td>11.814674</td>\n",
       "      <td>10.940605</td>\n",
       "      <td>13.609421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.884703  0.892643  0.934689  0.904012   6.704126    5.73522   \n",
       "1      0.777607  0.839667  0.903084   0.84012   9.311109   7.009273   \n",
       "2      0.657652  0.806375  0.865895  0.776641  11.553851   7.703399   \n",
       "3      0.528777  0.774943  0.829694  0.711138   13.55714   8.306188   \n",
       "4      0.404445   0.74743  0.796293   0.64939  15.240606   8.800932   \n",
       "5      0.290972  0.724987  0.763723  0.593228  16.626523   9.186337   \n",
       "6      0.193932  0.704764  0.730494  0.543063  17.724697   9.520858   \n",
       "7      0.113923  0.684738  0.696696  0.498452  18.579554   9.841437   \n",
       "8      0.046204  0.664302  0.660696  0.457067  19.270933  10.159451   \n",
       "9     -0.014245  0.643249  0.621479  0.416828  19.867771  10.476707   \n",
       "10    -0.072174   0.62095  0.577706  0.375494  20.424637  10.802127   \n",
       "11     -0.12889  0.596716  0.531564   0.33313  20.571391  11.145479   \n",
       "12    -0.183929  0.570672  0.485802  0.290848   20.14154  11.503948   \n",
       "13    -0.233632  0.544283  0.442132  0.250927  19.869264  11.856199   \n",
       "14    -0.279066  0.517547  0.401201  0.213227   19.90697  12.204407   \n",
       "15     -0.31975  0.492266  0.363046  0.178521   20.15455  12.526128   \n",
       "16    -0.355656  0.468535  0.328386  0.147088  20.029518  12.819252   \n",
       "17    -0.385938  0.445732  0.298408  0.119401  19.868924  13.095365   \n",
       "18    -0.412587  0.424849  0.273857  0.095373  19.766966  13.343829   \n",
       "19     -0.43714  0.405593  0.254949  0.074467  19.653763  13.567733   \n",
       "20    -0.459809  0.387731  0.239743  0.055888  19.607186  13.772249   \n",
       "21     -0.48145  0.371219   0.22686  0.038876  19.630259  13.958178   \n",
       "22    -0.501683  0.355614   0.21368  0.022537  19.496969  14.131523   \n",
       "23    -0.520756   0.34172  0.200715  0.007226  19.360188  14.285099   \n",
       "24    -0.539265  0.328278  0.189183 -0.007268  19.285305  14.432103   \n",
       "25    -0.555604  0.314712  0.180535 -0.020119  19.198111  14.578607   \n",
       "26    -0.570704  0.301421  0.172799 -0.032161  19.105298  14.719139   \n",
       "27    -0.584131  0.288126  0.161847 -0.044719  19.140321   14.85743   \n",
       "28     -0.59536  0.275183   0.14696 -0.057739  19.230497  14.990183   \n",
       "29    -0.605146  0.263162  0.127876 -0.071369  19.311576  15.111443   \n",
       "mean  -0.144623   0.52658     0.454  0.278652  18.072985  11.814674   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       3.935563   5.458303  \n",
       "1       4.794283   7.038222  \n",
       "2       5.639858   8.299036  \n",
       "3       6.355801   9.406376  \n",
       "4        6.95112  10.330886  \n",
       "5       7.485471  11.099444  \n",
       "6       7.993429  11.746328  \n",
       "7       8.478228   12.29974  \n",
       "8        8.96518  12.798521  \n",
       "9       9.467853  13.270777  \n",
       "10       9.99986  13.742208  \n",
       "11     10.532687  14.083186  \n",
       "12     11.036383   14.22729  \n",
       "13     11.496594  14.407353  \n",
       "14     11.911639  14.674339  \n",
       "15     12.285072  14.988583  \n",
       "16     12.614988  15.154586  \n",
       "17     12.893767  15.286019  \n",
       "18     13.118172  15.409656  \n",
       "19     13.288666  15.503387  \n",
       "20     13.424689  15.601375  \n",
       "21     13.539675  15.709371  \n",
       "22     13.656531  15.761675  \n",
       "23     13.770366  15.805218  \n",
       "24     13.870282  15.862563  \n",
       "25     13.945568  15.907429  \n",
       "26     14.013711  15.946049  \n",
       "27     14.110254  16.036001  \n",
       "28     14.239527  16.153402  \n",
       "29     14.402944  16.275321  \n",
       "mean   10.940605  13.609421  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "5th iteration\n",
      "history size: 30\n",
      "future size: 10\n",
      "Epoch 1/10000\n",
      "572/572 - 8s - loss: 0.2375 - val_loss: 0.1532 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "572/572 - 5s - loss: 0.1503 - val_loss: 0.1407 - 5s/epoch - 9ms/step\n",
      "Epoch 3/10000\n",
      "572/572 - 5s - loss: 0.1369 - val_loss: 0.1299 - 5s/epoch - 9ms/step\n",
      "Epoch 4/10000\n",
      "572/572 - 5s - loss: 0.1289 - val_loss: 0.1216 - 5s/epoch - 9ms/step\n",
      "Epoch 5/10000\n",
      "572/572 - 5s - loss: 0.1210 - val_loss: 0.1130 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "572/572 - 5s - loss: 0.1128 - val_loss: 0.1081 - 5s/epoch - 9ms/step\n",
      "Epoch 7/10000\n",
      "572/572 - 5s - loss: 0.1052 - val_loss: 0.1001 - 5s/epoch - 9ms/step\n",
      "Epoch 8/10000\n",
      "572/572 - 5s - loss: 0.1013 - val_loss: 0.0962 - 5s/epoch - 9ms/step\n",
      "Epoch 9/10000\n",
      "572/572 - 5s - loss: 0.0965 - val_loss: 0.0939 - 5s/epoch - 9ms/step\n",
      "Epoch 10/10000\n",
      "572/572 - 5s - loss: 0.0938 - val_loss: 0.0949 - 5s/epoch - 9ms/step\n",
      "Epoch 11/10000\n",
      "572/572 - 5s - loss: 0.0893 - val_loss: 0.0938 - 5s/epoch - 9ms/step\n",
      "Epoch 12/10000\n",
      "572/572 - 5s - loss: 0.0884 - val_loss: 0.0875 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "572/572 - 5s - loss: 0.0852 - val_loss: 0.0868 - 5s/epoch - 9ms/step\n",
      "Epoch 14/10000\n",
      "572/572 - 5s - loss: 0.0817 - val_loss: 0.0877 - 5s/epoch - 9ms/step\n",
      "Epoch 15/10000\n",
      "572/572 - 5s - loss: 0.0811 - val_loss: 0.0861 - 5s/epoch - 9ms/step\n",
      "Epoch 16/10000\n",
      "572/572 - 5s - loss: 0.0810 - val_loss: 0.0837 - 5s/epoch - 9ms/step\n",
      "Epoch 17/10000\n",
      "572/572 - 5s - loss: 0.0767 - val_loss: 0.0879 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "572/572 - 5s - loss: 0.0729 - val_loss: 0.0770 - 5s/epoch - 9ms/step\n",
      "Epoch 19/10000\n",
      "572/572 - 5s - loss: 0.0719 - val_loss: 0.0769 - 5s/epoch - 9ms/step\n",
      "Epoch 20/10000\n",
      "572/572 - 5s - loss: 0.0702 - val_loss: 0.0797 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "572/572 - 5s - loss: 0.0710 - val_loss: 0.0770 - 5s/epoch - 9ms/step\n",
      "Epoch 22/10000\n",
      "572/572 - 5s - loss: 0.0671 - val_loss: 0.0727 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "572/572 - 5s - loss: 0.0651 - val_loss: 0.0710 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "572/572 - 5s - loss: 0.0642 - val_loss: 0.0731 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "572/572 - 5s - loss: 0.0669 - val_loss: 0.0743 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "572/572 - 5s - loss: 0.0617 - val_loss: 0.0751 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "572/572 - 5s - loss: 0.0604 - val_loss: 0.0701 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "572/572 - 5s - loss: 0.0606 - val_loss: 0.0693 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "572/572 - 5s - loss: 0.0597 - val_loss: 0.0676 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "572/572 - 5s - loss: 0.0588 - val_loss: 0.0738 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "572/572 - 5s - loss: 0.0578 - val_loss: 0.0699 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "572/572 - 5s - loss: 0.0556 - val_loss: 0.0684 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "572/572 - 5s - loss: 0.0556 - val_loss: 0.0625 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "572/572 - 5s - loss: 0.0537 - val_loss: 0.0679 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "572/572 - 5s - loss: 0.0548 - val_loss: 0.0633 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "572/572 - 5s - loss: 0.0531 - val_loss: 0.0616 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "572/572 - 5s - loss: 0.0524 - val_loss: 0.0610 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "572/572 - 5s - loss: 0.0525 - val_loss: 0.0679 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "572/572 - 5s - loss: 0.0530 - val_loss: 0.0607 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "572/572 - 5s - loss: 0.0501 - val_loss: 0.0621 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "572/572 - 5s - loss: 0.0498 - val_loss: 0.0629 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "572/572 - 5s - loss: 0.0499 - val_loss: 0.0593 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "572/572 - 5s - loss: 0.0476 - val_loss: 0.0597 - 5s/epoch - 9ms/step\n",
      "Epoch 44/10000\n",
      "572/572 - 5s - loss: 0.0492 - val_loss: 0.0585 - 5s/epoch - 9ms/step\n",
      "Epoch 45/10000\n",
      "572/572 - 5s - loss: 0.0475 - val_loss: 0.0613 - 5s/epoch - 9ms/step\n",
      "Epoch 46/10000\n",
      "572/572 - 5s - loss: 0.0499 - val_loss: 0.0589 - 5s/epoch - 9ms/step\n",
      "Epoch 47/10000\n",
      "572/572 - 5s - loss: 0.0466 - val_loss: 0.0624 - 5s/epoch - 9ms/step\n",
      "Epoch 48/10000\n",
      "572/572 - 5s - loss: 0.0470 - val_loss: 0.0587 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "572/572 - 5s - loss: 0.0471 - val_loss: 0.0579 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "572/572 - 5s - loss: 0.0463 - val_loss: 0.0563 - 5s/epoch - 9ms/step\n",
      "Epoch 51/10000\n",
      "572/572 - 5s - loss: 0.0448 - val_loss: 0.0594 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "572/572 - 5s - loss: 0.0449 - val_loss: 0.0571 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "572/572 - 5s - loss: 0.0434 - val_loss: 0.0574 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "572/572 - 5s - loss: 0.0444 - val_loss: 0.0550 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "572/572 - 5s - loss: 0.0448 - val_loss: 0.0598 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "572/572 - 5s - loss: 0.0439 - val_loss: 0.0548 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "572/572 - 5s - loss: 0.0430 - val_loss: 0.0549 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "572/572 - 5s - loss: 0.0450 - val_loss: 0.0569 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "572/572 - 5s - loss: 0.0419 - val_loss: 0.0546 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "572/572 - 5s - loss: 0.0420 - val_loss: 0.0547 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "572/572 - 5s - loss: 0.0416 - val_loss: 0.0561 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "572/572 - 5s - loss: 0.0413 - val_loss: 0.0532 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "572/572 - 5s - loss: 0.0419 - val_loss: 0.0531 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "572/572 - 5s - loss: 0.0409 - val_loss: 0.0545 - 5s/epoch - 9ms/step\n",
      "Epoch 65/10000\n",
      "572/572 - 5s - loss: 0.0407 - val_loss: 0.0558 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "572/572 - 5s - loss: 0.0415 - val_loss: 0.0522 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "572/572 - 5s - loss: 0.0404 - val_loss: 0.0525 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "572/572 - 5s - loss: 0.0402 - val_loss: 0.0527 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "572/572 - 5s - loss: 0.0390 - val_loss: 0.0531 - 5s/epoch - 9ms/step\n",
      "Epoch 70/10000\n",
      "572/572 - 5s - loss: 0.0393 - val_loss: 0.0501 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "572/572 - 5s - loss: 0.0420 - val_loss: 0.0566 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "572/572 - 5s - loss: 0.0386 - val_loss: 0.0511 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "572/572 - 5s - loss: 0.0383 - val_loss: 0.0520 - 5s/epoch - 9ms/step\n",
      "Epoch 74/10000\n",
      "572/572 - 6s - loss: 0.0382 - val_loss: 0.0488 - 6s/epoch - 10ms/step\n",
      "Epoch 75/10000\n",
      "572/572 - 5s - loss: 0.0387 - val_loss: 0.0540 - 5s/epoch - 9ms/step\n",
      "Epoch 76/10000\n",
      "572/572 - 5s - loss: 0.0380 - val_loss: 0.0505 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "572/572 - 5s - loss: 0.0382 - val_loss: 0.0513 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "572/572 - 5s - loss: 0.0381 - val_loss: 0.0480 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "572/572 - 5s - loss: 0.0372 - val_loss: 0.0510 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "572/572 - 5s - loss: 0.0389 - val_loss: 0.0493 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "572/572 - 5s - loss: 0.0373 - val_loss: 0.0492 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "572/572 - 5s - loss: 0.0371 - val_loss: 0.0519 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "572/572 - 5s - loss: 0.0371 - val_loss: 0.0490 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "572/572 - 5s - loss: 0.0383 - val_loss: 0.0492 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "572/572 - 5s - loss: 0.0362 - val_loss: 0.0503 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "572/572 - 5s - loss: 0.0364 - val_loss: 0.0508 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "572/572 - 5s - loss: 0.0370 - val_loss: 0.0522 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "572/572 - 5s - loss: 0.0353 - val_loss: 0.0470 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "572/572 - 5s - loss: 0.0358 - val_loss: 0.0465 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "572/572 - 5s - loss: 0.0368 - val_loss: 0.0520 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "572/572 - 5s - loss: 0.0355 - val_loss: 0.0487 - 5s/epoch - 9ms/step\n",
      "Epoch 92/10000\n",
      "572/572 - 5s - loss: 0.0349 - val_loss: 0.0475 - 5s/epoch - 9ms/step\n",
      "Epoch 93/10000\n",
      "572/572 - 5s - loss: 0.0344 - val_loss: 0.0493 - 5s/epoch - 9ms/step\n",
      "Epoch 94/10000\n",
      "572/572 - 5s - loss: 0.0363 - val_loss: 0.0498 - 5s/epoch - 9ms/step\n",
      "Epoch 95/10000\n",
      "572/572 - 5s - loss: 0.0348 - val_loss: 0.0473 - 5s/epoch - 9ms/step\n",
      "Epoch 96/10000\n",
      "572/572 - 5s - loss: 0.0342 - val_loss: 0.0465 - 5s/epoch - 9ms/step\n",
      "Epoch 97/10000\n",
      "572/572 - 5s - loss: 0.0345 - val_loss: 0.0457 - 5s/epoch - 9ms/step\n",
      "Epoch 98/10000\n",
      "572/572 - 5s - loss: 0.0340 - val_loss: 0.0462 - 5s/epoch - 9ms/step\n",
      "Epoch 99/10000\n",
      "572/572 - 6s - loss: 0.0339 - val_loss: 0.0454 - 6s/epoch - 10ms/step\n",
      "Epoch 100/10000\n",
      "572/572 - 6s - loss: 0.0346 - val_loss: 0.0515 - 6s/epoch - 10ms/step\n",
      "Epoch 101/10000\n",
      "572/572 - 6s - loss: 0.0337 - val_loss: 0.0440 - 6s/epoch - 10ms/step\n",
      "Epoch 102/10000\n",
      "572/572 - 6s - loss: 0.0334 - val_loss: 0.0478 - 6s/epoch - 10ms/step\n",
      "Epoch 103/10000\n",
      "572/572 - 5s - loss: 0.0327 - val_loss: 0.0451 - 5s/epoch - 9ms/step\n",
      "Epoch 104/10000\n",
      "572/572 - 5s - loss: 0.0329 - val_loss: 0.0449 - 5s/epoch - 9ms/step\n",
      "Epoch 105/10000\n",
      "572/572 - 5s - loss: 0.0331 - val_loss: 0.0434 - 5s/epoch - 9ms/step\n",
      "Epoch 106/10000\n",
      "572/572 - 5s - loss: 0.0330 - val_loss: 0.0468 - 5s/epoch - 9ms/step\n",
      "Epoch 107/10000\n",
      "572/572 - 5s - loss: 0.0331 - val_loss: 0.0458 - 5s/epoch - 9ms/step\n",
      "Epoch 108/10000\n",
      "572/572 - 5s - loss: 0.0336 - val_loss: 0.0683 - 5s/epoch - 9ms/step\n",
      "Epoch 109/10000\n",
      "572/572 - 5s - loss: 0.0390 - val_loss: 0.0458 - 5s/epoch - 9ms/step\n",
      "Epoch 110/10000\n",
      "572/572 - 5s - loss: 0.0316 - val_loss: 0.0440 - 5s/epoch - 9ms/step\n",
      "Epoch 111/10000\n",
      "572/572 - 5s - loss: 0.0308 - val_loss: 0.0437 - 5s/epoch - 9ms/step\n",
      "Epoch 112/10000\n",
      "572/572 - 5s - loss: 0.0311 - val_loss: 0.0453 - 5s/epoch - 9ms/step\n",
      "Epoch 113/10000\n",
      "572/572 - 5s - loss: 0.0307 - val_loss: 0.0445 - 5s/epoch - 9ms/step\n",
      "Epoch 114/10000\n",
      "572/572 - 5s - loss: 0.0314 - val_loss: 0.0457 - 5s/epoch - 9ms/step\n",
      "Epoch 115/10000\n",
      "572/572 - 5s - loss: 0.0316 - val_loss: 0.0452 - 5s/epoch - 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_216_layer_call_fn, gru_cell_216_layer_call_and_return_conditional_losses, gru_cell_217_layer_call_fn, gru_cell_217_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_10_1_50_1_50_datt_seq2seq_gru_5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_10_1_50_1_50_datt_seq2seq_gru_5\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B4E9CE7F0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B505FE310> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.924327</td>\n",
       "      <td>0.939238</td>\n",
       "      <td>0.964059</td>\n",
       "      <td>0.942541</td>\n",
       "      <td>4.464704</td>\n",
       "      <td>4.329941</td>\n",
       "      <td>2.927633</td>\n",
       "      <td>3.907426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.838982</td>\n",
       "      <td>0.890529</td>\n",
       "      <td>0.913413</td>\n",
       "      <td>0.880975</td>\n",
       "      <td>6.469446</td>\n",
       "      <td>5.812566</td>\n",
       "      <td>4.543327</td>\n",
       "      <td>5.608446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.73022</td>\n",
       "      <td>0.845909</td>\n",
       "      <td>0.856926</td>\n",
       "      <td>0.811019</td>\n",
       "      <td>8.257909</td>\n",
       "      <td>6.896793</td>\n",
       "      <td>5.839749</td>\n",
       "      <td>6.99815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.618659</td>\n",
       "      <td>0.810308</td>\n",
       "      <td>0.822115</td>\n",
       "      <td>0.750361</td>\n",
       "      <td>9.685695</td>\n",
       "      <td>7.653146</td>\n",
       "      <td>6.511593</td>\n",
       "      <td>7.950144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.512948</td>\n",
       "      <td>0.779868</td>\n",
       "      <td>0.8093</td>\n",
       "      <td>0.700705</td>\n",
       "      <td>10.837513</td>\n",
       "      <td>8.245457</td>\n",
       "      <td>6.741878</td>\n",
       "      <td>8.608283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.418388</td>\n",
       "      <td>0.754449</td>\n",
       "      <td>0.807754</td>\n",
       "      <td>0.660197</td>\n",
       "      <td>11.728326</td>\n",
       "      <td>8.70983</td>\n",
       "      <td>6.769212</td>\n",
       "      <td>9.069123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.336055</td>\n",
       "      <td>0.734385</td>\n",
       "      <td>0.806868</td>\n",
       "      <td>0.625769</td>\n",
       "      <td>12.412526</td>\n",
       "      <td>9.059331</td>\n",
       "      <td>6.785116</td>\n",
       "      <td>9.418991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.265017</td>\n",
       "      <td>0.719828</td>\n",
       "      <td>0.80139</td>\n",
       "      <td>0.595412</td>\n",
       "      <td>13.030973</td>\n",
       "      <td>9.304513</td>\n",
       "      <td>6.881273</td>\n",
       "      <td>9.73892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.203457</td>\n",
       "      <td>0.708651</td>\n",
       "      <td>0.786828</td>\n",
       "      <td>0.566312</td>\n",
       "      <td>13.584765</td>\n",
       "      <td>9.488566</td>\n",
       "      <td>7.129562</td>\n",
       "      <td>10.067631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.150026</td>\n",
       "      <td>0.694341</td>\n",
       "      <td>0.763055</td>\n",
       "      <td>0.535808</td>\n",
       "      <td>14.053165</td>\n",
       "      <td>9.718849</td>\n",
       "      <td>7.517447</td>\n",
       "      <td>10.42982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.499808</td>\n",
       "      <td>0.787751</td>\n",
       "      <td>0.833171</td>\n",
       "      <td>0.70691</td>\n",
       "      <td>10.452502</td>\n",
       "      <td>7.921899</td>\n",
       "      <td>6.164679</td>\n",
       "      <td>8.179693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3 TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE     nRMSE     nRMSE   \n",
       "0      0.924327  0.939238  0.964059  0.942541   4.464704  4.329941  2.927633   \n",
       "1      0.838982  0.890529  0.913413  0.880975   6.469446  5.812566  4.543327   \n",
       "2       0.73022  0.845909  0.856926  0.811019   8.257909  6.896793  5.839749   \n",
       "3      0.618659  0.810308  0.822115  0.750361   9.685695  7.653146  6.511593   \n",
       "4      0.512948  0.779868    0.8093  0.700705  10.837513  8.245457  6.741878   \n",
       "5      0.418388  0.754449  0.807754  0.660197  11.728326   8.70983  6.769212   \n",
       "6      0.336055  0.734385  0.806868  0.625769  12.412526  9.059331  6.785116   \n",
       "7      0.265017  0.719828   0.80139  0.595412  13.030973  9.304513  6.881273   \n",
       "8      0.203457  0.708651  0.786828  0.566312  13.584765  9.488566  7.129562   \n",
       "9      0.150026  0.694341  0.763055  0.535808  14.053165  9.718849  7.517447   \n",
       "mean   0.499808  0.787751  0.833171   0.70691  10.452502  7.921899  6.164679   \n",
       "\n",
       "            mean  \n",
       "index      nRMSE  \n",
       "0       3.907426  \n",
       "1       5.608446  \n",
       "2        6.99815  \n",
       "3       7.950144  \n",
       "4       8.608283  \n",
       "5       9.069123  \n",
       "6       9.418991  \n",
       "7        9.73892  \n",
       "8      10.067631  \n",
       "9       10.42982  \n",
       "mean    8.179693  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/30_10_1_50_1_50_datt_seq2seq_gru_5.csv\n",
      "\n",
      "\n",
      "5th iteration\n",
      "history size: 30\n",
      "future size: 20\n",
      "Epoch 1/10000\n",
      "568/568 - 9s - loss: 0.3089 - val_loss: 0.2612 - 9s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "568/568 - 5s - loss: 0.2163 - val_loss: 0.2358 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "568/568 - 5s - loss: 0.2018 - val_loss: 0.2248 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "568/568 - 5s - loss: 0.1914 - val_loss: 0.2183 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "568/568 - 5s - loss: 0.1825 - val_loss: 0.2061 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "568/568 - 5s - loss: 0.1752 - val_loss: 0.1949 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "568/568 - 5s - loss: 0.1664 - val_loss: 0.1907 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "568/568 - 5s - loss: 0.1592 - val_loss: 0.1810 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "568/568 - 5s - loss: 0.1514 - val_loss: 0.1771 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "568/568 - 5s - loss: 0.1441 - val_loss: 0.1663 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "568/568 - 5s - loss: 0.1409 - val_loss: 0.1646 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "568/568 - 5s - loss: 0.1319 - val_loss: 0.1544 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "568/568 - 5s - loss: 0.1254 - val_loss: 0.1541 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "568/568 - 5s - loss: 0.1219 - val_loss: 0.1491 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "568/568 - 5s - loss: 0.1183 - val_loss: 0.1408 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "568/568 - 5s - loss: 0.1142 - val_loss: 0.1438 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "568/568 - 5s - loss: 0.1100 - val_loss: 0.1401 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "568/568 - 5s - loss: 0.1101 - val_loss: 0.1292 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "568/568 - 5s - loss: 0.1060 - val_loss: 0.1313 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "568/568 - 5s - loss: 0.1049 - val_loss: 0.1290 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "568/568 - 5s - loss: 0.0998 - val_loss: 0.1272 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "568/568 - 5s - loss: 0.1006 - val_loss: 0.1195 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "568/568 - 5s - loss: 0.0956 - val_loss: 0.1155 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "568/568 - 5s - loss: 0.0970 - val_loss: 0.1259 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "568/568 - 5s - loss: 0.0927 - val_loss: 0.1175 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "568/568 - 5s - loss: 0.0910 - val_loss: 0.1085 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "568/568 - 5s - loss: 0.0969 - val_loss: 0.1172 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "568/568 - 5s - loss: 0.0896 - val_loss: 0.1082 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "568/568 - 5s - loss: 0.0861 - val_loss: 0.1146 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "568/568 - 5s - loss: 0.0859 - val_loss: 0.1083 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "568/568 - 5s - loss: 0.0848 - val_loss: 0.1158 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "568/568 - 5s - loss: 0.0841 - val_loss: 0.1023 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "568/568 - 5s - loss: 0.0839 - val_loss: 0.1092 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "568/568 - 5s - loss: 0.0833 - val_loss: 0.1054 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "568/568 - 5s - loss: 0.0796 - val_loss: 0.1019 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "568/568 - 5s - loss: 0.0792 - val_loss: 0.1025 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "568/568 - 5s - loss: 0.0787 - val_loss: 0.1028 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "568/568 - 5s - loss: 0.0787 - val_loss: 0.1009 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "568/568 - 5s - loss: 0.0749 - val_loss: 0.0945 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "568/568 - 5s - loss: 0.0764 - val_loss: 0.0994 - 5s/epoch - 9ms/step\n",
      "Epoch 41/10000\n",
      "568/568 - 5s - loss: 0.0776 - val_loss: 0.0960 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "568/568 - 5s - loss: 0.0723 - val_loss: 0.0992 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "568/568 - 5s - loss: 0.0716 - val_loss: 0.0929 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "568/568 - 5s - loss: 0.0755 - val_loss: 0.0968 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "568/568 - 5s - loss: 0.0700 - val_loss: 0.0887 - 5s/epoch - 9ms/step\n",
      "Epoch 46/10000\n",
      "568/568 - 5s - loss: 0.0713 - val_loss: 0.0903 - 5s/epoch - 9ms/step\n",
      "Epoch 47/10000\n",
      "568/568 - 6s - loss: 0.0703 - val_loss: 0.0944 - 6s/epoch - 10ms/step\n",
      "Epoch 48/10000\n",
      "568/568 - 6s - loss: 0.0668 - val_loss: 0.1143 - 6s/epoch - 10ms/step\n",
      "Epoch 49/10000\n",
      "568/568 - 6s - loss: 0.0729 - val_loss: 0.0799 - 6s/epoch - 11ms/step\n",
      "Epoch 50/10000\n",
      "568/568 - 6s - loss: 0.0662 - val_loss: 0.0946 - 6s/epoch - 10ms/step\n",
      "Epoch 51/10000\n",
      "568/568 - 5s - loss: 0.0679 - val_loss: 0.0925 - 5s/epoch - 9ms/step\n",
      "Epoch 52/10000\n",
      "568/568 - 5s - loss: 0.0682 - val_loss: 0.0916 - 5s/epoch - 9ms/step\n",
      "Epoch 53/10000\n",
      "568/568 - 5s - loss: 0.0680 - val_loss: 0.0792 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "568/568 - 5s - loss: 0.0693 - val_loss: 0.0947 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "568/568 - 5s - loss: 0.0738 - val_loss: 0.0849 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "568/568 - 5s - loss: 0.0643 - val_loss: 0.0788 - 5s/epoch - 9ms/step\n",
      "Epoch 57/10000\n",
      "568/568 - 5s - loss: 0.0605 - val_loss: 0.0913 - 5s/epoch - 9ms/step\n",
      "Epoch 58/10000\n",
      "568/568 - 5s - loss: 0.0635 - val_loss: 0.0774 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "568/568 - 5s - loss: 0.0635 - val_loss: 0.0858 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "568/568 - 5s - loss: 0.0594 - val_loss: 0.0730 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "568/568 - 5s - loss: 0.0583 - val_loss: 0.0747 - 5s/epoch - 9ms/step\n",
      "Epoch 62/10000\n",
      "568/568 - 5s - loss: 0.0588 - val_loss: 0.0831 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "568/568 - 5s - loss: 0.0585 - val_loss: 0.0782 - 5s/epoch - 9ms/step\n",
      "Epoch 64/10000\n",
      "568/568 - 5s - loss: 0.0572 - val_loss: 0.0913 - 5s/epoch - 9ms/step\n",
      "Epoch 65/10000\n",
      "568/568 - 5s - loss: 0.0609 - val_loss: 0.0746 - 5s/epoch - 9ms/step\n",
      "Epoch 66/10000\n",
      "568/568 - 5s - loss: 0.0577 - val_loss: 0.0702 - 5s/epoch - 9ms/step\n",
      "Epoch 67/10000\n",
      "568/568 - 6s - loss: 0.0598 - val_loss: 0.0853 - 6s/epoch - 10ms/step\n",
      "Epoch 68/10000\n",
      "568/568 - 5s - loss: 0.0635 - val_loss: 0.0842 - 5s/epoch - 9ms/step\n",
      "Epoch 69/10000\n",
      "568/568 - 5s - loss: 0.0558 - val_loss: 0.0686 - 5s/epoch - 9ms/step\n",
      "Epoch 70/10000\n",
      "568/568 - 5s - loss: 0.0543 - val_loss: 0.0709 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "568/568 - 6s - loss: 0.0644 - val_loss: 0.0810 - 6s/epoch - 10ms/step\n",
      "Epoch 72/10000\n",
      "568/568 - 5s - loss: 0.0614 - val_loss: 0.0817 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "568/568 - 5s - loss: 0.0571 - val_loss: 0.0681 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "568/568 - 5s - loss: 0.0612 - val_loss: 0.0928 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "568/568 - 5s - loss: 0.0609 - val_loss: 0.0688 - 5s/epoch - 9ms/step\n",
      "Epoch 76/10000\n",
      "568/568 - 5s - loss: 0.0530 - val_loss: 0.0691 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "568/568 - 5s - loss: 0.0530 - val_loss: 0.0804 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "568/568 - 5s - loss: 0.0547 - val_loss: 0.0676 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "568/568 - 5s - loss: 0.0530 - val_loss: 0.0643 - 5s/epoch - 9ms/step\n",
      "Epoch 80/10000\n",
      "568/568 - 5s - loss: 0.0533 - val_loss: 0.0742 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "568/568 - 4s - loss: 0.0522 - val_loss: 0.0674 - 4s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "568/568 - 5s - loss: 0.0512 - val_loss: 0.0675 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "568/568 - 4s - loss: 0.0509 - val_loss: 0.0690 - 4s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "568/568 - 4s - loss: 0.0521 - val_loss: 0.0692 - 4s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "568/568 - 4s - loss: 0.0572 - val_loss: 0.0794 - 4s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "568/568 - 4s - loss: 0.0527 - val_loss: 0.0695 - 4s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "568/568 - 4s - loss: 0.0511 - val_loss: 0.0706 - 4s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "568/568 - 4s - loss: 0.0507 - val_loss: 0.0651 - 4s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "568/568 - 4s - loss: 0.0511 - val_loss: 0.0997 - 4s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_218_layer_call_fn, gru_cell_218_layer_call_and_return_conditional_losses, gru_cell_219_layer_call_fn, gru_cell_219_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_20_1_50_1_50_datt_seq2seq_gru_5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_20_1_50_1_50_datt_seq2seq_gru_5\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B7A2E74C0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B4EF6BCA0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.881247</td>\n",
       "      <td>0.904464</td>\n",
       "      <td>0.940562</td>\n",
       "      <td>0.908757</td>\n",
       "      <td>6.797426</td>\n",
       "      <td>5.423077</td>\n",
       "      <td>3.751632</td>\n",
       "      <td>5.324045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.798888</td>\n",
       "      <td>0.854292</td>\n",
       "      <td>0.894471</td>\n",
       "      <td>0.849217</td>\n",
       "      <td>8.682749</td>\n",
       "      <td>6.699381</td>\n",
       "      <td>4.999193</td>\n",
       "      <td>6.793774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.707163</td>\n",
       "      <td>0.813247</td>\n",
       "      <td>0.834907</td>\n",
       "      <td>0.785106</td>\n",
       "      <td>10.01712</td>\n",
       "      <td>7.587281</td>\n",
       "      <td>6.253538</td>\n",
       "      <td>7.952646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.614812</td>\n",
       "      <td>0.777787</td>\n",
       "      <td>0.787645</td>\n",
       "      <td>0.726748</td>\n",
       "      <td>11.102611</td>\n",
       "      <td>8.279079</td>\n",
       "      <td>7.093081</td>\n",
       "      <td>8.824924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.520799</td>\n",
       "      <td>0.746271</td>\n",
       "      <td>0.760912</td>\n",
       "      <td>0.675994</td>\n",
       "      <td>12.184775</td>\n",
       "      <td>8.85063</td>\n",
       "      <td>7.526789</td>\n",
       "      <td>9.520732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.432173</td>\n",
       "      <td>0.718111</td>\n",
       "      <td>0.746462</td>\n",
       "      <td>0.632249</td>\n",
       "      <td>13.22011</td>\n",
       "      <td>9.333365</td>\n",
       "      <td>7.750781</td>\n",
       "      <td>10.101419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.349304</td>\n",
       "      <td>0.692217</td>\n",
       "      <td>0.732489</td>\n",
       "      <td>0.591337</td>\n",
       "      <td>13.876651</td>\n",
       "      <td>9.755467</td>\n",
       "      <td>7.961557</td>\n",
       "      <td>10.531225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.272621</td>\n",
       "      <td>0.668229</td>\n",
       "      <td>0.706905</td>\n",
       "      <td>0.549252</td>\n",
       "      <td>14.394044</td>\n",
       "      <td>10.131566</td>\n",
       "      <td>8.333774</td>\n",
       "      <td>10.953128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.205279</td>\n",
       "      <td>0.644528</td>\n",
       "      <td>0.664191</td>\n",
       "      <td>0.504666</td>\n",
       "      <td>14.826539</td>\n",
       "      <td>10.490406</td>\n",
       "      <td>8.920891</td>\n",
       "      <td>11.412612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.147531</td>\n",
       "      <td>0.619227</td>\n",
       "      <td>0.608148</td>\n",
       "      <td>0.458302</td>\n",
       "      <td>15.136847</td>\n",
       "      <td>10.859222</td>\n",
       "      <td>9.637164</td>\n",
       "      <td>11.877744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.097401</td>\n",
       "      <td>0.590223</td>\n",
       "      <td>0.549538</td>\n",
       "      <td>0.412387</td>\n",
       "      <td>15.41752</td>\n",
       "      <td>11.266984</td>\n",
       "      <td>10.333631</td>\n",
       "      <td>12.339378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.054236</td>\n",
       "      <td>0.56112</td>\n",
       "      <td>0.501997</td>\n",
       "      <td>0.372451</td>\n",
       "      <td>15.684618</td>\n",
       "      <td>11.661425</td>\n",
       "      <td>10.866633</td>\n",
       "      <td>12.737559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.017339</td>\n",
       "      <td>0.532423</td>\n",
       "      <td>0.47476</td>\n",
       "      <td>0.341507</td>\n",
       "      <td>15.771747</td>\n",
       "      <td>12.037674</td>\n",
       "      <td>11.161429</td>\n",
       "      <td>12.990283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.015259</td>\n",
       "      <td>0.505158</td>\n",
       "      <td>0.465023</td>\n",
       "      <td>0.318307</td>\n",
       "      <td>15.81861</td>\n",
       "      <td>12.385433</td>\n",
       "      <td>11.265806</td>\n",
       "      <td>13.156616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.045914</td>\n",
       "      <td>0.477767</td>\n",
       "      <td>0.465017</td>\n",
       "      <td>0.298957</td>\n",
       "      <td>15.897094</td>\n",
       "      <td>12.725269</td>\n",
       "      <td>11.266634</td>\n",
       "      <td>13.296332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.07333</td>\n",
       "      <td>0.452709</td>\n",
       "      <td>0.463869</td>\n",
       "      <td>0.281082</td>\n",
       "      <td>15.946879</td>\n",
       "      <td>13.028327</td>\n",
       "      <td>11.279938</td>\n",
       "      <td>13.418381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.098194</td>\n",
       "      <td>0.42913</td>\n",
       "      <td>0.453248</td>\n",
       "      <td>0.261395</td>\n",
       "      <td>15.975192</td>\n",
       "      <td>13.305871</td>\n",
       "      <td>11.393126</td>\n",
       "      <td>13.558063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.12041</td>\n",
       "      <td>0.406934</td>\n",
       "      <td>0.433285</td>\n",
       "      <td>0.239936</td>\n",
       "      <td>16.0969</td>\n",
       "      <td>13.561056</td>\n",
       "      <td>11.602601</td>\n",
       "      <td>13.753519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.141616</td>\n",
       "      <td>0.385688</td>\n",
       "      <td>0.412937</td>\n",
       "      <td>0.219003</td>\n",
       "      <td>16.267518</td>\n",
       "      <td>13.800264</td>\n",
       "      <td>11.812817</td>\n",
       "      <td>13.9602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.160888</td>\n",
       "      <td>0.365342</td>\n",
       "      <td>0.399855</td>\n",
       "      <td>0.201436</td>\n",
       "      <td>16.423122</td>\n",
       "      <td>14.024574</td>\n",
       "      <td>11.947866</td>\n",
       "      <td>14.131854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.222159</td>\n",
       "      <td>0.607243</td>\n",
       "      <td>0.614811</td>\n",
       "      <td>0.481404</td>\n",
       "      <td>13.976904</td>\n",
       "      <td>10.760318</td>\n",
       "      <td>9.257944</td>\n",
       "      <td>11.331722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.881247  0.904464  0.940562  0.908757   6.797426   5.423077   \n",
       "1      0.798888  0.854292  0.894471  0.849217   8.682749   6.699381   \n",
       "2      0.707163  0.813247  0.834907  0.785106   10.01712   7.587281   \n",
       "3      0.614812  0.777787  0.787645  0.726748  11.102611   8.279079   \n",
       "4      0.520799  0.746271  0.760912  0.675994  12.184775    8.85063   \n",
       "5      0.432173  0.718111  0.746462  0.632249   13.22011   9.333365   \n",
       "6      0.349304  0.692217  0.732489  0.591337  13.876651   9.755467   \n",
       "7      0.272621  0.668229  0.706905  0.549252  14.394044  10.131566   \n",
       "8      0.205279  0.644528  0.664191  0.504666  14.826539  10.490406   \n",
       "9      0.147531  0.619227  0.608148  0.458302  15.136847  10.859222   \n",
       "10     0.097401  0.590223  0.549538  0.412387   15.41752  11.266984   \n",
       "11     0.054236   0.56112  0.501997  0.372451  15.684618  11.661425   \n",
       "12     0.017339  0.532423   0.47476  0.341507  15.771747  12.037674   \n",
       "13    -0.015259  0.505158  0.465023  0.318307   15.81861  12.385433   \n",
       "14    -0.045914  0.477767  0.465017  0.298957  15.897094  12.725269   \n",
       "15     -0.07333  0.452709  0.463869  0.281082  15.946879  13.028327   \n",
       "16    -0.098194   0.42913  0.453248  0.261395  15.975192  13.305871   \n",
       "17     -0.12041  0.406934  0.433285  0.239936    16.0969  13.561056   \n",
       "18    -0.141616  0.385688  0.412937  0.219003  16.267518  13.800264   \n",
       "19    -0.160888  0.365342  0.399855  0.201436  16.423122  14.024574   \n",
       "mean   0.222159  0.607243  0.614811  0.481404  13.976904  10.760318   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       3.751632   5.324045  \n",
       "1       4.999193   6.793774  \n",
       "2       6.253538   7.952646  \n",
       "3       7.093081   8.824924  \n",
       "4       7.526789   9.520732  \n",
       "5       7.750781  10.101419  \n",
       "6       7.961557  10.531225  \n",
       "7       8.333774  10.953128  \n",
       "8       8.920891  11.412612  \n",
       "9       9.637164  11.877744  \n",
       "10     10.333631  12.339378  \n",
       "11     10.866633  12.737559  \n",
       "12     11.161429  12.990283  \n",
       "13     11.265806  13.156616  \n",
       "14     11.266634  13.296332  \n",
       "15     11.279938  13.418381  \n",
       "16     11.393126  13.558063  \n",
       "17     11.602601  13.753519  \n",
       "18     11.812817    13.9602  \n",
       "19     11.947866  14.131854  \n",
       "mean    9.257944  11.331722  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/30_20_1_50_1_50_datt_seq2seq_gru_5.csv\n",
      "\n",
      "\n",
      "5th iteration\n",
      "history size: 30\n",
      "future size: 30\n",
      "Epoch 1/10000\n",
      "563/563 - 8s - loss: 0.3780 - val_loss: 0.3151 - 8s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.2889 - val_loss: 0.2934 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.2674 - val_loss: 0.2683 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 5s - loss: 0.2493 - val_loss: 0.2610 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.2321 - val_loss: 0.2591 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.2179 - val_loss: 0.2216 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.2043 - val_loss: 0.2178 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.1951 - val_loss: 0.2040 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.1845 - val_loss: 0.2025 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.1785 - val_loss: 0.1941 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.1710 - val_loss: 0.1899 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.1653 - val_loss: 0.1918 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.1623 - val_loss: 0.1803 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.1563 - val_loss: 0.1752 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.1523 - val_loss: 0.1726 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.1476 - val_loss: 0.1688 - 5s/epoch - 9ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.1445 - val_loss: 0.1669 - 5s/epoch - 9ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.1447 - val_loss: 0.1550 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.1392 - val_loss: 0.1573 - 5s/epoch - 9ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.1361 - val_loss: 0.1548 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.1348 - val_loss: 0.1492 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.1307 - val_loss: 0.1525 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.1278 - val_loss: 0.1439 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.1249 - val_loss: 0.1412 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.1239 - val_loss: 0.1460 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.1227 - val_loss: 0.1394 - 5s/epoch - 9ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.1201 - val_loss: 0.1357 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.1183 - val_loss: 0.1310 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.1156 - val_loss: 0.1273 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.1140 - val_loss: 0.1397 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.1144 - val_loss: 0.1226 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.1122 - val_loss: 0.1313 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.1060 - val_loss: 0.1179 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.1056 - val_loss: 0.1270 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.1063 - val_loss: 0.1161 - 5s/epoch - 9ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.1037 - val_loss: 0.1143 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.1010 - val_loss: 0.1095 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.1010 - val_loss: 0.1157 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.1031 - val_loss: 0.1508 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.1018 - val_loss: 0.1155 - 5s/epoch - 9ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0997 - val_loss: 0.1128 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0960 - val_loss: 0.1057 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0974 - val_loss: 0.1081 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0947 - val_loss: 0.1067 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0929 - val_loss: 0.1070 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0932 - val_loss: 0.1058 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0939 - val_loss: 0.1083 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0987 - val_loss: 0.1145 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0922 - val_loss: 0.1155 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0892 - val_loss: 0.1035 - 5s/epoch - 9ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0872 - val_loss: 0.0950 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0872 - val_loss: 0.1139 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0897 - val_loss: 0.0946 - 5s/epoch - 9ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0855 - val_loss: 0.0971 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0832 - val_loss: 0.1054 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0864 - val_loss: 0.0958 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0828 - val_loss: 0.0951 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0827 - val_loss: 0.1109 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0865 - val_loss: 0.0911 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0827 - val_loss: 0.0921 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0798 - val_loss: 0.0893 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0775 - val_loss: 0.0994 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 5s - loss: 0.0797 - val_loss: 0.0984 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0803 - val_loss: 0.0874 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0776 - val_loss: 0.0907 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0753 - val_loss: 0.0891 - 5s/epoch - 9ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0774 - val_loss: 0.0951 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 5s - loss: 0.0745 - val_loss: 0.0927 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 5s - loss: 0.0754 - val_loss: 0.0917 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 5s - loss: 0.0740 - val_loss: 0.0824 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 5s - loss: 0.0742 - val_loss: 0.0837 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 5s - loss: 0.0723 - val_loss: 0.0822 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 5s - loss: 0.0697 - val_loss: 0.0832 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 5s - loss: 0.0830 - val_loss: 0.1004 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 5s - loss: 0.0736 - val_loss: 0.0782 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 5s - loss: 0.0697 - val_loss: 0.0806 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 5s - loss: 0.0697 - val_loss: 0.0880 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 5s - loss: 0.0707 - val_loss: 0.0772 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 5s - loss: 0.0671 - val_loss: 0.0803 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 5s - loss: 0.0715 - val_loss: 0.0876 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 5s - loss: 0.0678 - val_loss: 0.0836 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 5s - loss: 0.0905 - val_loss: 0.0992 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 5s - loss: 0.0706 - val_loss: 0.0787 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "563/563 - 5s - loss: 0.0659 - val_loss: 0.0806 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "563/563 - 5s - loss: 0.0671 - val_loss: 0.0798 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "563/563 - 5s - loss: 0.0658 - val_loss: 0.0758 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "563/563 - 5s - loss: 0.0677 - val_loss: 0.0877 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "563/563 - 5s - loss: 0.0679 - val_loss: 0.0770 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "563/563 - 5s - loss: 0.0644 - val_loss: 0.0756 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "563/563 - 5s - loss: 0.0662 - val_loss: 0.0814 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "563/563 - 5s - loss: 0.0645 - val_loss: 0.0746 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "563/563 - 5s - loss: 0.0654 - val_loss: 0.0790 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "563/563 - 5s - loss: 0.0654 - val_loss: 0.0764 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "563/563 - 5s - loss: 0.0645 - val_loss: 0.0726 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "563/563 - 5s - loss: 0.0637 - val_loss: 0.0754 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "563/563 - 5s - loss: 0.0662 - val_loss: 0.0781 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "563/563 - 5s - loss: 0.0639 - val_loss: 0.0795 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "563/563 - 5s - loss: 0.0627 - val_loss: 0.0769 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "563/563 - 5s - loss: 0.0622 - val_loss: 0.0810 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "563/563 - 5s - loss: 0.0650 - val_loss: 0.0792 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "563/563 - 5s - loss: 0.0612 - val_loss: 0.0751 - 5s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "563/563 - 5s - loss: 0.0607 - val_loss: 0.0716 - 5s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "563/563 - 5s - loss: 0.0612 - val_loss: 0.0798 - 5s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "563/563 - 5s - loss: 0.0655 - val_loss: 0.0750 - 5s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "563/563 - 5s - loss: 0.0602 - val_loss: 0.0760 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "563/563 - 5s - loss: 0.0585 - val_loss: 0.0764 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "563/563 - 5s - loss: 0.0603 - val_loss: 0.0808 - 5s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "563/563 - 5s - loss: 0.0595 - val_loss: 0.0738 - 5s/epoch - 8ms/step\n",
      "Epoch 109/10000\n",
      "563/563 - 5s - loss: 0.0598 - val_loss: 0.0704 - 5s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "563/563 - 5s - loss: 0.0608 - val_loss: 0.0715 - 5s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "563/563 - 5s - loss: 0.0582 - val_loss: 0.0711 - 5s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "563/563 - 5s - loss: 0.0587 - val_loss: 0.0722 - 5s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "563/563 - 5s - loss: 0.0584 - val_loss: 0.0699 - 5s/epoch - 9ms/step\n",
      "Epoch 114/10000\n",
      "563/563 - 5s - loss: 0.0592 - val_loss: 0.0693 - 5s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "563/563 - 5s - loss: 0.0570 - val_loss: 0.0704 - 5s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "563/563 - 5s - loss: 0.0594 - val_loss: 0.0816 - 5s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "563/563 - 5s - loss: 0.0578 - val_loss: 0.0703 - 5s/epoch - 8ms/step\n",
      "Epoch 118/10000\n",
      "563/563 - 5s - loss: 0.0565 - val_loss: 0.0702 - 5s/epoch - 8ms/step\n",
      "Epoch 119/10000\n",
      "563/563 - 5s - loss: 0.0585 - val_loss: 0.0672 - 5s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "563/563 - 5s - loss: 0.0563 - val_loss: 0.0691 - 5s/epoch - 8ms/step\n",
      "Epoch 121/10000\n",
      "563/563 - 5s - loss: 0.0653 - val_loss: 0.0874 - 5s/epoch - 8ms/step\n",
      "Epoch 122/10000\n",
      "563/563 - 5s - loss: 0.0596 - val_loss: 0.0668 - 5s/epoch - 8ms/step\n",
      "Epoch 123/10000\n",
      "563/563 - 5s - loss: 0.0547 - val_loss: 0.0659 - 5s/epoch - 8ms/step\n",
      "Epoch 124/10000\n",
      "563/563 - 5s - loss: 0.0558 - val_loss: 0.0679 - 5s/epoch - 8ms/step\n",
      "Epoch 125/10000\n",
      "563/563 - 5s - loss: 0.0563 - val_loss: 0.0673 - 5s/epoch - 8ms/step\n",
      "Epoch 126/10000\n",
      "563/563 - 5s - loss: 0.0558 - val_loss: 0.0658 - 5s/epoch - 8ms/step\n",
      "Epoch 127/10000\n",
      "563/563 - 5s - loss: 0.0553 - val_loss: 0.0684 - 5s/epoch - 8ms/step\n",
      "Epoch 128/10000\n",
      "563/563 - 5s - loss: 0.0561 - val_loss: 0.0673 - 5s/epoch - 8ms/step\n",
      "Epoch 129/10000\n",
      "563/563 - 5s - loss: 0.0564 - val_loss: 0.0676 - 5s/epoch - 8ms/step\n",
      "Epoch 130/10000\n",
      "563/563 - 5s - loss: 0.0580 - val_loss: 0.0673 - 5s/epoch - 8ms/step\n",
      "Epoch 131/10000\n",
      "563/563 - 5s - loss: 0.0532 - val_loss: 0.0640 - 5s/epoch - 8ms/step\n",
      "Epoch 132/10000\n",
      "563/563 - 5s - loss: 0.0543 - val_loss: 0.0625 - 5s/epoch - 8ms/step\n",
      "Epoch 133/10000\n",
      "563/563 - 5s - loss: 0.0561 - val_loss: 0.0631 - 5s/epoch - 8ms/step\n",
      "Epoch 134/10000\n",
      "563/563 - 5s - loss: 0.0527 - val_loss: 0.0647 - 5s/epoch - 8ms/step\n",
      "Epoch 135/10000\n",
      "563/563 - 5s - loss: 0.0549 - val_loss: 0.0659 - 5s/epoch - 8ms/step\n",
      "Epoch 136/10000\n",
      "563/563 - 5s - loss: 0.0520 - val_loss: 0.0628 - 5s/epoch - 8ms/step\n",
      "Epoch 137/10000\n",
      "563/563 - 5s - loss: 0.0566 - val_loss: 0.0889 - 5s/epoch - 8ms/step\n",
      "Epoch 138/10000\n",
      "563/563 - 5s - loss: 0.0552 - val_loss: 0.0635 - 5s/epoch - 8ms/step\n",
      "Epoch 139/10000\n",
      "563/563 - 5s - loss: 0.0521 - val_loss: 0.0677 - 5s/epoch - 8ms/step\n",
      "Epoch 140/10000\n",
      "563/563 - 5s - loss: 0.0536 - val_loss: 0.0649 - 5s/epoch - 8ms/step\n",
      "Epoch 141/10000\n",
      "563/563 - 5s - loss: 0.0524 - val_loss: 0.0649 - 5s/epoch - 8ms/step\n",
      "Epoch 142/10000\n",
      "563/563 - 5s - loss: 0.0529 - val_loss: 0.0615 - 5s/epoch - 8ms/step\n",
      "Epoch 143/10000\n",
      "563/563 - 5s - loss: 0.0522 - val_loss: 0.0663 - 5s/epoch - 8ms/step\n",
      "Epoch 144/10000\n",
      "563/563 - 5s - loss: 0.0543 - val_loss: 0.0676 - 5s/epoch - 8ms/step\n",
      "Epoch 145/10000\n",
      "563/563 - 5s - loss: 0.0526 - val_loss: 0.0648 - 5s/epoch - 8ms/step\n",
      "Epoch 146/10000\n",
      "563/563 - 5s - loss: 0.0517 - val_loss: 0.0630 - 5s/epoch - 8ms/step\n",
      "Epoch 147/10000\n",
      "563/563 - 5s - loss: 0.0511 - val_loss: 0.0649 - 5s/epoch - 9ms/step\n",
      "Epoch 148/10000\n",
      "563/563 - 5s - loss: 0.0528 - val_loss: 0.0640 - 5s/epoch - 8ms/step\n",
      "Epoch 149/10000\n",
      "563/563 - 5s - loss: 0.0517 - val_loss: 0.0638 - 5s/epoch - 8ms/step\n",
      "Epoch 150/10000\n",
      "563/563 - 5s - loss: 0.0515 - val_loss: 0.0609 - 5s/epoch - 8ms/step\n",
      "Epoch 151/10000\n",
      "563/563 - 5s - loss: 0.0524 - val_loss: 0.0670 - 5s/epoch - 8ms/step\n",
      "Epoch 152/10000\n",
      "563/563 - 5s - loss: 0.0525 - val_loss: 0.0625 - 5s/epoch - 8ms/step\n",
      "Epoch 153/10000\n",
      "563/563 - 5s - loss: 0.0511 - val_loss: 0.0598 - 5s/epoch - 8ms/step\n",
      "Epoch 154/10000\n",
      "563/563 - 5s - loss: 0.0509 - val_loss: 0.0633 - 5s/epoch - 8ms/step\n",
      "Epoch 155/10000\n",
      "563/563 - 5s - loss: 0.0511 - val_loss: 0.0625 - 5s/epoch - 8ms/step\n",
      "Epoch 156/10000\n",
      "563/563 - 5s - loss: 0.0523 - val_loss: 0.0658 - 5s/epoch - 8ms/step\n",
      "Epoch 157/10000\n",
      "563/563 - 5s - loss: 0.0521 - val_loss: 0.0643 - 5s/epoch - 8ms/step\n",
      "Epoch 158/10000\n",
      "563/563 - 5s - loss: 0.0492 - val_loss: 0.0587 - 5s/epoch - 8ms/step\n",
      "Epoch 159/10000\n",
      "563/563 - 5s - loss: 0.0518 - val_loss: 0.0680 - 5s/epoch - 8ms/step\n",
      "Epoch 160/10000\n",
      "563/563 - 5s - loss: 0.0538 - val_loss: 0.0604 - 5s/epoch - 8ms/step\n",
      "Epoch 161/10000\n",
      "563/563 - 5s - loss: 0.0505 - val_loss: 0.0613 - 5s/epoch - 8ms/step\n",
      "Epoch 162/10000\n",
      "563/563 - 5s - loss: 0.0490 - val_loss: 0.0594 - 5s/epoch - 8ms/step\n",
      "Epoch 163/10000\n",
      "563/563 - 5s - loss: 0.0501 - val_loss: 0.0617 - 5s/epoch - 9ms/step\n",
      "Epoch 164/10000\n",
      "563/563 - 5s - loss: 0.0515 - val_loss: 0.0631 - 5s/epoch - 8ms/step\n",
      "Epoch 165/10000\n",
      "563/563 - 5s - loss: 0.0493 - val_loss: 0.0601 - 5s/epoch - 8ms/step\n",
      "Epoch 166/10000\n",
      "563/563 - 5s - loss: 0.0491 - val_loss: 0.0763 - 5s/epoch - 8ms/step\n",
      "Epoch 167/10000\n",
      "563/563 - 5s - loss: 0.0589 - val_loss: 0.0793 - 5s/epoch - 8ms/step\n",
      "Epoch 168/10000\n",
      "563/563 - 5s - loss: 0.0614 - val_loss: 0.0696 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_220_layer_call_fn, gru_cell_220_layer_call_and_return_conditional_losses, gru_cell_221_layer_call_fn, gru_cell_221_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_30_1_50_1_50_datt_seq2seq_gru_5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_30_1_50_1_50_datt_seq2seq_gru_5\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B4EC319D0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B651D9A30> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.874899</td>\n",
       "      <td>0.88871</td>\n",
       "      <td>0.904399</td>\n",
       "      <td>0.889336</td>\n",
       "      <td>6.971855</td>\n",
       "      <td>5.851452</td>\n",
       "      <td>4.740554</td>\n",
       "      <td>5.85462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.804375</td>\n",
       "      <td>0.84586</td>\n",
       "      <td>0.847799</td>\n",
       "      <td>0.832678</td>\n",
       "      <td>8.716338</td>\n",
       "      <td>6.88691</td>\n",
       "      <td>5.983272</td>\n",
       "      <td>7.195507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.709005</td>\n",
       "      <td>0.812466</td>\n",
       "      <td>0.794134</td>\n",
       "      <td>0.771869</td>\n",
       "      <td>10.629361</td>\n",
       "      <td>7.597067</td>\n",
       "      <td>6.960569</td>\n",
       "      <td>8.395665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.604089</td>\n",
       "      <td>0.783138</td>\n",
       "      <td>0.754575</td>\n",
       "      <td>0.713934</td>\n",
       "      <td>12.398027</td>\n",
       "      <td>8.170696</td>\n",
       "      <td>7.601635</td>\n",
       "      <td>9.390119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.498187</td>\n",
       "      <td>0.755612</td>\n",
       "      <td>0.720303</td>\n",
       "      <td>0.658034</td>\n",
       "      <td>13.957955</td>\n",
       "      <td>8.675501</td>\n",
       "      <td>8.116138</td>\n",
       "      <td>10.249865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.395906</td>\n",
       "      <td>0.729809</td>\n",
       "      <td>0.679067</td>\n",
       "      <td>0.601594</td>\n",
       "      <td>15.315316</td>\n",
       "      <td>9.124695</td>\n",
       "      <td>8.694322</td>\n",
       "      <td>11.044778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.299667</td>\n",
       "      <td>0.705054</td>\n",
       "      <td>0.623937</td>\n",
       "      <td>0.542886</td>\n",
       "      <td>16.492021</td>\n",
       "      <td>9.536538</td>\n",
       "      <td>9.412327</td>\n",
       "      <td>11.813629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.217059</td>\n",
       "      <td>0.680236</td>\n",
       "      <td>0.563849</td>\n",
       "      <td>0.487048</td>\n",
       "      <td>17.440802</td>\n",
       "      <td>9.932868</td>\n",
       "      <td>10.137989</td>\n",
       "      <td>12.503886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.150918</td>\n",
       "      <td>0.650145</td>\n",
       "      <td>0.519969</td>\n",
       "      <td>0.440344</td>\n",
       "      <td>18.166779</td>\n",
       "      <td>10.393994</td>\n",
       "      <td>10.637457</td>\n",
       "      <td>13.066077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.095941</td>\n",
       "      <td>0.613548</td>\n",
       "      <td>0.493201</td>\n",
       "      <td>0.400897</td>\n",
       "      <td>18.752313</td>\n",
       "      <td>10.927731</td>\n",
       "      <td>10.932442</td>\n",
       "      <td>13.537496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.04713</td>\n",
       "      <td>0.57838</td>\n",
       "      <td>0.466256</td>\n",
       "      <td>0.363922</td>\n",
       "      <td>19.260682</td>\n",
       "      <td>11.417274</td>\n",
       "      <td>11.22204</td>\n",
       "      <td>13.966665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.546024</td>\n",
       "      <td>0.435844</td>\n",
       "      <td>0.327899</td>\n",
       "      <td>19.359093</td>\n",
       "      <td>11.851072</td>\n",
       "      <td>11.540713</td>\n",
       "      <td>14.250292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.039427</td>\n",
       "      <td>0.516092</td>\n",
       "      <td>0.403506</td>\n",
       "      <td>0.29339</td>\n",
       "      <td>18.894607</td>\n",
       "      <td>12.240239</td>\n",
       "      <td>11.870341</td>\n",
       "      <td>14.335062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.076255</td>\n",
       "      <td>0.490048</td>\n",
       "      <td>0.365861</td>\n",
       "      <td>0.259885</td>\n",
       "      <td>18.584281</td>\n",
       "      <td>12.569736</td>\n",
       "      <td>12.24188</td>\n",
       "      <td>14.465299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.107612</td>\n",
       "      <td>0.469337</td>\n",
       "      <td>0.329063</td>\n",
       "      <td>0.230263</td>\n",
       "      <td>18.551247</td>\n",
       "      <td>12.828157</td>\n",
       "      <td>12.594915</td>\n",
       "      <td>14.658106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.134891</td>\n",
       "      <td>0.449884</td>\n",
       "      <td>0.301454</td>\n",
       "      <td>0.205482</td>\n",
       "      <td>18.714939</td>\n",
       "      <td>13.067077</td>\n",
       "      <td>12.853675</td>\n",
       "      <td>14.878564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.16009</td>\n",
       "      <td>0.432191</td>\n",
       "      <td>0.279955</td>\n",
       "      <td>0.184019</td>\n",
       "      <td>18.550513</td>\n",
       "      <td>13.278553</td>\n",
       "      <td>13.052628</td>\n",
       "      <td>14.960565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.184212</td>\n",
       "      <td>0.41372</td>\n",
       "      <td>0.258283</td>\n",
       "      <td>0.162597</td>\n",
       "      <td>18.384077</td>\n",
       "      <td>13.495826</td>\n",
       "      <td>13.250178</td>\n",
       "      <td>15.04336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.20737</td>\n",
       "      <td>0.394159</td>\n",
       "      <td>0.232985</td>\n",
       "      <td>0.139924</td>\n",
       "      <td>18.288683</td>\n",
       "      <td>13.721514</td>\n",
       "      <td>13.476654</td>\n",
       "      <td>15.162284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.229693</td>\n",
       "      <td>0.373505</td>\n",
       "      <td>0.206884</td>\n",
       "      <td>0.116899</td>\n",
       "      <td>18.188606</td>\n",
       "      <td>13.953638</td>\n",
       "      <td>13.706127</td>\n",
       "      <td>15.28279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.250522</td>\n",
       "      <td>0.352301</td>\n",
       "      <td>0.183011</td>\n",
       "      <td>0.09493</td>\n",
       "      <td>18.149495</td>\n",
       "      <td>14.188596</td>\n",
       "      <td>13.912924</td>\n",
       "      <td>15.417005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.269469</td>\n",
       "      <td>0.331107</td>\n",
       "      <td>0.164127</td>\n",
       "      <td>0.075255</td>\n",
       "      <td>18.166929</td>\n",
       "      <td>14.418009</td>\n",
       "      <td>14.075145</td>\n",
       "      <td>15.553361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.286966</td>\n",
       "      <td>0.308069</td>\n",
       "      <td>0.154157</td>\n",
       "      <td>0.05842</td>\n",
       "      <td>18.037955</td>\n",
       "      <td>14.663131</td>\n",
       "      <td>14.162173</td>\n",
       "      <td>15.621087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.304734</td>\n",
       "      <td>0.283714</td>\n",
       "      <td>0.147666</td>\n",
       "      <td>0.042215</td>\n",
       "      <td>17.91484</td>\n",
       "      <td>14.920153</td>\n",
       "      <td>14.220758</td>\n",
       "      <td>15.685251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.322136</td>\n",
       "      <td>0.257291</td>\n",
       "      <td>0.138652</td>\n",
       "      <td>0.024602</td>\n",
       "      <td>17.849854</td>\n",
       "      <td>15.193437</td>\n",
       "      <td>14.300038</td>\n",
       "      <td>15.78111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.33699</td>\n",
       "      <td>0.229515</td>\n",
       "      <td>0.124646</td>\n",
       "      <td>0.005724</td>\n",
       "      <td>17.770367</td>\n",
       "      <td>15.474141</td>\n",
       "      <td>14.420838</td>\n",
       "      <td>15.888449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.349049</td>\n",
       "      <td>0.202224</td>\n",
       "      <td>0.105047</td>\n",
       "      <td>-0.013926</td>\n",
       "      <td>17.676563</td>\n",
       "      <td>15.743461</td>\n",
       "      <td>14.586911</td>\n",
       "      <td>16.002312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.362342</td>\n",
       "      <td>0.174092</td>\n",
       "      <td>0.085864</td>\n",
       "      <td>-0.034129</td>\n",
       "      <td>17.720006</td>\n",
       "      <td>16.015413</td>\n",
       "      <td>14.748583</td>\n",
       "      <td>16.161334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.37623</td>\n",
       "      <td>0.14781</td>\n",
       "      <td>0.072784</td>\n",
       "      <td>-0.051879</td>\n",
       "      <td>17.831659</td>\n",
       "      <td>16.266048</td>\n",
       "      <td>14.859934</td>\n",
       "      <td>16.319214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.389504</td>\n",
       "      <td>0.120258</td>\n",
       "      <td>0.06926</td>\n",
       "      <td>-0.066662</td>\n",
       "      <td>17.94257</td>\n",
       "      <td>16.525233</td>\n",
       "      <td>14.894026</td>\n",
       "      <td>16.453943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.010384</td>\n",
       "      <td>0.484477</td>\n",
       "      <td>0.380885</td>\n",
       "      <td>0.291915</td>\n",
       "      <td>16.822591</td>\n",
       "      <td>12.297605</td>\n",
       "      <td>11.773573</td>\n",
       "      <td>13.631257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.874899   0.88871  0.904399  0.889336   6.971855   5.851452   \n",
       "1      0.804375   0.84586  0.847799  0.832678   8.716338    6.88691   \n",
       "2      0.709005  0.812466  0.794134  0.771869  10.629361   7.597067   \n",
       "3      0.604089  0.783138  0.754575  0.713934  12.398027   8.170696   \n",
       "4      0.498187  0.755612  0.720303  0.658034  13.957955   8.675501   \n",
       "5      0.395906  0.729809  0.679067  0.601594  15.315316   9.124695   \n",
       "6      0.299667  0.705054  0.623937  0.542886  16.492021   9.536538   \n",
       "7      0.217059  0.680236  0.563849  0.487048  17.440802   9.932868   \n",
       "8      0.150918  0.650145  0.519969  0.440344  18.166779  10.393994   \n",
       "9      0.095941  0.613548  0.493201  0.400897  18.752313  10.927731   \n",
       "10      0.04713   0.57838  0.466256  0.363922  19.260682  11.417274   \n",
       "11     0.001829  0.546024  0.435844  0.327899  19.359093  11.851072   \n",
       "12    -0.039427  0.516092  0.403506   0.29339  18.894607  12.240239   \n",
       "13    -0.076255  0.490048  0.365861  0.259885  18.584281  12.569736   \n",
       "14    -0.107612  0.469337  0.329063  0.230263  18.551247  12.828157   \n",
       "15    -0.134891  0.449884  0.301454  0.205482  18.714939  13.067077   \n",
       "16     -0.16009  0.432191  0.279955  0.184019  18.550513  13.278553   \n",
       "17    -0.184212   0.41372  0.258283  0.162597  18.384077  13.495826   \n",
       "18     -0.20737  0.394159  0.232985  0.139924  18.288683  13.721514   \n",
       "19    -0.229693  0.373505  0.206884  0.116899  18.188606  13.953638   \n",
       "20    -0.250522  0.352301  0.183011   0.09493  18.149495  14.188596   \n",
       "21    -0.269469  0.331107  0.164127  0.075255  18.166929  14.418009   \n",
       "22    -0.286966  0.308069  0.154157   0.05842  18.037955  14.663131   \n",
       "23    -0.304734  0.283714  0.147666  0.042215   17.91484  14.920153   \n",
       "24    -0.322136  0.257291  0.138652  0.024602  17.849854  15.193437   \n",
       "25     -0.33699  0.229515  0.124646  0.005724  17.770367  15.474141   \n",
       "26    -0.349049  0.202224  0.105047 -0.013926  17.676563  15.743461   \n",
       "27    -0.362342  0.174092  0.085864 -0.034129  17.720006  16.015413   \n",
       "28     -0.37623   0.14781  0.072784 -0.051879  17.831659  16.266048   \n",
       "29    -0.389504  0.120258   0.06926 -0.066662   17.94257  16.525233   \n",
       "mean   0.010384  0.484477  0.380885  0.291915  16.822591  12.297605   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       4.740554    5.85462  \n",
       "1       5.983272   7.195507  \n",
       "2       6.960569   8.395665  \n",
       "3       7.601635   9.390119  \n",
       "4       8.116138  10.249865  \n",
       "5       8.694322  11.044778  \n",
       "6       9.412327  11.813629  \n",
       "7      10.137989  12.503886  \n",
       "8      10.637457  13.066077  \n",
       "9      10.932442  13.537496  \n",
       "10      11.22204  13.966665  \n",
       "11     11.540713  14.250292  \n",
       "12     11.870341  14.335062  \n",
       "13      12.24188  14.465299  \n",
       "14     12.594915  14.658106  \n",
       "15     12.853675  14.878564  \n",
       "16     13.052628  14.960565  \n",
       "17     13.250178   15.04336  \n",
       "18     13.476654  15.162284  \n",
       "19     13.706127   15.28279  \n",
       "20     13.912924  15.417005  \n",
       "21     14.075145  15.553361  \n",
       "22     14.162173  15.621087  \n",
       "23     14.220758  15.685251  \n",
       "24     14.300038   15.78111  \n",
       "25     14.420838  15.888449  \n",
       "26     14.586911  16.002312  \n",
       "27     14.748583  16.161334  \n",
       "28     14.859934  16.319214  \n",
       "29     14.894026  16.453943  \n",
       "mean   11.773573  13.631257  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/30_30_1_50_1_50_datt_seq2seq_gru_5.csv\n",
      "\n",
      "\n",
      "5th iteration\n",
      "history size: 40\n",
      "future size: 10\n",
      "Epoch 1/10000\n",
      "568/568 - 8s - loss: 0.2365 - val_loss: 0.1695 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "568/568 - 5s - loss: 0.1470 - val_loss: 0.1532 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "568/568 - 4s - loss: 0.1347 - val_loss: 0.1379 - 4s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "568/568 - 4s - loss: 0.1262 - val_loss: 0.1277 - 4s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "568/568 - 4s - loss: 0.1166 - val_loss: 0.1193 - 4s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "568/568 - 4s - loss: 0.1082 - val_loss: 0.1141 - 4s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "568/568 - 5s - loss: 0.1040 - val_loss: 0.1090 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "568/568 - 5s - loss: 0.0994 - val_loss: 0.1088 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "568/568 - 4s - loss: 0.0956 - val_loss: 0.1025 - 4s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "568/568 - 4s - loss: 0.0924 - val_loss: 0.1017 - 4s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "568/568 - 4s - loss: 0.0904 - val_loss: 0.0995 - 4s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "568/568 - 4s - loss: 0.0872 - val_loss: 0.0987 - 4s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "568/568 - 4s - loss: 0.0842 - val_loss: 0.0990 - 4s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "568/568 - 4s - loss: 0.0796 - val_loss: 0.0917 - 4s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "568/568 - 5s - loss: 0.0812 - val_loss: 0.0933 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "568/568 - 4s - loss: 0.0776 - val_loss: 0.0901 - 4s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "568/568 - 4s - loss: 0.0751 - val_loss: 0.0868 - 4s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "568/568 - 4s - loss: 0.0747 - val_loss: 0.0866 - 4s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "568/568 - 5s - loss: 0.0716 - val_loss: 0.0857 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "568/568 - 4s - loss: 0.0700 - val_loss: 0.0813 - 4s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "568/568 - 4s - loss: 0.0684 - val_loss: 0.0814 - 4s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "568/568 - 4s - loss: 0.0665 - val_loss: 0.0853 - 4s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "568/568 - 4s - loss: 0.0682 - val_loss: 0.0803 - 4s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "568/568 - 4s - loss: 0.0655 - val_loss: 0.0777 - 4s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "568/568 - 4s - loss: 0.0637 - val_loss: 0.0806 - 4s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "568/568 - 4s - loss: 0.0651 - val_loss: 0.0791 - 4s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "568/568 - 4s - loss: 0.0606 - val_loss: 0.0780 - 4s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "568/568 - 4s - loss: 0.0610 - val_loss: 0.0746 - 4s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "568/568 - 4s - loss: 0.0588 - val_loss: 0.0796 - 4s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "568/568 - 5s - loss: 0.0590 - val_loss: 0.0739 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "568/568 - 4s - loss: 0.0572 - val_loss: 0.0931 - 4s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "568/568 - 4s - loss: 0.0586 - val_loss: 0.0711 - 4s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "568/568 - 5s - loss: 0.0566 - val_loss: 0.0704 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "568/568 - 5s - loss: 0.0545 - val_loss: 0.0697 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "568/568 - 5s - loss: 0.0539 - val_loss: 0.0679 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "568/568 - 4s - loss: 0.0547 - val_loss: 0.0671 - 4s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "568/568 - 4s - loss: 0.0551 - val_loss: 0.0725 - 4s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "568/568 - 4s - loss: 0.0534 - val_loss: 0.0634 - 4s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "568/568 - 4s - loss: 0.0506 - val_loss: 0.0669 - 4s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "568/568 - 4s - loss: 0.0503 - val_loss: 0.0659 - 4s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "568/568 - 4s - loss: 0.0506 - val_loss: 0.0686 - 4s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "568/568 - 4s - loss: 0.0518 - val_loss: 0.0643 - 4s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "568/568 - 4s - loss: 0.0495 - val_loss: 0.0784 - 4s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "568/568 - 4s - loss: 0.0540 - val_loss: 0.0621 - 4s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "568/568 - 4s - loss: 0.0481 - val_loss: 0.0645 - 4s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "568/568 - 5s - loss: 0.0469 - val_loss: 0.0634 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "568/568 - 4s - loss: 0.0478 - val_loss: 0.0626 - 4s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "568/568 - 4s - loss: 0.0464 - val_loss: 0.0724 - 4s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "568/568 - 4s - loss: 0.0500 - val_loss: 0.0656 - 4s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "568/568 - 5s - loss: 0.0452 - val_loss: 0.0592 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "568/568 - 4s - loss: 0.0458 - val_loss: 0.0578 - 4s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "568/568 - 5s - loss: 0.0446 - val_loss: 0.0601 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "568/568 - 5s - loss: 0.0468 - val_loss: 0.0590 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "568/568 - 4s - loss: 0.0457 - val_loss: 0.0614 - 4s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "568/568 - 4s - loss: 0.0443 - val_loss: 0.0615 - 4s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "568/568 - 4s - loss: 0.0429 - val_loss: 0.0544 - 4s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "568/568 - 4s - loss: 0.0431 - val_loss: 0.0559 - 4s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "568/568 - 4s - loss: 0.0429 - val_loss: 0.0600 - 4s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "568/568 - 5s - loss: 0.0438 - val_loss: 0.0641 - 5s/epoch - 9ms/step\n",
      "Epoch 60/10000\n",
      "568/568 - 5s - loss: 0.0432 - val_loss: 0.0537 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "568/568 - 4s - loss: 0.0408 - val_loss: 0.0554 - 4s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "568/568 - 4s - loss: 0.0410 - val_loss: 0.0603 - 4s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "568/568 - 4s - loss: 0.0440 - val_loss: 0.0569 - 4s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "568/568 - 4s - loss: 0.0403 - val_loss: 0.0592 - 4s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "568/568 - 4s - loss: 0.0422 - val_loss: 0.0542 - 4s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "568/568 - 4s - loss: 0.0396 - val_loss: 0.0595 - 4s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "568/568 - 4s - loss: 0.0418 - val_loss: 0.0555 - 4s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "568/568 - 5s - loss: 0.0394 - val_loss: 0.0519 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "568/568 - 4s - loss: 0.0385 - val_loss: 0.0578 - 4s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "568/568 - 4s - loss: 0.0395 - val_loss: 0.0568 - 4s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "568/568 - 4s - loss: 0.0402 - val_loss: 0.0521 - 4s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "568/568 - 4s - loss: 0.0377 - val_loss: 0.0533 - 4s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "568/568 - 5s - loss: 0.0375 - val_loss: 0.0518 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "568/568 - 5s - loss: 0.0371 - val_loss: 0.0506 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "568/568 - 5s - loss: 0.0382 - val_loss: 0.0508 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "568/568 - 4s - loss: 0.0363 - val_loss: 0.0495 - 4s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "568/568 - 5s - loss: 0.0356 - val_loss: 0.0504 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "568/568 - 4s - loss: 0.0358 - val_loss: 0.0499 - 4s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "568/568 - 4s - loss: 0.0366 - val_loss: 0.0506 - 4s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "568/568 - 4s - loss: 0.0349 - val_loss: 0.0481 - 4s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "568/568 - 4s - loss: 0.0348 - val_loss: 0.0503 - 4s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "568/568 - 4s - loss: 0.0359 - val_loss: 0.0523 - 4s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "568/568 - 4s - loss: 0.0342 - val_loss: 0.0506 - 4s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "568/568 - 4s - loss: 0.0334 - val_loss: 0.0490 - 4s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "568/568 - 4s - loss: 0.0330 - val_loss: 0.0495 - 4s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "568/568 - 5s - loss: 0.0331 - val_loss: 0.0506 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "568/568 - 4s - loss: 0.0334 - val_loss: 0.0478 - 4s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "568/568 - 4s - loss: 0.0327 - val_loss: 0.0471 - 4s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "568/568 - 4s - loss: 0.0333 - val_loss: 0.0477 - 4s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "568/568 - 4s - loss: 0.0329 - val_loss: 0.0540 - 4s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "568/568 - 4s - loss: 0.0356 - val_loss: 0.0499 - 4s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "568/568 - 4s - loss: 0.0360 - val_loss: 0.0563 - 4s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "568/568 - 4s - loss: 0.0317 - val_loss: 0.0469 - 4s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "568/568 - 4s - loss: 0.0302 - val_loss: 0.0448 - 4s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "568/568 - 4s - loss: 0.0317 - val_loss: 0.0585 - 4s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "568/568 - 4s - loss: 0.0353 - val_loss: 0.0465 - 4s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "568/568 - 5s - loss: 0.0305 - val_loss: 0.0466 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "568/568 - 4s - loss: 0.0301 - val_loss: 0.0443 - 4s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "568/568 - 5s - loss: 0.0321 - val_loss: 0.0504 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "568/568 - 4s - loss: 0.0315 - val_loss: 0.0454 - 4s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "568/568 - 4s - loss: 0.0306 - val_loss: 0.0462 - 4s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "568/568 - 4s - loss: 0.0304 - val_loss: 0.0428 - 4s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "568/568 - 4s - loss: 0.0296 - val_loss: 0.0442 - 4s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "568/568 - 4s - loss: 0.0297 - val_loss: 0.0465 - 4s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "568/568 - 4s - loss: 0.0299 - val_loss: 0.0436 - 4s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "568/568 - 4s - loss: 0.0304 - val_loss: 0.0496 - 4s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "568/568 - 4s - loss: 0.0314 - val_loss: 0.0495 - 4s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "568/568 - 4s - loss: 0.0293 - val_loss: 0.0443 - 4s/epoch - 8ms/step\n",
      "Epoch 109/10000\n",
      "568/568 - 4s - loss: 0.0287 - val_loss: 0.0423 - 4s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "568/568 - 4s - loss: 0.0302 - val_loss: 0.0437 - 4s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "568/568 - 4s - loss: 0.0289 - val_loss: 0.0437 - 4s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "568/568 - 4s - loss: 0.0288 - val_loss: 0.0434 - 4s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "568/568 - 5s - loss: 0.0285 - val_loss: 0.0433 - 5s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "568/568 - 4s - loss: 0.0300 - val_loss: 0.0436 - 4s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "568/568 - 4s - loss: 0.0288 - val_loss: 0.0436 - 4s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "568/568 - 5s - loss: 0.0286 - val_loss: 0.0458 - 5s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "568/568 - 5s - loss: 0.0287 - val_loss: 0.0442 - 5s/epoch - 8ms/step\n",
      "Epoch 118/10000\n",
      "568/568 - 5s - loss: 0.0282 - val_loss: 0.0483 - 5s/epoch - 8ms/step\n",
      "Epoch 119/10000\n",
      "568/568 - 5s - loss: 0.0310 - val_loss: 0.0476 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_222_layer_call_fn, gru_cell_222_layer_call_and_return_conditional_losses, gru_cell_223_layer_call_fn, gru_cell_223_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_10_1_50_1_50_datt_seq2seq_gru_5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_10_1_50_1_50_datt_seq2seq_gru_5\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B4F31A0A0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B590B58B0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.932549</td>\n",
       "      <td>0.933064</td>\n",
       "      <td>0.951588</td>\n",
       "      <td>0.939067</td>\n",
       "      <td>4.214658</td>\n",
       "      <td>4.553691</td>\n",
       "      <td>3.38768</td>\n",
       "      <td>4.05201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.866398</td>\n",
       "      <td>0.863267</td>\n",
       "      <td>0.901757</td>\n",
       "      <td>0.877141</td>\n",
       "      <td>5.895065</td>\n",
       "      <td>6.509017</td>\n",
       "      <td>4.826485</td>\n",
       "      <td>5.743522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.769604</td>\n",
       "      <td>0.800454</td>\n",
       "      <td>0.843243</td>\n",
       "      <td>0.804434</td>\n",
       "      <td>7.636878</td>\n",
       "      <td>7.863884</td>\n",
       "      <td>6.097538</td>\n",
       "      <td>7.199433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.66369</td>\n",
       "      <td>0.756628</td>\n",
       "      <td>0.798292</td>\n",
       "      <td>0.739537</td>\n",
       "      <td>9.10437</td>\n",
       "      <td>8.685872</td>\n",
       "      <td>6.917605</td>\n",
       "      <td>8.235949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.559252</td>\n",
       "      <td>0.722766</td>\n",
       "      <td>0.770821</td>\n",
       "      <td>0.68428</td>\n",
       "      <td>10.319654</td>\n",
       "      <td>9.271674</td>\n",
       "      <td>7.374134</td>\n",
       "      <td>8.988487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.460544</td>\n",
       "      <td>0.703049</td>\n",
       "      <td>0.762434</td>\n",
       "      <td>0.642009</td>\n",
       "      <td>11.305423</td>\n",
       "      <td>9.596702</td>\n",
       "      <td>7.50867</td>\n",
       "      <td>9.470265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.372357</td>\n",
       "      <td>0.69529</td>\n",
       "      <td>0.768665</td>\n",
       "      <td>0.612104</td>\n",
       "      <td>12.077113</td>\n",
       "      <td>9.721154</td>\n",
       "      <td>7.41086</td>\n",
       "      <td>9.736376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.299437</td>\n",
       "      <td>0.686768</td>\n",
       "      <td>0.78244</td>\n",
       "      <td>0.589548</td>\n",
       "      <td>12.728502</td>\n",
       "      <td>9.855423</td>\n",
       "      <td>7.1889</td>\n",
       "      <td>9.924275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.242435</td>\n",
       "      <td>0.673419</td>\n",
       "      <td>0.792132</td>\n",
       "      <td>0.569329</td>\n",
       "      <td>13.25169</td>\n",
       "      <td>10.062098</td>\n",
       "      <td>7.029178</td>\n",
       "      <td>10.114322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.194375</td>\n",
       "      <td>0.657715</td>\n",
       "      <td>0.788255</td>\n",
       "      <td>0.546781</td>\n",
       "      <td>13.681291</td>\n",
       "      <td>10.299452</td>\n",
       "      <td>7.096905</td>\n",
       "      <td>10.359216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.536064</td>\n",
       "      <td>0.749242</td>\n",
       "      <td>0.815963</td>\n",
       "      <td>0.700423</td>\n",
       "      <td>10.021464</td>\n",
       "      <td>8.641897</td>\n",
       "      <td>6.483795</td>\n",
       "      <td>8.382386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE     nRMSE   \n",
       "0      0.932549  0.933064  0.951588  0.939067   4.214658   4.553691   3.38768   \n",
       "1      0.866398  0.863267  0.901757  0.877141   5.895065   6.509017  4.826485   \n",
       "2      0.769604  0.800454  0.843243  0.804434   7.636878   7.863884  6.097538   \n",
       "3       0.66369  0.756628  0.798292  0.739537    9.10437   8.685872  6.917605   \n",
       "4      0.559252  0.722766  0.770821   0.68428  10.319654   9.271674  7.374134   \n",
       "5      0.460544  0.703049  0.762434  0.642009  11.305423   9.596702   7.50867   \n",
       "6      0.372357   0.69529  0.768665  0.612104  12.077113   9.721154   7.41086   \n",
       "7      0.299437  0.686768   0.78244  0.589548  12.728502   9.855423    7.1889   \n",
       "8      0.242435  0.673419  0.792132  0.569329   13.25169  10.062098  7.029178   \n",
       "9      0.194375  0.657715  0.788255  0.546781  13.681291  10.299452  7.096905   \n",
       "mean   0.536064  0.749242  0.815963  0.700423  10.021464   8.641897  6.483795   \n",
       "\n",
       "            mean  \n",
       "index      nRMSE  \n",
       "0        4.05201  \n",
       "1       5.743522  \n",
       "2       7.199433  \n",
       "3       8.235949  \n",
       "4       8.988487  \n",
       "5       9.470265  \n",
       "6       9.736376  \n",
       "7       9.924275  \n",
       "8      10.114322  \n",
       "9      10.359216  \n",
       "mean    8.382386  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/40_10_1_50_1_50_datt_seq2seq_gru_5.csv\n",
      "\n",
      "\n",
      "5th iteration\n",
      "history size: 40\n",
      "future size: 20\n",
      "Epoch 1/10000\n",
      "563/563 - 8s - loss: 0.3123 - val_loss: 0.2546 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.2219 - val_loss: 0.2291 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.2058 - val_loss: 0.2223 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 5s - loss: 0.1936 - val_loss: 0.2046 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.1830 - val_loss: 0.1948 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.1715 - val_loss: 0.1908 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.1657 - val_loss: 0.1834 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.1589 - val_loss: 0.1773 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.1494 - val_loss: 0.1701 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.1439 - val_loss: 0.1604 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.1405 - val_loss: 0.1583 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.1349 - val_loss: 0.1598 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.1287 - val_loss: 0.1425 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.1266 - val_loss: 0.1523 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.1208 - val_loss: 0.1408 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.1176 - val_loss: 0.1399 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.1132 - val_loss: 0.1338 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.1124 - val_loss: 0.1232 - 5s/epoch - 9ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.1101 - val_loss: 0.1237 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.1039 - val_loss: 0.1212 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.1011 - val_loss: 0.1188 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.0987 - val_loss: 0.1162 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.0995 - val_loss: 0.1127 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.0935 - val_loss: 0.1128 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.0963 - val_loss: 0.1109 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.0940 - val_loss: 0.1120 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.0919 - val_loss: 0.1187 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0883 - val_loss: 0.1060 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.0872 - val_loss: 0.1044 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.0886 - val_loss: 0.1041 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.0837 - val_loss: 0.1049 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.0832 - val_loss: 0.1021 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.0815 - val_loss: 0.0992 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.0831 - val_loss: 0.1096 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.0811 - val_loss: 0.1079 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0811 - val_loss: 0.0958 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0778 - val_loss: 0.0985 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0824 - val_loss: 0.0975 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.0756 - val_loss: 0.0951 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0752 - val_loss: 0.0968 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0731 - val_loss: 0.0964 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0740 - val_loss: 0.0951 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0728 - val_loss: 0.0919 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0726 - val_loss: 0.0935 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0754 - val_loss: 0.0881 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0721 - val_loss: 0.0866 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0702 - val_loss: 0.0866 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0691 - val_loss: 0.0927 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0671 - val_loss: 0.0912 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0694 - val_loss: 0.0842 - 5s/epoch - 9ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0686 - val_loss: 0.0847 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0676 - val_loss: 0.0870 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0647 - val_loss: 0.0878 - 5s/epoch - 9ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0692 - val_loss: 0.0855 - 5s/epoch - 9ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0630 - val_loss: 0.0810 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0665 - val_loss: 0.0880 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0627 - val_loss: 0.0797 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0619 - val_loss: 0.0785 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0617 - val_loss: 0.0720 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0709 - val_loss: 0.0924 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0690 - val_loss: 0.0783 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0591 - val_loss: 0.0833 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 5s - loss: 0.0609 - val_loss: 0.0845 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0618 - val_loss: 0.0804 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0614 - val_loss: 0.0906 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0600 - val_loss: 0.0782 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0600 - val_loss: 0.0751 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 5s - loss: 0.0592 - val_loss: 0.0688 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 5s - loss: 0.0564 - val_loss: 0.0785 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 5s - loss: 0.0574 - val_loss: 0.0687 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 5s - loss: 0.0581 - val_loss: 0.0712 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 5s - loss: 0.0573 - val_loss: 0.0758 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 5s - loss: 0.0543 - val_loss: 0.0732 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 5s - loss: 0.0547 - val_loss: 0.0668 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 5s - loss: 0.0576 - val_loss: 0.0669 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 5s - loss: 0.0570 - val_loss: 0.0718 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 5s - loss: 0.0582 - val_loss: 0.0889 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 5s - loss: 0.0570 - val_loss: 0.0737 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 5s - loss: 0.0554 - val_loss: 0.0757 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 5s - loss: 0.0539 - val_loss: 0.0667 - 5s/epoch - 9ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 5s - loss: 0.0532 - val_loss: 0.0638 - 5s/epoch - 9ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 5s - loss: 0.0524 - val_loss: 0.0648 - 5s/epoch - 9ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 5s - loss: 0.0518 - val_loss: 0.0648 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "563/563 - 5s - loss: 0.0530 - val_loss: 0.0725 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "563/563 - 5s - loss: 0.0546 - val_loss: 0.0629 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "563/563 - 5s - loss: 0.0512 - val_loss: 0.0661 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "563/563 - 5s - loss: 0.0535 - val_loss: 0.0780 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "563/563 - 5s - loss: 0.0533 - val_loss: 0.0802 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "563/563 - 5s - loss: 0.0556 - val_loss: 0.0684 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "563/563 - 5s - loss: 0.0512 - val_loss: 0.0659 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "563/563 - 5s - loss: 0.0494 - val_loss: 0.0639 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "563/563 - 5s - loss: 0.0502 - val_loss: 0.0678 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "563/563 - 5s - loss: 0.0518 - val_loss: 0.0685 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "563/563 - 5s - loss: 0.0506 - val_loss: 0.0844 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "563/563 - 5s - loss: 0.0541 - val_loss: 0.0617 - 5s/epoch - 9ms/step\n",
      "Epoch 96/10000\n",
      "563/563 - 5s - loss: 0.0482 - val_loss: 0.0647 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "563/563 - 5s - loss: 0.0487 - val_loss: 0.0591 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "563/563 - 5s - loss: 0.0497 - val_loss: 0.0618 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "563/563 - 5s - loss: 0.0480 - val_loss: 0.0631 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "563/563 - 5s - loss: 0.0502 - val_loss: 0.0601 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "563/563 - 5s - loss: 0.0473 - val_loss: 0.0603 - 5s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "563/563 - 5s - loss: 0.0475 - val_loss: 0.0592 - 5s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "563/563 - 5s - loss: 0.0473 - val_loss: 0.0611 - 5s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "563/563 - 5s - loss: 0.0473 - val_loss: 0.0603 - 5s/epoch - 9ms/step\n",
      "Epoch 105/10000\n",
      "563/563 - 5s - loss: 0.0470 - val_loss: 0.0585 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "563/563 - 5s - loss: 0.0475 - val_loss: 0.0595 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "563/563 - 5s - loss: 0.0454 - val_loss: 0.0581 - 5s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "563/563 - 5s - loss: 0.0483 - val_loss: 0.0622 - 5s/epoch - 9ms/step\n",
      "Epoch 109/10000\n",
      "563/563 - 5s - loss: 0.0503 - val_loss: 0.0650 - 5s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "563/563 - 5s - loss: 0.0482 - val_loss: 0.0652 - 5s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "563/563 - 5s - loss: 0.0464 - val_loss: 0.0581 - 5s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "563/563 - 5s - loss: 0.0475 - val_loss: 0.0587 - 5s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "563/563 - 5s - loss: 0.0451 - val_loss: 0.0574 - 5s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "563/563 - 5s - loss: 0.0447 - val_loss: 0.0559 - 5s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "563/563 - 5s - loss: 0.0446 - val_loss: 0.0644 - 5s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "563/563 - 5s - loss: 0.0461 - val_loss: 0.0572 - 5s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "563/563 - 5s - loss: 0.0438 - val_loss: 0.0689 - 5s/epoch - 8ms/step\n",
      "Epoch 118/10000\n",
      "563/563 - 5s - loss: 0.0499 - val_loss: 0.0615 - 5s/epoch - 8ms/step\n",
      "Epoch 119/10000\n",
      "563/563 - 5s - loss: 0.0448 - val_loss: 0.0585 - 5s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "563/563 - 5s - loss: 0.0442 - val_loss: 0.0572 - 5s/epoch - 8ms/step\n",
      "Epoch 121/10000\n",
      "563/563 - 5s - loss: 0.0442 - val_loss: 0.0531 - 5s/epoch - 8ms/step\n",
      "Epoch 122/10000\n",
      "563/563 - 5s - loss: 0.0435 - val_loss: 0.0612 - 5s/epoch - 9ms/step\n",
      "Epoch 123/10000\n",
      "563/563 - 6s - loss: 0.0445 - val_loss: 0.0561 - 6s/epoch - 11ms/step\n",
      "Epoch 124/10000\n",
      "563/563 - 6s - loss: 0.0460 - val_loss: 0.0645 - 6s/epoch - 10ms/step\n",
      "Epoch 125/10000\n",
      "563/563 - 5s - loss: 0.0444 - val_loss: 0.0573 - 5s/epoch - 10ms/step\n",
      "Epoch 126/10000\n",
      "563/563 - 6s - loss: 0.0435 - val_loss: 0.0543 - 6s/epoch - 10ms/step\n",
      "Epoch 127/10000\n",
      "563/563 - 5s - loss: 0.0475 - val_loss: 0.0557 - 5s/epoch - 9ms/step\n",
      "Epoch 128/10000\n",
      "563/563 - 5s - loss: 0.0439 - val_loss: 0.0739 - 5s/epoch - 8ms/step\n",
      "Epoch 129/10000\n",
      "563/563 - 5s - loss: 0.0439 - val_loss: 0.0515 - 5s/epoch - 8ms/step\n",
      "Epoch 130/10000\n",
      "563/563 - 5s - loss: 0.0420 - val_loss: 0.0540 - 5s/epoch - 8ms/step\n",
      "Epoch 131/10000\n",
      "563/563 - 5s - loss: 0.0411 - val_loss: 0.0576 - 5s/epoch - 8ms/step\n",
      "Epoch 132/10000\n",
      "563/563 - 5s - loss: 0.0430 - val_loss: 0.0549 - 5s/epoch - 8ms/step\n",
      "Epoch 133/10000\n",
      "563/563 - 5s - loss: 0.0431 - val_loss: 0.0612 - 5s/epoch - 8ms/step\n",
      "Epoch 134/10000\n",
      "563/563 - 5s - loss: 0.0428 - val_loss: 0.0542 - 5s/epoch - 8ms/step\n",
      "Epoch 135/10000\n",
      "563/563 - 5s - loss: 0.0420 - val_loss: 0.0545 - 5s/epoch - 8ms/step\n",
      "Epoch 136/10000\n",
      "563/563 - 5s - loss: 0.0423 - val_loss: 0.0533 - 5s/epoch - 8ms/step\n",
      "Epoch 137/10000\n",
      "563/563 - 5s - loss: 0.0422 - val_loss: 0.0514 - 5s/epoch - 8ms/step\n",
      "Epoch 138/10000\n",
      "563/563 - 5s - loss: 0.0409 - val_loss: 0.0520 - 5s/epoch - 8ms/step\n",
      "Epoch 139/10000\n",
      "563/563 - 5s - loss: 0.0460 - val_loss: 0.0522 - 5s/epoch - 8ms/step\n",
      "Epoch 140/10000\n",
      "563/563 - 5s - loss: 0.0410 - val_loss: 0.0519 - 5s/epoch - 8ms/step\n",
      "Epoch 141/10000\n",
      "563/563 - 5s - loss: 0.0412 - val_loss: 0.0556 - 5s/epoch - 8ms/step\n",
      "Epoch 142/10000\n",
      "563/563 - 5s - loss: 0.0412 - val_loss: 0.0515 - 5s/epoch - 8ms/step\n",
      "Epoch 143/10000\n",
      "563/563 - 5s - loss: 0.0410 - val_loss: 0.0558 - 5s/epoch - 8ms/step\n",
      "Epoch 144/10000\n",
      "563/563 - 5s - loss: 0.0410 - val_loss: 0.0527 - 5s/epoch - 8ms/step\n",
      "Epoch 145/10000\n",
      "563/563 - 5s - loss: 0.0409 - val_loss: 0.0531 - 5s/epoch - 9ms/step\n",
      "Epoch 146/10000\n",
      "563/563 - 5s - loss: 0.0402 - val_loss: 0.0544 - 5s/epoch - 9ms/step\n",
      "Epoch 147/10000\n",
      "563/563 - 5s - loss: 0.0409 - val_loss: 0.0513 - 5s/epoch - 8ms/step\n",
      "Epoch 148/10000\n",
      "563/563 - 5s - loss: 0.0414 - val_loss: 0.0508 - 5s/epoch - 8ms/step\n",
      "Epoch 149/10000\n",
      "563/563 - 5s - loss: 0.0398 - val_loss: 0.0542 - 5s/epoch - 8ms/step\n",
      "Epoch 150/10000\n",
      "563/563 - 5s - loss: 0.0399 - val_loss: 0.0509 - 5s/epoch - 9ms/step\n",
      "Epoch 151/10000\n",
      "563/563 - 5s - loss: 0.0409 - val_loss: 0.0540 - 5s/epoch - 8ms/step\n",
      "Epoch 152/10000\n",
      "563/563 - 5s - loss: 0.0397 - val_loss: 0.0535 - 5s/epoch - 8ms/step\n",
      "Epoch 153/10000\n",
      "563/563 - 5s - loss: 0.0400 - val_loss: 0.0502 - 5s/epoch - 8ms/step\n",
      "Epoch 154/10000\n",
      "563/563 - 5s - loss: 0.0401 - val_loss: 0.0510 - 5s/epoch - 8ms/step\n",
      "Epoch 155/10000\n",
      "563/563 - 5s - loss: 0.0399 - val_loss: 0.0531 - 5s/epoch - 8ms/step\n",
      "Epoch 156/10000\n",
      "563/563 - 5s - loss: 0.0401 - val_loss: 0.0518 - 5s/epoch - 8ms/step\n",
      "Epoch 157/10000\n",
      "563/563 - 5s - loss: 0.0391 - val_loss: 0.0514 - 5s/epoch - 8ms/step\n",
      "Epoch 158/10000\n",
      "563/563 - 5s - loss: 0.0394 - val_loss: 0.0518 - 5s/epoch - 8ms/step\n",
      "Epoch 159/10000\n",
      "563/563 - 5s - loss: 0.0387 - val_loss: 0.0495 - 5s/epoch - 8ms/step\n",
      "Epoch 160/10000\n",
      "563/563 - 5s - loss: 0.0419 - val_loss: 0.0554 - 5s/epoch - 8ms/step\n",
      "Epoch 161/10000\n",
      "563/563 - 5s - loss: 0.0531 - val_loss: 0.0609 - 5s/epoch - 8ms/step\n",
      "Epoch 162/10000\n",
      "563/563 - 5s - loss: 0.0461 - val_loss: 0.0528 - 5s/epoch - 8ms/step\n",
      "Epoch 163/10000\n",
      "563/563 - 5s - loss: 0.0388 - val_loss: 0.0547 - 5s/epoch - 8ms/step\n",
      "Epoch 164/10000\n",
      "563/563 - 5s - loss: 0.0373 - val_loss: 0.0500 - 5s/epoch - 8ms/step\n",
      "Epoch 165/10000\n",
      "563/563 - 5s - loss: 0.0380 - val_loss: 0.0513 - 5s/epoch - 8ms/step\n",
      "Epoch 166/10000\n",
      "563/563 - 5s - loss: 0.0376 - val_loss: 0.0494 - 5s/epoch - 8ms/step\n",
      "Epoch 167/10000\n",
      "563/563 - 5s - loss: 0.0467 - val_loss: 0.0546 - 5s/epoch - 8ms/step\n",
      "Epoch 168/10000\n",
      "563/563 - 5s - loss: 0.0501 - val_loss: 0.0708 - 5s/epoch - 8ms/step\n",
      "Epoch 169/10000\n",
      "563/563 - 5s - loss: 0.0421 - val_loss: 0.0526 - 5s/epoch - 8ms/step\n",
      "Epoch 170/10000\n",
      "563/563 - 5s - loss: 0.0368 - val_loss: 0.0495 - 5s/epoch - 8ms/step\n",
      "Epoch 171/10000\n",
      "563/563 - 5s - loss: 0.0369 - val_loss: 0.0507 - 5s/epoch - 8ms/step\n",
      "Epoch 172/10000\n",
      "563/563 - 5s - loss: 0.0390 - val_loss: 0.0503 - 5s/epoch - 8ms/step\n",
      "Epoch 173/10000\n",
      "563/563 - 5s - loss: 0.0390 - val_loss: 0.0497 - 5s/epoch - 8ms/step\n",
      "Epoch 174/10000\n",
      "563/563 - 5s - loss: 0.0394 - val_loss: 0.0506 - 5s/epoch - 8ms/step\n",
      "Epoch 175/10000\n",
      "563/563 - 5s - loss: 0.0386 - val_loss: 0.0505 - 5s/epoch - 8ms/step\n",
      "Epoch 176/10000\n",
      "563/563 - 5s - loss: 0.0382 - val_loss: 0.0496 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_224_layer_call_fn, gru_cell_224_layer_call_and_return_conditional_losses, gru_cell_225_layer_call_fn, gru_cell_225_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_20_1_50_1_50_datt_seq2seq_gru_5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_20_1_50_1_50_datt_seq2seq_gru_5\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B512BFE50> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B514F1070> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.877774</td>\n",
       "      <td>0.880796</td>\n",
       "      <td>0.920989</td>\n",
       "      <td>0.893186</td>\n",
       "      <td>6.898204</td>\n",
       "      <td>6.070811</td>\n",
       "      <td>4.317675</td>\n",
       "      <td>5.76223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.803031</td>\n",
       "      <td>0.802474</td>\n",
       "      <td>0.883149</td>\n",
       "      <td>0.829551</td>\n",
       "      <td>8.599657</td>\n",
       "      <td>7.817236</td>\n",
       "      <td>5.252303</td>\n",
       "      <td>7.223065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.704876</td>\n",
       "      <td>0.747472</td>\n",
       "      <td>0.84213</td>\n",
       "      <td>0.764826</td>\n",
       "      <td>10.067996</td>\n",
       "      <td>8.842249</td>\n",
       "      <td>6.10675</td>\n",
       "      <td>8.338998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.613055</td>\n",
       "      <td>0.699634</td>\n",
       "      <td>0.81467</td>\n",
       "      <td>0.709119</td>\n",
       "      <td>11.143281</td>\n",
       "      <td>9.646889</td>\n",
       "      <td>6.618032</td>\n",
       "      <td>9.136067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.528408</td>\n",
       "      <td>0.664042</td>\n",
       "      <td>0.800606</td>\n",
       "      <td>0.664352</td>\n",
       "      <td>12.104936</td>\n",
       "      <td>10.206989</td>\n",
       "      <td>6.866106</td>\n",
       "      <td>9.72601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.450846</td>\n",
       "      <td>0.639988</td>\n",
       "      <td>0.787545</td>\n",
       "      <td>0.626126</td>\n",
       "      <td>13.018425</td>\n",
       "      <td>10.570847</td>\n",
       "      <td>7.088652</td>\n",
       "      <td>10.225975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.381085</td>\n",
       "      <td>0.622474</td>\n",
       "      <td>0.771113</td>\n",
       "      <td>0.591557</td>\n",
       "      <td>13.549573</td>\n",
       "      <td>10.827381</td>\n",
       "      <td>7.359174</td>\n",
       "      <td>10.578709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.319875</td>\n",
       "      <td>0.608178</td>\n",
       "      <td>0.748834</td>\n",
       "      <td>0.558963</td>\n",
       "      <td>13.932251</td>\n",
       "      <td>11.032938</td>\n",
       "      <td>7.710503</td>\n",
       "      <td>10.891897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.265821</td>\n",
       "      <td>0.592869</td>\n",
       "      <td>0.720138</td>\n",
       "      <td>0.526276</td>\n",
       "      <td>14.261436</td>\n",
       "      <td>11.248373</td>\n",
       "      <td>8.140512</td>\n",
       "      <td>11.216773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.217288</td>\n",
       "      <td>0.577071</td>\n",
       "      <td>0.684054</td>\n",
       "      <td>0.492804</td>\n",
       "      <td>14.511156</td>\n",
       "      <td>11.464685</td>\n",
       "      <td>8.65073</td>\n",
       "      <td>11.54219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.174256</td>\n",
       "      <td>0.559607</td>\n",
       "      <td>0.645217</td>\n",
       "      <td>0.459693</td>\n",
       "      <td>14.748287</td>\n",
       "      <td>11.699649</td>\n",
       "      <td>9.168366</td>\n",
       "      <td>11.872101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.137468</td>\n",
       "      <td>0.540827</td>\n",
       "      <td>0.610302</td>\n",
       "      <td>0.429532</td>\n",
       "      <td>14.9747</td>\n",
       "      <td>11.945804</td>\n",
       "      <td>9.610521</td>\n",
       "      <td>12.177008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.104197</td>\n",
       "      <td>0.519537</td>\n",
       "      <td>0.578898</td>\n",
       "      <td>0.400877</td>\n",
       "      <td>15.049089</td>\n",
       "      <td>12.218717</td>\n",
       "      <td>9.992607</td>\n",
       "      <td>12.420138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.074289</td>\n",
       "      <td>0.497394</td>\n",
       "      <td>0.546072</td>\n",
       "      <td>0.372585</td>\n",
       "      <td>15.09002</td>\n",
       "      <td>12.498093</td>\n",
       "      <td>10.377945</td>\n",
       "      <td>12.655353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.047491</td>\n",
       "      <td>0.474041</td>\n",
       "      <td>0.51176</td>\n",
       "      <td>0.344431</td>\n",
       "      <td>15.150643</td>\n",
       "      <td>12.785642</td>\n",
       "      <td>10.766251</td>\n",
       "      <td>12.900845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.022846</td>\n",
       "      <td>0.44561</td>\n",
       "      <td>0.481624</td>\n",
       "      <td>0.316693</td>\n",
       "      <td>15.191963</td>\n",
       "      <td>13.125999</td>\n",
       "      <td>11.097393</td>\n",
       "      <td>13.138452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.000467</td>\n",
       "      <td>0.413637</td>\n",
       "      <td>0.458397</td>\n",
       "      <td>0.290522</td>\n",
       "      <td>15.222476</td>\n",
       "      <td>13.497179</td>\n",
       "      <td>11.347597</td>\n",
       "      <td>13.355751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.022896</td>\n",
       "      <td>0.384058</td>\n",
       "      <td>0.444088</td>\n",
       "      <td>0.268417</td>\n",
       "      <td>15.354525</td>\n",
       "      <td>13.830639</td>\n",
       "      <td>11.501324</td>\n",
       "      <td>13.562163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.04395</td>\n",
       "      <td>0.356652</td>\n",
       "      <td>0.43666</td>\n",
       "      <td>0.249787</td>\n",
       "      <td>15.530518</td>\n",
       "      <td>14.133085</td>\n",
       "      <td>11.582753</td>\n",
       "      <td>13.748785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.062709</td>\n",
       "      <td>0.33029</td>\n",
       "      <td>0.429775</td>\n",
       "      <td>0.232452</td>\n",
       "      <td>15.69141</td>\n",
       "      <td>14.418273</td>\n",
       "      <td>11.657923</td>\n",
       "      <td>13.922535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.279629</td>\n",
       "      <td>0.567832</td>\n",
       "      <td>0.655801</td>\n",
       "      <td>0.501088</td>\n",
       "      <td>13.504527</td>\n",
       "      <td>11.394074</td>\n",
       "      <td>8.760656</td>\n",
       "      <td>11.219752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.877774  0.880796  0.920989  0.893186   6.898204   6.070811   \n",
       "1      0.803031  0.802474  0.883149  0.829551   8.599657   7.817236   \n",
       "2      0.704876  0.747472   0.84213  0.764826  10.067996   8.842249   \n",
       "3      0.613055  0.699634   0.81467  0.709119  11.143281   9.646889   \n",
       "4      0.528408  0.664042  0.800606  0.664352  12.104936  10.206989   \n",
       "5      0.450846  0.639988  0.787545  0.626126  13.018425  10.570847   \n",
       "6      0.381085  0.622474  0.771113  0.591557  13.549573  10.827381   \n",
       "7      0.319875  0.608178  0.748834  0.558963  13.932251  11.032938   \n",
       "8      0.265821  0.592869  0.720138  0.526276  14.261436  11.248373   \n",
       "9      0.217288  0.577071  0.684054  0.492804  14.511156  11.464685   \n",
       "10     0.174256  0.559607  0.645217  0.459693  14.748287  11.699649   \n",
       "11     0.137468  0.540827  0.610302  0.429532    14.9747  11.945804   \n",
       "12     0.104197  0.519537  0.578898  0.400877  15.049089  12.218717   \n",
       "13     0.074289  0.497394  0.546072  0.372585   15.09002  12.498093   \n",
       "14     0.047491  0.474041   0.51176  0.344431  15.150643  12.785642   \n",
       "15     0.022846   0.44561  0.481624  0.316693  15.191963  13.125999   \n",
       "16    -0.000467  0.413637  0.458397  0.290522  15.222476  13.497179   \n",
       "17    -0.022896  0.384058  0.444088  0.268417  15.354525  13.830639   \n",
       "18     -0.04395  0.356652   0.43666  0.249787  15.530518  14.133085   \n",
       "19    -0.062709   0.33029  0.429775  0.232452   15.69141  14.418273   \n",
       "mean   0.279629  0.567832  0.655801  0.501088  13.504527  11.394074   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       4.317675    5.76223  \n",
       "1       5.252303   7.223065  \n",
       "2        6.10675   8.338998  \n",
       "3       6.618032   9.136067  \n",
       "4       6.866106    9.72601  \n",
       "5       7.088652  10.225975  \n",
       "6       7.359174  10.578709  \n",
       "7       7.710503  10.891897  \n",
       "8       8.140512  11.216773  \n",
       "9        8.65073   11.54219  \n",
       "10      9.168366  11.872101  \n",
       "11      9.610521  12.177008  \n",
       "12      9.992607  12.420138  \n",
       "13     10.377945  12.655353  \n",
       "14     10.766251  12.900845  \n",
       "15     11.097393  13.138452  \n",
       "16     11.347597  13.355751  \n",
       "17     11.501324  13.562163  \n",
       "18     11.582753  13.748785  \n",
       "19     11.657923  13.922535  \n",
       "mean    8.760656  11.219752  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/40_20_1_50_1_50_datt_seq2seq_gru_5.csv\n",
      "\n",
      "\n",
      "5th iteration\n",
      "history size: 40\n",
      "future size: 30\n",
      "Epoch 1/10000\n",
      "559/559 - 7s - loss: 0.3852 - val_loss: 0.3103 - 7s/epoch - 12ms/step\n",
      "Epoch 2/10000\n",
      "559/559 - 4s - loss: 0.2866 - val_loss: 0.2873 - 4s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "559/559 - 4s - loss: 0.2658 - val_loss: 0.2736 - 4s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "559/559 - 4s - loss: 0.2509 - val_loss: 0.2679 - 4s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "559/559 - 4s - loss: 0.2358 - val_loss: 0.2455 - 4s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "559/559 - 5s - loss: 0.2225 - val_loss: 0.2307 - 5s/epoch - 9ms/step\n",
      "Epoch 7/10000\n",
      "559/559 - 5s - loss: 0.2111 - val_loss: 0.2134 - 5s/epoch - 9ms/step\n",
      "Epoch 8/10000\n",
      "559/559 - 5s - loss: 0.1977 - val_loss: 0.1986 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "559/559 - 4s - loss: 0.1869 - val_loss: 0.1933 - 4s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "559/559 - 4s - loss: 0.1793 - val_loss: 0.1909 - 4s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "559/559 - 4s - loss: 0.1709 - val_loss: 0.1765 - 4s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "559/559 - 4s - loss: 0.1643 - val_loss: 0.1710 - 4s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "559/559 - 4s - loss: 0.1604 - val_loss: 0.1672 - 4s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "559/559 - 4s - loss: 0.1579 - val_loss: 0.1634 - 4s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "559/559 - 4s - loss: 0.1503 - val_loss: 0.1552 - 4s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "559/559 - 4s - loss: 0.1484 - val_loss: 0.1520 - 4s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "559/559 - 4s - loss: 0.1431 - val_loss: 0.1496 - 4s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "559/559 - 4s - loss: 0.1406 - val_loss: 0.1489 - 4s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "559/559 - 5s - loss: 0.1374 - val_loss: 0.1439 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "559/559 - 4s - loss: 0.1372 - val_loss: 0.1375 - 4s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "559/559 - 4s - loss: 0.1312 - val_loss: 0.1438 - 4s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "559/559 - 4s - loss: 0.1289 - val_loss: 0.1374 - 4s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "559/559 - 4s - loss: 0.1289 - val_loss: 0.1337 - 4s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "559/559 - 4s - loss: 0.1261 - val_loss: 0.1397 - 4s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "559/559 - 4s - loss: 0.1222 - val_loss: 0.1317 - 4s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "559/559 - 4s - loss: 0.1205 - val_loss: 0.1313 - 4s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "559/559 - 4s - loss: 0.1243 - val_loss: 0.1311 - 4s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "559/559 - 4s - loss: 0.1161 - val_loss: 0.1208 - 4s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "559/559 - 4s - loss: 0.1117 - val_loss: 0.1229 - 4s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "559/559 - 4s - loss: 0.1127 - val_loss: 0.1238 - 4s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "559/559 - 4s - loss: 0.1103 - val_loss: 0.1201 - 4s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "559/559 - 4s - loss: 0.1090 - val_loss: 0.1232 - 4s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "559/559 - 4s - loss: 0.1103 - val_loss: 0.1171 - 4s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "559/559 - 4s - loss: 0.1047 - val_loss: 0.1170 - 4s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "559/559 - 4s - loss: 0.1074 - val_loss: 0.1169 - 4s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "559/559 - 4s - loss: 0.1015 - val_loss: 0.1200 - 4s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "559/559 - 4s - loss: 0.1020 - val_loss: 0.1138 - 4s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "559/559 - 4s - loss: 0.1040 - val_loss: 0.1155 - 4s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "559/559 - 4s - loss: 0.0982 - val_loss: 0.1132 - 4s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "559/559 - 4s - loss: 0.0959 - val_loss: 0.1065 - 4s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "559/559 - 4s - loss: 0.0976 - val_loss: 0.1148 - 4s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "559/559 - 4s - loss: 0.0941 - val_loss: 0.1091 - 4s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "559/559 - 4s - loss: 0.0927 - val_loss: 0.1021 - 4s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "559/559 - 4s - loss: 0.0936 - val_loss: 0.1088 - 4s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "559/559 - 4s - loss: 0.0922 - val_loss: 0.1087 - 4s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "559/559 - 4s - loss: 0.0900 - val_loss: 0.1040 - 4s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "559/559 - 4s - loss: 0.0929 - val_loss: 0.0991 - 4s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "559/559 - 4s - loss: 0.0922 - val_loss: 0.0939 - 4s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "559/559 - 4s - loss: 0.0869 - val_loss: 0.0986 - 4s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "559/559 - 4s - loss: 0.0891 - val_loss: 0.1020 - 4s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "559/559 - 4s - loss: 0.0866 - val_loss: 0.0932 - 4s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "559/559 - 4s - loss: 0.0848 - val_loss: 0.0998 - 4s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "559/559 - 4s - loss: 0.0911 - val_loss: 0.0929 - 4s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "559/559 - 4s - loss: 0.0840 - val_loss: 0.0961 - 4s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "559/559 - 4s - loss: 0.0839 - val_loss: 0.0940 - 4s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "559/559 - 4s - loss: 0.0815 - val_loss: 0.0974 - 4s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "559/559 - 5s - loss: 0.0902 - val_loss: 0.1098 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "559/559 - 4s - loss: 0.0836 - val_loss: 0.0924 - 4s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "559/559 - 4s - loss: 0.0805 - val_loss: 0.0920 - 4s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "559/559 - 4s - loss: 0.0804 - val_loss: 0.0894 - 4s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "559/559 - 4s - loss: 0.0781 - val_loss: 0.0907 - 4s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "559/559 - 4s - loss: 0.0800 - val_loss: 0.0880 - 4s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "559/559 - 4s - loss: 0.0768 - val_loss: 0.0871 - 4s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "559/559 - 4s - loss: 0.0784 - val_loss: 0.0877 - 4s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "559/559 - 4s - loss: 0.0763 - val_loss: 0.0932 - 4s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "559/559 - 4s - loss: 0.0784 - val_loss: 0.0839 - 4s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "559/559 - 4s - loss: 0.0748 - val_loss: 0.0858 - 4s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "559/559 - 4s - loss: 0.0752 - val_loss: 0.0966 - 4s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "559/559 - 4s - loss: 0.0770 - val_loss: 0.0828 - 4s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "559/559 - 4s - loss: 0.0738 - val_loss: 0.0820 - 4s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "559/559 - 4s - loss: 0.0737 - val_loss: 0.0857 - 4s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "559/559 - 4s - loss: 0.0711 - val_loss: 0.0786 - 4s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "559/559 - 4s - loss: 0.0728 - val_loss: 0.0806 - 4s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "559/559 - 4s - loss: 0.0739 - val_loss: 0.0821 - 4s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "559/559 - 5s - loss: 0.0709 - val_loss: 0.0797 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "559/559 - 4s - loss: 0.0703 - val_loss: 0.0856 - 4s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "559/559 - 4s - loss: 0.0712 - val_loss: 0.0866 - 4s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "559/559 - 4s - loss: 0.0703 - val_loss: 0.0788 - 4s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "559/559 - 4s - loss: 0.0703 - val_loss: 0.0791 - 4s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "559/559 - 5s - loss: 0.0690 - val_loss: 0.0793 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "559/559 - 4s - loss: 0.0679 - val_loss: 0.1009 - 4s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "559/559 - 4s - loss: 0.0719 - val_loss: 0.0746 - 4s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "559/559 - 4s - loss: 0.0681 - val_loss: 0.0772 - 4s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "559/559 - 4s - loss: 0.0670 - val_loss: 0.0763 - 4s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "559/559 - 4s - loss: 0.0675 - val_loss: 0.0765 - 4s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "559/559 - 4s - loss: 0.0652 - val_loss: 0.0763 - 4s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "559/559 - 4s - loss: 0.0675 - val_loss: 0.0764 - 4s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "559/559 - 4s - loss: 0.0656 - val_loss: 0.0756 - 4s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "559/559 - 4s - loss: 0.0662 - val_loss: 0.0809 - 4s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "559/559 - 4s - loss: 0.0657 - val_loss: 0.0781 - 4s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "559/559 - 4s - loss: 0.0665 - val_loss: 0.0781 - 4s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "559/559 - 4s - loss: 0.0633 - val_loss: 0.0738 - 4s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "559/559 - 4s - loss: 0.0625 - val_loss: 0.0758 - 4s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "559/559 - 4s - loss: 0.0672 - val_loss: 0.0763 - 4s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "559/559 - 4s - loss: 0.0657 - val_loss: 0.0729 - 4s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "559/559 - 4s - loss: 0.0625 - val_loss: 0.0709 - 4s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "559/559 - 4s - loss: 0.0640 - val_loss: 0.0690 - 4s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "559/559 - 4s - loss: 0.0626 - val_loss: 0.0799 - 4s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "559/559 - 4s - loss: 0.0628 - val_loss: 0.0714 - 4s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "559/559 - 4s - loss: 0.0614 - val_loss: 0.0730 - 4s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "559/559 - 4s - loss: 0.0624 - val_loss: 0.0741 - 4s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "559/559 - 4s - loss: 0.0611 - val_loss: 0.0702 - 4s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "559/559 - 4s - loss: 0.0605 - val_loss: 0.0697 - 4s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "559/559 - 4s - loss: 0.0631 - val_loss: 0.0823 - 4s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "559/559 - 4s - loss: 0.0631 - val_loss: 0.0691 - 4s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "559/559 - 4s - loss: 0.0593 - val_loss: 0.0741 - 4s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "559/559 - 4s - loss: 0.0602 - val_loss: 0.0708 - 4s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_226_layer_call_fn, gru_cell_226_layer_call_and_return_conditional_losses, gru_cell_227_layer_call_fn, gru_cell_227_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_30_1_50_1_50_datt_seq2seq_gru_5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_30_1_50_1_50_datt_seq2seq_gru_5\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021CE31DD130> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B50FE1370> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.79776</td>\n",
       "      <td>0.887319</td>\n",
       "      <td>0.921463</td>\n",
       "      <td>0.868847</td>\n",
       "      <td>8.871525</td>\n",
       "      <td>5.902364</td>\n",
       "      <td>4.292679</td>\n",
       "      <td>6.355523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.74129</td>\n",
       "      <td>0.811596</td>\n",
       "      <td>0.849566</td>\n",
       "      <td>0.800818</td>\n",
       "      <td>10.036115</td>\n",
       "      <td>7.632605</td>\n",
       "      <td>5.943984</td>\n",
       "      <td>7.870901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.672128</td>\n",
       "      <td>0.76049</td>\n",
       "      <td>0.801921</td>\n",
       "      <td>0.744846</td>\n",
       "      <td>11.300634</td>\n",
       "      <td>8.606566</td>\n",
       "      <td>6.823842</td>\n",
       "      <td>8.910347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.601093</td>\n",
       "      <td>0.721455</td>\n",
       "      <td>0.782262</td>\n",
       "      <td>0.701603</td>\n",
       "      <td>12.466932</td>\n",
       "      <td>9.282843</td>\n",
       "      <td>7.157246</td>\n",
       "      <td>9.635674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.526194</td>\n",
       "      <td>0.693327</td>\n",
       "      <td>0.772696</td>\n",
       "      <td>0.664072</td>\n",
       "      <td>13.587506</td>\n",
       "      <td>9.742203</td>\n",
       "      <td>7.315275</td>\n",
       "      <td>10.214995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.451339</td>\n",
       "      <td>0.673365</td>\n",
       "      <td>0.756581</td>\n",
       "      <td>0.627095</td>\n",
       "      <td>14.620727</td>\n",
       "      <td>10.056798</td>\n",
       "      <td>7.572047</td>\n",
       "      <td>10.749858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.38064</td>\n",
       "      <td>0.655696</td>\n",
       "      <td>0.731399</td>\n",
       "      <td>0.589245</td>\n",
       "      <td>15.532805</td>\n",
       "      <td>10.327699</td>\n",
       "      <td>7.95593</td>\n",
       "      <td>11.272145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.314323</td>\n",
       "      <td>0.636888</td>\n",
       "      <td>0.700954</td>\n",
       "      <td>0.550722</td>\n",
       "      <td>16.341955</td>\n",
       "      <td>10.608388</td>\n",
       "      <td>8.396859</td>\n",
       "      <td>11.782401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.25271</td>\n",
       "      <td>0.614533</td>\n",
       "      <td>0.666986</td>\n",
       "      <td>0.51141</td>\n",
       "      <td>17.059348</td>\n",
       "      <td>10.932964</td>\n",
       "      <td>8.862936</td>\n",
       "      <td>12.285083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.196366</td>\n",
       "      <td>0.590581</td>\n",
       "      <td>0.630381</td>\n",
       "      <td>0.472443</td>\n",
       "      <td>17.690581</td>\n",
       "      <td>11.269369</td>\n",
       "      <td>9.34002</td>\n",
       "      <td>12.766657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.145826</td>\n",
       "      <td>0.567368</td>\n",
       "      <td>0.593113</td>\n",
       "      <td>0.435436</td>\n",
       "      <td>18.239378</td>\n",
       "      <td>11.586348</td>\n",
       "      <td>9.802443</td>\n",
       "      <td>13.20939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.101921</td>\n",
       "      <td>0.543823</td>\n",
       "      <td>0.556319</td>\n",
       "      <td>0.400688</td>\n",
       "      <td>18.358697</td>\n",
       "      <td>11.899298</td>\n",
       "      <td>10.239493</td>\n",
       "      <td>13.499162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.064892</td>\n",
       "      <td>0.518189</td>\n",
       "      <td>0.523036</td>\n",
       "      <td>0.368706</td>\n",
       "      <td>17.910068</td>\n",
       "      <td>12.232035</td>\n",
       "      <td>10.620622</td>\n",
       "      <td>13.587575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.031214</td>\n",
       "      <td>0.489295</td>\n",
       "      <td>0.495458</td>\n",
       "      <td>0.338656</td>\n",
       "      <td>17.614403</td>\n",
       "      <td>12.597022</td>\n",
       "      <td>10.927436</td>\n",
       "      <td>13.712954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.457235</td>\n",
       "      <td>0.473498</td>\n",
       "      <td>0.310738</td>\n",
       "      <td>17.589942</td>\n",
       "      <td>12.991064</td>\n",
       "      <td>11.167009</td>\n",
       "      <td>13.916005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.025872</td>\n",
       "      <td>0.422313</td>\n",
       "      <td>0.455744</td>\n",
       "      <td>0.284061</td>\n",
       "      <td>17.76413</td>\n",
       "      <td>13.406531</td>\n",
       "      <td>11.357492</td>\n",
       "      <td>14.176051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.050121</td>\n",
       "      <td>0.38737</td>\n",
       "      <td>0.438264</td>\n",
       "      <td>0.258504</td>\n",
       "      <td>17.617327</td>\n",
       "      <td>13.807204</td>\n",
       "      <td>11.542275</td>\n",
       "      <td>14.322269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.074029</td>\n",
       "      <td>0.351562</td>\n",
       "      <td>0.420248</td>\n",
       "      <td>0.232594</td>\n",
       "      <td>17.474434</td>\n",
       "      <td>14.206232</td>\n",
       "      <td>11.729215</td>\n",
       "      <td>14.46996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.0997</td>\n",
       "      <td>0.315004</td>\n",
       "      <td>0.403182</td>\n",
       "      <td>0.206162</td>\n",
       "      <td>17.420508</td>\n",
       "      <td>14.603265</td>\n",
       "      <td>11.903651</td>\n",
       "      <td>14.642475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.12478</td>\n",
       "      <td>0.283218</td>\n",
       "      <td>0.38696</td>\n",
       "      <td>0.181799</td>\n",
       "      <td>17.365869</td>\n",
       "      <td>14.939313</td>\n",
       "      <td>12.066794</td>\n",
       "      <td>14.790659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.148453</td>\n",
       "      <td>0.257398</td>\n",
       "      <td>0.371752</td>\n",
       "      <td>0.160232</td>\n",
       "      <td>17.370345</td>\n",
       "      <td>15.208212</td>\n",
       "      <td>12.218069</td>\n",
       "      <td>14.932209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.170993</td>\n",
       "      <td>0.236641</td>\n",
       "      <td>0.357748</td>\n",
       "      <td>0.141132</td>\n",
       "      <td>17.433263</td>\n",
       "      <td>15.420143</td>\n",
       "      <td>12.35633</td>\n",
       "      <td>15.069912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.192227</td>\n",
       "      <td>0.21789</td>\n",
       "      <td>0.344458</td>\n",
       "      <td>0.123374</td>\n",
       "      <td>17.353678</td>\n",
       "      <td>15.608203</td>\n",
       "      <td>12.48665</td>\n",
       "      <td>15.149511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.213017</td>\n",
       "      <td>0.199738</td>\n",
       "      <td>0.331406</td>\n",
       "      <td>0.106042</td>\n",
       "      <td>17.272687</td>\n",
       "      <td>15.790205</td>\n",
       "      <td>12.613627</td>\n",
       "      <td>15.225507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.231105</td>\n",
       "      <td>0.181492</td>\n",
       "      <td>0.316739</td>\n",
       "      <td>0.089042</td>\n",
       "      <td>17.229846</td>\n",
       "      <td>15.970917</td>\n",
       "      <td>12.753963</td>\n",
       "      <td>15.318242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.247372</td>\n",
       "      <td>0.162191</td>\n",
       "      <td>0.300337</td>\n",
       "      <td>0.071719</td>\n",
       "      <td>17.174734</td>\n",
       "      <td>16.159109</td>\n",
       "      <td>12.909189</td>\n",
       "      <td>15.414344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.261892</td>\n",
       "      <td>0.142445</td>\n",
       "      <td>0.284133</td>\n",
       "      <td>0.054896</td>\n",
       "      <td>17.10881</td>\n",
       "      <td>16.34685</td>\n",
       "      <td>13.061297</td>\n",
       "      <td>15.505653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.273828</td>\n",
       "      <td>0.123726</td>\n",
       "      <td>0.26836</td>\n",
       "      <td>0.039419</td>\n",
       "      <td>17.148745</td>\n",
       "      <td>16.522643</td>\n",
       "      <td>13.208508</td>\n",
       "      <td>15.626632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.281701</td>\n",
       "      <td>0.104712</td>\n",
       "      <td>0.254813</td>\n",
       "      <td>0.025941</td>\n",
       "      <td>17.224099</td>\n",
       "      <td>16.700297</td>\n",
       "      <td>13.333967</td>\n",
       "      <td>15.752788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.285618</td>\n",
       "      <td>0.080513</td>\n",
       "      <td>0.244471</td>\n",
       "      <td>0.013122</td>\n",
       "      <td>17.275313</td>\n",
       "      <td>16.924427</td>\n",
       "      <td>13.42962</td>\n",
       "      <td>15.876453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.086616</td>\n",
       "      <td>0.436246</td>\n",
       "      <td>0.514475</td>\n",
       "      <td>0.345779</td>\n",
       "      <td>16.24848</td>\n",
       "      <td>12.909371</td>\n",
       "      <td>10.446282</td>\n",
       "      <td>13.201378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0       0.79776  0.887319  0.921463  0.868847   8.871525   5.902364   \n",
       "1       0.74129  0.811596  0.849566  0.800818  10.036115   7.632605   \n",
       "2      0.672128   0.76049  0.801921  0.744846  11.300634   8.606566   \n",
       "3      0.601093  0.721455  0.782262  0.701603  12.466932   9.282843   \n",
       "4      0.526194  0.693327  0.772696  0.664072  13.587506   9.742203   \n",
       "5      0.451339  0.673365  0.756581  0.627095  14.620727  10.056798   \n",
       "6       0.38064  0.655696  0.731399  0.589245  15.532805  10.327699   \n",
       "7      0.314323  0.636888  0.700954  0.550722  16.341955  10.608388   \n",
       "8       0.25271  0.614533  0.666986   0.51141  17.059348  10.932964   \n",
       "9      0.196366  0.590581  0.630381  0.472443  17.690581  11.269369   \n",
       "10     0.145826  0.567368  0.593113  0.435436  18.239378  11.586348   \n",
       "11     0.101921  0.543823  0.556319  0.400688  18.358697  11.899298   \n",
       "12     0.064892  0.518189  0.523036  0.368706  17.910068  12.232035   \n",
       "13     0.031214  0.489295  0.495458  0.338656  17.614403  12.597022   \n",
       "14     0.001481  0.457235  0.473498  0.310738  17.589942  12.991064   \n",
       "15    -0.025872  0.422313  0.455744  0.284061   17.76413  13.406531   \n",
       "16    -0.050121   0.38737  0.438264  0.258504  17.617327  13.807204   \n",
       "17    -0.074029  0.351562  0.420248  0.232594  17.474434  14.206232   \n",
       "18      -0.0997  0.315004  0.403182  0.206162  17.420508  14.603265   \n",
       "19     -0.12478  0.283218   0.38696  0.181799  17.365869  14.939313   \n",
       "20    -0.148453  0.257398  0.371752  0.160232  17.370345  15.208212   \n",
       "21    -0.170993  0.236641  0.357748  0.141132  17.433263  15.420143   \n",
       "22    -0.192227   0.21789  0.344458  0.123374  17.353678  15.608203   \n",
       "23    -0.213017  0.199738  0.331406  0.106042  17.272687  15.790205   \n",
       "24    -0.231105  0.181492  0.316739  0.089042  17.229846  15.970917   \n",
       "25    -0.247372  0.162191  0.300337  0.071719  17.174734  16.159109   \n",
       "26    -0.261892  0.142445  0.284133  0.054896   17.10881   16.34685   \n",
       "27    -0.273828  0.123726   0.26836  0.039419  17.148745  16.522643   \n",
       "28    -0.281701  0.104712  0.254813  0.025941  17.224099  16.700297   \n",
       "29    -0.285618  0.080513  0.244471  0.013122  17.275313  16.924427   \n",
       "mean   0.086616  0.436246  0.514475  0.345779   16.24848  12.909371   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       4.292679   6.355523  \n",
       "1       5.943984   7.870901  \n",
       "2       6.823842   8.910347  \n",
       "3       7.157246   9.635674  \n",
       "4       7.315275  10.214995  \n",
       "5       7.572047  10.749858  \n",
       "6        7.95593  11.272145  \n",
       "7       8.396859  11.782401  \n",
       "8       8.862936  12.285083  \n",
       "9        9.34002  12.766657  \n",
       "10      9.802443   13.20939  \n",
       "11     10.239493  13.499162  \n",
       "12     10.620622  13.587575  \n",
       "13     10.927436  13.712954  \n",
       "14     11.167009  13.916005  \n",
       "15     11.357492  14.176051  \n",
       "16     11.542275  14.322269  \n",
       "17     11.729215   14.46996  \n",
       "18     11.903651  14.642475  \n",
       "19     12.066794  14.790659  \n",
       "20     12.218069  14.932209  \n",
       "21      12.35633  15.069912  \n",
       "22      12.48665  15.149511  \n",
       "23     12.613627  15.225507  \n",
       "24     12.753963  15.318242  \n",
       "25     12.909189  15.414344  \n",
       "26     13.061297  15.505653  \n",
       "27     13.208508  15.626632  \n",
       "28     13.333967  15.752788  \n",
       "29      13.42962  15.876453  \n",
       "mean   10.446282  13.201378  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/40_30_1_50_1_50_datt_seq2seq_gru_5.csv\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6th iteration\n",
      "history size: 10\n",
      "future size: 10\n",
      "Epoch 1/10000\n",
      "581/581 - 8s - loss: 0.2103 - val_loss: 0.1528 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "581/581 - 5s - loss: 0.1516 - val_loss: 0.1414 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "581/581 - 5s - loss: 0.1408 - val_loss: 0.1349 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "581/581 - 5s - loss: 0.1337 - val_loss: 0.1214 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "581/581 - 5s - loss: 0.1256 - val_loss: 0.1135 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "581/581 - 5s - loss: 0.1179 - val_loss: 0.1076 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "581/581 - 5s - loss: 0.1132 - val_loss: 0.1064 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "581/581 - 5s - loss: 0.1074 - val_loss: 0.0988 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "581/581 - 5s - loss: 0.1041 - val_loss: 0.0942 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "581/581 - 5s - loss: 0.1006 - val_loss: 0.0926 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "581/581 - 5s - loss: 0.0965 - val_loss: 0.0933 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "581/581 - 5s - loss: 0.0953 - val_loss: 0.0874 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "581/581 - 5s - loss: 0.0925 - val_loss: 0.0898 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "581/581 - 5s - loss: 0.0894 - val_loss: 0.0899 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "581/581 - 5s - loss: 0.0886 - val_loss: 0.0909 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "581/581 - 5s - loss: 0.0875 - val_loss: 0.0821 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "581/581 - 5s - loss: 0.0846 - val_loss: 0.0813 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "581/581 - 5s - loss: 0.0828 - val_loss: 0.0822 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "581/581 - 5s - loss: 0.0808 - val_loss: 0.0791 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "581/581 - 5s - loss: 0.0783 - val_loss: 0.0816 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "581/581 - 5s - loss: 0.0778 - val_loss: 0.0780 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "581/581 - 5s - loss: 0.0763 - val_loss: 0.0770 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "581/581 - 5s - loss: 0.0759 - val_loss: 0.0747 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "581/581 - 5s - loss: 0.0740 - val_loss: 0.0751 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "581/581 - 5s - loss: 0.0739 - val_loss: 0.0769 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "581/581 - 5s - loss: 0.0713 - val_loss: 0.0746 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "581/581 - 5s - loss: 0.0695 - val_loss: 0.0747 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "581/581 - 5s - loss: 0.0688 - val_loss: 0.0729 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "581/581 - 5s - loss: 0.0684 - val_loss: 0.0751 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "581/581 - 5s - loss: 0.0674 - val_loss: 0.0729 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "581/581 - 5s - loss: 0.0674 - val_loss: 0.0730 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "581/581 - 5s - loss: 0.0657 - val_loss: 0.0700 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "581/581 - 5s - loss: 0.0628 - val_loss: 0.0757 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "581/581 - 5s - loss: 0.0649 - val_loss: 0.0685 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "581/581 - 5s - loss: 0.0646 - val_loss: 0.0721 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "581/581 - 5s - loss: 0.0633 - val_loss: 0.0712 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "581/581 - 5s - loss: 0.0607 - val_loss: 0.0665 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "581/581 - 5s - loss: 0.0615 - val_loss: 0.0742 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "581/581 - 5s - loss: 0.0628 - val_loss: 0.0664 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "581/581 - 5s - loss: 0.0581 - val_loss: 0.0688 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "581/581 - 5s - loss: 0.0655 - val_loss: 0.0678 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "581/581 - 5s - loss: 0.0582 - val_loss: 0.0666 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "581/581 - 5s - loss: 0.0564 - val_loss: 0.0734 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "581/581 - 5s - loss: 0.0567 - val_loss: 0.0661 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "581/581 - 5s - loss: 0.0575 - val_loss: 0.0706 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "581/581 - 5s - loss: 0.0612 - val_loss: 0.0669 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "581/581 - 5s - loss: 0.0554 - val_loss: 0.0641 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "581/581 - 5s - loss: 0.0541 - val_loss: 0.0684 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "581/581 - 5s - loss: 0.0561 - val_loss: 0.0669 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "581/581 - 5s - loss: 0.0532 - val_loss: 0.0625 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "581/581 - 5s - loss: 0.0546 - val_loss: 0.0635 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "581/581 - 5s - loss: 0.0563 - val_loss: 0.0679 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "581/581 - 5s - loss: 0.0530 - val_loss: 0.0624 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "581/581 - 5s - loss: 0.0519 - val_loss: 0.0661 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "581/581 - 5s - loss: 0.0545 - val_loss: 0.0607 - 5s/epoch - 9ms/step\n",
      "Epoch 56/10000\n",
      "581/581 - 6s - loss: 0.0499 - val_loss: 0.0656 - 6s/epoch - 10ms/step\n",
      "Epoch 57/10000\n",
      "581/581 - 5s - loss: 0.0543 - val_loss: 0.0691 - 5s/epoch - 9ms/step\n",
      "Epoch 58/10000\n",
      "581/581 - 5s - loss: 0.0515 - val_loss: 0.0608 - 5s/epoch - 9ms/step\n",
      "Epoch 59/10000\n",
      "581/581 - 5s - loss: 0.0507 - val_loss: 0.0623 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "581/581 - 5s - loss: 0.0501 - val_loss: 0.0615 - 5s/epoch - 9ms/step\n",
      "Epoch 61/10000\n",
      "581/581 - 5s - loss: 0.0512 - val_loss: 0.0600 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "581/581 - 5s - loss: 0.0503 - val_loss: 0.0639 - 5s/epoch - 9ms/step\n",
      "Epoch 63/10000\n",
      "581/581 - 5s - loss: 0.0496 - val_loss: 0.0605 - 5s/epoch - 9ms/step\n",
      "Epoch 64/10000\n",
      "581/581 - 6s - loss: 0.0488 - val_loss: 0.0597 - 6s/epoch - 10ms/step\n",
      "Epoch 65/10000\n",
      "581/581 - 5s - loss: 0.0488 - val_loss: 0.0601 - 5s/epoch - 9ms/step\n",
      "Epoch 66/10000\n",
      "581/581 - 6s - loss: 0.0487 - val_loss: 0.0581 - 6s/epoch - 10ms/step\n",
      "Epoch 67/10000\n",
      "581/581 - 5s - loss: 0.0507 - val_loss: 0.0650 - 5s/epoch - 9ms/step\n",
      "Epoch 68/10000\n",
      "581/581 - 5s - loss: 0.0480 - val_loss: 0.0583 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "581/581 - 5s - loss: 0.0461 - val_loss: 0.0584 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "581/581 - 5s - loss: 0.0475 - val_loss: 0.0591 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "581/581 - 5s - loss: 0.0472 - val_loss: 0.0584 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "581/581 - 5s - loss: 0.0470 - val_loss: 0.0597 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "581/581 - 5s - loss: 0.0479 - val_loss: 0.0606 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "581/581 - 5s - loss: 0.0459 - val_loss: 0.0579 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "581/581 - 5s - loss: 0.0461 - val_loss: 0.0567 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "581/581 - 5s - loss: 0.0462 - val_loss: 0.0599 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "581/581 - 5s - loss: 0.0450 - val_loss: 0.0585 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "581/581 - 5s - loss: 0.0450 - val_loss: 0.0568 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "581/581 - 5s - loss: 0.0444 - val_loss: 0.0581 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "581/581 - 5s - loss: 0.0441 - val_loss: 0.0588 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "581/581 - 5s - loss: 0.0459 - val_loss: 0.0573 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "581/581 - 5s - loss: 0.0454 - val_loss: 0.0571 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "581/581 - 5s - loss: 0.0436 - val_loss: 0.0561 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "581/581 - 5s - loss: 0.0438 - val_loss: 0.0576 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "581/581 - 5s - loss: 0.0431 - val_loss: 0.0576 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "581/581 - 5s - loss: 0.0432 - val_loss: 0.0570 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "581/581 - 5s - loss: 0.0434 - val_loss: 0.0550 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "581/581 - 5s - loss: 0.0435 - val_loss: 0.0560 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "581/581 - 5s - loss: 0.0422 - val_loss: 0.0556 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "581/581 - 5s - loss: 0.0435 - val_loss: 0.0575 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "581/581 - 5s - loss: 0.0421 - val_loss: 0.0558 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "581/581 - 5s - loss: 0.0425 - val_loss: 0.0564 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "581/581 - 5s - loss: 0.0424 - val_loss: 0.0553 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "581/581 - 5s - loss: 0.0411 - val_loss: 0.0556 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "581/581 - 5s - loss: 0.0432 - val_loss: 0.0561 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "581/581 - 5s - loss: 0.0412 - val_loss: 0.0535 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "581/581 - 5s - loss: 0.0401 - val_loss: 0.0530 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "581/581 - 5s - loss: 0.0418 - val_loss: 0.0561 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "581/581 - 5s - loss: 0.0414 - val_loss: 0.0552 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "581/581 - 5s - loss: 0.0429 - val_loss: 0.0558 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "581/581 - 5s - loss: 0.0399 - val_loss: 0.0560 - 5s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "581/581 - 5s - loss: 0.0397 - val_loss: 0.0540 - 5s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "581/581 - 5s - loss: 0.0448 - val_loss: 0.0574 - 5s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "581/581 - 5s - loss: 0.0406 - val_loss: 0.0607 - 5s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "581/581 - 5s - loss: 0.0397 - val_loss: 0.0545 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "581/581 - 5s - loss: 0.0396 - val_loss: 0.0575 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "581/581 - 5s - loss: 0.0400 - val_loss: 0.0562 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_228_layer_call_fn, gru_cell_228_layer_call_and_return_conditional_losses, gru_cell_229_layer_call_fn, gru_cell_229_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_10_1_50_1_50_datt_seq2seq_gru_6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_10_1_50_1_50_datt_seq2seq_gru_6\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B515DF160> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B720791F0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.921616</td>\n",
       "      <td>0.946618</td>\n",
       "      <td>0.96927</td>\n",
       "      <td>0.945834</td>\n",
       "      <td>4.545796</td>\n",
       "      <td>4.040363</td>\n",
       "      <td>2.733193</td>\n",
       "      <td>3.773117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.847189</td>\n",
       "      <td>0.893962</td>\n",
       "      <td>0.940099</td>\n",
       "      <td>0.89375</td>\n",
       "      <td>6.306412</td>\n",
       "      <td>5.695097</td>\n",
       "      <td>3.814998</td>\n",
       "      <td>5.272169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.752296</td>\n",
       "      <td>0.839567</td>\n",
       "      <td>0.907911</td>\n",
       "      <td>0.833258</td>\n",
       "      <td>7.919912</td>\n",
       "      <td>7.005771</td>\n",
       "      <td>4.729087</td>\n",
       "      <td>6.55159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.65331</td>\n",
       "      <td>0.798429</td>\n",
       "      <td>0.877515</td>\n",
       "      <td>0.776418</td>\n",
       "      <td>9.245444</td>\n",
       "      <td>7.853861</td>\n",
       "      <td>5.452574</td>\n",
       "      <td>7.517293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.55467</td>\n",
       "      <td>0.765889</td>\n",
       "      <td>0.854019</td>\n",
       "      <td>0.72486</td>\n",
       "      <td>10.376065</td>\n",
       "      <td>8.465263</td>\n",
       "      <td>5.950715</td>\n",
       "      <td>8.264014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.461868</td>\n",
       "      <td>0.737785</td>\n",
       "      <td>0.836303</td>\n",
       "      <td>0.678652</td>\n",
       "      <td>11.296299</td>\n",
       "      <td>8.960487</td>\n",
       "      <td>6.299888</td>\n",
       "      <td>8.852225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.374819</td>\n",
       "      <td>0.714908</td>\n",
       "      <td>0.81999</td>\n",
       "      <td>0.636573</td>\n",
       "      <td>12.060335</td>\n",
       "      <td>9.343994</td>\n",
       "      <td>6.605148</td>\n",
       "      <td>9.336492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.29779</td>\n",
       "      <td>0.695762</td>\n",
       "      <td>0.806171</td>\n",
       "      <td>0.599907</td>\n",
       "      <td>12.751399</td>\n",
       "      <td>9.653016</td>\n",
       "      <td>6.853319</td>\n",
       "      <td>9.752578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.234582</td>\n",
       "      <td>0.677442</td>\n",
       "      <td>0.79505</td>\n",
       "      <td>0.569025</td>\n",
       "      <td>13.328336</td>\n",
       "      <td>9.939865</td>\n",
       "      <td>7.046647</td>\n",
       "      <td>10.10495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.186055</td>\n",
       "      <td>0.658068</td>\n",
       "      <td>0.782914</td>\n",
       "      <td>0.542345</td>\n",
       "      <td>13.760489</td>\n",
       "      <td>10.234352</td>\n",
       "      <td>7.251761</td>\n",
       "      <td>10.415534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.528419</td>\n",
       "      <td>0.772843</td>\n",
       "      <td>0.858924</td>\n",
       "      <td>0.720062</td>\n",
       "      <td>10.159049</td>\n",
       "      <td>8.119207</td>\n",
       "      <td>5.673733</td>\n",
       "      <td>7.983996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE     nRMSE   \n",
       "0      0.921616  0.946618   0.96927  0.945834   4.545796   4.040363  2.733193   \n",
       "1      0.847189  0.893962  0.940099   0.89375   6.306412   5.695097  3.814998   \n",
       "2      0.752296  0.839567  0.907911  0.833258   7.919912   7.005771  4.729087   \n",
       "3       0.65331  0.798429  0.877515  0.776418   9.245444   7.853861  5.452574   \n",
       "4       0.55467  0.765889  0.854019   0.72486  10.376065   8.465263  5.950715   \n",
       "5      0.461868  0.737785  0.836303  0.678652  11.296299   8.960487  6.299888   \n",
       "6      0.374819  0.714908   0.81999  0.636573  12.060335   9.343994  6.605148   \n",
       "7       0.29779  0.695762  0.806171  0.599907  12.751399   9.653016  6.853319   \n",
       "8      0.234582  0.677442   0.79505  0.569025  13.328336   9.939865  7.046647   \n",
       "9      0.186055  0.658068  0.782914  0.542345  13.760489  10.234352  7.251761   \n",
       "mean   0.528419  0.772843  0.858924  0.720062  10.159049   8.119207  5.673733   \n",
       "\n",
       "            mean  \n",
       "index      nRMSE  \n",
       "0       3.773117  \n",
       "1       5.272169  \n",
       "2        6.55159  \n",
       "3       7.517293  \n",
       "4       8.264014  \n",
       "5       8.852225  \n",
       "6       9.336492  \n",
       "7       9.752578  \n",
       "8       10.10495  \n",
       "9      10.415534  \n",
       "mean    7.983996  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/10_10_1_50_1_50_datt_seq2seq_gru_6.csv\n",
      "\n",
      "\n",
      "6th iteration\n",
      "history size: 10\n",
      "future size: 20\n",
      "Epoch 1/10000\n",
      "577/577 - 8s - loss: 0.3147 - val_loss: 0.2552 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "577/577 - 5s - loss: 0.2255 - val_loss: 0.2407 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "577/577 - 5s - loss: 0.2116 - val_loss: 0.2265 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "577/577 - 5s - loss: 0.2023 - val_loss: 0.2194 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "577/577 - 5s - loss: 0.1925 - val_loss: 0.2044 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "577/577 - 5s - loss: 0.1858 - val_loss: 0.2008 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "577/577 - 5s - loss: 0.1805 - val_loss: 0.1945 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "577/577 - 5s - loss: 0.1734 - val_loss: 0.1887 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "577/577 - 5s - loss: 0.1682 - val_loss: 0.1836 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "577/577 - 5s - loss: 0.1631 - val_loss: 0.1783 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "577/577 - 5s - loss: 0.1573 - val_loss: 0.1799 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "577/577 - 5s - loss: 0.1514 - val_loss: 0.1710 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "577/577 - 5s - loss: 0.1486 - val_loss: 0.1645 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "577/577 - 5s - loss: 0.1442 - val_loss: 0.1596 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "577/577 - 5s - loss: 0.1402 - val_loss: 0.1628 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "577/577 - 5s - loss: 0.1356 - val_loss: 0.1499 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "577/577 - 5s - loss: 0.1307 - val_loss: 0.1521 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "577/577 - 5s - loss: 0.1278 - val_loss: 0.1446 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "577/577 - 5s - loss: 0.1267 - val_loss: 0.1480 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "577/577 - 5s - loss: 0.1213 - val_loss: 0.1410 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "577/577 - 5s - loss: 0.1179 - val_loss: 0.1397 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "577/577 - 5s - loss: 0.1189 - val_loss: 0.1382 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "577/577 - 5s - loss: 0.1128 - val_loss: 0.1341 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "577/577 - 5s - loss: 0.1110 - val_loss: 0.1279 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "577/577 - 5s - loss: 0.1101 - val_loss: 0.1324 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "577/577 - 5s - loss: 0.1098 - val_loss: 0.1269 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "577/577 - 5s - loss: 0.1063 - val_loss: 0.1266 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "577/577 - 4s - loss: 0.1069 - val_loss: 0.1399 - 4s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "577/577 - 5s - loss: 0.1050 - val_loss: 0.1214 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "577/577 - 5s - loss: 0.1030 - val_loss: 0.1318 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "577/577 - 5s - loss: 0.1009 - val_loss: 0.1207 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "577/577 - 5s - loss: 0.1033 - val_loss: 0.1223 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "577/577 - 5s - loss: 0.1001 - val_loss: 0.1240 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "577/577 - 4s - loss: 0.0966 - val_loss: 0.1155 - 4s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "577/577 - 5s - loss: 0.0972 - val_loss: 0.1162 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "577/577 - 5s - loss: 0.0960 - val_loss: 0.1176 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "577/577 - 5s - loss: 0.0928 - val_loss: 0.1175 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "577/577 - 5s - loss: 0.0929 - val_loss: 0.1155 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "577/577 - 5s - loss: 0.0931 - val_loss: 0.1133 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "577/577 - 4s - loss: 0.0901 - val_loss: 0.1116 - 4s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "577/577 - 5s - loss: 0.0912 - val_loss: 0.1137 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "577/577 - 5s - loss: 0.0908 - val_loss: 0.1187 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "577/577 - 5s - loss: 0.0878 - val_loss: 0.1098 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "577/577 - 5s - loss: 0.0866 - val_loss: 0.1106 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "577/577 - 5s - loss: 0.0895 - val_loss: 0.1070 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "577/577 - 5s - loss: 0.0879 - val_loss: 0.1112 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "577/577 - 5s - loss: 0.0885 - val_loss: 0.1100 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "577/577 - 5s - loss: 0.0835 - val_loss: 0.1054 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "577/577 - 5s - loss: 0.0834 - val_loss: 0.1064 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "577/577 - 5s - loss: 0.0824 - val_loss: 0.1058 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "577/577 - 5s - loss: 0.0830 - val_loss: 0.1039 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "577/577 - 5s - loss: 0.0826 - val_loss: 0.1049 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "577/577 - 5s - loss: 0.0807 - val_loss: 0.1039 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "577/577 - 5s - loss: 0.0806 - val_loss: 0.1089 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "577/577 - 5s - loss: 0.0830 - val_loss: 0.1008 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "577/577 - 5s - loss: 0.0816 - val_loss: 0.1038 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "577/577 - 5s - loss: 0.0814 - val_loss: 0.1036 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "577/577 - 5s - loss: 0.0776 - val_loss: 0.0961 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "577/577 - 5s - loss: 0.0776 - val_loss: 0.0998 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "577/577 - 5s - loss: 0.0765 - val_loss: 0.0971 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "577/577 - 5s - loss: 0.0770 - val_loss: 0.0957 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "577/577 - 5s - loss: 0.0758 - val_loss: 0.0992 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "577/577 - 5s - loss: 0.0757 - val_loss: 0.1009 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "577/577 - 5s - loss: 0.0763 - val_loss: 0.0959 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "577/577 - 5s - loss: 0.0778 - val_loss: 0.0979 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "577/577 - 5s - loss: 0.0738 - val_loss: 0.0957 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "577/577 - 5s - loss: 0.0730 - val_loss: 0.0999 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "577/577 - 5s - loss: 0.0741 - val_loss: 0.0966 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "577/577 - 5s - loss: 0.0735 - val_loss: 0.1199 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "577/577 - 5s - loss: 0.0755 - val_loss: 0.0977 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "577/577 - 5s - loss: 0.0706 - val_loss: 0.0979 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_230_layer_call_fn, gru_cell_230_layer_call_and_return_conditional_losses, gru_cell_231_layer_call_fn, gru_cell_231_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_20_1_50_1_50_datt_seq2seq_gru_6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_20_1_50_1_50_datt_seq2seq_gru_6\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B6FAE5070> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B651F5310> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.921665</td>\n",
       "      <td>0.937186</td>\n",
       "      <td>0.956842</td>\n",
       "      <td>0.938564</td>\n",
       "      <td>5.528168</td>\n",
       "      <td>4.378675</td>\n",
       "      <td>3.232314</td>\n",
       "      <td>4.379719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.848652</td>\n",
       "      <td>0.882906</td>\n",
       "      <td>0.914164</td>\n",
       "      <td>0.881907</td>\n",
       "      <td>7.544198</td>\n",
       "      <td>5.980189</td>\n",
       "      <td>4.557939</td>\n",
       "      <td>6.027442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.740588</td>\n",
       "      <td>0.840417</td>\n",
       "      <td>0.873614</td>\n",
       "      <td>0.818206</td>\n",
       "      <td>9.445647</td>\n",
       "      <td>6.983969</td>\n",
       "      <td>5.529981</td>\n",
       "      <td>7.319866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.631517</td>\n",
       "      <td>0.803489</td>\n",
       "      <td>0.842666</td>\n",
       "      <td>0.759224</td>\n",
       "      <td>10.88171</td>\n",
       "      <td>7.752672</td>\n",
       "      <td>6.168543</td>\n",
       "      <td>8.267642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.529363</td>\n",
       "      <td>0.77267</td>\n",
       "      <td>0.82434</td>\n",
       "      <td>0.708791</td>\n",
       "      <td>12.101926</td>\n",
       "      <td>8.342208</td>\n",
       "      <td>6.515913</td>\n",
       "      <td>8.986682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.432834</td>\n",
       "      <td>0.746842</td>\n",
       "      <td>0.811675</td>\n",
       "      <td>0.663784</td>\n",
       "      <td>13.241738</td>\n",
       "      <td>8.807674</td>\n",
       "      <td>6.744275</td>\n",
       "      <td>9.597896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.341599</td>\n",
       "      <td>0.722812</td>\n",
       "      <td>0.797031</td>\n",
       "      <td>0.62048</td>\n",
       "      <td>13.988732</td>\n",
       "      <td>9.219019</td>\n",
       "      <td>6.999663</td>\n",
       "      <td>10.069138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.257441</td>\n",
       "      <td>0.70072</td>\n",
       "      <td>0.779915</td>\n",
       "      <td>0.579359</td>\n",
       "      <td>14.571729</td>\n",
       "      <td>9.582408</td>\n",
       "      <td>7.28718</td>\n",
       "      <td>10.480439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.182596</td>\n",
       "      <td>0.67913</td>\n",
       "      <td>0.758178</td>\n",
       "      <td>0.539968</td>\n",
       "      <td>15.061665</td>\n",
       "      <td>9.925308</td>\n",
       "      <td>7.636917</td>\n",
       "      <td>10.87463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.116942</td>\n",
       "      <td>0.656592</td>\n",
       "      <td>0.727247</td>\n",
       "      <td>0.50026</td>\n",
       "      <td>15.427265</td>\n",
       "      <td>10.269942</td>\n",
       "      <td>8.108365</td>\n",
       "      <td>11.268524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.057196</td>\n",
       "      <td>0.633273</td>\n",
       "      <td>0.689259</td>\n",
       "      <td>0.45991</td>\n",
       "      <td>15.774948</td>\n",
       "      <td>10.61465</td>\n",
       "      <td>8.652663</td>\n",
       "      <td>11.680754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.610403</td>\n",
       "      <td>0.654327</td>\n",
       "      <td>0.421609</td>\n",
       "      <td>16.142369</td>\n",
       "      <td>10.941988</td>\n",
       "      <td>9.124401</td>\n",
       "      <td>12.069586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.056105</td>\n",
       "      <td>0.586966</td>\n",
       "      <td>0.626338</td>\n",
       "      <td>0.385733</td>\n",
       "      <td>16.364279</td>\n",
       "      <td>11.267455</td>\n",
       "      <td>9.484642</td>\n",
       "      <td>12.372125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.109586</td>\n",
       "      <td>0.564479</td>\n",
       "      <td>0.602208</td>\n",
       "      <td>0.352367</td>\n",
       "      <td>16.550452</td>\n",
       "      <td>11.571893</td>\n",
       "      <td>9.783976</td>\n",
       "      <td>12.63544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.161665</td>\n",
       "      <td>0.54069</td>\n",
       "      <td>0.578709</td>\n",
       "      <td>0.319245</td>\n",
       "      <td>16.766222</td>\n",
       "      <td>11.885382</td>\n",
       "      <td>10.0664</td>\n",
       "      <td>12.906002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.210024</td>\n",
       "      <td>0.515783</td>\n",
       "      <td>0.554784</td>\n",
       "      <td>0.286848</td>\n",
       "      <td>16.942922</td>\n",
       "      <td>12.205207</td>\n",
       "      <td>10.346355</td>\n",
       "      <td>13.164828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.253173</td>\n",
       "      <td>0.490587</td>\n",
       "      <td>0.532045</td>\n",
       "      <td>0.256486</td>\n",
       "      <td>17.075273</td>\n",
       "      <td>12.519358</td>\n",
       "      <td>10.606094</td>\n",
       "      <td>13.400242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.291751</td>\n",
       "      <td>0.467043</td>\n",
       "      <td>0.508732</td>\n",
       "      <td>0.228008</td>\n",
       "      <td>17.292373</td>\n",
       "      <td>12.805673</td>\n",
       "      <td>10.86658</td>\n",
       "      <td>13.654875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.323778</td>\n",
       "      <td>0.445695</td>\n",
       "      <td>0.482798</td>\n",
       "      <td>0.201572</td>\n",
       "      <td>17.522775</td>\n",
       "      <td>13.060108</td>\n",
       "      <td>11.149284</td>\n",
       "      <td>13.910722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.352138</td>\n",
       "      <td>0.425921</td>\n",
       "      <td>0.454406</td>\n",
       "      <td>0.176063</td>\n",
       "      <td>17.726783</td>\n",
       "      <td>13.291242</td>\n",
       "      <td>11.450925</td>\n",
       "      <td>14.156317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.165114</td>\n",
       "      <td>0.65118</td>\n",
       "      <td>0.698464</td>\n",
       "      <td>0.504919</td>\n",
       "      <td>14.297559</td>\n",
       "      <td>10.070251</td>\n",
       "      <td>8.21562</td>\n",
       "      <td>10.861143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.921665  0.937186  0.956842  0.938564   5.528168   4.378675   \n",
       "1      0.848652  0.882906  0.914164  0.881907   7.544198   5.980189   \n",
       "2      0.740588  0.840417  0.873614  0.818206   9.445647   6.983969   \n",
       "3      0.631517  0.803489  0.842666  0.759224   10.88171   7.752672   \n",
       "4      0.529363   0.77267   0.82434  0.708791  12.101926   8.342208   \n",
       "5      0.432834  0.746842  0.811675  0.663784  13.241738   8.807674   \n",
       "6      0.341599  0.722812  0.797031   0.62048  13.988732   9.219019   \n",
       "7      0.257441   0.70072  0.779915  0.579359  14.571729   9.582408   \n",
       "8      0.182596   0.67913  0.758178  0.539968  15.061665   9.925308   \n",
       "9      0.116942  0.656592  0.727247   0.50026  15.427265  10.269942   \n",
       "10     0.057196  0.633273  0.689259   0.45991  15.774948   10.61465   \n",
       "11     0.000098  0.610403  0.654327  0.421609  16.142369  10.941988   \n",
       "12    -0.056105  0.586966  0.626338  0.385733  16.364279  11.267455   \n",
       "13    -0.109586  0.564479  0.602208  0.352367  16.550452  11.571893   \n",
       "14    -0.161665   0.54069  0.578709  0.319245  16.766222  11.885382   \n",
       "15    -0.210024  0.515783  0.554784  0.286848  16.942922  12.205207   \n",
       "16    -0.253173  0.490587  0.532045  0.256486  17.075273  12.519358   \n",
       "17    -0.291751  0.467043  0.508732  0.228008  17.292373  12.805673   \n",
       "18    -0.323778  0.445695  0.482798  0.201572  17.522775  13.060108   \n",
       "19    -0.352138  0.425921  0.454406  0.176063  17.726783  13.291242   \n",
       "mean   0.165114   0.65118  0.698464  0.504919  14.297559  10.070251   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       3.232314   4.379719  \n",
       "1       4.557939   6.027442  \n",
       "2       5.529981   7.319866  \n",
       "3       6.168543   8.267642  \n",
       "4       6.515913   8.986682  \n",
       "5       6.744275   9.597896  \n",
       "6       6.999663  10.069138  \n",
       "7        7.28718  10.480439  \n",
       "8       7.636917   10.87463  \n",
       "9       8.108365  11.268524  \n",
       "10      8.652663  11.680754  \n",
       "11      9.124401  12.069586  \n",
       "12      9.484642  12.372125  \n",
       "13      9.783976   12.63544  \n",
       "14       10.0664  12.906002  \n",
       "15     10.346355  13.164828  \n",
       "16     10.606094  13.400242  \n",
       "17      10.86658  13.654875  \n",
       "18     11.149284  13.910722  \n",
       "19     11.450925  14.156317  \n",
       "mean     8.21562  10.861143  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/10_20_1_50_1_50_datt_seq2seq_gru_6.csv\n",
      "\n",
      "\n",
      "6th iteration\n",
      "history size: 10\n",
      "future size: 30\n",
      "Epoch 1/10000\n",
      "572/572 - 8s - loss: 0.3855 - val_loss: 0.2992 - 8s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "572/572 - 5s - loss: 0.3011 - val_loss: 0.2806 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "572/572 - 5s - loss: 0.2821 - val_loss: 0.2801 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "572/572 - 5s - loss: 0.2665 - val_loss: 0.2567 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "572/572 - 5s - loss: 0.2523 - val_loss: 0.2470 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "572/572 - 5s - loss: 0.2394 - val_loss: 0.2334 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "572/572 - 5s - loss: 0.2273 - val_loss: 0.2268 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "572/572 - 5s - loss: 0.2188 - val_loss: 0.2201 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "572/572 - 5s - loss: 0.2083 - val_loss: 0.2081 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "572/572 - 5s - loss: 0.2010 - val_loss: 0.2075 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "572/572 - 5s - loss: 0.1944 - val_loss: 0.1948 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "572/572 - 5s - loss: 0.1863 - val_loss: 0.1918 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "572/572 - 5s - loss: 0.1824 - val_loss: 0.1849 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "572/572 - 5s - loss: 0.1738 - val_loss: 0.1839 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "572/572 - 5s - loss: 0.1746 - val_loss: 0.1812 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "572/572 - 5s - loss: 0.1669 - val_loss: 0.1742 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "572/572 - 5s - loss: 0.1631 - val_loss: 0.1717 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "572/572 - 5s - loss: 0.1612 - val_loss: 0.1746 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "572/572 - 5s - loss: 0.1560 - val_loss: 0.1655 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "572/572 - 5s - loss: 0.1588 - val_loss: 0.1629 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "572/572 - 5s - loss: 0.1548 - val_loss: 0.1619 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "572/572 - 5s - loss: 0.1510 - val_loss: 0.1652 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "572/572 - 5s - loss: 0.1487 - val_loss: 0.1574 - 5s/epoch - 9ms/step\n",
      "Epoch 24/10000\n",
      "572/572 - 8s - loss: 0.1475 - val_loss: 0.1526 - 8s/epoch - 13ms/step\n",
      "Epoch 25/10000\n",
      "572/572 - 8s - loss: 0.1448 - val_loss: 0.1604 - 8s/epoch - 14ms/step\n",
      "Epoch 26/10000\n",
      "572/572 - 6s - loss: 0.1434 - val_loss: 0.1526 - 6s/epoch - 10ms/step\n",
      "Epoch 27/10000\n",
      "572/572 - 5s - loss: 0.1426 - val_loss: 0.1492 - 5s/epoch - 9ms/step\n",
      "Epoch 28/10000\n",
      "572/572 - 5s - loss: 0.1376 - val_loss: 0.1579 - 5s/epoch - 9ms/step\n",
      "Epoch 29/10000\n",
      "572/572 - 5s - loss: 0.1373 - val_loss: 0.1578 - 5s/epoch - 9ms/step\n",
      "Epoch 30/10000\n",
      "572/572 - 5s - loss: 0.1381 - val_loss: 0.1434 - 5s/epoch - 9ms/step\n",
      "Epoch 31/10000\n",
      "572/572 - 5s - loss: 0.1327 - val_loss: 0.1471 - 5s/epoch - 9ms/step\n",
      "Epoch 32/10000\n",
      "572/572 - 7s - loss: 0.1313 - val_loss: 0.1481 - 7s/epoch - 12ms/step\n",
      "Epoch 33/10000\n",
      "572/572 - 7s - loss: 0.1295 - val_loss: 0.1386 - 7s/epoch - 12ms/step\n",
      "Epoch 34/10000\n",
      "572/572 - 6s - loss: 0.1298 - val_loss: 0.1446 - 6s/epoch - 10ms/step\n",
      "Epoch 35/10000\n",
      "572/572 - 5s - loss: 0.1278 - val_loss: 0.1438 - 5s/epoch - 9ms/step\n",
      "Epoch 36/10000\n",
      "572/572 - 5s - loss: 0.1247 - val_loss: 0.1419 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "572/572 - 5s - loss: 0.1229 - val_loss: 0.1373 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "572/572 - 5s - loss: 0.1256 - val_loss: 0.1372 - 5s/epoch - 9ms/step\n",
      "Epoch 39/10000\n",
      "572/572 - 5s - loss: 0.1277 - val_loss: 0.1325 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "572/572 - 5s - loss: 0.1178 - val_loss: 0.1326 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "572/572 - 5s - loss: 0.1201 - val_loss: 0.1436 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "572/572 - 5s - loss: 0.1160 - val_loss: 0.1308 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "572/572 - 5s - loss: 0.1177 - val_loss: 0.1282 - 5s/epoch - 9ms/step\n",
      "Epoch 44/10000\n",
      "572/572 - 5s - loss: 0.1162 - val_loss: 0.1265 - 5s/epoch - 9ms/step\n",
      "Epoch 45/10000\n",
      "572/572 - 5s - loss: 0.1138 - val_loss: 0.1342 - 5s/epoch - 9ms/step\n",
      "Epoch 46/10000\n",
      "572/572 - 5s - loss: 0.1153 - val_loss: 0.1272 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "572/572 - 5s - loss: 0.1113 - val_loss: 0.1272 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "572/572 - 5s - loss: 0.1092 - val_loss: 0.1351 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "572/572 - 5s - loss: 0.1132 - val_loss: 0.1321 - 5s/epoch - 9ms/step\n",
      "Epoch 50/10000\n",
      "572/572 - 5s - loss: 0.1117 - val_loss: 0.1224 - 5s/epoch - 9ms/step\n",
      "Epoch 51/10000\n",
      "572/572 - 5s - loss: 0.1103 - val_loss: 0.1273 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "572/572 - 5s - loss: 0.1187 - val_loss: 0.1239 - 5s/epoch - 9ms/step\n",
      "Epoch 53/10000\n",
      "572/572 - 6s - loss: 0.1049 - val_loss: 0.1242 - 6s/epoch - 10ms/step\n",
      "Epoch 54/10000\n",
      "572/572 - 6s - loss: 0.1039 - val_loss: 0.1215 - 6s/epoch - 10ms/step\n",
      "Epoch 55/10000\n",
      "572/572 - 5s - loss: 0.1076 - val_loss: 0.1302 - 5s/epoch - 9ms/step\n",
      "Epoch 56/10000\n",
      "572/572 - 5s - loss: 0.1072 - val_loss: 0.1277 - 5s/epoch - 9ms/step\n",
      "Epoch 57/10000\n",
      "572/572 - 5s - loss: 0.1068 - val_loss: 0.1167 - 5s/epoch - 9ms/step\n",
      "Epoch 58/10000\n",
      "572/572 - 5s - loss: 0.1012 - val_loss: 0.1171 - 5s/epoch - 10ms/step\n",
      "Epoch 59/10000\n",
      "572/572 - 5s - loss: 0.1043 - val_loss: 0.1250 - 5s/epoch - 9ms/step\n",
      "Epoch 60/10000\n",
      "572/572 - 5s - loss: 0.1015 - val_loss: 0.1163 - 5s/epoch - 9ms/step\n",
      "Epoch 61/10000\n",
      "572/572 - 6s - loss: 0.0987 - val_loss: 0.1172 - 6s/epoch - 10ms/step\n",
      "Epoch 62/10000\n",
      "572/572 - 5s - loss: 0.0972 - val_loss: 0.1183 - 5s/epoch - 10ms/step\n",
      "Epoch 63/10000\n",
      "572/572 - 6s - loss: 0.0975 - val_loss: 0.1216 - 6s/epoch - 11ms/step\n",
      "Epoch 64/10000\n",
      "572/572 - 5s - loss: 0.1022 - val_loss: 0.1328 - 5s/epoch - 9ms/step\n",
      "Epoch 65/10000\n",
      "572/572 - 5s - loss: 0.0973 - val_loss: 0.1145 - 5s/epoch - 9ms/step\n",
      "Epoch 66/10000\n",
      "572/572 - 5s - loss: 0.0957 - val_loss: 0.1176 - 5s/epoch - 9ms/step\n",
      "Epoch 67/10000\n",
      "572/572 - 5s - loss: 0.0973 - val_loss: 0.1149 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "572/572 - 5s - loss: 0.0999 - val_loss: 0.1223 - 5s/epoch - 9ms/step\n",
      "Epoch 69/10000\n",
      "572/572 - 5s - loss: 0.0954 - val_loss: 0.1191 - 5s/epoch - 9ms/step\n",
      "Epoch 70/10000\n",
      "572/572 - 5s - loss: 0.0933 - val_loss: 0.1126 - 5s/epoch - 9ms/step\n",
      "Epoch 71/10000\n",
      "572/572 - 5s - loss: 0.0964 - val_loss: 0.1208 - 5s/epoch - 9ms/step\n",
      "Epoch 72/10000\n",
      "572/572 - 5s - loss: 0.0937 - val_loss: 0.1083 - 5s/epoch - 9ms/step\n",
      "Epoch 73/10000\n",
      "572/572 - 5s - loss: 0.0904 - val_loss: 0.1097 - 5s/epoch - 9ms/step\n",
      "Epoch 74/10000\n",
      "572/572 - 5s - loss: 0.0898 - val_loss: 0.1073 - 5s/epoch - 9ms/step\n",
      "Epoch 75/10000\n",
      "572/572 - 5s - loss: 0.0916 - val_loss: 0.1095 - 5s/epoch - 9ms/step\n",
      "Epoch 76/10000\n",
      "572/572 - 5s - loss: 0.0904 - val_loss: 0.1175 - 5s/epoch - 9ms/step\n",
      "Epoch 77/10000\n",
      "572/572 - 5s - loss: 0.0921 - val_loss: 0.1126 - 5s/epoch - 9ms/step\n",
      "Epoch 78/10000\n",
      "572/572 - 5s - loss: 0.0884 - val_loss: 0.1133 - 5s/epoch - 9ms/step\n",
      "Epoch 79/10000\n",
      "572/572 - 5s - loss: 0.0948 - val_loss: 0.1115 - 5s/epoch - 9ms/step\n",
      "Epoch 80/10000\n",
      "572/572 - 5s - loss: 0.0889 - val_loss: 0.1056 - 5s/epoch - 9ms/step\n",
      "Epoch 81/10000\n",
      "572/572 - 5s - loss: 0.0865 - val_loss: 0.1100 - 5s/epoch - 9ms/step\n",
      "Epoch 82/10000\n",
      "572/572 - 5s - loss: 0.0970 - val_loss: 0.1179 - 5s/epoch - 9ms/step\n",
      "Epoch 83/10000\n",
      "572/572 - 5s - loss: 0.0888 - val_loss: 0.1031 - 5s/epoch - 9ms/step\n",
      "Epoch 84/10000\n",
      "572/572 - 5s - loss: 0.0849 - val_loss: 0.1042 - 5s/epoch - 9ms/step\n",
      "Epoch 85/10000\n",
      "572/572 - 5s - loss: 0.0848 - val_loss: 0.1091 - 5s/epoch - 9ms/step\n",
      "Epoch 86/10000\n",
      "572/572 - 5s - loss: 0.0841 - val_loss: 0.1064 - 5s/epoch - 9ms/step\n",
      "Epoch 87/10000\n",
      "572/572 - 5s - loss: 0.0956 - val_loss: 0.1098 - 5s/epoch - 9ms/step\n",
      "Epoch 88/10000\n",
      "572/572 - 5s - loss: 0.0918 - val_loss: 0.1056 - 5s/epoch - 9ms/step\n",
      "Epoch 89/10000\n",
      "572/572 - 5s - loss: 0.0821 - val_loss: 0.0977 - 5s/epoch - 9ms/step\n",
      "Epoch 90/10000\n",
      "572/572 - 5s - loss: 0.0827 - val_loss: 0.1063 - 5s/epoch - 9ms/step\n",
      "Epoch 91/10000\n",
      "572/572 - 5s - loss: 0.0872 - val_loss: 0.1077 - 5s/epoch - 9ms/step\n",
      "Epoch 92/10000\n",
      "572/572 - 5s - loss: 0.0822 - val_loss: 0.0990 - 5s/epoch - 9ms/step\n",
      "Epoch 93/10000\n",
      "572/572 - 5s - loss: 0.0801 - val_loss: 0.1043 - 5s/epoch - 9ms/step\n",
      "Epoch 94/10000\n",
      "572/572 - 5s - loss: 0.0854 - val_loss: 0.1130 - 5s/epoch - 9ms/step\n",
      "Epoch 95/10000\n",
      "572/572 - 5s - loss: 0.0843 - val_loss: 0.1018 - 5s/epoch - 9ms/step\n",
      "Epoch 96/10000\n",
      "572/572 - 5s - loss: 0.0786 - val_loss: 0.1000 - 5s/epoch - 9ms/step\n",
      "Epoch 97/10000\n",
      "572/572 - 5s - loss: 0.0792 - val_loss: 0.1034 - 5s/epoch - 9ms/step\n",
      "Epoch 98/10000\n",
      "572/572 - 5s - loss: 0.0826 - val_loss: 0.1014 - 5s/epoch - 9ms/step\n",
      "Epoch 99/10000\n",
      "572/572 - 5s - loss: 0.0798 - val_loss: 0.1082 - 5s/epoch - 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_232_layer_call_fn, gru_cell_232_layer_call_and_return_conditional_losses, gru_cell_233_layer_call_fn, gru_cell_233_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_30_1_50_1_50_datt_seq2seq_gru_6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/10_30_1_50_1_50_datt_seq2seq_gru_6\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B5928ECA0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B4EA20940> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.880196</td>\n",
       "      <td>0.928104</td>\n",
       "      <td>0.935046</td>\n",
       "      <td>0.914449</td>\n",
       "      <td>6.833087</td>\n",
       "      <td>4.683607</td>\n",
       "      <td>3.950392</td>\n",
       "      <td>5.155696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.809378</td>\n",
       "      <td>0.894747</td>\n",
       "      <td>0.910928</td>\n",
       "      <td>0.871684</td>\n",
       "      <td>8.61932</td>\n",
       "      <td>5.667171</td>\n",
       "      <td>4.626077</td>\n",
       "      <td>6.304189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.712163</td>\n",
       "      <td>0.859896</td>\n",
       "      <td>0.881333</td>\n",
       "      <td>0.817797</td>\n",
       "      <td>10.593176</td>\n",
       "      <td>6.538931</td>\n",
       "      <td>5.339071</td>\n",
       "      <td>7.490393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.613736</td>\n",
       "      <td>0.833791</td>\n",
       "      <td>0.856189</td>\n",
       "      <td>0.767905</td>\n",
       "      <td>12.273763</td>\n",
       "      <td>7.123012</td>\n",
       "      <td>5.876115</td>\n",
       "      <td>8.424296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.520035</td>\n",
       "      <td>0.810729</td>\n",
       "      <td>0.838822</td>\n",
       "      <td>0.723195</td>\n",
       "      <td>13.683458</td>\n",
       "      <td>7.602534</td>\n",
       "      <td>6.21889</td>\n",
       "      <td>9.168294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.430827</td>\n",
       "      <td>0.789368</td>\n",
       "      <td>0.824124</td>\n",
       "      <td>0.68144</td>\n",
       "      <td>14.902228</td>\n",
       "      <td>8.022325</td>\n",
       "      <td>6.494251</td>\n",
       "      <td>9.806268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.344959</td>\n",
       "      <td>0.768502</td>\n",
       "      <td>0.806575</td>\n",
       "      <td>0.640012</td>\n",
       "      <td>15.988189</td>\n",
       "      <td>8.412793</td>\n",
       "      <td>6.808971</td>\n",
       "      <td>10.403318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.260391</td>\n",
       "      <td>0.747489</td>\n",
       "      <td>0.786139</td>\n",
       "      <td>0.598006</td>\n",
       "      <td>16.988767</td>\n",
       "      <td>8.788969</td>\n",
       "      <td>7.158469</td>\n",
       "      <td>10.978735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.177479</td>\n",
       "      <td>0.72408</td>\n",
       "      <td>0.763797</td>\n",
       "      <td>0.555119</td>\n",
       "      <td>17.914821</td>\n",
       "      <td>9.19103</td>\n",
       "      <td>7.521679</td>\n",
       "      <td>11.54251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.100362</td>\n",
       "      <td>0.698151</td>\n",
       "      <td>0.739179</td>\n",
       "      <td>0.512564</td>\n",
       "      <td>18.736828</td>\n",
       "      <td>9.61646</td>\n",
       "      <td>7.902394</td>\n",
       "      <td>12.085227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.031149</td>\n",
       "      <td>0.670778</td>\n",
       "      <td>0.713159</td>\n",
       "      <td>0.471695</td>\n",
       "      <td>19.448523</td>\n",
       "      <td>10.045734</td>\n",
       "      <td>8.285904</td>\n",
       "      <td>12.593387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.029714</td>\n",
       "      <td>0.644723</td>\n",
       "      <td>0.689243</td>\n",
       "      <td>0.434751</td>\n",
       "      <td>19.686365</td>\n",
       "      <td>10.439017</td>\n",
       "      <td>8.623242</td>\n",
       "      <td>12.916208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.084866</td>\n",
       "      <td>0.619959</td>\n",
       "      <td>0.670171</td>\n",
       "      <td>0.401755</td>\n",
       "      <td>19.325021</td>\n",
       "      <td>10.800746</td>\n",
       "      <td>8.882674</td>\n",
       "      <td>13.002814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.13216</td>\n",
       "      <td>0.597046</td>\n",
       "      <td>0.654965</td>\n",
       "      <td>0.373284</td>\n",
       "      <td>19.082253</td>\n",
       "      <td>11.125482</td>\n",
       "      <td>9.0839</td>\n",
       "      <td>13.097212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.173836</td>\n",
       "      <td>0.575604</td>\n",
       "      <td>0.641309</td>\n",
       "      <td>0.347692</td>\n",
       "      <td>19.118443</td>\n",
       "      <td>11.422781</td>\n",
       "      <td>9.261007</td>\n",
       "      <td>13.267411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.211006</td>\n",
       "      <td>0.554579</td>\n",
       "      <td>0.627431</td>\n",
       "      <td>0.323668</td>\n",
       "      <td>19.351221</td>\n",
       "      <td>11.708139</td>\n",
       "      <td>9.437027</td>\n",
       "      <td>13.498796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.242949</td>\n",
       "      <td>0.53252</td>\n",
       "      <td>0.612246</td>\n",
       "      <td>0.300606</td>\n",
       "      <td>19.218289</td>\n",
       "      <td>11.997995</td>\n",
       "      <td>9.626138</td>\n",
       "      <td>13.614141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.271522</td>\n",
       "      <td>0.509565</td>\n",
       "      <td>0.596142</td>\n",
       "      <td>0.278062</td>\n",
       "      <td>19.062975</td>\n",
       "      <td>12.292943</td>\n",
       "      <td>9.821998</td>\n",
       "      <td>13.725972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.298995</td>\n",
       "      <td>0.486361</td>\n",
       "      <td>0.580279</td>\n",
       "      <td>0.255882</td>\n",
       "      <td>18.978328</td>\n",
       "      <td>12.584556</td>\n",
       "      <td>10.010527</td>\n",
       "      <td>13.857804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.326305</td>\n",
       "      <td>0.464336</td>\n",
       "      <td>0.565356</td>\n",
       "      <td>0.234462</td>\n",
       "      <td>18.893689</td>\n",
       "      <td>12.854011</td>\n",
       "      <td>10.18372</td>\n",
       "      <td>13.97714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.354925</td>\n",
       "      <td>0.443082</td>\n",
       "      <td>0.550472</td>\n",
       "      <td>0.212876</td>\n",
       "      <td>18.89212</td>\n",
       "      <td>13.108755</td>\n",
       "      <td>10.353811</td>\n",
       "      <td>14.118229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.385234</td>\n",
       "      <td>0.423376</td>\n",
       "      <td>0.534433</td>\n",
       "      <td>0.190858</td>\n",
       "      <td>18.975444</td>\n",
       "      <td>13.340275</td>\n",
       "      <td>10.535097</td>\n",
       "      <td>14.283605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.416324</td>\n",
       "      <td>0.403948</td>\n",
       "      <td>0.51724</td>\n",
       "      <td>0.168288</td>\n",
       "      <td>18.921115</td>\n",
       "      <td>13.564422</td>\n",
       "      <td>10.727033</td>\n",
       "      <td>14.40419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.448669</td>\n",
       "      <td>0.383777</td>\n",
       "      <td>0.499745</td>\n",
       "      <td>0.144951</td>\n",
       "      <td>18.878126</td>\n",
       "      <td>13.79383</td>\n",
       "      <td>10.919776</td>\n",
       "      <td>14.530577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.483044</td>\n",
       "      <td>0.363368</td>\n",
       "      <td>0.484051</td>\n",
       "      <td>0.121458</td>\n",
       "      <td>18.911201</td>\n",
       "      <td>14.022235</td>\n",
       "      <td>11.089428</td>\n",
       "      <td>14.674288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.516761</td>\n",
       "      <td>0.344308</td>\n",
       "      <td>0.46993</td>\n",
       "      <td>0.099159</td>\n",
       "      <td>18.939903</td>\n",
       "      <td>14.232755</td>\n",
       "      <td>11.240241</td>\n",
       "      <td>14.804299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.547085</td>\n",
       "      <td>0.326275</td>\n",
       "      <td>0.457653</td>\n",
       "      <td>0.078948</td>\n",
       "      <td>18.947467</td>\n",
       "      <td>14.428175</td>\n",
       "      <td>11.370204</td>\n",
       "      <td>14.915282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.576742</td>\n",
       "      <td>0.310262</td>\n",
       "      <td>0.446984</td>\n",
       "      <td>0.060168</td>\n",
       "      <td>19.086155</td>\n",
       "      <td>14.599016</td>\n",
       "      <td>11.482515</td>\n",
       "      <td>15.055895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.605887</td>\n",
       "      <td>0.294555</td>\n",
       "      <td>0.437316</td>\n",
       "      <td>0.041995</td>\n",
       "      <td>19.288787</td>\n",
       "      <td>14.764717</td>\n",
       "      <td>11.583227</td>\n",
       "      <td>15.212244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.632548</td>\n",
       "      <td>0.279482</td>\n",
       "      <td>0.427685</td>\n",
       "      <td>0.024873</td>\n",
       "      <td>19.476208</td>\n",
       "      <td>14.921709</td>\n",
       "      <td>11.683265</td>\n",
       "      <td>15.360394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.06193</td>\n",
       "      <td>0.576092</td>\n",
       "      <td>0.650598</td>\n",
       "      <td>0.388253</td>\n",
       "      <td>17.300509</td>\n",
       "      <td>11.056471</td>\n",
       "      <td>8.869901</td>\n",
       "      <td>12.40896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.880196  0.928104  0.935046  0.914449   6.833087   4.683607   \n",
       "1      0.809378  0.894747  0.910928  0.871684    8.61932   5.667171   \n",
       "2      0.712163  0.859896  0.881333  0.817797  10.593176   6.538931   \n",
       "3      0.613736  0.833791  0.856189  0.767905  12.273763   7.123012   \n",
       "4      0.520035  0.810729  0.838822  0.723195  13.683458   7.602534   \n",
       "5      0.430827  0.789368  0.824124   0.68144  14.902228   8.022325   \n",
       "6      0.344959  0.768502  0.806575  0.640012  15.988189   8.412793   \n",
       "7      0.260391  0.747489  0.786139  0.598006  16.988767   8.788969   \n",
       "8      0.177479   0.72408  0.763797  0.555119  17.914821    9.19103   \n",
       "9      0.100362  0.698151  0.739179  0.512564  18.736828    9.61646   \n",
       "10     0.031149  0.670778  0.713159  0.471695  19.448523  10.045734   \n",
       "11    -0.029714  0.644723  0.689243  0.434751  19.686365  10.439017   \n",
       "12    -0.084866  0.619959  0.670171  0.401755  19.325021  10.800746   \n",
       "13     -0.13216  0.597046  0.654965  0.373284  19.082253  11.125482   \n",
       "14    -0.173836  0.575604  0.641309  0.347692  19.118443  11.422781   \n",
       "15    -0.211006  0.554579  0.627431  0.323668  19.351221  11.708139   \n",
       "16    -0.242949   0.53252  0.612246  0.300606  19.218289  11.997995   \n",
       "17    -0.271522  0.509565  0.596142  0.278062  19.062975  12.292943   \n",
       "18    -0.298995  0.486361  0.580279  0.255882  18.978328  12.584556   \n",
       "19    -0.326305  0.464336  0.565356  0.234462  18.893689  12.854011   \n",
       "20    -0.354925  0.443082  0.550472  0.212876   18.89212  13.108755   \n",
       "21    -0.385234  0.423376  0.534433  0.190858  18.975444  13.340275   \n",
       "22    -0.416324  0.403948   0.51724  0.168288  18.921115  13.564422   \n",
       "23    -0.448669  0.383777  0.499745  0.144951  18.878126   13.79383   \n",
       "24    -0.483044  0.363368  0.484051  0.121458  18.911201  14.022235   \n",
       "25    -0.516761  0.344308   0.46993  0.099159  18.939903  14.232755   \n",
       "26    -0.547085  0.326275  0.457653  0.078948  18.947467  14.428175   \n",
       "27    -0.576742  0.310262  0.446984  0.060168  19.086155  14.599016   \n",
       "28    -0.605887  0.294555  0.437316  0.041995  19.288787  14.764717   \n",
       "29    -0.632548  0.279482  0.427685  0.024873  19.476208  14.921709   \n",
       "mean   -0.06193  0.576092  0.650598  0.388253  17.300509  11.056471   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       3.950392   5.155696  \n",
       "1       4.626077   6.304189  \n",
       "2       5.339071   7.490393  \n",
       "3       5.876115   8.424296  \n",
       "4        6.21889   9.168294  \n",
       "5       6.494251   9.806268  \n",
       "6       6.808971  10.403318  \n",
       "7       7.158469  10.978735  \n",
       "8       7.521679   11.54251  \n",
       "9       7.902394  12.085227  \n",
       "10      8.285904  12.593387  \n",
       "11      8.623242  12.916208  \n",
       "12      8.882674  13.002814  \n",
       "13        9.0839  13.097212  \n",
       "14      9.261007  13.267411  \n",
       "15      9.437027  13.498796  \n",
       "16      9.626138  13.614141  \n",
       "17      9.821998  13.725972  \n",
       "18     10.010527  13.857804  \n",
       "19      10.18372   13.97714  \n",
       "20     10.353811  14.118229  \n",
       "21     10.535097  14.283605  \n",
       "22     10.727033   14.40419  \n",
       "23     10.919776  14.530577  \n",
       "24     11.089428  14.674288  \n",
       "25     11.240241  14.804299  \n",
       "26     11.370204  14.915282  \n",
       "27     11.482515  15.055895  \n",
       "28     11.583227  15.212244  \n",
       "29     11.683265  15.360394  \n",
       "mean    8.869901   12.40896  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/10_30_1_50_1_50_datt_seq2seq_gru_6.csv\n",
      "\n",
      "\n",
      "6th iteration\n",
      "history size: 20\n",
      "future size: 10\n",
      "Epoch 1/10000\n",
      "577/577 - 8s - loss: 0.2261 - val_loss: 0.1623 - 8s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "577/577 - 5s - loss: 0.1491 - val_loss: 0.1482 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "577/577 - 5s - loss: 0.1379 - val_loss: 0.1406 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "577/577 - 5s - loss: 0.1292 - val_loss: 0.1277 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "577/577 - 5s - loss: 0.1220 - val_loss: 0.1319 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "577/577 - 5s - loss: 0.1150 - val_loss: 0.1240 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "577/577 - 5s - loss: 0.1078 - val_loss: 0.1107 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "577/577 - 5s - loss: 0.1028 - val_loss: 0.1093 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "577/577 - 5s - loss: 0.0982 - val_loss: 0.1061 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "577/577 - 5s - loss: 0.0950 - val_loss: 0.0990 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "577/577 - 5s - loss: 0.0918 - val_loss: 0.0982 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "577/577 - 5s - loss: 0.0870 - val_loss: 0.0961 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "577/577 - 5s - loss: 0.0841 - val_loss: 0.0920 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "577/577 - 5s - loss: 0.0823 - val_loss: 0.0985 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "577/577 - 5s - loss: 0.0807 - val_loss: 0.1015 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "577/577 - 5s - loss: 0.0784 - val_loss: 0.0863 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "577/577 - 5s - loss: 0.0750 - val_loss: 0.0838 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "577/577 - 5s - loss: 0.0735 - val_loss: 0.0839 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "577/577 - 5s - loss: 0.0715 - val_loss: 0.0832 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "577/577 - 5s - loss: 0.0692 - val_loss: 0.0779 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "577/577 - 5s - loss: 0.0667 - val_loss: 0.0812 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "577/577 - 5s - loss: 0.0665 - val_loss: 0.0757 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "577/577 - 4s - loss: 0.0643 - val_loss: 0.0784 - 4s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "577/577 - 5s - loss: 0.0645 - val_loss: 0.0843 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "577/577 - 5s - loss: 0.0629 - val_loss: 0.0735 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "577/577 - 5s - loss: 0.0633 - val_loss: 0.0710 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "577/577 - 5s - loss: 0.0602 - val_loss: 0.0734 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "577/577 - 4s - loss: 0.0602 - val_loss: 0.0737 - 4s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "577/577 - 4s - loss: 0.0579 - val_loss: 0.0674 - 4s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "577/577 - 5s - loss: 0.0576 - val_loss: 0.0686 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "577/577 - 5s - loss: 0.0575 - val_loss: 0.0698 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "577/577 - 5s - loss: 0.0565 - val_loss: 0.0718 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "577/577 - 5s - loss: 0.0581 - val_loss: 0.0695 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "577/577 - 5s - loss: 0.0550 - val_loss: 0.0687 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "577/577 - 5s - loss: 0.0539 - val_loss: 0.0665 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "577/577 - 5s - loss: 0.0530 - val_loss: 0.0646 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "577/577 - 5s - loss: 0.0532 - val_loss: 0.0667 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "577/577 - 5s - loss: 0.0525 - val_loss: 0.0686 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "577/577 - 5s - loss: 0.0507 - val_loss: 0.0647 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "577/577 - 5s - loss: 0.0500 - val_loss: 0.0654 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "577/577 - 5s - loss: 0.0499 - val_loss: 0.0612 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "577/577 - 5s - loss: 0.0500 - val_loss: 0.0597 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "577/577 - 5s - loss: 0.0495 - val_loss: 0.0621 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "577/577 - 5s - loss: 0.0488 - val_loss: 0.0622 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "577/577 - 4s - loss: 0.0475 - val_loss: 0.0599 - 4s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "577/577 - 5s - loss: 0.0491 - val_loss: 0.0617 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "577/577 - 5s - loss: 0.0461 - val_loss: 0.0586 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "577/577 - 5s - loss: 0.0458 - val_loss: 0.0628 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "577/577 - 5s - loss: 0.0470 - val_loss: 0.0623 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "577/577 - 5s - loss: 0.0472 - val_loss: 0.0589 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "577/577 - 5s - loss: 0.0458 - val_loss: 0.0595 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "577/577 - 5s - loss: 0.0441 - val_loss: 0.0581 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "577/577 - 5s - loss: 0.0446 - val_loss: 0.0596 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "577/577 - 5s - loss: 0.0432 - val_loss: 0.0570 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "577/577 - 5s - loss: 0.0433 - val_loss: 0.0554 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "577/577 - 5s - loss: 0.0432 - val_loss: 0.0564 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "577/577 - 5s - loss: 0.0434 - val_loss: 0.0568 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "577/577 - 5s - loss: 0.0438 - val_loss: 0.0538 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "577/577 - 5s - loss: 0.0421 - val_loss: 0.0555 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "577/577 - 5s - loss: 0.0417 - val_loss: 0.0544 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "577/577 - 5s - loss: 0.0417 - val_loss: 0.0559 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "577/577 - 4s - loss: 0.0426 - val_loss: 0.0566 - 4s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "577/577 - 5s - loss: 0.0414 - val_loss: 0.0540 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "577/577 - 5s - loss: 0.0416 - val_loss: 0.0530 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "577/577 - 5s - loss: 0.0412 - val_loss: 0.0550 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "577/577 - 5s - loss: 0.0402 - val_loss: 0.0538 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "577/577 - 5s - loss: 0.0397 - val_loss: 0.0532 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "577/577 - 4s - loss: 0.0409 - val_loss: 0.0528 - 4s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "577/577 - 5s - loss: 0.0391 - val_loss: 0.0544 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "577/577 - 5s - loss: 0.0414 - val_loss: 0.0522 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "577/577 - 5s - loss: 0.0393 - val_loss: 0.0528 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "577/577 - 5s - loss: 0.0385 - val_loss: 0.0573 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "577/577 - 5s - loss: 0.0393 - val_loss: 0.0561 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "577/577 - 5s - loss: 0.0389 - val_loss: 0.0541 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "577/577 - 5s - loss: 0.0389 - val_loss: 0.0510 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "577/577 - 5s - loss: 0.0379 - val_loss: 0.0516 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "577/577 - 5s - loss: 0.0372 - val_loss: 0.0551 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "577/577 - 4s - loss: 0.0387 - val_loss: 0.0536 - 4s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "577/577 - 5s - loss: 0.0398 - val_loss: 0.0525 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "577/577 - 5s - loss: 0.0369 - val_loss: 0.0517 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "577/577 - 5s - loss: 0.0367 - val_loss: 0.0492 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "577/577 - 5s - loss: 0.0352 - val_loss: 0.0505 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "577/577 - 4s - loss: 0.0356 - val_loss: 0.0549 - 4s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "577/577 - 4s - loss: 0.0356 - val_loss: 0.0523 - 4s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "577/577 - 4s - loss: 0.0372 - val_loss: 0.0493 - 4s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "577/577 - 5s - loss: 0.0383 - val_loss: 0.0534 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "577/577 - 5s - loss: 0.0389 - val_loss: 0.0533 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "577/577 - 5s - loss: 0.0354 - val_loss: 0.0487 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "577/577 - 4s - loss: 0.0353 - val_loss: 0.0480 - 4s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "577/577 - 5s - loss: 0.0349 - val_loss: 0.0499 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "577/577 - 4s - loss: 0.0352 - val_loss: 0.0508 - 4s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "577/577 - 4s - loss: 0.0339 - val_loss: 0.0488 - 4s/epoch - 7ms/step\n",
      "Epoch 93/10000\n",
      "577/577 - 4s - loss: 0.0341 - val_loss: 0.0505 - 4s/epoch - 7ms/step\n",
      "Epoch 94/10000\n",
      "577/577 - 4s - loss: 0.0353 - val_loss: 0.0489 - 4s/epoch - 7ms/step\n",
      "Epoch 95/10000\n",
      "577/577 - 4s - loss: 0.0347 - val_loss: 0.0485 - 4s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "577/577 - 4s - loss: 0.0352 - val_loss: 0.0493 - 4s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "577/577 - 4s - loss: 0.0334 - val_loss: 0.0489 - 4s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "577/577 - 4s - loss: 0.0336 - val_loss: 0.0490 - 4s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "577/577 - 4s - loss: 0.0341 - val_loss: 0.0489 - 4s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_234_layer_call_fn, gru_cell_234_layer_call_and_return_conditional_losses, gru_cell_235_layer_call_fn, gru_cell_235_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_10_1_50_1_50_datt_seq2seq_gru_6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_10_1_50_1_50_datt_seq2seq_gru_6\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B4E890400> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B4E488DC0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.935277</td>\n",
       "      <td>0.944684</td>\n",
       "      <td>0.953347</td>\n",
       "      <td>0.944436</td>\n",
       "      <td>4.133194</td>\n",
       "      <td>4.122488</td>\n",
       "      <td>3.352658</td>\n",
       "      <td>3.869447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.858737</td>\n",
       "      <td>0.89761</td>\n",
       "      <td>0.911141</td>\n",
       "      <td>0.889163</td>\n",
       "      <td>6.067396</td>\n",
       "      <td>5.609416</td>\n",
       "      <td>4.626179</td>\n",
       "      <td>5.43433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.758912</td>\n",
       "      <td>0.850517</td>\n",
       "      <td>0.860909</td>\n",
       "      <td>0.823446</td>\n",
       "      <td>7.818643</td>\n",
       "      <td>6.778425</td>\n",
       "      <td>5.7867</td>\n",
       "      <td>6.79459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.655053</td>\n",
       "      <td>0.808074</td>\n",
       "      <td>0.828129</td>\n",
       "      <td>0.763752</td>\n",
       "      <td>9.227953</td>\n",
       "      <td>7.681872</td>\n",
       "      <td>6.431156</td>\n",
       "      <td>7.780327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.555621</td>\n",
       "      <td>0.773784</td>\n",
       "      <td>0.805531</td>\n",
       "      <td>0.711645</td>\n",
       "      <td>10.369827</td>\n",
       "      <td>8.341076</td>\n",
       "      <td>6.839249</td>\n",
       "      <td>8.516717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.466943</td>\n",
       "      <td>0.749617</td>\n",
       "      <td>0.786503</td>\n",
       "      <td>0.667688</td>\n",
       "      <td>11.24548</td>\n",
       "      <td>8.77662</td>\n",
       "      <td>7.164697</td>\n",
       "      <td>9.062266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.39071</td>\n",
       "      <td>0.732679</td>\n",
       "      <td>0.766285</td>\n",
       "      <td>0.629891</td>\n",
       "      <td>11.90623</td>\n",
       "      <td>9.069085</td>\n",
       "      <td>7.495423</td>\n",
       "      <td>9.490246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.327723</td>\n",
       "      <td>0.720348</td>\n",
       "      <td>0.742303</td>\n",
       "      <td>0.596791</td>\n",
       "      <td>12.474964</td>\n",
       "      <td>9.276092</td>\n",
       "      <td>7.870245</td>\n",
       "      <td>9.873767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.277833</td>\n",
       "      <td>0.709466</td>\n",
       "      <td>0.714327</td>\n",
       "      <td>0.567208</td>\n",
       "      <td>12.942383</td>\n",
       "      <td>9.455211</td>\n",
       "      <td>8.286123</td>\n",
       "      <td>10.227906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.240106</td>\n",
       "      <td>0.69283</td>\n",
       "      <td>0.691527</td>\n",
       "      <td>0.541488</td>\n",
       "      <td>13.289117</td>\n",
       "      <td>9.722298</td>\n",
       "      <td>8.61022</td>\n",
       "      <td>10.540545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.546691</td>\n",
       "      <td>0.787961</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.713551</td>\n",
       "      <td>9.947519</td>\n",
       "      <td>7.883258</td>\n",
       "      <td>6.646265</td>\n",
       "      <td>8.159014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3 TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE     nRMSE     nRMSE   \n",
       "0      0.935277  0.944684  0.953347  0.944436   4.133194  4.122488  3.352658   \n",
       "1      0.858737   0.89761  0.911141  0.889163   6.067396  5.609416  4.626179   \n",
       "2      0.758912  0.850517  0.860909  0.823446   7.818643  6.778425    5.7867   \n",
       "3      0.655053  0.808074  0.828129  0.763752   9.227953  7.681872  6.431156   \n",
       "4      0.555621  0.773784  0.805531  0.711645  10.369827  8.341076  6.839249   \n",
       "5      0.466943  0.749617  0.786503  0.667688   11.24548   8.77662  7.164697   \n",
       "6       0.39071  0.732679  0.766285  0.629891   11.90623  9.069085  7.495423   \n",
       "7      0.327723  0.720348  0.742303  0.596791  12.474964  9.276092  7.870245   \n",
       "8      0.277833  0.709466  0.714327  0.567208  12.942383  9.455211  8.286123   \n",
       "9      0.240106   0.69283  0.691527  0.541488  13.289117  9.722298   8.61022   \n",
       "mean   0.546691  0.787961     0.806  0.713551   9.947519  7.883258  6.646265   \n",
       "\n",
       "            mean  \n",
       "index      nRMSE  \n",
       "0       3.869447  \n",
       "1        5.43433  \n",
       "2        6.79459  \n",
       "3       7.780327  \n",
       "4       8.516717  \n",
       "5       9.062266  \n",
       "6       9.490246  \n",
       "7       9.873767  \n",
       "8      10.227906  \n",
       "9      10.540545  \n",
       "mean    8.159014  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/20_10_1_50_1_50_datt_seq2seq_gru_6.csv\n",
      "\n",
      "\n",
      "6th iteration\n",
      "history size: 20\n",
      "future size: 20\n",
      "Epoch 1/10000\n",
      "572/572 - 8s - loss: 0.3151 - val_loss: 0.2381 - 8s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "572/572 - 5s - loss: 0.2274 - val_loss: 0.2206 - 5s/epoch - 9ms/step\n",
      "Epoch 3/10000\n",
      "572/572 - 5s - loss: 0.2124 - val_loss: 0.2093 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "572/572 - 5s - loss: 0.2012 - val_loss: 0.1997 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "572/572 - 5s - loss: 0.1924 - val_loss: 0.2051 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "572/572 - 5s - loss: 0.1828 - val_loss: 0.1991 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "572/572 - 5s - loss: 0.1758 - val_loss: 0.1793 - 5s/epoch - 9ms/step\n",
      "Epoch 8/10000\n",
      "572/572 - 5s - loss: 0.1686 - val_loss: 0.1761 - 5s/epoch - 9ms/step\n",
      "Epoch 9/10000\n",
      "572/572 - 5s - loss: 0.1611 - val_loss: 0.1710 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "572/572 - 5s - loss: 0.1541 - val_loss: 0.1601 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "572/572 - 5s - loss: 0.1480 - val_loss: 0.1580 - 5s/epoch - 9ms/step\n",
      "Epoch 12/10000\n",
      "572/572 - 5s - loss: 0.1433 - val_loss: 0.1567 - 5s/epoch - 9ms/step\n",
      "Epoch 13/10000\n",
      "572/572 - 5s - loss: 0.1369 - val_loss: 0.1510 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "572/572 - 5s - loss: 0.1333 - val_loss: 0.1436 - 5s/epoch - 9ms/step\n",
      "Epoch 15/10000\n",
      "572/572 - 5s - loss: 0.1299 - val_loss: 0.1416 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "572/572 - 5s - loss: 0.1246 - val_loss: 0.1393 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "572/572 - 5s - loss: 0.1199 - val_loss: 0.1325 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "572/572 - 5s - loss: 0.1193 - val_loss: 0.1322 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "572/572 - 5s - loss: 0.1157 - val_loss: 0.1252 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "572/572 - 5s - loss: 0.1114 - val_loss: 0.1230 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "572/572 - 5s - loss: 0.1088 - val_loss: 0.1207 - 5s/epoch - 9ms/step\n",
      "Epoch 22/10000\n",
      "572/572 - 5s - loss: 0.1049 - val_loss: 0.1210 - 5s/epoch - 9ms/step\n",
      "Epoch 23/10000\n",
      "572/572 - 5s - loss: 0.1084 - val_loss: 0.1206 - 5s/epoch - 9ms/step\n",
      "Epoch 24/10000\n",
      "572/572 - 5s - loss: 0.1033 - val_loss: 0.1141 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "572/572 - 5s - loss: 0.1007 - val_loss: 0.1185 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "572/572 - 5s - loss: 0.1005 - val_loss: 0.1153 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "572/572 - 5s - loss: 0.0955 - val_loss: 0.1115 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "572/572 - 5s - loss: 0.0969 - val_loss: 0.1090 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "572/572 - 5s - loss: 0.0945 - val_loss: 0.1104 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "572/572 - 5s - loss: 0.0919 - val_loss: 0.1098 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "572/572 - 5s - loss: 0.0941 - val_loss: 0.1069 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "572/572 - 5s - loss: 0.0894 - val_loss: 0.1052 - 5s/epoch - 9ms/step\n",
      "Epoch 33/10000\n",
      "572/572 - 5s - loss: 0.0890 - val_loss: 0.1056 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "572/572 - 5s - loss: 0.0876 - val_loss: 0.1049 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "572/572 - 5s - loss: 0.0866 - val_loss: 0.1033 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "572/572 - 5s - loss: 0.0839 - val_loss: 0.1046 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "572/572 - 5s - loss: 0.0850 - val_loss: 0.1010 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "572/572 - 5s - loss: 0.0842 - val_loss: 0.1005 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "572/572 - 5s - loss: 0.0829 - val_loss: 0.0986 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "572/572 - 5s - loss: 0.0807 - val_loss: 0.0968 - 5s/epoch - 9ms/step\n",
      "Epoch 41/10000\n",
      "572/572 - 5s - loss: 0.0814 - val_loss: 0.1064 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "572/572 - 5s - loss: 0.0793 - val_loss: 0.0930 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "572/572 - 5s - loss: 0.0782 - val_loss: 0.0955 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "572/572 - 5s - loss: 0.0785 - val_loss: 0.0949 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "572/572 - 5s - loss: 0.0774 - val_loss: 0.0936 - 5s/epoch - 9ms/step\n",
      "Epoch 46/10000\n",
      "572/572 - 5s - loss: 0.0769 - val_loss: 0.0935 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "572/572 - 5s - loss: 0.0752 - val_loss: 0.0917 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "572/572 - 5s - loss: 0.0737 - val_loss: 0.0901 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "572/572 - 5s - loss: 0.0741 - val_loss: 0.0889 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "572/572 - 5s - loss: 0.0728 - val_loss: 0.0909 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "572/572 - 5s - loss: 0.0723 - val_loss: 0.0907 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "572/572 - 5s - loss: 0.0733 - val_loss: 0.0911 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "572/572 - 5s - loss: 0.0705 - val_loss: 0.0881 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "572/572 - 5s - loss: 0.0777 - val_loss: 0.0911 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "572/572 - 5s - loss: 0.0688 - val_loss: 0.0848 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "572/572 - 5s - loss: 0.0678 - val_loss: 0.0835 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "572/572 - 5s - loss: 0.0686 - val_loss: 0.0866 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "572/572 - 5s - loss: 0.0690 - val_loss: 0.0851 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "572/572 - 5s - loss: 0.0662 - val_loss: 0.0833 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "572/572 - 5s - loss: 0.0672 - val_loss: 0.0820 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "572/572 - 5s - loss: 0.0657 - val_loss: 0.0841 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "572/572 - 5s - loss: 0.0699 - val_loss: 0.0852 - 5s/epoch - 9ms/step\n",
      "Epoch 63/10000\n",
      "572/572 - 5s - loss: 0.0659 - val_loss: 0.0794 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "572/572 - 5s - loss: 0.0644 - val_loss: 0.0824 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "572/572 - 5s - loss: 0.0640 - val_loss: 0.0833 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "572/572 - 5s - loss: 0.0636 - val_loss: 0.0800 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "572/572 - 5s - loss: 0.0637 - val_loss: 0.0811 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "572/572 - 5s - loss: 0.0642 - val_loss: 0.0786 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "572/572 - 5s - loss: 0.0624 - val_loss: 0.0801 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "572/572 - 5s - loss: 0.0614 - val_loss: 0.0768 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "572/572 - 5s - loss: 0.0653 - val_loss: 0.0819 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "572/572 - 5s - loss: 0.0659 - val_loss: 0.0852 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "572/572 - 5s - loss: 0.0602 - val_loss: 0.0774 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "572/572 - 5s - loss: 0.0592 - val_loss: 0.0783 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "572/572 - 5s - loss: 0.0615 - val_loss: 0.0789 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "572/572 - 5s - loss: 0.0589 - val_loss: 0.0776 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "572/572 - 5s - loss: 0.0591 - val_loss: 0.0780 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "572/572 - 5s - loss: 0.0604 - val_loss: 0.0736 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "572/572 - 5s - loss: 0.0607 - val_loss: 0.0777 - 5s/epoch - 9ms/step\n",
      "Epoch 80/10000\n",
      "572/572 - 5s - loss: 0.0582 - val_loss: 0.0718 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "572/572 - 5s - loss: 0.0562 - val_loss: 0.0703 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "572/572 - 5s - loss: 0.0550 - val_loss: 0.0701 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "572/572 - 5s - loss: 0.0570 - val_loss: 0.0753 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "572/572 - 5s - loss: 0.0549 - val_loss: 0.0721 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "572/572 - 5s - loss: 0.0545 - val_loss: 0.0716 - 5s/epoch - 9ms/step\n",
      "Epoch 86/10000\n",
      "572/572 - 5s - loss: 0.0560 - val_loss: 0.0748 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "572/572 - 5s - loss: 0.0590 - val_loss: 0.0707 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "572/572 - 5s - loss: 0.0623 - val_loss: 0.0723 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "572/572 - 5s - loss: 0.0543 - val_loss: 0.0659 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "572/572 - 5s - loss: 0.0531 - val_loss: 0.0675 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "572/572 - 5s - loss: 0.0544 - val_loss: 0.0679 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "572/572 - 5s - loss: 0.0533 - val_loss: 0.0678 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "572/572 - 5s - loss: 0.0524 - val_loss: 0.0673 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "572/572 - 5s - loss: 0.0516 - val_loss: 0.0672 - 5s/epoch - 9ms/step\n",
      "Epoch 95/10000\n",
      "572/572 - 5s - loss: 0.0510 - val_loss: 0.0695 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "572/572 - 5s - loss: 0.0579 - val_loss: 0.0702 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "572/572 - 5s - loss: 0.0507 - val_loss: 0.0640 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "572/572 - 5s - loss: 0.0495 - val_loss: 0.0656 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "572/572 - 5s - loss: 0.0500 - val_loss: 0.0666 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "572/572 - 5s - loss: 0.0559 - val_loss: 0.0778 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "572/572 - 5s - loss: 0.0529 - val_loss: 0.0640 - 5s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "572/572 - 5s - loss: 0.0485 - val_loss: 0.0654 - 5s/epoch - 9ms/step\n",
      "Epoch 103/10000\n",
      "572/572 - 5s - loss: 0.0486 - val_loss: 0.0656 - 5s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "572/572 - 5s - loss: 0.0516 - val_loss: 0.0659 - 5s/epoch - 9ms/step\n",
      "Epoch 105/10000\n",
      "572/572 - 5s - loss: 0.0496 - val_loss: 0.0632 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "572/572 - 5s - loss: 0.0488 - val_loss: 0.0639 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "572/572 - 5s - loss: 0.0470 - val_loss: 0.0656 - 5s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "572/572 - 5s - loss: 0.0506 - val_loss: 0.0680 - 5s/epoch - 9ms/step\n",
      "Epoch 109/10000\n",
      "572/572 - 5s - loss: 0.0488 - val_loss: 0.0657 - 5s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "572/572 - 5s - loss: 0.0538 - val_loss: 0.0762 - 5s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "572/572 - 5s - loss: 0.0540 - val_loss: 0.0690 - 5s/epoch - 9ms/step\n",
      "Epoch 112/10000\n",
      "572/572 - 5s - loss: 0.0537 - val_loss: 0.0670 - 5s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "572/572 - 5s - loss: 0.0703 - val_loss: 0.0762 - 5s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "572/572 - 5s - loss: 0.0583 - val_loss: 0.0742 - 5s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "572/572 - 5s - loss: 0.0539 - val_loss: 0.0667 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_236_layer_call_fn, gru_cell_236_layer_call_and_return_conditional_losses, gru_cell_237_layer_call_fn, gru_cell_237_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_20_1_50_1_50_datt_seq2seq_gru_6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_20_1_50_1_50_datt_seq2seq_gru_6\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B515E2B20> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B4EF8E610> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.888316</td>\n",
       "      <td>0.913126</td>\n",
       "      <td>0.954129</td>\n",
       "      <td>0.918524</td>\n",
       "      <td>6.603198</td>\n",
       "      <td>5.16037</td>\n",
       "      <td>3.313498</td>\n",
       "      <td>5.025689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.795255</td>\n",
       "      <td>0.863101</td>\n",
       "      <td>0.913753</td>\n",
       "      <td>0.857369</td>\n",
       "      <td>8.778379</td>\n",
       "      <td>6.480021</td>\n",
       "      <td>4.542897</td>\n",
       "      <td>6.600432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.687446</td>\n",
       "      <td>0.8226</td>\n",
       "      <td>0.866826</td>\n",
       "      <td>0.792291</td>\n",
       "      <td>10.372765</td>\n",
       "      <td>7.379308</td>\n",
       "      <td>5.644277</td>\n",
       "      <td>7.798783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.585904</td>\n",
       "      <td>0.791448</td>\n",
       "      <td>0.835043</td>\n",
       "      <td>0.737465</td>\n",
       "      <td>11.54054</td>\n",
       "      <td>8.003849</td>\n",
       "      <td>6.28096</td>\n",
       "      <td>8.60845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.493697</td>\n",
       "      <td>0.766018</td>\n",
       "      <td>0.819002</td>\n",
       "      <td>0.692906</td>\n",
       "      <td>12.556078</td>\n",
       "      <td>8.481588</td>\n",
       "      <td>6.578621</td>\n",
       "      <td>9.205429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.407526</td>\n",
       "      <td>0.744052</td>\n",
       "      <td>0.811853</td>\n",
       "      <td>0.654477</td>\n",
       "      <td>13.535368</td>\n",
       "      <td>8.875208</td>\n",
       "      <td>6.706251</td>\n",
       "      <td>9.705609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.325613</td>\n",
       "      <td>0.723346</td>\n",
       "      <td>0.805363</td>\n",
       "      <td>0.618107</td>\n",
       "      <td>14.156072</td>\n",
       "      <td>9.229866</td>\n",
       "      <td>6.820029</td>\n",
       "      <td>10.068655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.246761</td>\n",
       "      <td>0.702907</td>\n",
       "      <td>0.795727</td>\n",
       "      <td>0.581798</td>\n",
       "      <td>14.672194</td>\n",
       "      <td>9.567781</td>\n",
       "      <td>6.985392</td>\n",
       "      <td>10.408456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.172964</td>\n",
       "      <td>0.683704</td>\n",
       "      <td>0.777364</td>\n",
       "      <td>0.544677</td>\n",
       "      <td>15.143159</td>\n",
       "      <td>9.875432</td>\n",
       "      <td>7.290786</td>\n",
       "      <td>10.769792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.107088</td>\n",
       "      <td>0.663741</td>\n",
       "      <td>0.745696</td>\n",
       "      <td>0.505509</td>\n",
       "      <td>15.502416</td>\n",
       "      <td>10.184247</td>\n",
       "      <td>7.789616</td>\n",
       "      <td>11.15876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.050826</td>\n",
       "      <td>0.639882</td>\n",
       "      <td>0.705374</td>\n",
       "      <td>0.465361</td>\n",
       "      <td>15.812333</td>\n",
       "      <td>10.541156</td>\n",
       "      <td>8.382188</td>\n",
       "      <td>11.578559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.004616</td>\n",
       "      <td>0.615199</td>\n",
       "      <td>0.668088</td>\n",
       "      <td>0.429301</td>\n",
       "      <td>16.085172</td>\n",
       "      <td>10.897741</td>\n",
       "      <td>8.895268</td>\n",
       "      <td>11.959394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.032931</td>\n",
       "      <td>0.592596</td>\n",
       "      <td>0.64168</td>\n",
       "      <td>0.400448</td>\n",
       "      <td>16.158498</td>\n",
       "      <td>11.21429</td>\n",
       "      <td>9.241649</td>\n",
       "      <td>12.204812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.063092</td>\n",
       "      <td>0.574557</td>\n",
       "      <td>0.624607</td>\n",
       "      <td>0.378691</td>\n",
       "      <td>16.17185</td>\n",
       "      <td>11.461361</td>\n",
       "      <td>9.459338</td>\n",
       "      <td>12.364183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.089236</td>\n",
       "      <td>0.559153</td>\n",
       "      <td>0.609653</td>\n",
       "      <td>0.359857</td>\n",
       "      <td>16.207012</td>\n",
       "      <td>11.668551</td>\n",
       "      <td>9.645646</td>\n",
       "      <td>12.50707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.111369</td>\n",
       "      <td>0.545316</td>\n",
       "      <td>0.590561</td>\n",
       "      <td>0.341503</td>\n",
       "      <td>16.212436</td>\n",
       "      <td>11.852059</td>\n",
       "      <td>9.878788</td>\n",
       "      <td>12.647761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.130702</td>\n",
       "      <td>0.531939</td>\n",
       "      <td>0.563518</td>\n",
       "      <td>0.321585</td>\n",
       "      <td>16.198252</td>\n",
       "      <td>12.025999</td>\n",
       "      <td>10.200295</td>\n",
       "      <td>12.808182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.148013</td>\n",
       "      <td>0.517312</td>\n",
       "      <td>0.52802</td>\n",
       "      <td>0.299106</td>\n",
       "      <td>16.285891</td>\n",
       "      <td>12.212779</td>\n",
       "      <td>10.607912</td>\n",
       "      <td>13.035527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.164806</td>\n",
       "      <td>0.500611</td>\n",
       "      <td>0.490741</td>\n",
       "      <td>0.275515</td>\n",
       "      <td>16.427607</td>\n",
       "      <td>12.422615</td>\n",
       "      <td>11.019626</td>\n",
       "      <td>13.289949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.181521</td>\n",
       "      <td>0.483429</td>\n",
       "      <td>0.459526</td>\n",
       "      <td>0.253811</td>\n",
       "      <td>16.568845</td>\n",
       "      <td>12.63459</td>\n",
       "      <td>11.353613</td>\n",
       "      <td>13.519016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.192217</td>\n",
       "      <td>0.661702</td>\n",
       "      <td>0.710326</td>\n",
       "      <td>0.521415</td>\n",
       "      <td>14.249403</td>\n",
       "      <td>10.008441</td>\n",
       "      <td>8.031833</td>\n",
       "      <td>10.763225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.888316  0.913126  0.954129  0.918524   6.603198    5.16037   \n",
       "1      0.795255  0.863101  0.913753  0.857369   8.778379   6.480021   \n",
       "2      0.687446    0.8226  0.866826  0.792291  10.372765   7.379308   \n",
       "3      0.585904  0.791448  0.835043  0.737465   11.54054   8.003849   \n",
       "4      0.493697  0.766018  0.819002  0.692906  12.556078   8.481588   \n",
       "5      0.407526  0.744052  0.811853  0.654477  13.535368   8.875208   \n",
       "6      0.325613  0.723346  0.805363  0.618107  14.156072   9.229866   \n",
       "7      0.246761  0.702907  0.795727  0.581798  14.672194   9.567781   \n",
       "8      0.172964  0.683704  0.777364  0.544677  15.143159   9.875432   \n",
       "9      0.107088  0.663741  0.745696  0.505509  15.502416  10.184247   \n",
       "10     0.050826  0.639882  0.705374  0.465361  15.812333  10.541156   \n",
       "11     0.004616  0.615199  0.668088  0.429301  16.085172  10.897741   \n",
       "12    -0.032931  0.592596   0.64168  0.400448  16.158498   11.21429   \n",
       "13    -0.063092  0.574557  0.624607  0.378691   16.17185  11.461361   \n",
       "14    -0.089236  0.559153  0.609653  0.359857  16.207012  11.668551   \n",
       "15    -0.111369  0.545316  0.590561  0.341503  16.212436  11.852059   \n",
       "16    -0.130702  0.531939  0.563518  0.321585  16.198252  12.025999   \n",
       "17    -0.148013  0.517312   0.52802  0.299106  16.285891  12.212779   \n",
       "18    -0.164806  0.500611  0.490741  0.275515  16.427607  12.422615   \n",
       "19    -0.181521  0.483429  0.459526  0.253811  16.568845   12.63459   \n",
       "mean   0.192217  0.661702  0.710326  0.521415  14.249403  10.008441   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       3.313498   5.025689  \n",
       "1       4.542897   6.600432  \n",
       "2       5.644277   7.798783  \n",
       "3        6.28096    8.60845  \n",
       "4       6.578621   9.205429  \n",
       "5       6.706251   9.705609  \n",
       "6       6.820029  10.068655  \n",
       "7       6.985392  10.408456  \n",
       "8       7.290786  10.769792  \n",
       "9       7.789616   11.15876  \n",
       "10      8.382188  11.578559  \n",
       "11      8.895268  11.959394  \n",
       "12      9.241649  12.204812  \n",
       "13      9.459338  12.364183  \n",
       "14      9.645646   12.50707  \n",
       "15      9.878788  12.647761  \n",
       "16     10.200295  12.808182  \n",
       "17     10.607912  13.035527  \n",
       "18     11.019626  13.289949  \n",
       "19     11.353613  13.519016  \n",
       "mean    8.031833  10.763225  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/20_20_1_50_1_50_datt_seq2seq_gru_6.csv\n",
      "\n",
      "\n",
      "6th iteration\n",
      "history size: 20\n",
      "future size: 30\n",
      "Epoch 1/10000\n",
      "568/568 - 8s - loss: 0.3958 - val_loss: 0.3322 - 8s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "568/568 - 5s - loss: 0.2886 - val_loss: 0.3009 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "568/568 - 5s - loss: 0.2675 - val_loss: 0.2934 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "568/568 - 5s - loss: 0.2547 - val_loss: 0.2647 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "568/568 - 5s - loss: 0.2376 - val_loss: 0.2467 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "568/568 - 5s - loss: 0.2255 - val_loss: 0.2432 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "568/568 - 5s - loss: 0.2152 - val_loss: 0.2397 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "568/568 - 5s - loss: 0.2051 - val_loss: 0.2217 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "568/568 - 5s - loss: 0.1961 - val_loss: 0.2123 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "568/568 - 5s - loss: 0.1868 - val_loss: 0.2111 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "568/568 - 5s - loss: 0.1809 - val_loss: 0.2110 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "568/568 - 5s - loss: 0.1741 - val_loss: 0.1942 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "568/568 - 5s - loss: 0.1677 - val_loss: 0.1938 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "568/568 - 5s - loss: 0.1616 - val_loss: 0.1893 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "568/568 - 5s - loss: 0.1577 - val_loss: 0.1787 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "568/568 - 5s - loss: 0.1530 - val_loss: 0.1776 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "568/568 - 5s - loss: 0.1485 - val_loss: 0.1723 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "568/568 - 5s - loss: 0.1467 - val_loss: 0.1644 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "568/568 - 5s - loss: 0.1419 - val_loss: 0.1628 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "568/568 - 5s - loss: 0.1412 - val_loss: 0.1617 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "568/568 - 5s - loss: 0.1354 - val_loss: 0.1625 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "568/568 - 5s - loss: 0.1349 - val_loss: 0.1548 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "568/568 - 5s - loss: 0.1310 - val_loss: 0.1443 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "568/568 - 5s - loss: 0.1276 - val_loss: 0.1460 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "568/568 - 5s - loss: 0.1253 - val_loss: 0.1448 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "568/568 - 5s - loss: 0.1246 - val_loss: 0.1419 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "568/568 - 5s - loss: 0.1211 - val_loss: 0.1298 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "568/568 - 5s - loss: 0.1185 - val_loss: 0.1303 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "568/568 - 5s - loss: 0.1192 - val_loss: 0.1396 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "568/568 - 5s - loss: 0.1154 - val_loss: 0.1396 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "568/568 - 5s - loss: 0.1165 - val_loss: 0.1298 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "568/568 - 5s - loss: 0.1111 - val_loss: 0.1351 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "568/568 - 5s - loss: 0.1118 - val_loss: 0.1462 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "568/568 - 5s - loss: 0.1152 - val_loss: 0.1242 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "568/568 - 4s - loss: 0.1095 - val_loss: 0.1204 - 4s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "568/568 - 5s - loss: 0.1036 - val_loss: 0.1163 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "568/568 - 5s - loss: 0.1071 - val_loss: 0.1299 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "568/568 - 5s - loss: 0.1049 - val_loss: 0.1165 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "568/568 - 4s - loss: 0.1049 - val_loss: 0.1185 - 4s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "568/568 - 5s - loss: 0.1017 - val_loss: 0.1169 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "568/568 - 5s - loss: 0.1000 - val_loss: 0.1128 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "568/568 - 5s - loss: 0.1016 - val_loss: 0.1184 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "568/568 - 4s - loss: 0.0976 - val_loss: 0.1192 - 4s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "568/568 - 4s - loss: 0.0963 - val_loss: 0.1107 - 4s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "568/568 - 4s - loss: 0.1042 - val_loss: 0.1087 - 4s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "568/568 - 5s - loss: 0.0958 - val_loss: 0.1108 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "568/568 - 5s - loss: 0.0949 - val_loss: 0.1065 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "568/568 - 5s - loss: 0.0929 - val_loss: 0.1035 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "568/568 - 4s - loss: 0.0921 - val_loss: 0.1074 - 4s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "568/568 - 5s - loss: 0.0920 - val_loss: 0.1078 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "568/568 - 5s - loss: 0.0925 - val_loss: 0.1070 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "568/568 - 4s - loss: 0.0882 - val_loss: 0.1075 - 4s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "568/568 - 5s - loss: 0.0903 - val_loss: 0.1071 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "568/568 - 4s - loss: 0.1024 - val_loss: 0.1221 - 4s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "568/568 - 5s - loss: 0.0950 - val_loss: 0.1035 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "568/568 - 5s - loss: 0.0899 - val_loss: 0.1276 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "568/568 - 5s - loss: 0.0959 - val_loss: 0.1098 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "568/568 - 4s - loss: 0.0853 - val_loss: 0.1005 - 4s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "568/568 - 5s - loss: 0.0836 - val_loss: 0.1000 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "568/568 - 5s - loss: 0.0840 - val_loss: 0.1071 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "568/568 - 5s - loss: 0.0836 - val_loss: 0.1020 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "568/568 - 5s - loss: 0.0842 - val_loss: 0.0970 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "568/568 - 4s - loss: 0.0816 - val_loss: 0.0939 - 4s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "568/568 - 4s - loss: 0.0813 - val_loss: 0.0984 - 4s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "568/568 - 4s - loss: 0.0838 - val_loss: 0.1080 - 4s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "568/568 - 4s - loss: 0.0811 - val_loss: 0.0946 - 4s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "568/568 - 5s - loss: 0.0802 - val_loss: 0.0926 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "568/568 - 4s - loss: 0.0785 - val_loss: 0.0938 - 4s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "568/568 - 4s - loss: 0.0792 - val_loss: 0.0926 - 4s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "568/568 - 5s - loss: 0.0787 - val_loss: 0.0909 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "568/568 - 5s - loss: 0.0793 - val_loss: 0.1089 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "568/568 - 5s - loss: 0.0777 - val_loss: 0.0938 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "568/568 - 5s - loss: 0.0785 - val_loss: 0.0964 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "568/568 - 4s - loss: 0.0785 - val_loss: 0.0910 - 4s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "568/568 - 5s - loss: 0.0753 - val_loss: 0.0876 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "568/568 - 4s - loss: 0.0738 - val_loss: 0.0870 - 4s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "568/568 - 4s - loss: 0.0778 - val_loss: 0.0956 - 4s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "568/568 - 4s - loss: 0.0738 - val_loss: 0.0854 - 4s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "568/568 - 5s - loss: 0.0740 - val_loss: 0.0860 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "568/568 - 4s - loss: 0.0718 - val_loss: 0.0865 - 4s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "568/568 - 4s - loss: 0.0759 - val_loss: 0.0885 - 4s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "568/568 - 4s - loss: 0.0720 - val_loss: 0.0866 - 4s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "568/568 - 5s - loss: 0.0745 - val_loss: 0.0818 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "568/568 - 5s - loss: 0.0710 - val_loss: 0.0969 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "568/568 - 5s - loss: 0.0757 - val_loss: 0.1026 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "568/568 - 4s - loss: 0.0703 - val_loss: 0.0807 - 4s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "568/568 - 4s - loss: 0.0700 - val_loss: 0.0833 - 4s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "568/568 - 5s - loss: 0.0726 - val_loss: 0.0999 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "568/568 - 4s - loss: 0.0704 - val_loss: 0.0896 - 4s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "568/568 - 5s - loss: 0.0698 - val_loss: 0.0965 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "568/568 - 4s - loss: 0.0693 - val_loss: 0.0839 - 4s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "568/568 - 5s - loss: 0.0687 - val_loss: 0.0822 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "568/568 - 4s - loss: 0.0724 - val_loss: 0.0846 - 4s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "568/568 - 5s - loss: 0.0679 - val_loss: 0.0798 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "568/568 - 5s - loss: 0.0688 - val_loss: 0.0996 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "568/568 - 5s - loss: 0.0685 - val_loss: 0.0800 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "568/568 - 5s - loss: 0.0664 - val_loss: 0.0806 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "568/568 - 5s - loss: 0.0673 - val_loss: 0.0809 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "568/568 - 5s - loss: 0.0664 - val_loss: 0.0877 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "568/568 - 5s - loss: 0.0666 - val_loss: 0.0821 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "568/568 - 5s - loss: 0.0656 - val_loss: 0.0826 - 5s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "568/568 - 5s - loss: 0.0659 - val_loss: 0.0779 - 5s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "568/568 - 4s - loss: 0.0654 - val_loss: 0.0766 - 4s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "568/568 - 5s - loss: 0.0662 - val_loss: 0.0773 - 5s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "568/568 - 5s - loss: 0.0660 - val_loss: 0.0814 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "568/568 - 5s - loss: 0.0639 - val_loss: 0.0797 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "568/568 - 4s - loss: 0.0659 - val_loss: 0.0789 - 4s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "568/568 - 4s - loss: 0.0803 - val_loss: 0.1444 - 4s/epoch - 8ms/step\n",
      "Epoch 109/10000\n",
      "568/568 - 4s - loss: 0.0859 - val_loss: 0.0846 - 4s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "568/568 - 4s - loss: 0.0668 - val_loss: 0.0775 - 4s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "568/568 - 4s - loss: 0.0627 - val_loss: 0.0768 - 4s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "568/568 - 5s - loss: 0.0613 - val_loss: 0.0731 - 5s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "568/568 - 5s - loss: 0.0622 - val_loss: 0.0754 - 5s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "568/568 - 5s - loss: 0.0626 - val_loss: 0.0775 - 5s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "568/568 - 5s - loss: 0.0639 - val_loss: 0.0969 - 5s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "568/568 - 5s - loss: 0.0678 - val_loss: 0.0752 - 5s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "568/568 - 5s - loss: 0.0610 - val_loss: 0.0789 - 5s/epoch - 8ms/step\n",
      "Epoch 118/10000\n",
      "568/568 - 5s - loss: 0.0641 - val_loss: 0.0930 - 5s/epoch - 8ms/step\n",
      "Epoch 119/10000\n",
      "568/568 - 4s - loss: 0.0628 - val_loss: 0.0816 - 4s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "568/568 - 5s - loss: 0.0613 - val_loss: 0.0795 - 5s/epoch - 8ms/step\n",
      "Epoch 121/10000\n",
      "568/568 - 5s - loss: 0.0607 - val_loss: 0.0717 - 5s/epoch - 8ms/step\n",
      "Epoch 122/10000\n",
      "568/568 - 5s - loss: 0.0607 - val_loss: 0.0784 - 5s/epoch - 8ms/step\n",
      "Epoch 123/10000\n",
      "568/568 - 5s - loss: 0.0650 - val_loss: 0.0798 - 5s/epoch - 8ms/step\n",
      "Epoch 124/10000\n",
      "568/568 - 5s - loss: 0.0607 - val_loss: 0.0760 - 5s/epoch - 8ms/step\n",
      "Epoch 125/10000\n",
      "568/568 - 5s - loss: 0.0607 - val_loss: 0.0764 - 5s/epoch - 8ms/step\n",
      "Epoch 126/10000\n",
      "568/568 - 4s - loss: 0.0622 - val_loss: 0.0758 - 4s/epoch - 8ms/step\n",
      "Epoch 127/10000\n",
      "568/568 - 4s - loss: 0.0619 - val_loss: 0.0746 - 4s/epoch - 8ms/step\n",
      "Epoch 128/10000\n",
      "568/568 - 4s - loss: 0.0590 - val_loss: 0.0726 - 4s/epoch - 8ms/step\n",
      "Epoch 129/10000\n",
      "568/568 - 4s - loss: 0.0591 - val_loss: 0.0739 - 4s/epoch - 8ms/step\n",
      "Epoch 130/10000\n",
      "568/568 - 4s - loss: 0.0608 - val_loss: 0.0732 - 4s/epoch - 8ms/step\n",
      "Epoch 131/10000\n",
      "568/568 - 4s - loss: 0.0596 - val_loss: 0.0749 - 4s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_238_layer_call_fn, gru_cell_238_layer_call_and_return_conditional_losses, gru_cell_239_layer_call_fn, gru_cell_239_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_30_1_50_1_50_datt_seq2seq_gru_6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/20_30_1_50_1_50_datt_seq2seq_gru_6\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B51D41520> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B4E21AA60> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.862357</td>\n",
       "      <td>0.91184</td>\n",
       "      <td>0.921265</td>\n",
       "      <td>0.898487</td>\n",
       "      <td>7.32505</td>\n",
       "      <td>5.197219</td>\n",
       "      <td>4.321141</td>\n",
       "      <td>5.61447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.705275</td>\n",
       "      <td>0.839046</td>\n",
       "      <td>0.881572</td>\n",
       "      <td>0.808631</td>\n",
       "      <td>10.718879</td>\n",
       "      <td>7.022837</td>\n",
       "      <td>5.299731</td>\n",
       "      <td>7.680482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.537571</td>\n",
       "      <td>0.778689</td>\n",
       "      <td>0.837949</td>\n",
       "      <td>0.718069</td>\n",
       "      <td>13.428136</td>\n",
       "      <td>8.235742</td>\n",
       "      <td>6.199716</td>\n",
       "      <td>9.287865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.38846</td>\n",
       "      <td>0.732954</td>\n",
       "      <td>0.803188</td>\n",
       "      <td>0.641534</td>\n",
       "      <td>15.444275</td>\n",
       "      <td>9.047922</td>\n",
       "      <td>6.832521</td>\n",
       "      <td>10.441572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.256005</td>\n",
       "      <td>0.69663</td>\n",
       "      <td>0.770508</td>\n",
       "      <td>0.574381</td>\n",
       "      <td>17.034381</td>\n",
       "      <td>9.645487</td>\n",
       "      <td>7.377952</td>\n",
       "      <td>11.352607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.145241</td>\n",
       "      <td>0.669713</td>\n",
       "      <td>0.740222</td>\n",
       "      <td>0.518392</td>\n",
       "      <td>18.255411</td>\n",
       "      <td>10.067267</td>\n",
       "      <td>7.848914</td>\n",
       "      <td>12.057197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.056152</td>\n",
       "      <td>0.651435</td>\n",
       "      <td>0.71131</td>\n",
       "      <td>0.472966</td>\n",
       "      <td>19.1798</td>\n",
       "      <td>10.345066</td>\n",
       "      <td>8.273034</td>\n",
       "      <td>12.5993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.014523</td>\n",
       "      <td>0.638331</td>\n",
       "      <td>0.684313</td>\n",
       "      <td>0.43604</td>\n",
       "      <td>19.880651</td>\n",
       "      <td>10.540912</td>\n",
       "      <td>8.64958</td>\n",
       "      <td>13.023714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.069416</td>\n",
       "      <td>0.625495</td>\n",
       "      <td>0.658578</td>\n",
       "      <td>0.404886</td>\n",
       "      <td>20.405548</td>\n",
       "      <td>10.730607</td>\n",
       "      <td>8.993123</td>\n",
       "      <td>13.376426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.112006</td>\n",
       "      <td>0.611245</td>\n",
       "      <td>0.633384</td>\n",
       "      <td>0.377541</td>\n",
       "      <td>20.803254</td>\n",
       "      <td>10.936557</td>\n",
       "      <td>9.317779</td>\n",
       "      <td>13.685863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.145078</td>\n",
       "      <td>0.594755</td>\n",
       "      <td>0.608941</td>\n",
       "      <td>0.352873</td>\n",
       "      <td>21.107614</td>\n",
       "      <td>11.169141</td>\n",
       "      <td>9.622939</td>\n",
       "      <td>13.966565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.173439</td>\n",
       "      <td>0.576676</td>\n",
       "      <td>0.585755</td>\n",
       "      <td>0.329664</td>\n",
       "      <td>20.973362</td>\n",
       "      <td>11.419045</td>\n",
       "      <td>9.904735</td>\n",
       "      <td>14.099047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.19878</td>\n",
       "      <td>0.557438</td>\n",
       "      <td>0.56327</td>\n",
       "      <td>0.307309</td>\n",
       "      <td>20.267473</td>\n",
       "      <td>11.679911</td>\n",
       "      <td>10.171107</td>\n",
       "      <td>14.039497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.222471</td>\n",
       "      <td>0.537591</td>\n",
       "      <td>0.540307</td>\n",
       "      <td>0.285142</td>\n",
       "      <td>19.779183</td>\n",
       "      <td>11.942934</td>\n",
       "      <td>10.436075</td>\n",
       "      <td>14.052731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.2465</td>\n",
       "      <td>0.517551</td>\n",
       "      <td>0.516671</td>\n",
       "      <td>0.262574</td>\n",
       "      <td>19.651916</td>\n",
       "      <td>12.20436</td>\n",
       "      <td>10.701696</td>\n",
       "      <td>14.185991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.270412</td>\n",
       "      <td>0.496835</td>\n",
       "      <td>0.494414</td>\n",
       "      <td>0.240279</td>\n",
       "      <td>19.774226</td>\n",
       "      <td>12.469645</td>\n",
       "      <td>10.945138</td>\n",
       "      <td>14.396337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.291331</td>\n",
       "      <td>0.475608</td>\n",
       "      <td>0.47414</td>\n",
       "      <td>0.219472</td>\n",
       "      <td>19.548549</td>\n",
       "      <td>12.733662</td>\n",
       "      <td>11.162515</td>\n",
       "      <td>14.481575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.308846</td>\n",
       "      <td>0.455077</td>\n",
       "      <td>0.4549</td>\n",
       "      <td>0.200377</td>\n",
       "      <td>19.308426</td>\n",
       "      <td>12.984497</td>\n",
       "      <td>11.365159</td>\n",
       "      <td>14.552694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.323486</td>\n",
       "      <td>0.43608</td>\n",
       "      <td>0.436388</td>\n",
       "      <td>0.182994</td>\n",
       "      <td>19.133398</td>\n",
       "      <td>13.212906</td>\n",
       "      <td>11.557193</td>\n",
       "      <td>14.634499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.336614</td>\n",
       "      <td>0.418108</td>\n",
       "      <td>0.415918</td>\n",
       "      <td>0.165804</td>\n",
       "      <td>18.953927</td>\n",
       "      <td>13.424147</td>\n",
       "      <td>11.765905</td>\n",
       "      <td>14.714659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.350848</td>\n",
       "      <td>0.400635</td>\n",
       "      <td>0.393941</td>\n",
       "      <td>0.147909</td>\n",
       "      <td>18.861252</td>\n",
       "      <td>13.626347</td>\n",
       "      <td>11.986196</td>\n",
       "      <td>14.824598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.365198</td>\n",
       "      <td>0.382924</td>\n",
       "      <td>0.370678</td>\n",
       "      <td>0.129468</td>\n",
       "      <td>18.844315</td>\n",
       "      <td>13.827646</td>\n",
       "      <td>12.215623</td>\n",
       "      <td>14.962528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.382032</td>\n",
       "      <td>0.364852</td>\n",
       "      <td>0.348153</td>\n",
       "      <td>0.110324</td>\n",
       "      <td>18.704109</td>\n",
       "      <td>14.029867</td>\n",
       "      <td>12.434078</td>\n",
       "      <td>15.056018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.399413</td>\n",
       "      <td>0.348411</td>\n",
       "      <td>0.327363</td>\n",
       "      <td>0.09212</td>\n",
       "      <td>18.57175</td>\n",
       "      <td>14.212313</td>\n",
       "      <td>12.632372</td>\n",
       "      <td>15.138811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.417369</td>\n",
       "      <td>0.33269</td>\n",
       "      <td>0.309872</td>\n",
       "      <td>0.075064</td>\n",
       "      <td>18.505943</td>\n",
       "      <td>14.384637</td>\n",
       "      <td>12.79643</td>\n",
       "      <td>15.229003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.434294</td>\n",
       "      <td>0.317861</td>\n",
       "      <td>0.294637</td>\n",
       "      <td>0.059401</td>\n",
       "      <td>18.43436</td>\n",
       "      <td>14.545068</td>\n",
       "      <td>12.938308</td>\n",
       "      <td>15.305912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.44918</td>\n",
       "      <td>0.303557</td>\n",
       "      <td>0.280556</td>\n",
       "      <td>0.044977</td>\n",
       "      <td>18.351342</td>\n",
       "      <td>14.696627</td>\n",
       "      <td>13.069113</td>\n",
       "      <td>15.372361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.463686</td>\n",
       "      <td>0.28879</td>\n",
       "      <td>0.266811</td>\n",
       "      <td>0.030638</td>\n",
       "      <td>18.398299</td>\n",
       "      <td>14.850495</td>\n",
       "      <td>13.19718</td>\n",
       "      <td>15.481991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.479151</td>\n",
       "      <td>0.274138</td>\n",
       "      <td>0.252708</td>\n",
       "      <td>0.015898</td>\n",
       "      <td>18.516867</td>\n",
       "      <td>15.000991</td>\n",
       "      <td>13.327731</td>\n",
       "      <td>15.615196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.493767</td>\n",
       "      <td>0.257943</td>\n",
       "      <td>0.237009</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>18.629533</td>\n",
       "      <td>15.16487</td>\n",
       "      <td>13.471683</td>\n",
       "      <td>15.755362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.133226</td>\n",
       "      <td>0.51643</td>\n",
       "      <td>0.527157</td>\n",
       "      <td>0.303454</td>\n",
       "      <td>18.226374</td>\n",
       "      <td>11.978291</td>\n",
       "      <td>10.293822</td>\n",
       "      <td>13.499496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.862357   0.91184  0.921265  0.898487    7.32505   5.197219   \n",
       "1      0.705275  0.839046  0.881572  0.808631  10.718879   7.022837   \n",
       "2      0.537571  0.778689  0.837949  0.718069  13.428136   8.235742   \n",
       "3       0.38846  0.732954  0.803188  0.641534  15.444275   9.047922   \n",
       "4      0.256005   0.69663  0.770508  0.574381  17.034381   9.645487   \n",
       "5      0.145241  0.669713  0.740222  0.518392  18.255411  10.067267   \n",
       "6      0.056152  0.651435   0.71131  0.472966    19.1798  10.345066   \n",
       "7     -0.014523  0.638331  0.684313   0.43604  19.880651  10.540912   \n",
       "8     -0.069416  0.625495  0.658578  0.404886  20.405548  10.730607   \n",
       "9     -0.112006  0.611245  0.633384  0.377541  20.803254  10.936557   \n",
       "10    -0.145078  0.594755  0.608941  0.352873  21.107614  11.169141   \n",
       "11    -0.173439  0.576676  0.585755  0.329664  20.973362  11.419045   \n",
       "12     -0.19878  0.557438   0.56327  0.307309  20.267473  11.679911   \n",
       "13    -0.222471  0.537591  0.540307  0.285142  19.779183  11.942934   \n",
       "14      -0.2465  0.517551  0.516671  0.262574  19.651916   12.20436   \n",
       "15    -0.270412  0.496835  0.494414  0.240279  19.774226  12.469645   \n",
       "16    -0.291331  0.475608   0.47414  0.219472  19.548549  12.733662   \n",
       "17    -0.308846  0.455077    0.4549  0.200377  19.308426  12.984497   \n",
       "18    -0.323486   0.43608  0.436388  0.182994  19.133398  13.212906   \n",
       "19    -0.336614  0.418108  0.415918  0.165804  18.953927  13.424147   \n",
       "20    -0.350848  0.400635  0.393941  0.147909  18.861252  13.626347   \n",
       "21    -0.365198  0.382924  0.370678  0.129468  18.844315  13.827646   \n",
       "22    -0.382032  0.364852  0.348153  0.110324  18.704109  14.029867   \n",
       "23    -0.399413  0.348411  0.327363   0.09212   18.57175  14.212313   \n",
       "24    -0.417369   0.33269  0.309872  0.075064  18.505943  14.384637   \n",
       "25    -0.434294  0.317861  0.294637  0.059401   18.43436  14.545068   \n",
       "26     -0.44918  0.303557  0.280556  0.044977  18.351342  14.696627   \n",
       "27    -0.463686   0.28879  0.266811  0.030638  18.398299  14.850495   \n",
       "28    -0.479151  0.274138  0.252708  0.015898  18.516867  15.000991   \n",
       "29    -0.493767  0.257943  0.237009  0.000395  18.629533   15.16487   \n",
       "mean  -0.133226   0.51643  0.527157  0.303454  18.226374  11.978291   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       4.321141    5.61447  \n",
       "1       5.299731   7.680482  \n",
       "2       6.199716   9.287865  \n",
       "3       6.832521  10.441572  \n",
       "4       7.377952  11.352607  \n",
       "5       7.848914  12.057197  \n",
       "6       8.273034    12.5993  \n",
       "7        8.64958  13.023714  \n",
       "8       8.993123  13.376426  \n",
       "9       9.317779  13.685863  \n",
       "10      9.622939  13.966565  \n",
       "11      9.904735  14.099047  \n",
       "12     10.171107  14.039497  \n",
       "13     10.436075  14.052731  \n",
       "14     10.701696  14.185991  \n",
       "15     10.945138  14.396337  \n",
       "16     11.162515  14.481575  \n",
       "17     11.365159  14.552694  \n",
       "18     11.557193  14.634499  \n",
       "19     11.765905  14.714659  \n",
       "20     11.986196  14.824598  \n",
       "21     12.215623  14.962528  \n",
       "22     12.434078  15.056018  \n",
       "23     12.632372  15.138811  \n",
       "24      12.79643  15.229003  \n",
       "25     12.938308  15.305912  \n",
       "26     13.069113  15.372361  \n",
       "27      13.19718  15.481991  \n",
       "28     13.327731  15.615196  \n",
       "29     13.471683  15.755362  \n",
       "mean   10.293822  13.499496  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/20_30_1_50_1_50_datt_seq2seq_gru_6.csv\n",
      "\n",
      "\n",
      "6th iteration\n",
      "history size: 30\n",
      "future size: 10\n",
      "Epoch 1/10000\n",
      "572/572 - 8s - loss: 0.2481 - val_loss: 0.1497 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "572/572 - 5s - loss: 0.1506 - val_loss: 0.1369 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "572/572 - 5s - loss: 0.1387 - val_loss: 0.1331 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "572/572 - 5s - loss: 0.1294 - val_loss: 0.1217 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "572/572 - 5s - loss: 0.1199 - val_loss: 0.1143 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "572/572 - 5s - loss: 0.1106 - val_loss: 0.1045 - 5s/epoch - 9ms/step\n",
      "Epoch 7/10000\n",
      "572/572 - 5s - loss: 0.1043 - val_loss: 0.0993 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "572/572 - 5s - loss: 0.1000 - val_loss: 0.0972 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "572/572 - 5s - loss: 0.0953 - val_loss: 0.0959 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "572/572 - 5s - loss: 0.0926 - val_loss: 0.0956 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "572/572 - 5s - loss: 0.0897 - val_loss: 0.0896 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "572/572 - 5s - loss: 0.0850 - val_loss: 0.0966 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "572/572 - 5s - loss: 0.0837 - val_loss: 0.0861 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "572/572 - 5s - loss: 0.0819 - val_loss: 0.0854 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "572/572 - 5s - loss: 0.0778 - val_loss: 0.0848 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "572/572 - 5s - loss: 0.0803 - val_loss: 0.0819 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "572/572 - 5s - loss: 0.0753 - val_loss: 0.0833 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "572/572 - 5s - loss: 0.0719 - val_loss: 0.0795 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "572/572 - 5s - loss: 0.0696 - val_loss: 0.0761 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "572/572 - 5s - loss: 0.0689 - val_loss: 0.0808 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "572/572 - 5s - loss: 0.0672 - val_loss: 0.0738 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "572/572 - 5s - loss: 0.0651 - val_loss: 0.0727 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "572/572 - 5s - loss: 0.0661 - val_loss: 0.0737 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "572/572 - 5s - loss: 0.0643 - val_loss: 0.0681 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "572/572 - 5s - loss: 0.0611 - val_loss: 0.0716 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "572/572 - 5s - loss: 0.0597 - val_loss: 0.0698 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "572/572 - 5s - loss: 0.0603 - val_loss: 0.0689 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "572/572 - 5s - loss: 0.0593 - val_loss: 0.0789 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "572/572 - 5s - loss: 0.0594 - val_loss: 0.0656 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "572/572 - 5s - loss: 0.0567 - val_loss: 0.0661 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "572/572 - 5s - loss: 0.0554 - val_loss: 0.0634 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "572/572 - 5s - loss: 0.0537 - val_loss: 0.0638 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "572/572 - 5s - loss: 0.0535 - val_loss: 0.0637 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "572/572 - 5s - loss: 0.0553 - val_loss: 0.0631 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "572/572 - 5s - loss: 0.0521 - val_loss: 0.0615 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "572/572 - 5s - loss: 0.0528 - val_loss: 0.0600 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "572/572 - 5s - loss: 0.0521 - val_loss: 0.0628 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "572/572 - 5s - loss: 0.0511 - val_loss: 0.0602 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "572/572 - 5s - loss: 0.0492 - val_loss: 0.0614 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "572/572 - 5s - loss: 0.0492 - val_loss: 0.0615 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "572/572 - 5s - loss: 0.0504 - val_loss: 0.0624 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "572/572 - 5s - loss: 0.0479 - val_loss: 0.0617 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "572/572 - 5s - loss: 0.0480 - val_loss: 0.0671 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "572/572 - 5s - loss: 0.0488 - val_loss: 0.0573 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "572/572 - 5s - loss: 0.0469 - val_loss: 0.0591 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "572/572 - 5s - loss: 0.0459 - val_loss: 0.0584 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "572/572 - 5s - loss: 0.0454 - val_loss: 0.0594 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "572/572 - 5s - loss: 0.0469 - val_loss: 0.0580 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "572/572 - 5s - loss: 0.0462 - val_loss: 0.0640 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "572/572 - 5s - loss: 0.0456 - val_loss: 0.0559 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "572/572 - 5s - loss: 0.0434 - val_loss: 0.0564 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "572/572 - 5s - loss: 0.0437 - val_loss: 0.0551 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "572/572 - 5s - loss: 0.0434 - val_loss: 0.0566 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "572/572 - 5s - loss: 0.0448 - val_loss: 0.0577 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "572/572 - 5s - loss: 0.0452 - val_loss: 0.0536 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "572/572 - 5s - loss: 0.0417 - val_loss: 0.0554 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "572/572 - 5s - loss: 0.0418 - val_loss: 0.0560 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "572/572 - 5s - loss: 0.0419 - val_loss: 0.0534 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "572/572 - 5s - loss: 0.0423 - val_loss: 0.0535 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "572/572 - 5s - loss: 0.0409 - val_loss: 0.0534 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "572/572 - 5s - loss: 0.0412 - val_loss: 0.0550 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "572/572 - 5s - loss: 0.0425 - val_loss: 0.0547 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "572/572 - 5s - loss: 0.0396 - val_loss: 0.0505 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "572/572 - 5s - loss: 0.0392 - val_loss: 0.0519 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "572/572 - 5s - loss: 0.0407 - val_loss: 0.0503 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "572/572 - 5s - loss: 0.0395 - val_loss: 0.0526 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "572/572 - 5s - loss: 0.0392 - val_loss: 0.0515 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "572/572 - 5s - loss: 0.0400 - val_loss: 0.0531 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "572/572 - 5s - loss: 0.0385 - val_loss: 0.0554 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "572/572 - 5s - loss: 0.0415 - val_loss: 0.0544 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "572/572 - 5s - loss: 0.0382 - val_loss: 0.0505 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "572/572 - 5s - loss: 0.0373 - val_loss: 0.0513 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "572/572 - 5s - loss: 0.0378 - val_loss: 0.0494 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "572/572 - 5s - loss: 0.0378 - val_loss: 0.0498 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "572/572 - 5s - loss: 0.0381 - val_loss: 0.0503 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "572/572 - 5s - loss: 0.0372 - val_loss: 0.0503 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "572/572 - 5s - loss: 0.0372 - val_loss: 0.0509 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "572/572 - 5s - loss: 0.0372 - val_loss: 0.0486 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "572/572 - 5s - loss: 0.0365 - val_loss: 0.0500 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "572/572 - 5s - loss: 0.0369 - val_loss: 0.0493 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "572/572 - 5s - loss: 0.0377 - val_loss: 0.0517 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "572/572 - 5s - loss: 0.0359 - val_loss: 0.0469 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "572/572 - 5s - loss: 0.0354 - val_loss: 0.0460 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "572/572 - 5s - loss: 0.0353 - val_loss: 0.0471 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "572/572 - 5s - loss: 0.0359 - val_loss: 0.0502 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "572/572 - 5s - loss: 0.0358 - val_loss: 0.0529 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "572/572 - 5s - loss: 0.0357 - val_loss: 0.0460 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "572/572 - 5s - loss: 0.0347 - val_loss: 0.0467 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "572/572 - 5s - loss: 0.0350 - val_loss: 0.0476 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "572/572 - 5s - loss: 0.0350 - val_loss: 0.0472 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "572/572 - 5s - loss: 0.0343 - val_loss: 0.0460 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "572/572 - 5s - loss: 0.0341 - val_loss: 0.0448 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "572/572 - 5s - loss: 0.0340 - val_loss: 0.0472 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "572/572 - 5s - loss: 0.0338 - val_loss: 0.0484 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "572/572 - 5s - loss: 0.0341 - val_loss: 0.0486 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "572/572 - 5s - loss: 0.0340 - val_loss: 0.0471 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "572/572 - 5s - loss: 0.0333 - val_loss: 0.0454 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "572/572 - 5s - loss: 0.0328 - val_loss: 0.0456 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "572/572 - 5s - loss: 0.0326 - val_loss: 0.0455 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "572/572 - 5s - loss: 0.0333 - val_loss: 0.0458 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "572/572 - 5s - loss: 0.0333 - val_loss: 0.0441 - 5s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "572/572 - 5s - loss: 0.0317 - val_loss: 0.0469 - 5s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "572/572 - 5s - loss: 0.0332 - val_loss: 0.0451 - 5s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "572/572 - 5s - loss: 0.0311 - val_loss: 0.0453 - 5s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "572/572 - 5s - loss: 0.0311 - val_loss: 0.0468 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "572/572 - 5s - loss: 0.0316 - val_loss: 0.0430 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "572/572 - 5s - loss: 0.0327 - val_loss: 0.0602 - 5s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "572/572 - 5s - loss: 0.0345 - val_loss: 0.0433 - 5s/epoch - 8ms/step\n",
      "Epoch 109/10000\n",
      "572/572 - 5s - loss: 0.0302 - val_loss: 0.0448 - 5s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "572/572 - 5s - loss: 0.0307 - val_loss: 0.0428 - 5s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "572/572 - 5s - loss: 0.0304 - val_loss: 0.0428 - 5s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "572/572 - 5s - loss: 0.0298 - val_loss: 0.0434 - 5s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "572/572 - 5s - loss: 0.0300 - val_loss: 0.0439 - 5s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "572/572 - 5s - loss: 0.0305 - val_loss: 0.0431 - 5s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "572/572 - 5s - loss: 0.0296 - val_loss: 0.0432 - 5s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "572/572 - 5s - loss: 0.0299 - val_loss: 0.0451 - 5s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "572/572 - 5s - loss: 0.0303 - val_loss: 0.0436 - 5s/epoch - 8ms/step\n",
      "Epoch 118/10000\n",
      "572/572 - 5s - loss: 0.0305 - val_loss: 0.0431 - 5s/epoch - 8ms/step\n",
      "Epoch 119/10000\n",
      "572/572 - 5s - loss: 0.0288 - val_loss: 0.0429 - 5s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "572/572 - 5s - loss: 0.0304 - val_loss: 0.0420 - 5s/epoch - 8ms/step\n",
      "Epoch 121/10000\n",
      "572/572 - 5s - loss: 0.0374 - val_loss: 0.0442 - 5s/epoch - 8ms/step\n",
      "Epoch 122/10000\n",
      "572/572 - 5s - loss: 0.0290 - val_loss: 0.0421 - 5s/epoch - 8ms/step\n",
      "Epoch 123/10000\n",
      "572/572 - 5s - loss: 0.0281 - val_loss: 0.0407 - 5s/epoch - 8ms/step\n",
      "Epoch 124/10000\n",
      "572/572 - 5s - loss: 0.0284 - val_loss: 0.0427 - 5s/epoch - 8ms/step\n",
      "Epoch 125/10000\n",
      "572/572 - 5s - loss: 0.0281 - val_loss: 0.0402 - 5s/epoch - 8ms/step\n",
      "Epoch 126/10000\n",
      "572/572 - 5s - loss: 0.0286 - val_loss: 0.0416 - 5s/epoch - 8ms/step\n",
      "Epoch 127/10000\n",
      "572/572 - 5s - loss: 0.0288 - val_loss: 0.0448 - 5s/epoch - 8ms/step\n",
      "Epoch 128/10000\n",
      "572/572 - 5s - loss: 0.0285 - val_loss: 0.0410 - 5s/epoch - 8ms/step\n",
      "Epoch 129/10000\n",
      "572/572 - 5s - loss: 0.0274 - val_loss: 0.0426 - 5s/epoch - 8ms/step\n",
      "Epoch 130/10000\n",
      "572/572 - 5s - loss: 0.0282 - val_loss: 0.0442 - 5s/epoch - 8ms/step\n",
      "Epoch 131/10000\n",
      "572/572 - 5s - loss: 0.0284 - val_loss: 0.0428 - 5s/epoch - 8ms/step\n",
      "Epoch 132/10000\n",
      "572/572 - 5s - loss: 0.0279 - val_loss: 0.0420 - 5s/epoch - 8ms/step\n",
      "Epoch 133/10000\n",
      "572/572 - 5s - loss: 0.0284 - val_loss: 0.0394 - 5s/epoch - 8ms/step\n",
      "Epoch 134/10000\n",
      "572/572 - 5s - loss: 0.0277 - val_loss: 0.0406 - 5s/epoch - 8ms/step\n",
      "Epoch 135/10000\n",
      "572/572 - 5s - loss: 0.0283 - val_loss: 0.0422 - 5s/epoch - 8ms/step\n",
      "Epoch 136/10000\n",
      "572/572 - 5s - loss: 0.0282 - val_loss: 0.0397 - 5s/epoch - 8ms/step\n",
      "Epoch 137/10000\n",
      "572/572 - 5s - loss: 0.0269 - val_loss: 0.0399 - 5s/epoch - 8ms/step\n",
      "Epoch 138/10000\n",
      "572/572 - 5s - loss: 0.0264 - val_loss: 0.0391 - 5s/epoch - 8ms/step\n",
      "Epoch 139/10000\n",
      "572/572 - 5s - loss: 0.0273 - val_loss: 0.0424 - 5s/epoch - 8ms/step\n",
      "Epoch 140/10000\n",
      "572/572 - 5s - loss: 0.0279 - val_loss: 0.0421 - 5s/epoch - 8ms/step\n",
      "Epoch 141/10000\n",
      "572/572 - 5s - loss: 0.0274 - val_loss: 0.0398 - 5s/epoch - 8ms/step\n",
      "Epoch 142/10000\n",
      "572/572 - 5s - loss: 0.0272 - val_loss: 0.0404 - 5s/epoch - 8ms/step\n",
      "Epoch 143/10000\n",
      "572/572 - 5s - loss: 0.0302 - val_loss: 0.0409 - 5s/epoch - 8ms/step\n",
      "Epoch 144/10000\n",
      "572/572 - 5s - loss: 0.0259 - val_loss: 0.0393 - 5s/epoch - 8ms/step\n",
      "Epoch 145/10000\n",
      "572/572 - 5s - loss: 0.0261 - val_loss: 0.0410 - 5s/epoch - 8ms/step\n",
      "Epoch 146/10000\n",
      "572/572 - 5s - loss: 0.0265 - val_loss: 0.0436 - 5s/epoch - 8ms/step\n",
      "Epoch 147/10000\n",
      "572/572 - 5s - loss: 0.0263 - val_loss: 0.0388 - 5s/epoch - 8ms/step\n",
      "Epoch 148/10000\n",
      "572/572 - 5s - loss: 0.0262 - val_loss: 0.0409 - 5s/epoch - 8ms/step\n",
      "Epoch 149/10000\n",
      "572/572 - 5s - loss: 0.0267 - val_loss: 0.0393 - 5s/epoch - 8ms/step\n",
      "Epoch 150/10000\n",
      "572/572 - 5s - loss: 0.0265 - val_loss: 0.0395 - 5s/epoch - 8ms/step\n",
      "Epoch 151/10000\n",
      "572/572 - 5s - loss: 0.0253 - val_loss: 0.0395 - 5s/epoch - 8ms/step\n",
      "Epoch 152/10000\n",
      "572/572 - 5s - loss: 0.0265 - val_loss: 0.0393 - 5s/epoch - 8ms/step\n",
      "Epoch 153/10000\n",
      "572/572 - 5s - loss: 0.0260 - val_loss: 0.0405 - 5s/epoch - 9ms/step\n",
      "Epoch 154/10000\n",
      "572/572 - 5s - loss: 0.0286 - val_loss: 0.0400 - 5s/epoch - 8ms/step\n",
      "Epoch 155/10000\n",
      "572/572 - 5s - loss: 0.0255 - val_loss: 0.0385 - 5s/epoch - 8ms/step\n",
      "Epoch 156/10000\n",
      "572/572 - 5s - loss: 0.0250 - val_loss: 0.0382 - 5s/epoch - 8ms/step\n",
      "Epoch 157/10000\n",
      "572/572 - 5s - loss: 0.0248 - val_loss: 0.0379 - 5s/epoch - 8ms/step\n",
      "Epoch 158/10000\n",
      "572/572 - 5s - loss: 0.0252 - val_loss: 0.0396 - 5s/epoch - 8ms/step\n",
      "Epoch 159/10000\n",
      "572/572 - 5s - loss: 0.0255 - val_loss: 0.0401 - 5s/epoch - 8ms/step\n",
      "Epoch 160/10000\n",
      "572/572 - 5s - loss: 0.0268 - val_loss: 0.0401 - 5s/epoch - 8ms/step\n",
      "Epoch 161/10000\n",
      "572/572 - 5s - loss: 0.0266 - val_loss: 0.0372 - 5s/epoch - 8ms/step\n",
      "Epoch 162/10000\n",
      "572/572 - 5s - loss: 0.0250 - val_loss: 0.0394 - 5s/epoch - 8ms/step\n",
      "Epoch 163/10000\n",
      "572/572 - 5s - loss: 0.0254 - val_loss: 0.0380 - 5s/epoch - 8ms/step\n",
      "Epoch 164/10000\n",
      "572/572 - 5s - loss: 0.0250 - val_loss: 0.0376 - 5s/epoch - 8ms/step\n",
      "Epoch 165/10000\n",
      "572/572 - 5s - loss: 0.0246 - val_loss: 0.0396 - 5s/epoch - 8ms/step\n",
      "Epoch 166/10000\n",
      "572/572 - 5s - loss: 0.0251 - val_loss: 0.0395 - 5s/epoch - 8ms/step\n",
      "Epoch 167/10000\n",
      "572/572 - 5s - loss: 0.0254 - val_loss: 0.0380 - 5s/epoch - 8ms/step\n",
      "Epoch 168/10000\n",
      "572/572 - 5s - loss: 0.0256 - val_loss: 0.0393 - 5s/epoch - 8ms/step\n",
      "Epoch 169/10000\n",
      "572/572 - 5s - loss: 0.0259 - val_loss: 0.0376 - 5s/epoch - 8ms/step\n",
      "Epoch 170/10000\n",
      "572/572 - 5s - loss: 0.0239 - val_loss: 0.0373 - 5s/epoch - 8ms/step\n",
      "Epoch 171/10000\n",
      "572/572 - 5s - loss: 0.0239 - val_loss: 0.0369 - 5s/epoch - 8ms/step\n",
      "Epoch 172/10000\n",
      "572/572 - 5s - loss: 0.0248 - val_loss: 0.0409 - 5s/epoch - 8ms/step\n",
      "Epoch 173/10000\n",
      "572/572 - 5s - loss: 0.0299 - val_loss: 0.0377 - 5s/epoch - 8ms/step\n",
      "Epoch 174/10000\n",
      "572/572 - 5s - loss: 0.0237 - val_loss: 0.0362 - 5s/epoch - 8ms/step\n",
      "Epoch 175/10000\n",
      "572/572 - 5s - loss: 0.0235 - val_loss: 0.0367 - 5s/epoch - 8ms/step\n",
      "Epoch 176/10000\n",
      "572/572 - 5s - loss: 0.0246 - val_loss: 0.0450 - 5s/epoch - 8ms/step\n",
      "Epoch 177/10000\n",
      "572/572 - 5s - loss: 0.0249 - val_loss: 0.0373 - 5s/epoch - 8ms/step\n",
      "Epoch 178/10000\n",
      "572/572 - 5s - loss: 0.0243 - val_loss: 0.0379 - 5s/epoch - 8ms/step\n",
      "Epoch 179/10000\n",
      "572/572 - 5s - loss: 0.0237 - val_loss: 0.0358 - 5s/epoch - 8ms/step\n",
      "Epoch 180/10000\n",
      "572/572 - 5s - loss: 0.0252 - val_loss: 0.0387 - 5s/epoch - 8ms/step\n",
      "Epoch 181/10000\n",
      "572/572 - 5s - loss: 0.0241 - val_loss: 0.0380 - 5s/epoch - 8ms/step\n",
      "Epoch 182/10000\n",
      "572/572 - 5s - loss: 0.0239 - val_loss: 0.0368 - 5s/epoch - 10ms/step\n",
      "Epoch 183/10000\n",
      "572/572 - 5s - loss: 0.0235 - val_loss: 0.0357 - 5s/epoch - 9ms/step\n",
      "Epoch 184/10000\n",
      "572/572 - 5s - loss: 0.0255 - val_loss: 0.0365 - 5s/epoch - 8ms/step\n",
      "Epoch 185/10000\n",
      "572/572 - 5s - loss: 0.0240 - val_loss: 0.0362 - 5s/epoch - 8ms/step\n",
      "Epoch 186/10000\n",
      "572/572 - 5s - loss: 0.0230 - val_loss: 0.0362 - 5s/epoch - 8ms/step\n",
      "Epoch 187/10000\n",
      "572/572 - 5s - loss: 0.0242 - val_loss: 0.0378 - 5s/epoch - 8ms/step\n",
      "Epoch 188/10000\n",
      "572/572 - 5s - loss: 0.0231 - val_loss: 0.0366 - 5s/epoch - 8ms/step\n",
      "Epoch 189/10000\n",
      "572/572 - 5s - loss: 0.0239 - val_loss: 0.0364 - 5s/epoch - 8ms/step\n",
      "Epoch 190/10000\n",
      "572/572 - 5s - loss: 0.0230 - val_loss: 0.0351 - 5s/epoch - 8ms/step\n",
      "Epoch 191/10000\n",
      "572/572 - 5s - loss: 0.0236 - val_loss: 0.0358 - 5s/epoch - 8ms/step\n",
      "Epoch 192/10000\n",
      "572/572 - 5s - loss: 0.0232 - val_loss: 0.0353 - 5s/epoch - 8ms/step\n",
      "Epoch 193/10000\n",
      "572/572 - 5s - loss: 0.0233 - val_loss: 0.0365 - 5s/epoch - 8ms/step\n",
      "Epoch 194/10000\n",
      "572/572 - 5s - loss: 0.0235 - val_loss: 0.0361 - 5s/epoch - 8ms/step\n",
      "Epoch 195/10000\n",
      "572/572 - 5s - loss: 0.0232 - val_loss: 0.0357 - 5s/epoch - 8ms/step\n",
      "Epoch 196/10000\n",
      "572/572 - 5s - loss: 0.0230 - val_loss: 0.0360 - 5s/epoch - 8ms/step\n",
      "Epoch 197/10000\n",
      "572/572 - 5s - loss: 0.0235 - val_loss: 0.0382 - 5s/epoch - 8ms/step\n",
      "Epoch 198/10000\n",
      "572/572 - 5s - loss: 0.0276 - val_loss: 0.0374 - 5s/epoch - 8ms/step\n",
      "Epoch 199/10000\n",
      "572/572 - 5s - loss: 0.0236 - val_loss: 0.0352 - 5s/epoch - 8ms/step\n",
      "Epoch 200/10000\n",
      "572/572 - 5s - loss: 0.0219 - val_loss: 0.0370 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_240_layer_call_fn, gru_cell_240_layer_call_and_return_conditional_losses, gru_cell_241_layer_call_fn, gru_cell_241_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_10_1_50_1_50_datt_seq2seq_gru_6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_10_1_50_1_50_datt_seq2seq_gru_6\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021CE312AEE0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B4FED4BE0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.922061</td>\n",
       "      <td>0.930456</td>\n",
       "      <td>0.943177</td>\n",
       "      <td>0.931898</td>\n",
       "      <td>4.531063</td>\n",
       "      <td>4.632278</td>\n",
       "      <td>3.681154</td>\n",
       "      <td>4.281498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.849172</td>\n",
       "      <td>0.86714</td>\n",
       "      <td>0.880314</td>\n",
       "      <td>0.865542</td>\n",
       "      <td>6.261404</td>\n",
       "      <td>6.40347</td>\n",
       "      <td>5.341565</td>\n",
       "      <td>6.002146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.754844</td>\n",
       "      <td>0.813394</td>\n",
       "      <td>0.810141</td>\n",
       "      <td>0.792793</td>\n",
       "      <td>7.872033</td>\n",
       "      <td>7.589659</td>\n",
       "      <td>6.727121</td>\n",
       "      <td>7.396271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.645478</td>\n",
       "      <td>0.777744</td>\n",
       "      <td>0.759847</td>\n",
       "      <td>0.72769</td>\n",
       "      <td>9.338899</td>\n",
       "      <td>8.284044</td>\n",
       "      <td>7.565916</td>\n",
       "      <td>8.396287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.536007</td>\n",
       "      <td>0.755892</td>\n",
       "      <td>0.741794</td>\n",
       "      <td>0.677898</td>\n",
       "      <td>10.577856</td>\n",
       "      <td>8.68289</td>\n",
       "      <td>7.84492</td>\n",
       "      <td>9.035222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.438642</td>\n",
       "      <td>0.740988</td>\n",
       "      <td>0.746785</td>\n",
       "      <td>0.642138</td>\n",
       "      <td>11.522298</td>\n",
       "      <td>8.945386</td>\n",
       "      <td>7.768805</td>\n",
       "      <td>9.412163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.357054</td>\n",
       "      <td>0.727707</td>\n",
       "      <td>0.758409</td>\n",
       "      <td>0.61439</td>\n",
       "      <td>12.214659</td>\n",
       "      <td>9.172513</td>\n",
       "      <td>7.588747</td>\n",
       "      <td>9.65864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.289898</td>\n",
       "      <td>0.712394</td>\n",
       "      <td>0.765718</td>\n",
       "      <td>0.589337</td>\n",
       "      <td>12.808504</td>\n",
       "      <td>9.427148</td>\n",
       "      <td>7.473734</td>\n",
       "      <td>9.903129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.234075</td>\n",
       "      <td>0.693825</td>\n",
       "      <td>0.760738</td>\n",
       "      <td>0.56288</td>\n",
       "      <td>13.321117</td>\n",
       "      <td>9.726984</td>\n",
       "      <td>7.553252</td>\n",
       "      <td>10.200451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.188453</td>\n",
       "      <td>0.672919</td>\n",
       "      <td>0.741736</td>\n",
       "      <td>0.534369</td>\n",
       "      <td>13.731822</td>\n",
       "      <td>10.053659</td>\n",
       "      <td>7.848353</td>\n",
       "      <td>10.544611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.521568</td>\n",
       "      <td>0.769246</td>\n",
       "      <td>0.790866</td>\n",
       "      <td>0.693893</td>\n",
       "      <td>10.217965</td>\n",
       "      <td>8.291803</td>\n",
       "      <td>6.939357</td>\n",
       "      <td>8.483042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE     nRMSE   \n",
       "0      0.922061  0.930456  0.943177  0.931898   4.531063   4.632278  3.681154   \n",
       "1      0.849172   0.86714  0.880314  0.865542   6.261404    6.40347  5.341565   \n",
       "2      0.754844  0.813394  0.810141  0.792793   7.872033   7.589659  6.727121   \n",
       "3      0.645478  0.777744  0.759847   0.72769   9.338899   8.284044  7.565916   \n",
       "4      0.536007  0.755892  0.741794  0.677898  10.577856    8.68289   7.84492   \n",
       "5      0.438642  0.740988  0.746785  0.642138  11.522298   8.945386  7.768805   \n",
       "6      0.357054  0.727707  0.758409   0.61439  12.214659   9.172513  7.588747   \n",
       "7      0.289898  0.712394  0.765718  0.589337  12.808504   9.427148  7.473734   \n",
       "8      0.234075  0.693825  0.760738   0.56288  13.321117   9.726984  7.553252   \n",
       "9      0.188453  0.672919  0.741736  0.534369  13.731822  10.053659  7.848353   \n",
       "mean   0.521568  0.769246  0.790866  0.693893  10.217965   8.291803  6.939357   \n",
       "\n",
       "            mean  \n",
       "index      nRMSE  \n",
       "0       4.281498  \n",
       "1       6.002146  \n",
       "2       7.396271  \n",
       "3       8.396287  \n",
       "4       9.035222  \n",
       "5       9.412163  \n",
       "6        9.65864  \n",
       "7       9.903129  \n",
       "8      10.200451  \n",
       "9      10.544611  \n",
       "mean    8.483042  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/30_10_1_50_1_50_datt_seq2seq_gru_6.csv\n",
      "\n",
      "\n",
      "6th iteration\n",
      "history size: 30\n",
      "future size: 20\n",
      "Epoch 1/10000\n",
      "568/568 - 8s - loss: 0.3009 - val_loss: 0.2601 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "568/568 - 5s - loss: 0.2164 - val_loss: 0.2418 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "568/568 - 5s - loss: 0.2030 - val_loss: 0.2235 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "568/568 - 5s - loss: 0.1918 - val_loss: 0.2145 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "568/568 - 5s - loss: 0.1830 - val_loss: 0.2050 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "568/568 - 5s - loss: 0.1740 - val_loss: 0.1988 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "568/568 - 5s - loss: 0.1663 - val_loss: 0.2003 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "568/568 - 5s - loss: 0.1593 - val_loss: 0.1859 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "568/568 - 5s - loss: 0.1514 - val_loss: 0.1782 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "568/568 - 5s - loss: 0.1460 - val_loss: 0.1669 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "568/568 - 5s - loss: 0.1386 - val_loss: 0.1599 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "568/568 - 5s - loss: 0.1338 - val_loss: 0.1520 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "568/568 - 5s - loss: 0.1315 - val_loss: 0.1504 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "568/568 - 5s - loss: 0.1218 - val_loss: 0.1426 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "568/568 - 5s - loss: 0.1200 - val_loss: 0.1615 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "568/568 - 5s - loss: 0.1193 - val_loss: 0.1344 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "568/568 - 5s - loss: 0.1135 - val_loss: 0.1342 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "568/568 - 5s - loss: 0.1086 - val_loss: 0.1301 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "568/568 - 5s - loss: 0.1052 - val_loss: 0.1266 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "568/568 - 5s - loss: 0.1039 - val_loss: 0.1251 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "568/568 - 5s - loss: 0.1035 - val_loss: 0.1220 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "568/568 - 5s - loss: 0.0990 - val_loss: 0.1187 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "568/568 - 5s - loss: 0.0961 - val_loss: 0.1212 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "568/568 - 5s - loss: 0.0967 - val_loss: 0.1182 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "568/568 - 4s - loss: 0.0942 - val_loss: 0.1209 - 4s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "568/568 - 5s - loss: 0.0922 - val_loss: 0.1125 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "568/568 - 5s - loss: 0.0912 - val_loss: 0.1151 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "568/568 - 5s - loss: 0.0899 - val_loss: 0.1116 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "568/568 - 5s - loss: 0.0874 - val_loss: 0.1068 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "568/568 - 5s - loss: 0.0855 - val_loss: 0.1132 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "568/568 - 5s - loss: 0.0854 - val_loss: 0.1127 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "568/568 - 5s - loss: 0.0843 - val_loss: 0.1291 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "568/568 - 4s - loss: 0.0862 - val_loss: 0.1036 - 4s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "568/568 - 5s - loss: 0.0817 - val_loss: 0.1039 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "568/568 - 5s - loss: 0.0820 - val_loss: 0.1071 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "568/568 - 5s - loss: 0.0796 - val_loss: 0.1020 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "568/568 - 5s - loss: 0.0778 - val_loss: 0.1032 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "568/568 - 5s - loss: 0.0779 - val_loss: 0.1052 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "568/568 - 5s - loss: 0.0790 - val_loss: 0.1019 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "568/568 - 5s - loss: 0.0757 - val_loss: 0.0981 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "568/568 - 5s - loss: 0.0748 - val_loss: 0.0995 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "568/568 - 5s - loss: 0.0753 - val_loss: 0.1005 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "568/568 - 4s - loss: 0.0724 - val_loss: 0.1005 - 4s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "568/568 - 5s - loss: 0.0737 - val_loss: 0.0986 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "568/568 - 5s - loss: 0.0713 - val_loss: 0.0911 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "568/568 - 5s - loss: 0.0739 - val_loss: 0.0939 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "568/568 - 5s - loss: 0.0707 - val_loss: 0.0972 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "568/568 - 4s - loss: 0.0715 - val_loss: 0.0966 - 4s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "568/568 - 5s - loss: 0.0690 - val_loss: 0.0887 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "568/568 - 5s - loss: 0.0705 - val_loss: 0.0922 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "568/568 - 5s - loss: 0.0663 - val_loss: 0.0892 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "568/568 - 5s - loss: 0.0674 - val_loss: 0.0914 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "568/568 - 5s - loss: 0.0670 - val_loss: 0.0877 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "568/568 - 5s - loss: 0.0660 - val_loss: 0.0891 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "568/568 - 5s - loss: 0.0656 - val_loss: 0.0911 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "568/568 - 5s - loss: 0.0641 - val_loss: 0.0850 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "568/568 - 4s - loss: 0.0640 - val_loss: 0.0868 - 4s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "568/568 - 4s - loss: 0.0611 - val_loss: 0.0823 - 4s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "568/568 - 5s - loss: 0.0638 - val_loss: 0.0912 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "568/568 - 5s - loss: 0.0615 - val_loss: 0.0843 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "568/568 - 5s - loss: 0.0604 - val_loss: 0.0845 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "568/568 - 5s - loss: 0.0614 - val_loss: 0.0842 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "568/568 - 5s - loss: 0.0599 - val_loss: 0.0852 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "568/568 - 5s - loss: 0.0637 - val_loss: 0.0913 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "568/568 - 4s - loss: 0.0606 - val_loss: 0.0807 - 4s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "568/568 - 4s - loss: 0.0588 - val_loss: 0.0784 - 4s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "568/568 - 4s - loss: 0.0577 - val_loss: 0.0737 - 4s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "568/568 - 5s - loss: 0.0566 - val_loss: 0.0761 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "568/568 - 4s - loss: 0.0592 - val_loss: 0.0771 - 4s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "568/568 - 4s - loss: 0.0595 - val_loss: 0.0824 - 4s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "568/568 - 4s - loss: 0.0566 - val_loss: 0.0805 - 4s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "568/568 - 4s - loss: 0.0564 - val_loss: 0.0749 - 4s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "568/568 - 5s - loss: 0.0560 - val_loss: 0.0786 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "568/568 - 5s - loss: 0.0535 - val_loss: 0.0815 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "568/568 - 5s - loss: 0.0610 - val_loss: 0.1031 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "568/568 - 5s - loss: 0.0638 - val_loss: 0.0738 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "568/568 - 5s - loss: 0.0541 - val_loss: 0.0743 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_242_layer_call_fn, gru_cell_242_layer_call_and_return_conditional_losses, gru_cell_243_layer_call_fn, gru_cell_243_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_20_1_50_1_50_datt_seq2seq_gru_6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_20_1_50_1_50_datt_seq2seq_gru_6\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B4ED00160> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B6FACE6D0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.879521</td>\n",
       "      <td>0.897967</td>\n",
       "      <td>0.93764</td>\n",
       "      <td>0.905043</td>\n",
       "      <td>6.846628</td>\n",
       "      <td>5.604437</td>\n",
       "      <td>3.842719</td>\n",
       "      <td>5.431261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.814903</td>\n",
       "      <td>0.831363</td>\n",
       "      <td>0.867428</td>\n",
       "      <td>0.837898</td>\n",
       "      <td>8.329847</td>\n",
       "      <td>7.207251</td>\n",
       "      <td>5.60325</td>\n",
       "      <td>7.046782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.739783</td>\n",
       "      <td>0.776057</td>\n",
       "      <td>0.794912</td>\n",
       "      <td>0.77025</td>\n",
       "      <td>9.442742</td>\n",
       "      <td>8.308478</td>\n",
       "      <td>6.969979</td>\n",
       "      <td>8.2404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.65689</td>\n",
       "      <td>0.735641</td>\n",
       "      <td>0.74769</td>\n",
       "      <td>0.713407</td>\n",
       "      <td>10.478652</td>\n",
       "      <td>9.030154</td>\n",
       "      <td>7.731624</td>\n",
       "      <td>9.080143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.566099</td>\n",
       "      <td>0.703805</td>\n",
       "      <td>0.732068</td>\n",
       "      <td>0.667324</td>\n",
       "      <td>11.594556</td>\n",
       "      <td>9.562645</td>\n",
       "      <td>7.96789</td>\n",
       "      <td>9.708364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.469799</td>\n",
       "      <td>0.676212</td>\n",
       "      <td>0.730085</td>\n",
       "      <td>0.625366</td>\n",
       "      <td>12.774604</td>\n",
       "      <td>10.00297</td>\n",
       "      <td>7.997184</td>\n",
       "      <td>10.258253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.375157</td>\n",
       "      <td>0.649414</td>\n",
       "      <td>0.722205</td>\n",
       "      <td>0.582259</td>\n",
       "      <td>13.598196</td>\n",
       "      <td>10.411722</td>\n",
       "      <td>8.113146</td>\n",
       "      <td>10.707688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.289852</td>\n",
       "      <td>0.626978</td>\n",
       "      <td>0.699445</td>\n",
       "      <td>0.538758</td>\n",
       "      <td>14.222536</td>\n",
       "      <td>10.742982</td>\n",
       "      <td>8.439159</td>\n",
       "      <td>11.134892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21613</td>\n",
       "      <td>0.607073</td>\n",
       "      <td>0.658911</td>\n",
       "      <td>0.494038</td>\n",
       "      <td>14.724971</td>\n",
       "      <td>11.029246</td>\n",
       "      <td>8.990756</td>\n",
       "      <td>11.581658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.154978</td>\n",
       "      <td>0.585621</td>\n",
       "      <td>0.602661</td>\n",
       "      <td>0.447753</td>\n",
       "      <td>15.070585</td>\n",
       "      <td>11.32828</td>\n",
       "      <td>9.70441</td>\n",
       "      <td>12.034425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.104248</td>\n",
       "      <td>0.561203</td>\n",
       "      <td>0.543881</td>\n",
       "      <td>0.403111</td>\n",
       "      <td>15.358934</td>\n",
       "      <td>11.659107</td>\n",
       "      <td>10.398321</td>\n",
       "      <td>12.47212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.062623</td>\n",
       "      <td>0.535286</td>\n",
       "      <td>0.504489</td>\n",
       "      <td>0.367466</td>\n",
       "      <td>15.614918</td>\n",
       "      <td>11.999737</td>\n",
       "      <td>10.839415</td>\n",
       "      <td>12.818023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.02531</td>\n",
       "      <td>0.507592</td>\n",
       "      <td>0.491226</td>\n",
       "      <td>0.341376</td>\n",
       "      <td>15.707649</td>\n",
       "      <td>12.353172</td>\n",
       "      <td>10.985083</td>\n",
       "      <td>13.015301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.009213</td>\n",
       "      <td>0.481459</td>\n",
       "      <td>0.492541</td>\n",
       "      <td>0.321595</td>\n",
       "      <td>15.771441</td>\n",
       "      <td>12.678552</td>\n",
       "      <td>10.972232</td>\n",
       "      <td>13.140742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.042579</td>\n",
       "      <td>0.45685</td>\n",
       "      <td>0.491875</td>\n",
       "      <td>0.302049</td>\n",
       "      <td>15.871727</td>\n",
       "      <td>12.977619</td>\n",
       "      <td>10.980173</td>\n",
       "      <td>13.276506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.072988</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.481496</td>\n",
       "      <td>0.281097</td>\n",
       "      <td>15.944335</td>\n",
       "      <td>13.239973</td>\n",
       "      <td>11.092952</td>\n",
       "      <td>13.425753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.098774</td>\n",
       "      <td>0.414135</td>\n",
       "      <td>0.460141</td>\n",
       "      <td>0.258501</td>\n",
       "      <td>15.979414</td>\n",
       "      <td>13.479491</td>\n",
       "      <td>11.321078</td>\n",
       "      <td>13.593328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.117218</td>\n",
       "      <td>0.39349</td>\n",
       "      <td>0.432423</td>\n",
       "      <td>0.236232</td>\n",
       "      <td>16.073952</td>\n",
       "      <td>13.71391</td>\n",
       "      <td>11.611423</td>\n",
       "      <td>13.799761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.13039</td>\n",
       "      <td>0.372795</td>\n",
       "      <td>0.406319</td>\n",
       "      <td>0.216241</td>\n",
       "      <td>16.187338</td>\n",
       "      <td>13.944332</td>\n",
       "      <td>11.879206</td>\n",
       "      <td>14.003625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.140949</td>\n",
       "      <td>0.350706</td>\n",
       "      <td>0.387763</td>\n",
       "      <td>0.199173</td>\n",
       "      <td>16.281471</td>\n",
       "      <td>14.185374</td>\n",
       "      <td>12.067628</td>\n",
       "      <td>14.178158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.237159</td>\n",
       "      <td>0.579921</td>\n",
       "      <td>0.60926</td>\n",
       "      <td>0.475447</td>\n",
       "      <td>13.793725</td>\n",
       "      <td>11.172972</td>\n",
       "      <td>9.375381</td>\n",
       "      <td>11.447359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.879521  0.897967   0.93764  0.905043   6.846628   5.604437   \n",
       "1      0.814903  0.831363  0.867428  0.837898   8.329847   7.207251   \n",
       "2      0.739783  0.776057  0.794912   0.77025   9.442742   8.308478   \n",
       "3       0.65689  0.735641   0.74769  0.713407  10.478652   9.030154   \n",
       "4      0.566099  0.703805  0.732068  0.667324  11.594556   9.562645   \n",
       "5      0.469799  0.676212  0.730085  0.625366  12.774604   10.00297   \n",
       "6      0.375157  0.649414  0.722205  0.582259  13.598196  10.411722   \n",
       "7      0.289852  0.626978  0.699445  0.538758  14.222536  10.742982   \n",
       "8       0.21613  0.607073  0.658911  0.494038  14.724971  11.029246   \n",
       "9      0.154978  0.585621  0.602661  0.447753  15.070585   11.32828   \n",
       "10     0.104248  0.561203  0.543881  0.403111  15.358934  11.659107   \n",
       "11     0.062623  0.535286  0.504489  0.367466  15.614918  11.999737   \n",
       "12      0.02531  0.507592  0.491226  0.341376  15.707649  12.353172   \n",
       "13    -0.009213  0.481459  0.492541  0.321595  15.771441  12.678552   \n",
       "14    -0.042579   0.45685  0.491875  0.302049  15.871727  12.977619   \n",
       "15    -0.072988  0.434783  0.481496  0.281097  15.944335  13.239973   \n",
       "16    -0.098774  0.414135  0.460141  0.258501  15.979414  13.479491   \n",
       "17    -0.117218   0.39349  0.432423  0.236232  16.073952   13.71391   \n",
       "18     -0.13039  0.372795  0.406319  0.216241  16.187338  13.944332   \n",
       "19    -0.140949  0.350706  0.387763  0.199173  16.281471  14.185374   \n",
       "mean   0.237159  0.579921   0.60926  0.475447  13.793725  11.172972   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       3.842719   5.431261  \n",
       "1        5.60325   7.046782  \n",
       "2       6.969979     8.2404  \n",
       "3       7.731624   9.080143  \n",
       "4        7.96789   9.708364  \n",
       "5       7.997184  10.258253  \n",
       "6       8.113146  10.707688  \n",
       "7       8.439159  11.134892  \n",
       "8       8.990756  11.581658  \n",
       "9        9.70441  12.034425  \n",
       "10     10.398321   12.47212  \n",
       "11     10.839415  12.818023  \n",
       "12     10.985083  13.015301  \n",
       "13     10.972232  13.140742  \n",
       "14     10.980173  13.276506  \n",
       "15     11.092952  13.425753  \n",
       "16     11.321078  13.593328  \n",
       "17     11.611423  13.799761  \n",
       "18     11.879206  14.003625  \n",
       "19     12.067628  14.178158  \n",
       "mean    9.375381  11.447359  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/30_20_1_50_1_50_datt_seq2seq_gru_6.csv\n",
      "\n",
      "\n",
      "6th iteration\n",
      "history size: 30\n",
      "future size: 30\n",
      "Epoch 1/10000\n",
      "563/563 - 9s - loss: 0.3900 - val_loss: 0.3159 - 9s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.2879 - val_loss: 0.2897 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.2664 - val_loss: 0.2703 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 5s - loss: 0.2504 - val_loss: 0.2556 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.2347 - val_loss: 0.2473 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.2204 - val_loss: 0.2248 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.2078 - val_loss: 0.2218 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.1967 - val_loss: 0.2125 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.1877 - val_loss: 0.2137 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.1776 - val_loss: 0.1892 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.1709 - val_loss: 0.1914 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.1637 - val_loss: 0.1844 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.1610 - val_loss: 0.1776 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.1533 - val_loss: 0.1783 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.1483 - val_loss: 0.1671 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.1463 - val_loss: 0.1703 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.1428 - val_loss: 0.1656 - 5s/epoch - 9ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.1393 - val_loss: 0.1548 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.1371 - val_loss: 0.1520 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.1333 - val_loss: 0.1558 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.1305 - val_loss: 0.1497 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.1288 - val_loss: 0.1438 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.1271 - val_loss: 0.1483 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.1246 - val_loss: 0.1479 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.1235 - val_loss: 0.1388 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.1201 - val_loss: 0.1276 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.1160 - val_loss: 0.1356 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.1168 - val_loss: 0.1414 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.1168 - val_loss: 0.1297 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.1086 - val_loss: 0.1246 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.1069 - val_loss: 0.1190 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.1064 - val_loss: 0.1171 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.1048 - val_loss: 0.1322 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.1050 - val_loss: 0.1134 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.1026 - val_loss: 0.1184 - 5s/epoch - 9ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.1018 - val_loss: 0.1103 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0980 - val_loss: 0.1117 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.1057 - val_loss: 0.1117 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.0963 - val_loss: 0.1076 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0982 - val_loss: 0.1114 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0914 - val_loss: 0.1169 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0971 - val_loss: 0.1036 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0907 - val_loss: 0.1055 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0917 - val_loss: 0.1118 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0896 - val_loss: 0.1052 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0884 - val_loss: 0.0976 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0882 - val_loss: 0.0988 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0858 - val_loss: 0.1027 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0862 - val_loss: 0.1028 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0839 - val_loss: 0.0954 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0854 - val_loss: 0.0983 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0838 - val_loss: 0.0935 - 5s/epoch - 9ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0818 - val_loss: 0.1076 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0808 - val_loss: 0.0928 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0811 - val_loss: 0.0990 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0801 - val_loss: 0.0982 - 5s/epoch - 9ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0813 - val_loss: 0.0968 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0780 - val_loss: 0.0908 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0760 - val_loss: 0.0972 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0793 - val_loss: 0.0980 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0814 - val_loss: 0.0953 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0770 - val_loss: 0.0854 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 5s - loss: 0.0766 - val_loss: 0.0917 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0757 - val_loss: 0.0855 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0728 - val_loss: 0.0878 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0742 - val_loss: 0.0835 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0741 - val_loss: 0.0815 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 5s - loss: 0.0732 - val_loss: 0.0855 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 5s - loss: 0.0761 - val_loss: 0.0914 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 5s - loss: 0.0719 - val_loss: 0.0798 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 5s - loss: 0.0690 - val_loss: 0.0816 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 5s - loss: 0.0726 - val_loss: 0.0829 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 5s - loss: 0.0726 - val_loss: 0.0793 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 5s - loss: 0.0690 - val_loss: 0.0845 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 5s - loss: 0.0710 - val_loss: 0.0941 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 5s - loss: 0.0703 - val_loss: 0.0791 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 5s - loss: 0.0698 - val_loss: 0.0785 - 5s/epoch - 9ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 5s - loss: 0.0674 - val_loss: 0.0801 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 5s - loss: 0.0695 - val_loss: 0.0874 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 5s - loss: 0.0675 - val_loss: 0.0805 - 5s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 5s - loss: 0.0680 - val_loss: 0.0801 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 5s - loss: 0.0782 - val_loss: 0.0812 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 5s - loss: 0.0650 - val_loss: 0.0755 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "563/563 - 5s - loss: 0.0644 - val_loss: 0.0748 - 5s/epoch - 9ms/step\n",
      "Epoch 85/10000\n",
      "563/563 - 5s - loss: 0.0651 - val_loss: 0.0756 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "563/563 - 5s - loss: 0.0657 - val_loss: 0.0775 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "563/563 - 5s - loss: 0.0661 - val_loss: 0.0798 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "563/563 - 5s - loss: 0.0659 - val_loss: 0.0875 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "563/563 - 5s - loss: 0.0639 - val_loss: 0.0737 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "563/563 - 5s - loss: 0.0636 - val_loss: 0.0807 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "563/563 - 5s - loss: 0.0640 - val_loss: 0.0815 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "563/563 - 5s - loss: 0.0638 - val_loss: 0.0830 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "563/563 - 5s - loss: 0.0636 - val_loss: 0.0737 - 5s/epoch - 9ms/step\n",
      "Epoch 94/10000\n",
      "563/563 - 5s - loss: 0.0891 - val_loss: 0.1027 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "563/563 - 5s - loss: 0.0653 - val_loss: 0.0903 - 5s/epoch - 9ms/step\n",
      "Epoch 96/10000\n",
      "563/563 - 5s - loss: 0.0639 - val_loss: 0.0777 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "563/563 - 5s - loss: 0.0623 - val_loss: 0.0754 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "563/563 - 5s - loss: 0.0615 - val_loss: 0.0727 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "563/563 - 5s - loss: 0.0608 - val_loss: 0.0757 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "563/563 - 5s - loss: 0.0620 - val_loss: 0.0790 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "563/563 - 5s - loss: 0.0646 - val_loss: 0.0700 - 5s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "563/563 - 5s - loss: 0.0598 - val_loss: 0.0786 - 5s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "563/563 - 5s - loss: 0.0607 - val_loss: 0.0730 - 5s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "563/563 - 5s - loss: 0.0592 - val_loss: 0.0717 - 5s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "563/563 - 5s - loss: 0.0602 - val_loss: 0.0765 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "563/563 - 5s - loss: 0.0629 - val_loss: 0.0717 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "563/563 - 5s - loss: 0.0592 - val_loss: 0.0699 - 5s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "563/563 - 5s - loss: 0.0573 - val_loss: 0.0704 - 5s/epoch - 8ms/step\n",
      "Epoch 109/10000\n",
      "563/563 - 5s - loss: 0.0603 - val_loss: 0.0709 - 5s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "563/563 - 5s - loss: 0.0602 - val_loss: 0.0752 - 5s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "563/563 - 5s - loss: 0.0594 - val_loss: 0.0793 - 5s/epoch - 8ms/step\n",
      "Epoch 112/10000\n",
      "563/563 - 5s - loss: 0.0587 - val_loss: 0.0699 - 5s/epoch - 8ms/step\n",
      "Epoch 113/10000\n",
      "563/563 - 5s - loss: 0.0578 - val_loss: 0.0677 - 5s/epoch - 8ms/step\n",
      "Epoch 114/10000\n",
      "563/563 - 5s - loss: 0.0584 - val_loss: 0.0674 - 5s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "563/563 - 5s - loss: 0.0582 - val_loss: 0.0842 - 5s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "563/563 - 5s - loss: 0.0593 - val_loss: 0.0685 - 5s/epoch - 9ms/step\n",
      "Epoch 117/10000\n",
      "563/563 - 5s - loss: 0.0572 - val_loss: 0.0672 - 5s/epoch - 9ms/step\n",
      "Epoch 118/10000\n",
      "563/563 - 5s - loss: 0.0567 - val_loss: 0.0663 - 5s/epoch - 8ms/step\n",
      "Epoch 119/10000\n",
      "563/563 - 5s - loss: 0.0579 - val_loss: 0.0672 - 5s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "563/563 - 5s - loss: 0.0563 - val_loss: 0.0673 - 5s/epoch - 8ms/step\n",
      "Epoch 121/10000\n",
      "563/563 - 5s - loss: 0.0562 - val_loss: 0.0755 - 5s/epoch - 8ms/step\n",
      "Epoch 122/10000\n",
      "563/563 - 5s - loss: 0.0572 - val_loss: 0.0703 - 5s/epoch - 8ms/step\n",
      "Epoch 123/10000\n",
      "563/563 - 5s - loss: 0.0560 - val_loss: 0.0652 - 5s/epoch - 8ms/step\n",
      "Epoch 124/10000\n",
      "563/563 - 5s - loss: 0.0562 - val_loss: 0.0664 - 5s/epoch - 8ms/step\n",
      "Epoch 125/10000\n",
      "563/563 - 5s - loss: 0.0562 - val_loss: 0.0746 - 5s/epoch - 8ms/step\n",
      "Epoch 126/10000\n",
      "563/563 - 5s - loss: 0.0563 - val_loss: 0.0708 - 5s/epoch - 8ms/step\n",
      "Epoch 127/10000\n",
      "563/563 - 5s - loss: 0.0582 - val_loss: 0.0668 - 5s/epoch - 8ms/step\n",
      "Epoch 128/10000\n",
      "563/563 - 5s - loss: 0.0575 - val_loss: 0.0720 - 5s/epoch - 8ms/step\n",
      "Epoch 129/10000\n",
      "563/563 - 5s - loss: 0.0569 - val_loss: 0.0806 - 5s/epoch - 8ms/step\n",
      "Epoch 130/10000\n",
      "563/563 - 5s - loss: 0.0560 - val_loss: 0.0640 - 5s/epoch - 8ms/step\n",
      "Epoch 131/10000\n",
      "563/563 - 5s - loss: 0.0544 - val_loss: 0.0636 - 5s/epoch - 8ms/step\n",
      "Epoch 132/10000\n",
      "563/563 - 5s - loss: 0.0537 - val_loss: 0.0662 - 5s/epoch - 8ms/step\n",
      "Epoch 133/10000\n",
      "563/563 - 5s - loss: 0.0543 - val_loss: 0.0654 - 5s/epoch - 8ms/step\n",
      "Epoch 134/10000\n",
      "563/563 - 5s - loss: 0.0545 - val_loss: 0.0635 - 5s/epoch - 8ms/step\n",
      "Epoch 135/10000\n",
      "563/563 - 5s - loss: 0.0558 - val_loss: 0.0693 - 5s/epoch - 8ms/step\n",
      "Epoch 136/10000\n",
      "563/563 - 5s - loss: 0.0548 - val_loss: 0.0642 - 5s/epoch - 8ms/step\n",
      "Epoch 137/10000\n",
      "563/563 - 5s - loss: 0.0524 - val_loss: 0.0658 - 5s/epoch - 8ms/step\n",
      "Epoch 138/10000\n",
      "563/563 - 5s - loss: 0.0543 - val_loss: 0.0641 - 5s/epoch - 8ms/step\n",
      "Epoch 139/10000\n",
      "563/563 - 5s - loss: 0.0532 - val_loss: 0.0647 - 5s/epoch - 9ms/step\n",
      "Epoch 140/10000\n",
      "563/563 - 5s - loss: 0.0530 - val_loss: 0.0682 - 5s/epoch - 8ms/step\n",
      "Epoch 141/10000\n",
      "563/563 - 5s - loss: 0.0533 - val_loss: 0.0623 - 5s/epoch - 8ms/step\n",
      "Epoch 142/10000\n",
      "563/563 - 5s - loss: 0.0530 - val_loss: 0.0614 - 5s/epoch - 8ms/step\n",
      "Epoch 143/10000\n",
      "563/563 - 5s - loss: 0.0531 - val_loss: 0.0624 - 5s/epoch - 8ms/step\n",
      "Epoch 144/10000\n",
      "563/563 - 5s - loss: 0.0535 - val_loss: 0.0647 - 5s/epoch - 8ms/step\n",
      "Epoch 145/10000\n",
      "563/563 - 5s - loss: 0.0529 - val_loss: 0.0645 - 5s/epoch - 8ms/step\n",
      "Epoch 146/10000\n",
      "563/563 - 5s - loss: 0.0536 - val_loss: 0.0625 - 5s/epoch - 8ms/step\n",
      "Epoch 147/10000\n",
      "563/563 - 5s - loss: 0.0528 - val_loss: 0.0613 - 5s/epoch - 8ms/step\n",
      "Epoch 148/10000\n",
      "563/563 - 5s - loss: 0.0530 - val_loss: 0.0690 - 5s/epoch - 8ms/step\n",
      "Epoch 149/10000\n",
      "563/563 - 5s - loss: 0.0516 - val_loss: 0.0589 - 5s/epoch - 8ms/step\n",
      "Epoch 150/10000\n",
      "563/563 - 5s - loss: 0.0514 - val_loss: 0.0655 - 5s/epoch - 8ms/step\n",
      "Epoch 151/10000\n",
      "563/563 - 5s - loss: 0.0723 - val_loss: 0.0882 - 5s/epoch - 9ms/step\n",
      "Epoch 152/10000\n",
      "563/563 - 5s - loss: 0.0567 - val_loss: 0.0738 - 5s/epoch - 9ms/step\n",
      "Epoch 153/10000\n",
      "563/563 - 5s - loss: 0.0510 - val_loss: 0.0621 - 5s/epoch - 8ms/step\n",
      "Epoch 154/10000\n",
      "563/563 - 5s - loss: 0.0546 - val_loss: 0.0637 - 5s/epoch - 8ms/step\n",
      "Epoch 155/10000\n",
      "563/563 - 5s - loss: 0.0513 - val_loss: 0.0651 - 5s/epoch - 9ms/step\n",
      "Epoch 156/10000\n",
      "563/563 - 5s - loss: 0.0506 - val_loss: 0.0633 - 5s/epoch - 8ms/step\n",
      "Epoch 157/10000\n",
      "563/563 - 5s - loss: 0.0503 - val_loss: 0.0637 - 5s/epoch - 8ms/step\n",
      "Epoch 158/10000\n",
      "563/563 - 5s - loss: 0.0513 - val_loss: 0.0637 - 5s/epoch - 8ms/step\n",
      "Epoch 159/10000\n",
      "563/563 - 5s - loss: 0.0527 - val_loss: 0.0641 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_244_layer_call_fn, gru_cell_244_layer_call_and_return_conditional_losses, gru_cell_245_layer_call_fn, gru_cell_245_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_30_1_50_1_50_datt_seq2seq_gru_6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/30_30_1_50_1_50_datt_seq2seq_gru_6\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021CE31EECA0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B4E56A1C0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.825952</td>\n",
       "      <td>0.869315</td>\n",
       "      <td>0.92129</td>\n",
       "      <td>0.872186</td>\n",
       "      <td>8.22342</td>\n",
       "      <td>6.340864</td>\n",
       "      <td>4.301427</td>\n",
       "      <td>6.28857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.749921</td>\n",
       "      <td>0.786192</td>\n",
       "      <td>0.868111</td>\n",
       "      <td>0.801408</td>\n",
       "      <td>9.855078</td>\n",
       "      <td>8.111088</td>\n",
       "      <td>5.569749</td>\n",
       "      <td>7.845305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.73229</td>\n",
       "      <td>0.818536</td>\n",
       "      <td>0.73599</td>\n",
       "      <td>11.53775</td>\n",
       "      <td>9.076924</td>\n",
       "      <td>6.53503</td>\n",
       "      <td>9.049901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.561855</td>\n",
       "      <td>0.69531</td>\n",
       "      <td>0.786149</td>\n",
       "      <td>0.681105</td>\n",
       "      <td>13.042556</td>\n",
       "      <td>9.684934</td>\n",
       "      <td>7.09583</td>\n",
       "      <td>9.941107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.473974</td>\n",
       "      <td>0.670846</td>\n",
       "      <td>0.765994</td>\n",
       "      <td>0.636938</td>\n",
       "      <td>14.290726</td>\n",
       "      <td>10.068241</td>\n",
       "      <td>7.42368</td>\n",
       "      <td>10.594216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.390547</td>\n",
       "      <td>0.651998</td>\n",
       "      <td>0.750945</td>\n",
       "      <td>0.59783</td>\n",
       "      <td>15.383097</td>\n",
       "      <td>10.355564</td>\n",
       "      <td>7.65907</td>\n",
       "      <td>11.132577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.310384</td>\n",
       "      <td>0.634809</td>\n",
       "      <td>0.734737</td>\n",
       "      <td>0.559977</td>\n",
       "      <td>16.36535</td>\n",
       "      <td>10.611574</td>\n",
       "      <td>7.905054</td>\n",
       "      <td>11.627326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.237672</td>\n",
       "      <td>0.615189</td>\n",
       "      <td>0.717491</td>\n",
       "      <td>0.52345</td>\n",
       "      <td>17.209692</td>\n",
       "      <td>10.896417</td>\n",
       "      <td>8.159235</td>\n",
       "      <td>12.088448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.170843</td>\n",
       "      <td>0.593166</td>\n",
       "      <td>0.698512</td>\n",
       "      <td>0.487507</td>\n",
       "      <td>17.952362</td>\n",
       "      <td>11.208496</td>\n",
       "      <td>8.430207</td>\n",
       "      <td>12.530355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.108477</td>\n",
       "      <td>0.570724</td>\n",
       "      <td>0.677676</td>\n",
       "      <td>0.452292</td>\n",
       "      <td>18.621848</td>\n",
       "      <td>11.517296</td>\n",
       "      <td>8.718578</td>\n",
       "      <td>12.952574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.049635</td>\n",
       "      <td>0.54977</td>\n",
       "      <td>0.653883</td>\n",
       "      <td>0.417763</td>\n",
       "      <td>19.235343</td>\n",
       "      <td>11.798281</td>\n",
       "      <td>9.036851</td>\n",
       "      <td>13.356825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.00636</td>\n",
       "      <td>0.532636</td>\n",
       "      <td>0.62901</td>\n",
       "      <td>0.385095</td>\n",
       "      <td>19.438343</td>\n",
       "      <td>12.02454</td>\n",
       "      <td>9.358675</td>\n",
       "      <td>13.607186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.060597</td>\n",
       "      <td>0.517854</td>\n",
       "      <td>0.606145</td>\n",
       "      <td>0.354467</td>\n",
       "      <td>19.086048</td>\n",
       "      <td>12.217937</td>\n",
       "      <td>9.64558</td>\n",
       "      <td>13.649855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.111294</td>\n",
       "      <td>0.504902</td>\n",
       "      <td>0.586151</td>\n",
       "      <td>0.326586</td>\n",
       "      <td>18.884382</td>\n",
       "      <td>12.385312</td>\n",
       "      <td>9.889562</td>\n",
       "      <td>13.719752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.157827</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>0.56879</td>\n",
       "      <td>0.301239</td>\n",
       "      <td>18.967113</td>\n",
       "      <td>12.541924</td>\n",
       "      <td>10.097151</td>\n",
       "      <td>13.868729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.197751</td>\n",
       "      <td>0.480153</td>\n",
       "      <td>0.553186</td>\n",
       "      <td>0.27853</td>\n",
       "      <td>19.22625</td>\n",
       "      <td>12.702491</td>\n",
       "      <td>10.28</td>\n",
       "      <td>14.06958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.231729</td>\n",
       "      <td>0.466582</td>\n",
       "      <td>0.538448</td>\n",
       "      <td>0.257767</td>\n",
       "      <td>19.114707</td>\n",
       "      <td>12.870151</td>\n",
       "      <td>10.450287</td>\n",
       "      <td>14.145049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.259082</td>\n",
       "      <td>0.451336</td>\n",
       "      <td>0.523941</td>\n",
       "      <td>0.238731</td>\n",
       "      <td>18.956322</td>\n",
       "      <td>13.055704</td>\n",
       "      <td>10.615324</td>\n",
       "      <td>14.209117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.283369</td>\n",
       "      <td>0.43588</td>\n",
       "      <td>0.510152</td>\n",
       "      <td>0.220888</td>\n",
       "      <td>18.855495</td>\n",
       "      <td>13.240618</td>\n",
       "      <td>10.769881</td>\n",
       "      <td>14.288665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.307249</td>\n",
       "      <td>0.420864</td>\n",
       "      <td>0.496474</td>\n",
       "      <td>0.203363</td>\n",
       "      <td>18.753405</td>\n",
       "      <td>13.415869</td>\n",
       "      <td>10.920877</td>\n",
       "      <td>14.363384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.330445</td>\n",
       "      <td>0.405688</td>\n",
       "      <td>0.483701</td>\n",
       "      <td>0.186315</td>\n",
       "      <td>18.720494</td>\n",
       "      <td>13.591269</td>\n",
       "      <td>11.060153</td>\n",
       "      <td>14.457305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.353007</td>\n",
       "      <td>0.389208</td>\n",
       "      <td>0.47175</td>\n",
       "      <td>0.169317</td>\n",
       "      <td>18.755154</td>\n",
       "      <td>13.777607</td>\n",
       "      <td>11.189287</td>\n",
       "      <td>14.574016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.372785</td>\n",
       "      <td>0.37078</td>\n",
       "      <td>0.459157</td>\n",
       "      <td>0.152384</td>\n",
       "      <td>18.629665</td>\n",
       "      <td>13.982878</td>\n",
       "      <td>11.32454</td>\n",
       "      <td>14.645694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.388897</td>\n",
       "      <td>0.351954</td>\n",
       "      <td>0.446364</td>\n",
       "      <td>0.136474</td>\n",
       "      <td>18.483621</td>\n",
       "      <td>14.191646</td>\n",
       "      <td>11.461194</td>\n",
       "      <td>14.712153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.404671</td>\n",
       "      <td>0.333584</td>\n",
       "      <td>0.433398</td>\n",
       "      <td>0.12077</td>\n",
       "      <td>18.398563</td>\n",
       "      <td>14.391947</td>\n",
       "      <td>11.598106</td>\n",
       "      <td>14.796205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.417698</td>\n",
       "      <td>0.3161</td>\n",
       "      <td>0.420666</td>\n",
       "      <td>0.106356</td>\n",
       "      <td>18.298867</td>\n",
       "      <td>14.578772</td>\n",
       "      <td>11.731765</td>\n",
       "      <td>14.869801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.42733</td>\n",
       "      <td>0.297876</td>\n",
       "      <td>0.408838</td>\n",
       "      <td>0.093128</td>\n",
       "      <td>18.182186</td>\n",
       "      <td>14.769528</td>\n",
       "      <td>11.855404</td>\n",
       "      <td>14.935706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.434406</td>\n",
       "      <td>0.280138</td>\n",
       "      <td>0.397901</td>\n",
       "      <td>0.081211</td>\n",
       "      <td>18.182631</td>\n",
       "      <td>14.951915</td>\n",
       "      <td>11.969577</td>\n",
       "      <td>15.034708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.437121</td>\n",
       "      <td>0.262908</td>\n",
       "      <td>0.38736</td>\n",
       "      <td>0.071049</td>\n",
       "      <td>18.221866</td>\n",
       "      <td>15.127765</td>\n",
       "      <td>12.078949</td>\n",
       "      <td>15.14286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.43823</td>\n",
       "      <td>0.24622</td>\n",
       "      <td>0.377699</td>\n",
       "      <td>0.061897</td>\n",
       "      <td>18.254457</td>\n",
       "      <td>15.296498</td>\n",
       "      <td>12.17862</td>\n",
       "      <td>15.243191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.036115</td>\n",
       "      <td>0.497568</td>\n",
       "      <td>0.589748</td>\n",
       "      <td>0.3504</td>\n",
       "      <td>17.23756</td>\n",
       "      <td>12.292802</td>\n",
       "      <td>9.643655</td>\n",
       "      <td>13.058005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.825952  0.869315   0.92129  0.872186    8.22342   6.340864   \n",
       "1      0.749921  0.786192  0.868111  0.801408   9.855078   8.111088   \n",
       "2      0.657143   0.73229  0.818536   0.73599   11.53775   9.076924   \n",
       "3      0.561855   0.69531  0.786149  0.681105  13.042556   9.684934   \n",
       "4      0.473974  0.670846  0.765994  0.636938  14.290726  10.068241   \n",
       "5      0.390547  0.651998  0.750945   0.59783  15.383097  10.355564   \n",
       "6      0.310384  0.634809  0.734737  0.559977   16.36535  10.611574   \n",
       "7      0.237672  0.615189  0.717491   0.52345  17.209692  10.896417   \n",
       "8      0.170843  0.593166  0.698512  0.487507  17.952362  11.208496   \n",
       "9      0.108477  0.570724  0.677676  0.452292  18.621848  11.517296   \n",
       "10     0.049635   0.54977  0.653883  0.417763  19.235343  11.798281   \n",
       "11     -0.00636  0.532636   0.62901  0.385095  19.438343   12.02454   \n",
       "12    -0.060597  0.517854  0.606145  0.354467  19.086048  12.217937   \n",
       "13    -0.111294  0.504902  0.586151  0.326586  18.884382  12.385312   \n",
       "14    -0.157827  0.492754   0.56879  0.301239  18.967113  12.541924   \n",
       "15    -0.197751  0.480153  0.553186   0.27853   19.22625  12.702491   \n",
       "16    -0.231729  0.466582  0.538448  0.257767  19.114707  12.870151   \n",
       "17    -0.259082  0.451336  0.523941  0.238731  18.956322  13.055704   \n",
       "18    -0.283369   0.43588  0.510152  0.220888  18.855495  13.240618   \n",
       "19    -0.307249  0.420864  0.496474  0.203363  18.753405  13.415869   \n",
       "20    -0.330445  0.405688  0.483701  0.186315  18.720494  13.591269   \n",
       "21    -0.353007  0.389208   0.47175  0.169317  18.755154  13.777607   \n",
       "22    -0.372785   0.37078  0.459157  0.152384  18.629665  13.982878   \n",
       "23    -0.388897  0.351954  0.446364  0.136474  18.483621  14.191646   \n",
       "24    -0.404671  0.333584  0.433398   0.12077  18.398563  14.391947   \n",
       "25    -0.417698    0.3161  0.420666  0.106356  18.298867  14.578772   \n",
       "26     -0.42733  0.297876  0.408838  0.093128  18.182186  14.769528   \n",
       "27    -0.434406  0.280138  0.397901  0.081211  18.182631  14.951915   \n",
       "28    -0.437121  0.262908   0.38736  0.071049  18.221866  15.127765   \n",
       "29     -0.43823   0.24622  0.377699  0.061897  18.254457  15.296498   \n",
       "mean  -0.036115  0.497568  0.589748    0.3504   17.23756  12.292802   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       4.301427    6.28857  \n",
       "1       5.569749   7.845305  \n",
       "2        6.53503   9.049901  \n",
       "3        7.09583   9.941107  \n",
       "4        7.42368  10.594216  \n",
       "5        7.65907  11.132577  \n",
       "6       7.905054  11.627326  \n",
       "7       8.159235  12.088448  \n",
       "8       8.430207  12.530355  \n",
       "9       8.718578  12.952574  \n",
       "10      9.036851  13.356825  \n",
       "11      9.358675  13.607186  \n",
       "12       9.64558  13.649855  \n",
       "13      9.889562  13.719752  \n",
       "14     10.097151  13.868729  \n",
       "15         10.28   14.06958  \n",
       "16     10.450287  14.145049  \n",
       "17     10.615324  14.209117  \n",
       "18     10.769881  14.288665  \n",
       "19     10.920877  14.363384  \n",
       "20     11.060153  14.457305  \n",
       "21     11.189287  14.574016  \n",
       "22      11.32454  14.645694  \n",
       "23     11.461194  14.712153  \n",
       "24     11.598106  14.796205  \n",
       "25     11.731765  14.869801  \n",
       "26     11.855404  14.935706  \n",
       "27     11.969577  15.034708  \n",
       "28     12.078949   15.14286  \n",
       "29      12.17862  15.243191  \n",
       "mean    9.643655  13.058005  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/30_30_1_50_1_50_datt_seq2seq_gru_6.csv\n",
      "\n",
      "\n",
      "6th iteration\n",
      "history size: 40\n",
      "future size: 10\n",
      "Epoch 1/10000\n",
      "568/568 - 8s - loss: 0.2444 - val_loss: 0.1680 - 8s/epoch - 14ms/step\n",
      "Epoch 2/10000\n",
      "568/568 - 5s - loss: 0.1506 - val_loss: 0.1494 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "568/568 - 5s - loss: 0.1380 - val_loss: 0.1435 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "568/568 - 5s - loss: 0.1297 - val_loss: 0.1311 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "568/568 - 5s - loss: 0.1198 - val_loss: 0.1211 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "568/568 - 5s - loss: 0.1112 - val_loss: 0.1161 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "568/568 - 5s - loss: 0.1056 - val_loss: 0.1099 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "568/568 - 5s - loss: 0.1002 - val_loss: 0.1072 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "568/568 - 4s - loss: 0.0969 - val_loss: 0.1058 - 4s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "568/568 - 5s - loss: 0.0942 - val_loss: 0.1006 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "568/568 - 5s - loss: 0.0905 - val_loss: 0.0988 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "568/568 - 5s - loss: 0.0879 - val_loss: 0.0982 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "568/568 - 5s - loss: 0.0853 - val_loss: 0.0987 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "568/568 - 5s - loss: 0.0827 - val_loss: 0.0973 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "568/568 - 5s - loss: 0.0806 - val_loss: 0.0978 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "568/568 - 5s - loss: 0.0775 - val_loss: 0.0907 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "568/568 - 4s - loss: 0.0764 - val_loss: 0.0861 - 4s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "568/568 - 4s - loss: 0.0740 - val_loss: 0.0897 - 4s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "568/568 - 5s - loss: 0.0734 - val_loss: 0.0891 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "568/568 - 5s - loss: 0.0712 - val_loss: 0.0832 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "568/568 - 5s - loss: 0.0694 - val_loss: 0.0820 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "568/568 - 4s - loss: 0.0662 - val_loss: 0.0804 - 4s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "568/568 - 5s - loss: 0.0691 - val_loss: 0.0807 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "568/568 - 5s - loss: 0.0664 - val_loss: 0.0890 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "568/568 - 5s - loss: 0.0697 - val_loss: 0.0809 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "568/568 - 5s - loss: 0.0620 - val_loss: 0.0757 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "568/568 - 5s - loss: 0.0614 - val_loss: 0.0774 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "568/568 - 5s - loss: 0.0616 - val_loss: 0.0803 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "568/568 - 4s - loss: 0.0592 - val_loss: 0.0787 - 4s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "568/568 - 5s - loss: 0.0602 - val_loss: 0.0722 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "568/568 - 5s - loss: 0.0592 - val_loss: 0.0824 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "568/568 - 5s - loss: 0.0572 - val_loss: 0.0704 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "568/568 - 5s - loss: 0.0616 - val_loss: 0.0747 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "568/568 - 5s - loss: 0.0576 - val_loss: 0.0738 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "568/568 - 5s - loss: 0.0564 - val_loss: 0.0686 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "568/568 - 5s - loss: 0.0538 - val_loss: 0.0663 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "568/568 - 4s - loss: 0.0535 - val_loss: 0.0657 - 4s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "568/568 - 5s - loss: 0.0535 - val_loss: 0.0674 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "568/568 - 5s - loss: 0.0511 - val_loss: 0.0771 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "568/568 - 5s - loss: 0.0514 - val_loss: 0.0679 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "568/568 - 5s - loss: 0.0527 - val_loss: 0.0820 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "568/568 - 5s - loss: 0.0529 - val_loss: 0.0661 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "568/568 - 4s - loss: 0.0488 - val_loss: 0.0626 - 4s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "568/568 - 4s - loss: 0.0483 - val_loss: 0.0666 - 4s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "568/568 - 4s - loss: 0.0477 - val_loss: 0.0619 - 4s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "568/568 - 4s - loss: 0.0479 - val_loss: 0.0633 - 4s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "568/568 - 4s - loss: 0.0492 - val_loss: 0.0639 - 4s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "568/568 - 5s - loss: 0.0488 - val_loss: 0.0665 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "568/568 - 5s - loss: 0.0496 - val_loss: 0.0666 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "568/568 - 5s - loss: 0.0469 - val_loss: 0.0605 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "568/568 - 5s - loss: 0.0450 - val_loss: 0.0598 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "568/568 - 5s - loss: 0.0484 - val_loss: 0.0699 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "568/568 - 5s - loss: 0.0472 - val_loss: 0.0617 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "568/568 - 5s - loss: 0.0435 - val_loss: 0.0560 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "568/568 - 4s - loss: 0.0428 - val_loss: 0.0590 - 4s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "568/568 - 5s - loss: 0.0438 - val_loss: 0.0571 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "568/568 - 5s - loss: 0.0435 - val_loss: 0.0587 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "568/568 - 4s - loss: 0.0443 - val_loss: 0.0634 - 4s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "568/568 - 5s - loss: 0.0491 - val_loss: 0.0598 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "568/568 - 5s - loss: 0.0420 - val_loss: 0.0566 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "568/568 - 5s - loss: 0.0417 - val_loss: 0.0613 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "568/568 - 4s - loss: 0.0415 - val_loss: 0.0565 - 4s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "568/568 - 5s - loss: 0.0408 - val_loss: 0.0566 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "568/568 - 4s - loss: 0.0406 - val_loss: 0.0578 - 4s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_246_layer_call_fn, gru_cell_246_layer_call_and_return_conditional_losses, gru_cell_247_layer_call_fn, gru_cell_247_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_10_1_50_1_50_datt_seq2seq_gru_6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_10_1_50_1_50_datt_seq2seq_gru_6\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B4ED32250> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B66CD7A00> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.929679</td>\n",
       "      <td>0.939977</td>\n",
       "      <td>0.949369</td>\n",
       "      <td>0.939675</td>\n",
       "      <td>4.303382</td>\n",
       "      <td>4.312139</td>\n",
       "      <td>3.464436</td>\n",
       "      <td>4.026653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.864718</td>\n",
       "      <td>0.873685</td>\n",
       "      <td>0.917835</td>\n",
       "      <td>0.885412</td>\n",
       "      <td>5.932026</td>\n",
       "      <td>6.256144</td>\n",
       "      <td>4.41391</td>\n",
       "      <td>5.534027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.782234</td>\n",
       "      <td>0.821357</td>\n",
       "      <td>0.876313</td>\n",
       "      <td>0.826635</td>\n",
       "      <td>7.424597</td>\n",
       "      <td>7.440622</td>\n",
       "      <td>5.416306</td>\n",
       "      <td>6.760508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.694118</td>\n",
       "      <td>0.782619</td>\n",
       "      <td>0.84289</td>\n",
       "      <td>0.773209</td>\n",
       "      <td>8.682737</td>\n",
       "      <td>8.208974</td>\n",
       "      <td>6.105155</td>\n",
       "      <td>7.665622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.610267</td>\n",
       "      <td>0.754525</td>\n",
       "      <td>0.820523</td>\n",
       "      <td>0.728438</td>\n",
       "      <td>9.704063</td>\n",
       "      <td>8.724455</td>\n",
       "      <td>6.525716</td>\n",
       "      <td>8.318078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.534542</td>\n",
       "      <td>0.734403</td>\n",
       "      <td>0.808258</td>\n",
       "      <td>0.692401</td>\n",
       "      <td>10.501447</td>\n",
       "      <td>9.075932</td>\n",
       "      <td>6.745751</td>\n",
       "      <td>8.774377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.468834</td>\n",
       "      <td>0.719155</td>\n",
       "      <td>0.802858</td>\n",
       "      <td>0.663616</td>\n",
       "      <td>11.110194</td>\n",
       "      <td>9.332723</td>\n",
       "      <td>6.841285</td>\n",
       "      <td>9.094734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.414079</td>\n",
       "      <td>0.707437</td>\n",
       "      <td>0.801465</td>\n",
       "      <td>0.640993</td>\n",
       "      <td>11.64054</td>\n",
       "      <td>9.524716</td>\n",
       "      <td>6.867396</td>\n",
       "      <td>9.344218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.370445</td>\n",
       "      <td>0.697283</td>\n",
       "      <td>0.798654</td>\n",
       "      <td>0.622127</td>\n",
       "      <td>12.08031</td>\n",
       "      <td>9.687486</td>\n",
       "      <td>6.918034</td>\n",
       "      <td>9.561944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.335437</td>\n",
       "      <td>0.68423</td>\n",
       "      <td>0.790954</td>\n",
       "      <td>0.60354</td>\n",
       "      <td>12.425919</td>\n",
       "      <td>9.892487</td>\n",
       "      <td>7.051519</td>\n",
       "      <td>9.789975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.600435</td>\n",
       "      <td>0.771467</td>\n",
       "      <td>0.840912</td>\n",
       "      <td>0.737605</td>\n",
       "      <td>9.380522</td>\n",
       "      <td>8.245568</td>\n",
       "      <td>6.034951</td>\n",
       "      <td>7.887013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3 TT-3061-5 LT-3061-2  \\\n",
       "index        R2        R2        R2        R2      nRMSE     nRMSE     nRMSE   \n",
       "0      0.929679  0.939977  0.949369  0.939675   4.303382  4.312139  3.464436   \n",
       "1      0.864718  0.873685  0.917835  0.885412   5.932026  6.256144   4.41391   \n",
       "2      0.782234  0.821357  0.876313  0.826635   7.424597  7.440622  5.416306   \n",
       "3      0.694118  0.782619   0.84289  0.773209   8.682737  8.208974  6.105155   \n",
       "4      0.610267  0.754525  0.820523  0.728438   9.704063  8.724455  6.525716   \n",
       "5      0.534542  0.734403  0.808258  0.692401  10.501447  9.075932  6.745751   \n",
       "6      0.468834  0.719155  0.802858  0.663616  11.110194  9.332723  6.841285   \n",
       "7      0.414079  0.707437  0.801465  0.640993   11.64054  9.524716  6.867396   \n",
       "8      0.370445  0.697283  0.798654  0.622127   12.08031  9.687486  6.918034   \n",
       "9      0.335437   0.68423  0.790954   0.60354  12.425919  9.892487  7.051519   \n",
       "mean   0.600435  0.771467  0.840912  0.737605   9.380522  8.245568  6.034951   \n",
       "\n",
       "           mean  \n",
       "index     nRMSE  \n",
       "0      4.026653  \n",
       "1      5.534027  \n",
       "2      6.760508  \n",
       "3      7.665622  \n",
       "4      8.318078  \n",
       "5      8.774377  \n",
       "6      9.094734  \n",
       "7      9.344218  \n",
       "8      9.561944  \n",
       "9      9.789975  \n",
       "mean   7.887013  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/40_10_1_50_1_50_datt_seq2seq_gru_6.csv\n",
      "\n",
      "\n",
      "6th iteration\n",
      "history size: 40\n",
      "future size: 20\n",
      "Epoch 1/10000\n",
      "563/563 - 9s - loss: 0.3043 - val_loss: 0.2569 - 9s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "563/563 - 5s - loss: 0.2203 - val_loss: 0.2319 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "563/563 - 5s - loss: 0.2049 - val_loss: 0.2305 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "563/563 - 5s - loss: 0.1924 - val_loss: 0.2051 - 5s/epoch - 9ms/step\n",
      "Epoch 5/10000\n",
      "563/563 - 5s - loss: 0.1825 - val_loss: 0.1957 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "563/563 - 5s - loss: 0.1748 - val_loss: 0.1918 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "563/563 - 5s - loss: 0.1682 - val_loss: 0.1831 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "563/563 - 5s - loss: 0.1608 - val_loss: 0.1777 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "563/563 - 5s - loss: 0.1557 - val_loss: 0.1742 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "563/563 - 5s - loss: 0.1499 - val_loss: 0.1629 - 5s/epoch - 9ms/step\n",
      "Epoch 11/10000\n",
      "563/563 - 5s - loss: 0.1450 - val_loss: 0.1684 - 5s/epoch - 9ms/step\n",
      "Epoch 12/10000\n",
      "563/563 - 5s - loss: 0.1386 - val_loss: 0.1531 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "563/563 - 5s - loss: 0.1344 - val_loss: 0.1503 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "563/563 - 5s - loss: 0.1296 - val_loss: 0.1618 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "563/563 - 5s - loss: 0.1254 - val_loss: 0.1432 - 5s/epoch - 9ms/step\n",
      "Epoch 16/10000\n",
      "563/563 - 5s - loss: 0.1193 - val_loss: 0.1354 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "563/563 - 5s - loss: 0.1138 - val_loss: 0.1335 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "563/563 - 5s - loss: 0.1105 - val_loss: 0.1283 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "563/563 - 5s - loss: 0.1090 - val_loss: 0.1257 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "563/563 - 5s - loss: 0.1048 - val_loss: 0.1170 - 5s/epoch - 9ms/step\n",
      "Epoch 21/10000\n",
      "563/563 - 5s - loss: 0.1012 - val_loss: 0.1184 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "563/563 - 5s - loss: 0.0996 - val_loss: 0.1189 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "563/563 - 5s - loss: 0.0969 - val_loss: 0.1164 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "563/563 - 5s - loss: 0.0954 - val_loss: 0.1132 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "563/563 - 5s - loss: 0.0949 - val_loss: 0.1167 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "563/563 - 5s - loss: 0.0896 - val_loss: 0.1083 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "563/563 - 5s - loss: 0.0888 - val_loss: 0.1047 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "563/563 - 5s - loss: 0.0877 - val_loss: 0.1059 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "563/563 - 5s - loss: 0.0854 - val_loss: 0.1122 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "563/563 - 5s - loss: 0.0856 - val_loss: 0.1016 - 5s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "563/563 - 5s - loss: 0.0818 - val_loss: 0.1012 - 5s/epoch - 9ms/step\n",
      "Epoch 32/10000\n",
      "563/563 - 5s - loss: 0.0822 - val_loss: 0.1030 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "563/563 - 5s - loss: 0.0799 - val_loss: 0.1026 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "563/563 - 5s - loss: 0.0794 - val_loss: 0.1119 - 5s/epoch - 9ms/step\n",
      "Epoch 35/10000\n",
      "563/563 - 5s - loss: 0.0800 - val_loss: 0.0991 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "563/563 - 5s - loss: 0.0787 - val_loss: 0.1454 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "563/563 - 5s - loss: 0.0810 - val_loss: 0.0974 - 5s/epoch - 9ms/step\n",
      "Epoch 38/10000\n",
      "563/563 - 5s - loss: 0.0741 - val_loss: 0.0930 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "563/563 - 5s - loss: 0.0752 - val_loss: 0.0982 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "563/563 - 5s - loss: 0.0754 - val_loss: 0.0993 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "563/563 - 5s - loss: 0.0747 - val_loss: 0.0930 - 5s/epoch - 9ms/step\n",
      "Epoch 42/10000\n",
      "563/563 - 5s - loss: 0.0720 - val_loss: 0.0941 - 5s/epoch - 9ms/step\n",
      "Epoch 43/10000\n",
      "563/563 - 5s - loss: 0.0727 - val_loss: 0.0889 - 5s/epoch - 9ms/step\n",
      "Epoch 44/10000\n",
      "563/563 - 5s - loss: 0.0736 - val_loss: 0.0941 - 5s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "563/563 - 5s - loss: 0.0694 - val_loss: 0.0954 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "563/563 - 5s - loss: 0.0709 - val_loss: 0.1067 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "563/563 - 5s - loss: 0.0707 - val_loss: 0.1047 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "563/563 - 5s - loss: 0.0700 - val_loss: 0.0856 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "563/563 - 5s - loss: 0.0705 - val_loss: 0.0812 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "563/563 - 5s - loss: 0.0645 - val_loss: 0.0812 - 5s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "563/563 - 5s - loss: 0.0681 - val_loss: 0.0906 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "563/563 - 5s - loss: 0.0668 - val_loss: 0.0899 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "563/563 - 5s - loss: 0.0648 - val_loss: 0.0814 - 5s/epoch - 9ms/step\n",
      "Epoch 54/10000\n",
      "563/563 - 5s - loss: 0.0637 - val_loss: 0.0802 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "563/563 - 5s - loss: 0.0626 - val_loss: 0.0860 - 5s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "563/563 - 5s - loss: 0.0645 - val_loss: 0.0792 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "563/563 - 5s - loss: 0.0610 - val_loss: 0.0818 - 5s/epoch - 9ms/step\n",
      "Epoch 58/10000\n",
      "563/563 - 5s - loss: 0.0610 - val_loss: 0.0817 - 5s/epoch - 9ms/step\n",
      "Epoch 59/10000\n",
      "563/563 - 5s - loss: 0.0602 - val_loss: 0.0807 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "563/563 - 5s - loss: 0.0673 - val_loss: 0.0848 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "563/563 - 5s - loss: 0.0641 - val_loss: 0.0767 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "563/563 - 5s - loss: 0.0580 - val_loss: 0.0739 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "563/563 - 5s - loss: 0.0583 - val_loss: 0.0721 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "563/563 - 5s - loss: 0.0638 - val_loss: 0.0794 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "563/563 - 5s - loss: 0.0565 - val_loss: 0.0739 - 5s/epoch - 9ms/step\n",
      "Epoch 66/10000\n",
      "563/563 - 5s - loss: 0.0560 - val_loss: 0.0825 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "563/563 - 5s - loss: 0.0601 - val_loss: 0.0845 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "563/563 - 5s - loss: 0.0619 - val_loss: 0.0765 - 5s/epoch - 9ms/step\n",
      "Epoch 69/10000\n",
      "563/563 - 5s - loss: 0.0550 - val_loss: 0.0720 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "563/563 - 5s - loss: 0.0543 - val_loss: 0.0756 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "563/563 - 5s - loss: 0.0546 - val_loss: 0.0712 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "563/563 - 5s - loss: 0.0539 - val_loss: 0.0752 - 5s/epoch - 9ms/step\n",
      "Epoch 73/10000\n",
      "563/563 - 5s - loss: 0.0617 - val_loss: 0.0782 - 5s/epoch - 9ms/step\n",
      "Epoch 74/10000\n",
      "563/563 - 5s - loss: 0.0548 - val_loss: 0.0713 - 5s/epoch - 9ms/step\n",
      "Epoch 75/10000\n",
      "563/563 - 5s - loss: 0.0510 - val_loss: 0.0735 - 5s/epoch - 9ms/step\n",
      "Epoch 76/10000\n",
      "563/563 - 5s - loss: 0.0522 - val_loss: 0.0648 - 5s/epoch - 9ms/step\n",
      "Epoch 77/10000\n",
      "563/563 - 5s - loss: 0.0517 - val_loss: 0.0721 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "563/563 - 5s - loss: 0.0532 - val_loss: 0.0653 - 5s/epoch - 9ms/step\n",
      "Epoch 79/10000\n",
      "563/563 - 5s - loss: 0.0524 - val_loss: 0.0640 - 5s/epoch - 9ms/step\n",
      "Epoch 80/10000\n",
      "563/563 - 5s - loss: 0.0509 - val_loss: 0.0636 - 5s/epoch - 9ms/step\n",
      "Epoch 81/10000\n",
      "563/563 - 5s - loss: 0.0510 - val_loss: 0.0664 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "563/563 - 5s - loss: 0.0520 - val_loss: 0.0727 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "563/563 - 5s - loss: 0.0543 - val_loss: 0.0628 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "563/563 - 5s - loss: 0.0523 - val_loss: 0.0681 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "563/563 - 5s - loss: 0.0482 - val_loss: 0.0652 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "563/563 - 5s - loss: 0.0490 - val_loss: 0.0659 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "563/563 - 5s - loss: 0.0487 - val_loss: 0.0605 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "563/563 - 5s - loss: 0.0487 - val_loss: 0.0715 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "563/563 - 5s - loss: 0.0484 - val_loss: 0.0644 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "563/563 - 5s - loss: 0.0481 - val_loss: 0.0643 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "563/563 - 5s - loss: 0.0499 - val_loss: 0.0878 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "563/563 - 5s - loss: 0.0571 - val_loss: 0.0733 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "563/563 - 5s - loss: 0.0524 - val_loss: 0.0737 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "563/563 - 5s - loss: 0.0497 - val_loss: 0.0602 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "563/563 - 5s - loss: 0.0466 - val_loss: 0.0638 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "563/563 - 5s - loss: 0.0460 - val_loss: 0.0614 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "563/563 - 5s - loss: 0.0467 - val_loss: 0.0638 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "563/563 - 5s - loss: 0.0480 - val_loss: 0.0659 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "563/563 - 5s - loss: 0.0488 - val_loss: 0.0624 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "563/563 - 5s - loss: 0.0446 - val_loss: 0.0666 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "563/563 - 5s - loss: 0.0461 - val_loss: 0.0614 - 5s/epoch - 9ms/step\n",
      "Epoch 102/10000\n",
      "563/563 - 5s - loss: 0.0454 - val_loss: 0.0593 - 5s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "563/563 - 5s - loss: 0.0449 - val_loss: 0.0639 - 5s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "563/563 - 5s - loss: 0.0451 - val_loss: 0.0654 - 5s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "563/563 - 5s - loss: 0.0480 - val_loss: 0.0658 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "563/563 - 5s - loss: 0.0481 - val_loss: 0.0574 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "563/563 - 5s - loss: 0.0438 - val_loss: 0.0549 - 5s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "563/563 - 5s - loss: 0.0443 - val_loss: 0.0596 - 5s/epoch - 9ms/step\n",
      "Epoch 109/10000\n",
      "563/563 - 5s - loss: 0.0437 - val_loss: 0.0569 - 5s/epoch - 8ms/step\n",
      "Epoch 110/10000\n",
      "563/563 - 5s - loss: 0.0441 - val_loss: 0.0569 - 5s/epoch - 8ms/step\n",
      "Epoch 111/10000\n",
      "563/563 - 5s - loss: 0.0427 - val_loss: 0.0616 - 5s/epoch - 9ms/step\n",
      "Epoch 112/10000\n",
      "563/563 - 5s - loss: 0.0463 - val_loss: 0.0601 - 5s/epoch - 9ms/step\n",
      "Epoch 113/10000\n",
      "563/563 - 5s - loss: 0.0430 - val_loss: 0.0596 - 5s/epoch - 9ms/step\n",
      "Epoch 114/10000\n",
      "563/563 - 5s - loss: 0.0432 - val_loss: 0.0602 - 5s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "563/563 - 5s - loss: 0.0435 - val_loss: 0.0604 - 5s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "563/563 - 5s - loss: 0.0439 - val_loss: 0.0661 - 5s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "563/563 - 5s - loss: 0.0446 - val_loss: 0.0588 - 5s/epoch - 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_248_layer_call_fn, gru_cell_248_layer_call_and_return_conditional_losses, gru_cell_249_layer_call_fn, gru_cell_249_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_20_1_50_1_50_datt_seq2seq_gru_6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_20_1_50_1_50_datt_seq2seq_gru_6\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B5135AC70> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B52A26A60> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.870734</td>\n",
       "      <td>0.903007</td>\n",
       "      <td>0.944785</td>\n",
       "      <td>0.906175</td>\n",
       "      <td>7.094088</td>\n",
       "      <td>5.476121</td>\n",
       "      <td>3.609382</td>\n",
       "      <td>5.393197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.793251</td>\n",
       "      <td>0.804834</td>\n",
       "      <td>0.896775</td>\n",
       "      <td>0.83162</td>\n",
       "      <td>8.810579</td>\n",
       "      <td>7.770392</td>\n",
       "      <td>4.936576</td>\n",
       "      <td>7.172515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.700022</td>\n",
       "      <td>0.739072</td>\n",
       "      <td>0.833317</td>\n",
       "      <td>0.757471</td>\n",
       "      <td>10.150446</td>\n",
       "      <td>8.988118</td>\n",
       "      <td>6.274879</td>\n",
       "      <td>8.471147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.607056</td>\n",
       "      <td>0.6907</td>\n",
       "      <td>0.769417</td>\n",
       "      <td>0.689058</td>\n",
       "      <td>11.229322</td>\n",
       "      <td>9.789297</td>\n",
       "      <td>7.38193</td>\n",
       "      <td>9.466849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.517725</td>\n",
       "      <td>0.660509</td>\n",
       "      <td>0.72664</td>\n",
       "      <td>0.634958</td>\n",
       "      <td>12.241272</td>\n",
       "      <td>10.260517</td>\n",
       "      <td>8.03936</td>\n",
       "      <td>10.180383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.436138</td>\n",
       "      <td>0.642123</td>\n",
       "      <td>0.715206</td>\n",
       "      <td>0.597822</td>\n",
       "      <td>13.191608</td>\n",
       "      <td>10.539456</td>\n",
       "      <td>8.207206</td>\n",
       "      <td>10.64609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.361304</td>\n",
       "      <td>0.628386</td>\n",
       "      <td>0.723715</td>\n",
       "      <td>0.571135</td>\n",
       "      <td>13.764394</td>\n",
       "      <td>10.742268</td>\n",
       "      <td>8.085315</td>\n",
       "      <td>10.863992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.292864</td>\n",
       "      <td>0.615123</td>\n",
       "      <td>0.732289</td>\n",
       "      <td>0.546758</td>\n",
       "      <td>14.206225</td>\n",
       "      <td>10.934734</td>\n",
       "      <td>7.960414</td>\n",
       "      <td>11.033791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.231018</td>\n",
       "      <td>0.598123</td>\n",
       "      <td>0.725748</td>\n",
       "      <td>0.518296</td>\n",
       "      <td>14.595548</td>\n",
       "      <td>11.175551</td>\n",
       "      <td>8.058517</td>\n",
       "      <td>11.276539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.177737</td>\n",
       "      <td>0.57668</td>\n",
       "      <td>0.696741</td>\n",
       "      <td>0.483719</td>\n",
       "      <td>14.873271</td>\n",
       "      <td>11.469992</td>\n",
       "      <td>8.475261</td>\n",
       "      <td>11.606175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1298</td>\n",
       "      <td>0.549553</td>\n",
       "      <td>0.647549</td>\n",
       "      <td>0.442301</td>\n",
       "      <td>15.140087</td>\n",
       "      <td>11.832445</td>\n",
       "      <td>9.138183</td>\n",
       "      <td>12.036905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.084599</td>\n",
       "      <td>0.520353</td>\n",
       "      <td>0.5902</td>\n",
       "      <td>0.398384</td>\n",
       "      <td>15.426811</td>\n",
       "      <td>12.209216</td>\n",
       "      <td>9.855276</td>\n",
       "      <td>12.497101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.04391</td>\n",
       "      <td>0.489847</td>\n",
       "      <td>0.543733</td>\n",
       "      <td>0.359163</td>\n",
       "      <td>15.547235</td>\n",
       "      <td>12.590582</td>\n",
       "      <td>10.401467</td>\n",
       "      <td>12.846428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.006809</td>\n",
       "      <td>0.459138</td>\n",
       "      <td>0.515978</td>\n",
       "      <td>0.327308</td>\n",
       "      <td>15.630336</td>\n",
       "      <td>12.965024</td>\n",
       "      <td>10.71644</td>\n",
       "      <td>13.103933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.028901</td>\n",
       "      <td>0.42812</td>\n",
       "      <td>0.498485</td>\n",
       "      <td>0.299234</td>\n",
       "      <td>15.746477</td>\n",
       "      <td>13.332117</td>\n",
       "      <td>10.91164</td>\n",
       "      <td>13.330078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.063418</td>\n",
       "      <td>0.397443</td>\n",
       "      <td>0.478822</td>\n",
       "      <td>0.270949</td>\n",
       "      <td>15.848362</td>\n",
       "      <td>13.684328</td>\n",
       "      <td>11.127347</td>\n",
       "      <td>13.553346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.096966</td>\n",
       "      <td>0.367367</td>\n",
       "      <td>0.449677</td>\n",
       "      <td>0.240026</td>\n",
       "      <td>15.939714</td>\n",
       "      <td>14.019603</td>\n",
       "      <td>11.438579</td>\n",
       "      <td>13.799299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.130037</td>\n",
       "      <td>0.336795</td>\n",
       "      <td>0.409391</td>\n",
       "      <td>0.205383</td>\n",
       "      <td>16.138644</td>\n",
       "      <td>14.351463</td>\n",
       "      <td>11.854819</td>\n",
       "      <td>14.114975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.159324</td>\n",
       "      <td>0.304979</td>\n",
       "      <td>0.363325</td>\n",
       "      <td>0.16966</td>\n",
       "      <td>16.366228</td>\n",
       "      <td>14.689695</td>\n",
       "      <td>12.313607</td>\n",
       "      <td>14.45651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.184676</td>\n",
       "      <td>0.271536</td>\n",
       "      <td>0.321251</td>\n",
       "      <td>0.136037</td>\n",
       "      <td>16.567406</td>\n",
       "      <td>15.037443</td>\n",
       "      <td>12.718989</td>\n",
       "      <td>14.774613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.229482</td>\n",
       "      <td>0.549184</td>\n",
       "      <td>0.629152</td>\n",
       "      <td>0.469273</td>\n",
       "      <td>13.925403</td>\n",
       "      <td>11.592918</td>\n",
       "      <td>9.075259</td>\n",
       "      <td>11.531193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.870734  0.903007  0.944785  0.906175   7.094088   5.476121   \n",
       "1      0.793251  0.804834  0.896775   0.83162   8.810579   7.770392   \n",
       "2      0.700022  0.739072  0.833317  0.757471  10.150446   8.988118   \n",
       "3      0.607056    0.6907  0.769417  0.689058  11.229322   9.789297   \n",
       "4      0.517725  0.660509   0.72664  0.634958  12.241272  10.260517   \n",
       "5      0.436138  0.642123  0.715206  0.597822  13.191608  10.539456   \n",
       "6      0.361304  0.628386  0.723715  0.571135  13.764394  10.742268   \n",
       "7      0.292864  0.615123  0.732289  0.546758  14.206225  10.934734   \n",
       "8      0.231018  0.598123  0.725748  0.518296  14.595548  11.175551   \n",
       "9      0.177737   0.57668  0.696741  0.483719  14.873271  11.469992   \n",
       "10       0.1298  0.549553  0.647549  0.442301  15.140087  11.832445   \n",
       "11     0.084599  0.520353    0.5902  0.398384  15.426811  12.209216   \n",
       "12      0.04391  0.489847  0.543733  0.359163  15.547235  12.590582   \n",
       "13     0.006809  0.459138  0.515978  0.327308  15.630336  12.965024   \n",
       "14    -0.028901   0.42812  0.498485  0.299234  15.746477  13.332117   \n",
       "15    -0.063418  0.397443  0.478822  0.270949  15.848362  13.684328   \n",
       "16    -0.096966  0.367367  0.449677  0.240026  15.939714  14.019603   \n",
       "17    -0.130037  0.336795  0.409391  0.205383  16.138644  14.351463   \n",
       "18    -0.159324  0.304979  0.363325   0.16966  16.366228  14.689695   \n",
       "19    -0.184676  0.271536  0.321251  0.136037  16.567406  15.037443   \n",
       "mean   0.229482  0.549184  0.629152  0.469273  13.925403  11.592918   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       3.609382   5.393197  \n",
       "1       4.936576   7.172515  \n",
       "2       6.274879   8.471147  \n",
       "3        7.38193   9.466849  \n",
       "4        8.03936  10.180383  \n",
       "5       8.207206   10.64609  \n",
       "6       8.085315  10.863992  \n",
       "7       7.960414  11.033791  \n",
       "8       8.058517  11.276539  \n",
       "9       8.475261  11.606175  \n",
       "10      9.138183  12.036905  \n",
       "11      9.855276  12.497101  \n",
       "12     10.401467  12.846428  \n",
       "13      10.71644  13.103933  \n",
       "14      10.91164  13.330078  \n",
       "15     11.127347  13.553346  \n",
       "16     11.438579  13.799299  \n",
       "17     11.854819  14.114975  \n",
       "18     12.313607   14.45651  \n",
       "19     12.718989  14.774613  \n",
       "mean    9.075259  11.531193  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/40_20_1_50_1_50_datt_seq2seq_gru_6.csv\n",
      "\n",
      "\n",
      "6th iteration\n",
      "history size: 40\n",
      "future size: 30\n",
      "Epoch 1/10000\n",
      "559/559 - 8s - loss: 0.3757 - val_loss: 0.3147 - 8s/epoch - 15ms/step\n",
      "Epoch 2/10000\n",
      "559/559 - 5s - loss: 0.2873 - val_loss: 0.2887 - 5s/epoch - 8ms/step\n",
      "Epoch 3/10000\n",
      "559/559 - 5s - loss: 0.2674 - val_loss: 0.2747 - 5s/epoch - 8ms/step\n",
      "Epoch 4/10000\n",
      "559/559 - 5s - loss: 0.2517 - val_loss: 0.2639 - 5s/epoch - 8ms/step\n",
      "Epoch 5/10000\n",
      "559/559 - 5s - loss: 0.2394 - val_loss: 0.2457 - 5s/epoch - 8ms/step\n",
      "Epoch 6/10000\n",
      "559/559 - 5s - loss: 0.2271 - val_loss: 0.2349 - 5s/epoch - 8ms/step\n",
      "Epoch 7/10000\n",
      "559/559 - 5s - loss: 0.2132 - val_loss: 0.2294 - 5s/epoch - 8ms/step\n",
      "Epoch 8/10000\n",
      "559/559 - 5s - loss: 0.2040 - val_loss: 0.2099 - 5s/epoch - 8ms/step\n",
      "Epoch 9/10000\n",
      "559/559 - 5s - loss: 0.1930 - val_loss: 0.2014 - 5s/epoch - 8ms/step\n",
      "Epoch 10/10000\n",
      "559/559 - 5s - loss: 0.1854 - val_loss: 0.1923 - 5s/epoch - 8ms/step\n",
      "Epoch 11/10000\n",
      "559/559 - 5s - loss: 0.1779 - val_loss: 0.1853 - 5s/epoch - 8ms/step\n",
      "Epoch 12/10000\n",
      "559/559 - 5s - loss: 0.1705 - val_loss: 0.1904 - 5s/epoch - 8ms/step\n",
      "Epoch 13/10000\n",
      "559/559 - 5s - loss: 0.1680 - val_loss: 0.1726 - 5s/epoch - 8ms/step\n",
      "Epoch 14/10000\n",
      "559/559 - 5s - loss: 0.1603 - val_loss: 0.1658 - 5s/epoch - 8ms/step\n",
      "Epoch 15/10000\n",
      "559/559 - 5s - loss: 0.1566 - val_loss: 0.1635 - 5s/epoch - 8ms/step\n",
      "Epoch 16/10000\n",
      "559/559 - 5s - loss: 0.1540 - val_loss: 0.1581 - 5s/epoch - 8ms/step\n",
      "Epoch 17/10000\n",
      "559/559 - 5s - loss: 0.1494 - val_loss: 0.1536 - 5s/epoch - 8ms/step\n",
      "Epoch 18/10000\n",
      "559/559 - 5s - loss: 0.1449 - val_loss: 0.1506 - 5s/epoch - 8ms/step\n",
      "Epoch 19/10000\n",
      "559/559 - 5s - loss: 0.1399 - val_loss: 0.1457 - 5s/epoch - 8ms/step\n",
      "Epoch 20/10000\n",
      "559/559 - 5s - loss: 0.1372 - val_loss: 0.1546 - 5s/epoch - 8ms/step\n",
      "Epoch 21/10000\n",
      "559/559 - 5s - loss: 0.1362 - val_loss: 0.1457 - 5s/epoch - 8ms/step\n",
      "Epoch 22/10000\n",
      "559/559 - 5s - loss: 0.1332 - val_loss: 0.1475 - 5s/epoch - 8ms/step\n",
      "Epoch 23/10000\n",
      "559/559 - 5s - loss: 0.1324 - val_loss: 0.1356 - 5s/epoch - 8ms/step\n",
      "Epoch 24/10000\n",
      "559/559 - 5s - loss: 0.1216 - val_loss: 0.1456 - 5s/epoch - 8ms/step\n",
      "Epoch 25/10000\n",
      "559/559 - 5s - loss: 0.1245 - val_loss: 0.1363 - 5s/epoch - 8ms/step\n",
      "Epoch 26/10000\n",
      "559/559 - 5s - loss: 0.1195 - val_loss: 0.1306 - 5s/epoch - 8ms/step\n",
      "Epoch 27/10000\n",
      "559/559 - 5s - loss: 0.1174 - val_loss: 0.1240 - 5s/epoch - 8ms/step\n",
      "Epoch 28/10000\n",
      "559/559 - 5s - loss: 0.1097 - val_loss: 0.1249 - 5s/epoch - 8ms/step\n",
      "Epoch 29/10000\n",
      "559/559 - 5s - loss: 0.1158 - val_loss: 0.1338 - 5s/epoch - 8ms/step\n",
      "Epoch 30/10000\n",
      "559/559 - 4s - loss: 0.1113 - val_loss: 0.1177 - 4s/epoch - 8ms/step\n",
      "Epoch 31/10000\n",
      "559/559 - 5s - loss: 0.1071 - val_loss: 0.1175 - 5s/epoch - 8ms/step\n",
      "Epoch 32/10000\n",
      "559/559 - 5s - loss: 0.1066 - val_loss: 0.1202 - 5s/epoch - 8ms/step\n",
      "Epoch 33/10000\n",
      "559/559 - 5s - loss: 0.1046 - val_loss: 0.1186 - 5s/epoch - 8ms/step\n",
      "Epoch 34/10000\n",
      "559/559 - 5s - loss: 0.1020 - val_loss: 0.1199 - 5s/epoch - 8ms/step\n",
      "Epoch 35/10000\n",
      "559/559 - 5s - loss: 0.1198 - val_loss: 0.1093 - 5s/epoch - 8ms/step\n",
      "Epoch 36/10000\n",
      "559/559 - 5s - loss: 0.0984 - val_loss: 0.1097 - 5s/epoch - 8ms/step\n",
      "Epoch 37/10000\n",
      "559/559 - 5s - loss: 0.0998 - val_loss: 0.1085 - 5s/epoch - 8ms/step\n",
      "Epoch 38/10000\n",
      "559/559 - 5s - loss: 0.0975 - val_loss: 0.1060 - 5s/epoch - 8ms/step\n",
      "Epoch 39/10000\n",
      "559/559 - 5s - loss: 0.0959 - val_loss: 0.1067 - 5s/epoch - 8ms/step\n",
      "Epoch 40/10000\n",
      "559/559 - 5s - loss: 0.0946 - val_loss: 0.1025 - 5s/epoch - 8ms/step\n",
      "Epoch 41/10000\n",
      "559/559 - 5s - loss: 0.0929 - val_loss: 0.1044 - 5s/epoch - 8ms/step\n",
      "Epoch 42/10000\n",
      "559/559 - 5s - loss: 0.0926 - val_loss: 0.1438 - 5s/epoch - 8ms/step\n",
      "Epoch 43/10000\n",
      "559/559 - 5s - loss: 0.0931 - val_loss: 0.1038 - 5s/epoch - 8ms/step\n",
      "Epoch 44/10000\n",
      "559/559 - 4s - loss: 0.0913 - val_loss: 0.1053 - 4s/epoch - 8ms/step\n",
      "Epoch 45/10000\n",
      "559/559 - 5s - loss: 0.0901 - val_loss: 0.1002 - 5s/epoch - 8ms/step\n",
      "Epoch 46/10000\n",
      "559/559 - 5s - loss: 0.0865 - val_loss: 0.0978 - 5s/epoch - 8ms/step\n",
      "Epoch 47/10000\n",
      "559/559 - 5s - loss: 0.0867 - val_loss: 0.1228 - 5s/epoch - 8ms/step\n",
      "Epoch 48/10000\n",
      "559/559 - 5s - loss: 0.0895 - val_loss: 0.0973 - 5s/epoch - 8ms/step\n",
      "Epoch 49/10000\n",
      "559/559 - 5s - loss: 0.0863 - val_loss: 0.1033 - 5s/epoch - 8ms/step\n",
      "Epoch 50/10000\n",
      "559/559 - 4s - loss: 0.0856 - val_loss: 0.1046 - 4s/epoch - 8ms/step\n",
      "Epoch 51/10000\n",
      "559/559 - 5s - loss: 0.0826 - val_loss: 0.0923 - 5s/epoch - 8ms/step\n",
      "Epoch 52/10000\n",
      "559/559 - 5s - loss: 0.0828 - val_loss: 0.0899 - 5s/epoch - 8ms/step\n",
      "Epoch 53/10000\n",
      "559/559 - 5s - loss: 0.0815 - val_loss: 0.0932 - 5s/epoch - 8ms/step\n",
      "Epoch 54/10000\n",
      "559/559 - 5s - loss: 0.0840 - val_loss: 0.0903 - 5s/epoch - 8ms/step\n",
      "Epoch 55/10000\n",
      "559/559 - 4s - loss: 0.0811 - val_loss: 0.0905 - 4s/epoch - 8ms/step\n",
      "Epoch 56/10000\n",
      "559/559 - 5s - loss: 0.0792 - val_loss: 0.0918 - 5s/epoch - 8ms/step\n",
      "Epoch 57/10000\n",
      "559/559 - 5s - loss: 0.0831 - val_loss: 0.0913 - 5s/epoch - 8ms/step\n",
      "Epoch 58/10000\n",
      "559/559 - 5s - loss: 0.0793 - val_loss: 0.0950 - 5s/epoch - 8ms/step\n",
      "Epoch 59/10000\n",
      "559/559 - 5s - loss: 0.0774 - val_loss: 0.0867 - 5s/epoch - 8ms/step\n",
      "Epoch 60/10000\n",
      "559/559 - 5s - loss: 0.0762 - val_loss: 0.0876 - 5s/epoch - 8ms/step\n",
      "Epoch 61/10000\n",
      "559/559 - 5s - loss: 0.0751 - val_loss: 0.0913 - 5s/epoch - 8ms/step\n",
      "Epoch 62/10000\n",
      "559/559 - 5s - loss: 0.0765 - val_loss: 0.0859 - 5s/epoch - 8ms/step\n",
      "Epoch 63/10000\n",
      "559/559 - 5s - loss: 0.0778 - val_loss: 0.0872 - 5s/epoch - 8ms/step\n",
      "Epoch 64/10000\n",
      "559/559 - 5s - loss: 0.0729 - val_loss: 0.0854 - 5s/epoch - 8ms/step\n",
      "Epoch 65/10000\n",
      "559/559 - 5s - loss: 0.0755 - val_loss: 0.0856 - 5s/epoch - 8ms/step\n",
      "Epoch 66/10000\n",
      "559/559 - 5s - loss: 0.0731 - val_loss: 0.0881 - 5s/epoch - 8ms/step\n",
      "Epoch 67/10000\n",
      "559/559 - 5s - loss: 0.0729 - val_loss: 0.0888 - 5s/epoch - 8ms/step\n",
      "Epoch 68/10000\n",
      "559/559 - 5s - loss: 0.0722 - val_loss: 0.0906 - 5s/epoch - 8ms/step\n",
      "Epoch 69/10000\n",
      "559/559 - 5s - loss: 0.0709 - val_loss: 0.0809 - 5s/epoch - 8ms/step\n",
      "Epoch 70/10000\n",
      "559/559 - 5s - loss: 0.0708 - val_loss: 0.0813 - 5s/epoch - 8ms/step\n",
      "Epoch 71/10000\n",
      "559/559 - 5s - loss: 0.0715 - val_loss: 0.0823 - 5s/epoch - 8ms/step\n",
      "Epoch 72/10000\n",
      "559/559 - 5s - loss: 0.0691 - val_loss: 0.0752 - 5s/epoch - 8ms/step\n",
      "Epoch 73/10000\n",
      "559/559 - 5s - loss: 0.0707 - val_loss: 0.0795 - 5s/epoch - 8ms/step\n",
      "Epoch 74/10000\n",
      "559/559 - 5s - loss: 0.0685 - val_loss: 0.0776 - 5s/epoch - 8ms/step\n",
      "Epoch 75/10000\n",
      "559/559 - 5s - loss: 0.0675 - val_loss: 0.0811 - 5s/epoch - 8ms/step\n",
      "Epoch 76/10000\n",
      "559/559 - 5s - loss: 0.0692 - val_loss: 0.0826 - 5s/epoch - 8ms/step\n",
      "Epoch 77/10000\n",
      "559/559 - 5s - loss: 0.0677 - val_loss: 0.0754 - 5s/epoch - 8ms/step\n",
      "Epoch 78/10000\n",
      "559/559 - 5s - loss: 0.0661 - val_loss: 0.0787 - 5s/epoch - 8ms/step\n",
      "Epoch 79/10000\n",
      "559/559 - 5s - loss: 0.0651 - val_loss: 0.0765 - 5s/epoch - 8ms/step\n",
      "Epoch 80/10000\n",
      "559/559 - 4s - loss: 0.0667 - val_loss: 0.0805 - 4s/epoch - 8ms/step\n",
      "Epoch 81/10000\n",
      "559/559 - 5s - loss: 0.0656 - val_loss: 0.0748 - 5s/epoch - 8ms/step\n",
      "Epoch 82/10000\n",
      "559/559 - 5s - loss: 0.0657 - val_loss: 0.0796 - 5s/epoch - 8ms/step\n",
      "Epoch 83/10000\n",
      "559/559 - 5s - loss: 0.0746 - val_loss: 0.0829 - 5s/epoch - 8ms/step\n",
      "Epoch 84/10000\n",
      "559/559 - 5s - loss: 0.0648 - val_loss: 0.0715 - 5s/epoch - 8ms/step\n",
      "Epoch 85/10000\n",
      "559/559 - 5s - loss: 0.0609 - val_loss: 0.0791 - 5s/epoch - 8ms/step\n",
      "Epoch 86/10000\n",
      "559/559 - 5s - loss: 0.0631 - val_loss: 0.0754 - 5s/epoch - 8ms/step\n",
      "Epoch 87/10000\n",
      "559/559 - 5s - loss: 0.0658 - val_loss: 0.0746 - 5s/epoch - 8ms/step\n",
      "Epoch 88/10000\n",
      "559/559 - 5s - loss: 0.0626 - val_loss: 0.0735 - 5s/epoch - 8ms/step\n",
      "Epoch 89/10000\n",
      "559/559 - 5s - loss: 0.0644 - val_loss: 0.0754 - 5s/epoch - 8ms/step\n",
      "Epoch 90/10000\n",
      "559/559 - 5s - loss: 0.0616 - val_loss: 0.0789 - 5s/epoch - 8ms/step\n",
      "Epoch 91/10000\n",
      "559/559 - 5s - loss: 0.0631 - val_loss: 0.0718 - 5s/epoch - 8ms/step\n",
      "Epoch 92/10000\n",
      "559/559 - 5s - loss: 0.0605 - val_loss: 0.0711 - 5s/epoch - 8ms/step\n",
      "Epoch 93/10000\n",
      "559/559 - 5s - loss: 0.0619 - val_loss: 0.0783 - 5s/epoch - 8ms/step\n",
      "Epoch 94/10000\n",
      "559/559 - 5s - loss: 0.0610 - val_loss: 0.0751 - 5s/epoch - 8ms/step\n",
      "Epoch 95/10000\n",
      "559/559 - 5s - loss: 0.0603 - val_loss: 0.0785 - 5s/epoch - 8ms/step\n",
      "Epoch 96/10000\n",
      "559/559 - 5s - loss: 0.0624 - val_loss: 0.0724 - 5s/epoch - 8ms/step\n",
      "Epoch 97/10000\n",
      "559/559 - 5s - loss: 0.0645 - val_loss: 0.0809 - 5s/epoch - 8ms/step\n",
      "Epoch 98/10000\n",
      "559/559 - 5s - loss: 0.0596 - val_loss: 0.0706 - 5s/epoch - 8ms/step\n",
      "Epoch 99/10000\n",
      "559/559 - 5s - loss: 0.0601 - val_loss: 0.0685 - 5s/epoch - 8ms/step\n",
      "Epoch 100/10000\n",
      "559/559 - 5s - loss: 0.0605 - val_loss: 0.0754 - 5s/epoch - 8ms/step\n",
      "Epoch 101/10000\n",
      "559/559 - 5s - loss: 0.0610 - val_loss: 0.0671 - 5s/epoch - 8ms/step\n",
      "Epoch 102/10000\n",
      "559/559 - 5s - loss: 0.0591 - val_loss: 0.0825 - 5s/epoch - 8ms/step\n",
      "Epoch 103/10000\n",
      "559/559 - 5s - loss: 0.0595 - val_loss: 0.0690 - 5s/epoch - 8ms/step\n",
      "Epoch 104/10000\n",
      "559/559 - 5s - loss: 0.0587 - val_loss: 0.0714 - 5s/epoch - 8ms/step\n",
      "Epoch 105/10000\n",
      "559/559 - 5s - loss: 0.0580 - val_loss: 0.0662 - 5s/epoch - 8ms/step\n",
      "Epoch 106/10000\n",
      "559/559 - 5s - loss: 0.0584 - val_loss: 0.0680 - 5s/epoch - 8ms/step\n",
      "Epoch 107/10000\n",
      "559/559 - 5s - loss: 0.0588 - val_loss: 0.0711 - 5s/epoch - 8ms/step\n",
      "Epoch 108/10000\n",
      "559/559 - 5s - loss: 0.0570 - val_loss: 0.0676 - 5s/epoch - 8ms/step\n",
      "Epoch 109/10000\n",
      "559/559 - 5s - loss: 0.0583 - val_loss: 0.0658 - 5s/epoch - 9ms/step\n",
      "Epoch 110/10000\n",
      "559/559 - 5s - loss: 0.0572 - val_loss: 0.0756 - 5s/epoch - 9ms/step\n",
      "Epoch 111/10000\n",
      "559/559 - 5s - loss: 0.0570 - val_loss: 0.0713 - 5s/epoch - 9ms/step\n",
      "Epoch 112/10000\n",
      "559/559 - 5s - loss: 0.0586 - val_loss: 0.0753 - 5s/epoch - 9ms/step\n",
      "Epoch 113/10000\n",
      "559/559 - 5s - loss: 0.0613 - val_loss: 0.0983 - 5s/epoch - 9ms/step\n",
      "Epoch 114/10000\n",
      "559/559 - 5s - loss: 0.0701 - val_loss: 0.0692 - 5s/epoch - 8ms/step\n",
      "Epoch 115/10000\n",
      "559/559 - 5s - loss: 0.0547 - val_loss: 0.0642 - 5s/epoch - 8ms/step\n",
      "Epoch 116/10000\n",
      "559/559 - 5s - loss: 0.0554 - val_loss: 0.0660 - 5s/epoch - 8ms/step\n",
      "Epoch 117/10000\n",
      "559/559 - 5s - loss: 0.0574 - val_loss: 0.0663 - 5s/epoch - 8ms/step\n",
      "Epoch 118/10000\n",
      "559/559 - 5s - loss: 0.0558 - val_loss: 0.0768 - 5s/epoch - 8ms/step\n",
      "Epoch 119/10000\n",
      "559/559 - 5s - loss: 0.0556 - val_loss: 0.0654 - 5s/epoch - 8ms/step\n",
      "Epoch 120/10000\n",
      "559/559 - 5s - loss: 0.0545 - val_loss: 0.0642 - 5s/epoch - 8ms/step\n",
      "Epoch 121/10000\n",
      "559/559 - 5s - loss: 0.0564 - val_loss: 0.0838 - 5s/epoch - 8ms/step\n",
      "Epoch 122/10000\n",
      "559/559 - 5s - loss: 0.0591 - val_loss: 0.0678 - 5s/epoch - 8ms/step\n",
      "Epoch 123/10000\n",
      "559/559 - 5s - loss: 0.0548 - val_loss: 0.0610 - 5s/epoch - 8ms/step\n",
      "Epoch 124/10000\n",
      "559/559 - 5s - loss: 0.0538 - val_loss: 0.0663 - 5s/epoch - 8ms/step\n",
      "Epoch 125/10000\n",
      "559/559 - 5s - loss: 0.0533 - val_loss: 0.0625 - 5s/epoch - 8ms/step\n",
      "Epoch 126/10000\n",
      "559/559 - 4s - loss: 0.0568 - val_loss: 0.0645 - 4s/epoch - 8ms/step\n",
      "Epoch 127/10000\n",
      "559/559 - 5s - loss: 0.0537 - val_loss: 0.0632 - 5s/epoch - 8ms/step\n",
      "Epoch 128/10000\n",
      "559/559 - 5s - loss: 0.0549 - val_loss: 0.0631 - 5s/epoch - 8ms/step\n",
      "Epoch 129/10000\n",
      "559/559 - 4s - loss: 0.0523 - val_loss: 0.0672 - 4s/epoch - 8ms/step\n",
      "Epoch 130/10000\n",
      "559/559 - 5s - loss: 0.0545 - val_loss: 0.0628 - 5s/epoch - 8ms/step\n",
      "Epoch 131/10000\n",
      "559/559 - 4s - loss: 0.0528 - val_loss: 0.0669 - 4s/epoch - 8ms/step\n",
      "Epoch 132/10000\n",
      "559/559 - 5s - loss: 0.0529 - val_loss: 0.0662 - 5s/epoch - 8ms/step\n",
      "Epoch 133/10000\n",
      "559/559 - 5s - loss: 0.0527 - val_loss: 0.0691 - 5s/epoch - 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_250_layer_call_fn, gru_cell_250_layer_call_and_return_conditional_losses, gru_cell_251_layer_call_fn, gru_cell_251_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_30_1_50_1_50_datt_seq2seq_gru_6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/40_30_1_50_1_50_datt_seq2seq_gru_6\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B4E05DAC0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x0000021B4FD82CA0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "      <th>TT-3061-3</th>\n",
       "      <th>TT-3061-5</th>\n",
       "      <th>LT-3061-2</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>R2</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "      <td>nRMSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.852236</td>\n",
       "      <td>0.868947</td>\n",
       "      <td>0.916844</td>\n",
       "      <td>0.879342</td>\n",
       "      <td>7.583139</td>\n",
       "      <td>6.365358</td>\n",
       "      <td>4.417085</td>\n",
       "      <td>6.121861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.768344</td>\n",
       "      <td>0.822075</td>\n",
       "      <td>0.872841</td>\n",
       "      <td>0.821086</td>\n",
       "      <td>9.496882</td>\n",
       "      <td>7.417326</td>\n",
       "      <td>5.464852</td>\n",
       "      <td>7.459687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.671438</td>\n",
       "      <td>0.770902</td>\n",
       "      <td>0.826208</td>\n",
       "      <td>0.756183</td>\n",
       "      <td>11.312529</td>\n",
       "      <td>8.417414</td>\n",
       "      <td>6.391821</td>\n",
       "      <td>8.707255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.567192</td>\n",
       "      <td>0.730297</td>\n",
       "      <td>0.779179</td>\n",
       "      <td>0.692223</td>\n",
       "      <td>12.985877</td>\n",
       "      <td>9.134309</td>\n",
       "      <td>7.207735</td>\n",
       "      <td>9.775974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.462332</td>\n",
       "      <td>0.700652</td>\n",
       "      <td>0.73121</td>\n",
       "      <td>0.631398</td>\n",
       "      <td>14.474273</td>\n",
       "      <td>9.625151</td>\n",
       "      <td>7.95489</td>\n",
       "      <td>10.684771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.361785</td>\n",
       "      <td>0.679041</td>\n",
       "      <td>0.682022</td>\n",
       "      <td>0.574283</td>\n",
       "      <td>15.768867</td>\n",
       "      <td>9.969036</td>\n",
       "      <td>8.654352</td>\n",
       "      <td>11.464085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.272064</td>\n",
       "      <td>0.66079</td>\n",
       "      <td>0.633353</td>\n",
       "      <td>0.522069</td>\n",
       "      <td>16.839339</td>\n",
       "      <td>10.251006</td>\n",
       "      <td>9.295261</td>\n",
       "      <td>12.128535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.197545</td>\n",
       "      <td>0.643655</td>\n",
       "      <td>0.588465</td>\n",
       "      <td>0.476555</td>\n",
       "      <td>17.678873</td>\n",
       "      <td>10.50907</td>\n",
       "      <td>9.850338</td>\n",
       "      <td>12.679427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.137116</td>\n",
       "      <td>0.624955</td>\n",
       "      <td>0.544748</td>\n",
       "      <td>0.435606</td>\n",
       "      <td>18.331332</td>\n",
       "      <td>10.784162</td>\n",
       "      <td>10.362683</td>\n",
       "      <td>13.159392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.087973</td>\n",
       "      <td>0.604193</td>\n",
       "      <td>0.497821</td>\n",
       "      <td>0.396662</td>\n",
       "      <td>18.845894</td>\n",
       "      <td>11.080458</td>\n",
       "      <td>10.886797</td>\n",
       "      <td>13.604383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.043341</td>\n",
       "      <td>0.581914</td>\n",
       "      <td>0.44676</td>\n",
       "      <td>0.357338</td>\n",
       "      <td>19.302579</td>\n",
       "      <td>11.389916</td>\n",
       "      <td>11.430205</td>\n",
       "      <td>14.0409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001426</td>\n",
       "      <td>0.559583</td>\n",
       "      <td>0.395375</td>\n",
       "      <td>0.318795</td>\n",
       "      <td>19.358637</td>\n",
       "      <td>11.69194</td>\n",
       "      <td>11.953249</td>\n",
       "      <td>14.334609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.036741</td>\n",
       "      <td>0.536888</td>\n",
       "      <td>0.348876</td>\n",
       "      <td>0.283008</td>\n",
       "      <td>18.858253</td>\n",
       "      <td>11.992323</td>\n",
       "      <td>12.409065</td>\n",
       "      <td>14.41988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.069359</td>\n",
       "      <td>0.514926</td>\n",
       "      <td>0.309843</td>\n",
       "      <td>0.251803</td>\n",
       "      <td>18.506144</td>\n",
       "      <td>12.27685</td>\n",
       "      <td>12.780379</td>\n",
       "      <td>14.521124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.095374</td>\n",
       "      <td>0.494725</td>\n",
       "      <td>0.278135</td>\n",
       "      <td>0.225828</td>\n",
       "      <td>18.423309</td>\n",
       "      <td>12.534379</td>\n",
       "      <td>13.0757</td>\n",
       "      <td>14.677796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.115912</td>\n",
       "      <td>0.475787</td>\n",
       "      <td>0.253504</td>\n",
       "      <td>0.20446</td>\n",
       "      <td>18.527305</td>\n",
       "      <td>12.770976</td>\n",
       "      <td>13.301313</td>\n",
       "      <td>14.866531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.131426</td>\n",
       "      <td>0.458058</td>\n",
       "      <td>0.236853</td>\n",
       "      <td>0.187828</td>\n",
       "      <td>18.286621</td>\n",
       "      <td>12.986228</td>\n",
       "      <td>13.453323</td>\n",
       "      <td>14.908724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.14346</td>\n",
       "      <td>0.440814</td>\n",
       "      <td>0.227312</td>\n",
       "      <td>0.174889</td>\n",
       "      <td>18.030408</td>\n",
       "      <td>13.192369</td>\n",
       "      <td>13.540978</td>\n",
       "      <td>14.921252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.155063</td>\n",
       "      <td>0.424501</td>\n",
       "      <td>0.223384</td>\n",
       "      <td>0.164274</td>\n",
       "      <td>17.853632</td>\n",
       "      <td>13.385302</td>\n",
       "      <td>13.578831</td>\n",
       "      <td>14.939255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.167617</td>\n",
       "      <td>0.408494</td>\n",
       "      <td>0.224489</td>\n",
       "      <td>0.155122</td>\n",
       "      <td>17.693467</td>\n",
       "      <td>13.571148</td>\n",
       "      <td>13.571927</td>\n",
       "      <td>14.945514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.181607</td>\n",
       "      <td>0.392263</td>\n",
       "      <td>0.229538</td>\n",
       "      <td>0.146731</td>\n",
       "      <td>17.61929</td>\n",
       "      <td>13.758084</td>\n",
       "      <td>13.530464</td>\n",
       "      <td>14.969279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.196114</td>\n",
       "      <td>0.37704</td>\n",
       "      <td>0.236699</td>\n",
       "      <td>0.139208</td>\n",
       "      <td>17.619272</td>\n",
       "      <td>13.930091</td>\n",
       "      <td>13.470527</td>\n",
       "      <td>15.00663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.210062</td>\n",
       "      <td>0.362761</td>\n",
       "      <td>0.243558</td>\n",
       "      <td>0.132086</td>\n",
       "      <td>17.482995</td>\n",
       "      <td>14.088681</td>\n",
       "      <td>13.413236</td>\n",
       "      <td>14.99497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.223093</td>\n",
       "      <td>0.348578</td>\n",
       "      <td>0.247869</td>\n",
       "      <td>0.124451</td>\n",
       "      <td>17.344275</td>\n",
       "      <td>14.246331</td>\n",
       "      <td>13.378444</td>\n",
       "      <td>14.989683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.235573</td>\n",
       "      <td>0.334675</td>\n",
       "      <td>0.248192</td>\n",
       "      <td>0.115764</td>\n",
       "      <td>17.261087</td>\n",
       "      <td>14.399102</td>\n",
       "      <td>13.378442</td>\n",
       "      <td>15.012877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.246048</td>\n",
       "      <td>0.320354</td>\n",
       "      <td>0.244634</td>\n",
       "      <td>0.106313</td>\n",
       "      <td>17.16562</td>\n",
       "      <td>14.55413</td>\n",
       "      <td>13.413229</td>\n",
       "      <td>15.044326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.254337</td>\n",
       "      <td>0.306877</td>\n",
       "      <td>0.238816</td>\n",
       "      <td>0.097119</td>\n",
       "      <td>17.05752</td>\n",
       "      <td>14.69631</td>\n",
       "      <td>13.468369</td>\n",
       "      <td>15.074066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.262411</td>\n",
       "      <td>0.295078</td>\n",
       "      <td>0.231568</td>\n",
       "      <td>0.088078</td>\n",
       "      <td>17.071724</td>\n",
       "      <td>14.819384</td>\n",
       "      <td>13.536547</td>\n",
       "      <td>15.142551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.270318</td>\n",
       "      <td>0.28444</td>\n",
       "      <td>0.223914</td>\n",
       "      <td>0.079346</td>\n",
       "      <td>17.147441</td>\n",
       "      <td>14.930206</td>\n",
       "      <td>13.607599</td>\n",
       "      <td>15.228416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.277967</td>\n",
       "      <td>0.274382</td>\n",
       "      <td>0.217283</td>\n",
       "      <td>0.071233</td>\n",
       "      <td>17.22383</td>\n",
       "      <td>15.034717</td>\n",
       "      <td>13.669114</td>\n",
       "      <td>15.30922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.038344</td>\n",
       "      <td>0.509921</td>\n",
       "      <td>0.412643</td>\n",
       "      <td>0.320303</td>\n",
       "      <td>16.705014</td>\n",
       "      <td>12.126725</td>\n",
       "      <td>11.481559</td>\n",
       "      <td>13.437766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TT-3061-3 TT-3061-5 LT-3061-2      mean  TT-3061-3  TT-3061-5  \\\n",
       "index        R2        R2        R2        R2      nRMSE      nRMSE   \n",
       "0      0.852236  0.868947  0.916844  0.879342   7.583139   6.365358   \n",
       "1      0.768344  0.822075  0.872841  0.821086   9.496882   7.417326   \n",
       "2      0.671438  0.770902  0.826208  0.756183  11.312529   8.417414   \n",
       "3      0.567192  0.730297  0.779179  0.692223  12.985877   9.134309   \n",
       "4      0.462332  0.700652   0.73121  0.631398  14.474273   9.625151   \n",
       "5      0.361785  0.679041  0.682022  0.574283  15.768867   9.969036   \n",
       "6      0.272064   0.66079  0.633353  0.522069  16.839339  10.251006   \n",
       "7      0.197545  0.643655  0.588465  0.476555  17.678873   10.50907   \n",
       "8      0.137116  0.624955  0.544748  0.435606  18.331332  10.784162   \n",
       "9      0.087973  0.604193  0.497821  0.396662  18.845894  11.080458   \n",
       "10     0.043341  0.581914   0.44676  0.357338  19.302579  11.389916   \n",
       "11     0.001426  0.559583  0.395375  0.318795  19.358637   11.69194   \n",
       "12    -0.036741  0.536888  0.348876  0.283008  18.858253  11.992323   \n",
       "13    -0.069359  0.514926  0.309843  0.251803  18.506144   12.27685   \n",
       "14    -0.095374  0.494725  0.278135  0.225828  18.423309  12.534379   \n",
       "15    -0.115912  0.475787  0.253504   0.20446  18.527305  12.770976   \n",
       "16    -0.131426  0.458058  0.236853  0.187828  18.286621  12.986228   \n",
       "17     -0.14346  0.440814  0.227312  0.174889  18.030408  13.192369   \n",
       "18    -0.155063  0.424501  0.223384  0.164274  17.853632  13.385302   \n",
       "19    -0.167617  0.408494  0.224489  0.155122  17.693467  13.571148   \n",
       "20    -0.181607  0.392263  0.229538  0.146731   17.61929  13.758084   \n",
       "21    -0.196114   0.37704  0.236699  0.139208  17.619272  13.930091   \n",
       "22    -0.210062  0.362761  0.243558  0.132086  17.482995  14.088681   \n",
       "23    -0.223093  0.348578  0.247869  0.124451  17.344275  14.246331   \n",
       "24    -0.235573  0.334675  0.248192  0.115764  17.261087  14.399102   \n",
       "25    -0.246048  0.320354  0.244634  0.106313   17.16562   14.55413   \n",
       "26    -0.254337  0.306877  0.238816  0.097119   17.05752   14.69631   \n",
       "27    -0.262411  0.295078  0.231568  0.088078  17.071724  14.819384   \n",
       "28    -0.270318   0.28444  0.223914  0.079346  17.147441  14.930206   \n",
       "29    -0.277967  0.274382  0.217283  0.071233   17.22383  15.034717   \n",
       "mean   0.038344  0.509921  0.412643  0.320303  16.705014  12.126725   \n",
       "\n",
       "       LT-3061-2       mean  \n",
       "index      nRMSE      nRMSE  \n",
       "0       4.417085   6.121861  \n",
       "1       5.464852   7.459687  \n",
       "2       6.391821   8.707255  \n",
       "3       7.207735   9.775974  \n",
       "4        7.95489  10.684771  \n",
       "5       8.654352  11.464085  \n",
       "6       9.295261  12.128535  \n",
       "7       9.850338  12.679427  \n",
       "8      10.362683  13.159392  \n",
       "9      10.886797  13.604383  \n",
       "10     11.430205    14.0409  \n",
       "11     11.953249  14.334609  \n",
       "12     12.409065   14.41988  \n",
       "13     12.780379  14.521124  \n",
       "14       13.0757  14.677796  \n",
       "15     13.301313  14.866531  \n",
       "16     13.453323  14.908724  \n",
       "17     13.540978  14.921252  \n",
       "18     13.578831  14.939255  \n",
       "19     13.571927  14.945514  \n",
       "20     13.530464  14.969279  \n",
       "21     13.470527   15.00663  \n",
       "22     13.413236   14.99497  \n",
       "23     13.378444  14.989683  \n",
       "24     13.378442  15.012877  \n",
       "25     13.413229  15.044326  \n",
       "26     13.468369  15.074066  \n",
       "27     13.536547  15.142551  \n",
       "28     13.607599  15.228416  \n",
       "29     13.669114   15.30922  \n",
       "mean   11.481559  13.437766  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file is saved to: ./result/40_30_1_50_1_50_datt_seq2seq_gru_6.csv\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "end opitmization\n"
     ]
    }
   ],
   "source": [
    "target_list = cts_list\n",
    "\n",
    "# history size and future size\n",
    "history_list = [10, 20, 30, 40]\n",
    "future_list = [10, 20, 30]\n",
    "step = 1\n",
    "\n",
    "# variable selection\n",
    "history_var = process_var\n",
    "future_var = output_var\n",
    "\n",
    "history_num = len(history_var)\n",
    "future_num = len(future_var)\n",
    "\n",
    "# supervised attention factor\n",
    "delta = 1\n",
    "att_type = 'linear'\n",
    "        \n",
    "# test data split        \n",
    "test_size = 0.2\n",
    "test_num = -1\n",
    "\n",
    "# model structure\n",
    "num_layers = 1\n",
    "num_neurons = 50\n",
    "dense_layers = 1\n",
    "dense_neurons = 50\n",
    "model_type = 'datt_seq2seq_gru'\n",
    "\n",
    "iteration_list = [5, 6]\n",
    "for iteration in iteration_list:\n",
    "    for history_size in history_list:\n",
    "        for future_size in future_list:\n",
    "            print(f\"{iteration}th iteration\")\n",
    "            print(f\"history size: {history_size}\")\n",
    "            print(f\"future size: {future_size}\")\n",
    "            history_series = []\n",
    "            future_series = []\n",
    "\n",
    "            # data to series\n",
    "            for i in range(len(target_list)):\n",
    "                history, future = data2series(target_list[i], history_size, history_var, future_size, future_var,\n",
    "                                            step, start_idx=0, end_idx=None)\n",
    "                if not i:\n",
    "                    history_series = history\n",
    "                    future_series = future\n",
    "                else:\n",
    "                    history_series = np.concatenate([history_series, history], axis=0)\n",
    "                    future_series = np.concatenate([future_series, future], axis=0)\n",
    "            \n",
    "            factor = rnn.super_attention(delta, future_size, future_num, att_type)\n",
    "            # Dual-attention Seq2Seq model\n",
    "            DATT_seq2seq_GRU = rnn.RNN(history_series, history_var, future_series, future_var)\n",
    "            # TT split\n",
    "            DATT_seq2seq_GRU.train_test(test_size=test_size, test_num=test_num)\n",
    "            # TV split\n",
    "            valid_size = DATT_seq2seq_GRU.history_test.shape[0]/DATT_seq2seq_GRU.history_train.shape[0]\n",
    "            DATT_seq2seq_GRU.train_valid(valid_size=valid_size)\n",
    "            # scaling\n",
    "            DATT_seq2seq_GRU.scaling()\n",
    "            # modeling\n",
    "            DATT_seq2seq_GRU.build_model(num_layers=num_layers, num_neurons=num_neurons, dense_layers=dense_layers, dense_neurons=dense_neurons, model_type=model_type, factor=factor)\n",
    "            # training\n",
    "            model_num = iteration\n",
    "            model_name = f\"{history_size}_{future_size}_{num_layers}_{num_neurons}_{dense_layers}_{dense_neurons}_{model_type}_{model_num}\"\n",
    "            if not exists(f\"./model/{model_name}.h5\"):\n",
    "                DATT_seq2seq_GRU.train()\n",
    "                DATT_seq2seq_GRU.save_model(f\"./model/{model_name}\", 'weights')\n",
    "                \n",
    "            else:\n",
    "                DATT_seq2seq_GRU.model.load_weights(f\"./model/{model_name}.h5\")\n",
    "            # test\n",
    "            test_result = DATT_seq2seq_GRU.test()\n",
    "            if not exists(f'./result/{model_name}.csv', ):\n",
    "                savefile(test_result, './result', model_name)\n",
    "            print(\"\\n\")\n",
    "    print('\\n\\n')\n",
    "print('end opitmization')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3cf56d631085086b1721b0064da1454dfad7e026414ec6e1cab7db73e55aa6df"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
